{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'NN III -- Stochastic Gradient Descent, Batches and Convolutional Neural Networks'\n",
        "jupyter: python3\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mp\n",
        "import pandas as pd\n",
        "from IPython.display import Image, HTML"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recap\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/25-NN-III-CNNs.ipynb)\n",
        "\n",
        "We have covered the following topics\n",
        "\n",
        "* Gradients, gradient descent, and back propagation\n",
        "* Fully connected neural networks (Multi-Layer Perceptron)\n",
        "* Training of MLPs using back propagation\n",
        "\n",
        "Now we cover\n",
        "\n",
        "* _Stochastic_ gradient descent (SGD)\n",
        "* Convolutional Neural Networks (CNNs)\n",
        "* Training a CNN with SGD\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "\n",
        "## Batches and Stochastic Gradient Descent\n",
        "\n",
        "\n",
        "* Compute the gradient (e.g., forward pass and backward pass) with only a _random subset_\n",
        "of the input data.\n",
        "\n",
        "> This subset is called a _batch_.\n",
        "\n",
        "* Work through the dataset by _randomly sampling without replacement_. This is the _stochastic_ part.\n",
        "* One forward and backward pass through all the batches of data is called an _epoch_.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Batches and Stochastic Gradient Descent\n",
        ":::\n",
        "\n",
        "The squared error loss for (full-batch) gradient descent for $N$ input samples is\n",
        "\n",
        "$$\n",
        "L = \\sum_{i=1}^{N} \\ell_i = \\sum_{i=1}^{N} \\left( y_i - \\hat{y}_i  \\right)^2.\n",
        "$$\n",
        "\n",
        "In _Stochastic Gradient Descent_, the loss is calculated for a single _batch_ of data, i.e.,\n",
        "\n",
        "$$\n",
        "L_t = \\sum_{i \\in \\mathcal{B}_t} \\ell_i = \\sum_{i \\in \\mathcal{B}_t} \\left( y_i - \\hat{y}_i  \\right)^2,\n",
        "$$\n",
        "\n",
        "where $\\mathcal{B}_t$ is the $t$-th batch.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Batches\n",
        ":::\n",
        "\n",
        "Here is an example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate 12 evenly spaced x values between 1 and 4\n",
        "x = np.linspace(1, 4, 12)\n",
        "\n",
        "# Add normally distributed noise to the x values\n",
        "x += np.random.normal(0, 1.0, 12)\n",
        "\n",
        "# Calculate the corresponding y values for the line y = 2x\n",
        "y = 2 * x\n",
        "\n",
        "# Add normally distributed noise to the y values\n",
        "y += np.random.normal(0, 1.0, 12)\n",
        "\n",
        "# Shuffle the points and split them into 3 groups of 4\n",
        "indices = np.random.permutation(12)\n",
        "colors = ['red', 'green', 'blue', 'purple']\n",
        "labels = ['Batch 1', 'Batch 2', 'Batch 3', 'Batch 4']\n",
        "\n",
        "# Plot each group of points with a different color and label\n",
        "for i in range(4):\n",
        "    plt.scatter(x[indices[i*3:(i+1)*3]], y[indices[i*3:(i+1)*3]], color=colors[i], label=labels[i])\n",
        "\n",
        "# Display the legend\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given a training data set of 12 points and we want to use a _batch size_ of 3.\n",
        "\n",
        "The 12 points are divided into batches of 3 by randomly selecting points without replacement.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Batches\n",
        ":::\n",
        "\n",
        "The points can be resampled again to create a different set of batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "# Shuffle the points and split them into 3 groups of 4\n",
        "indices = np.random.permutation(12)\n",
        "colors = ['red', 'green', 'blue', 'purple']\n",
        "labels = ['Batch 1', 'Batch 2', 'Batch 3', 'Batch 4']\n",
        "\n",
        "# Plot each group of points with a different color and label\n",
        "for i in range(4):\n",
        "    plt.scatter(x[indices[i*3:(i+1)*3]], y[indices[i*3:(i+1)*3]], color=colors[i], label=labels[i])\n",
        "\n",
        "# Display the legend\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Batches\n",
        ":::\n",
        "\n",
        "For every training iteration, you calculate the loss after a forward and backward pass with the data from a single batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "colors = ['red', 'lightgray', 'lightgray', 'lightgray']\n",
        "labels = ['Batch 1', 'Batch 2', 'Batch 3', 'Batch 4']\n",
        "\n",
        "fig, axs = plt.subplots(2, 2, figsize=(8, 8))\n",
        "\n",
        "# Plot each group of points with a different color and label\n",
        "for i in range(4):\n",
        "    axs[0, 0].scatter(x[indices[i*3:(i+1)*3]], y[indices[i*3:(i+1)*3]], color=colors[i], label=labels[i])\n",
        "\n",
        "axs[0, 0].legend()\n",
        "axs[0, 0].set_title(\"Batch 1\")\n",
        "\n",
        "colors = ['lightgray', 'green', 'lightgray', 'lightgray']\n",
        "labels = ['Batch 1', 'Batch 2', 'Batch 3', 'Batch 4']\n",
        "\n",
        "# Plot each group of points with a different color and label\n",
        "for i in range(4):\n",
        "    axs[0, 1].scatter(x[indices[i*3:(i+1)*3]], y[indices[i*3:(i+1)*3]], color=colors[i], label=labels[i])\n",
        "\n",
        "axs[0, 1].legend()\n",
        "axs[0, 1].set_title(\"Batch 2\")\n",
        "\n",
        "colors = ['lightgray', 'lightgray', 'blue', 'lightgray']\n",
        "labels = ['Batch 1', 'Batch 2', 'Batch 3', 'Batch 4']\n",
        "\n",
        "# Plot each group of points with a different color and label\n",
        "for i in range(4):\n",
        "    axs[1, 0].scatter(x[indices[i*3:(i+1)*3]], y[indices[i*3:(i+1)*3]], color=colors[i], label=labels[i])\n",
        "\n",
        "axs[1, 0].legend()\n",
        "axs[1, 0].set_title(\"Batch 3\")\n",
        "\n",
        "colors = ['lightgray', 'lightgray', 'lightgray', 'purple']\n",
        "labels = ['Batch 1', 'Batch 2', 'Batch 3', 'Batch 4']\n",
        "\n",
        "# Plot each group of points with a different color and label\n",
        "for i in range(4):\n",
        "    axs[1, 1].scatter(x[indices[i*3:(i+1)*3]], y[indices[i*3:(i+1)*3]], color=colors[i], label=labels[i])\n",
        "\n",
        "axs[1, 1].legend()\n",
        "axs[1, 1].set_title(\"Batch 4\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vocabulary Summary\n",
        "\n",
        "We have introduced the following terms:\n",
        "\n",
        "- **batch** - a subset of the full training data\n",
        "- **batch size** - the number of data points in the batch\n",
        "- **iteration** - a forward and backward pass with a single batch of data\n",
        "- **epoch** - a forward and backward pass over all the batches of data.\n",
        "\n",
        "With 12 instances of data split into 4 batches, the batch size is 3, and it takes 4 iterations for a single epoch.\n",
        "\n",
        "\n",
        "## Advantages of SGD\n",
        "\n",
        "There are two main advantages to _Stochastic Gradient Descent_.\n",
        "\n",
        "1. Avoid reading and computing on every input data sample for every training iteration.\n",
        "    * Speeds up the iterations while still making optimization progress.\n",
        "    * Works better with limited GPU memory and CPU cache. Avoid slow downs by thrashing limited memory.\n",
        "\n",
        "2. Improve training convergence by adding _noise_ to the weight updates.\n",
        "    * Possibly avoid getting stuck in a local minima.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Advantages of SGD\n",
        ":::\n",
        "\n",
        "Consider the following example.\n",
        "\n",
        "<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n",
        "\n",
        "![](figs/NN-figs/L25-GD-vs-SGD.png){width=\"75%\" fig-align=\"center\"}\n",
        "\n",
        "This contour plot shows a loss surface for a model with only 2 parameters.\n",
        "\n",
        "With full-batch gradient descent, starting points 1 and 3 converge to the\n",
        "global minimum, but starting point 2 gets stuck in a local minimum.\n",
        "\n",
        "With SGD, starting point 1 converges to the global minimum. \n",
        "However, starting point 2 now avoids the local minimum and converges to\n",
        "the global minimum.\n",
        "\n",
        "# Load an Image Dataset in Batches in PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DataSet and DataLoader\n",
        "\n",
        "![](figs/DataSetDataLoader.png)\n",
        "\n",
        "---\n",
        "\n",
        "- **`Dataset` Object**:\n",
        "  - Abstract class representing a dataset.\n",
        "  - Custom datasets are created by subclassing `Dataset` and implementing `__len__` and `__getitem__`.\n",
        "\n",
        "  - **`DataLoader` Object**:\n",
        "  - Provides an iterable over a dataset.\n",
        "  - Handles batching, shuffling, and loading data in parallel.\n",
        "\n",
        "  - **Key Features**:\n",
        "    - **Batching**: Efficiently groups data samples into batches.\n",
        "    - **Shuffling**: Randomizes the order of data samples.\n",
        "    - **Parallel Loading**: Uses multiple workers to load data in parallel, improving performance.\n",
        "\n",
        "\n",
        "## 1. Load and Scale MNIST\n",
        "\n",
        "Load MNIST handwritten digit dataset with 60K training samples and 10K test samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Define a transform to scale the pixel values from [0, 255] to [-1, 1]\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = torchvision.datasets.MNIST('./data/MNIST_data/', download=True,\n",
        "                                    train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n",
        "                                          shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = torchvision.datasets.MNIST('./data/MNIST_data/', download=True,\n",
        "                                    train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, \n",
        "                                         shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* `torchvision.dataset.MNIST` is a convenience class which inherits from\n",
        "  `torch.utils.data.Dataset` (see [doc](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset))\n",
        "   that wraps a particular dataset and overwrites a `__getitem__()` method which\n",
        "   retrieves a data sample given an \n",
        "   index or a key.\n",
        "\n",
        "* If we give the argument `train=True`, it returns the training set, while the \n",
        "  argument `train=False` returns the test set.\n",
        "\n",
        "* `torch.utils.data.DataLoader()` takes a dataset as in the previous line and\n",
        "  returns a python _iterable_ which lets you loop through the data.\n",
        "\n",
        "* We give `DataLoader` the _batch size_, and it will return a batch of data samples\n",
        "  on each iteration.\n",
        "\n",
        "* By passing `shuffle=True`, we are telling the data loader to shuffle the batches\n",
        "  after every epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f\"No. of training images: {len(trainset)}\")\n",
        "print(f\"No. of test images: {len(testset)}\")\n",
        "print(\"The dataset classes are:\")\n",
        "print(trainset.classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 1. Load and Scale MNIST\n",
        ":::\n",
        "\n",
        "We can see the data loader, `trainloader` in action in the code below to\n",
        "get a batch and visualize it along with the labels.\n",
        "\n",
        "Everytime we rerun the cell we will get a different batch.\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"40%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: {.column width=\"60%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "# Assuming batch_size is 64 and images are displayed in an 8x8 grid\n",
        "labels_grid = [trainset.classes[labels[j]] for j in range(64)]\n",
        "labels_grid = np.array(labels_grid).reshape(8, 8)\n",
        "\n",
        "df = pd.DataFrame(labels_grid)\n",
        "\n",
        "# Generate HTML representation of DataFrame with border\n",
        "html = df.to_html(border=1)\n",
        "\n",
        "# Add CSS to shrink the size of the table\n",
        "html = f\"\"\"\n",
        "<style>\n",
        "    table {{\n",
        "        font-size: 14px;\n",
        "    }}\n",
        "</style>\n",
        "{html}\n",
        "\"\"\"\n",
        "\n",
        "# Display the DataFrame\n",
        "display(HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "# Convolutional Neural Networks\n",
        "\n",
        "## Problems with Fully-Connected Networks\n",
        "\n",
        "* Size\n",
        "    * 224x224 RGB image = 150,528 dimensions\n",
        "    * Hidden layers generally larger than inputs\n",
        "    * One hidden layer = $150,520x150,528\\approx 22$ billion weights  22 billion weigth\n",
        "* Nearby pixels statistically related\n",
        "    * But fully connected network doesn't exploit spatial correlation\n",
        "* Should be stable under transformations\n",
        "    * Donâ€™t want to re-learn appearance at different parts of image\n",
        "\n",
        "## Convolutional Neural Network (CNN)\n",
        "\n",
        "- **Definition**:\n",
        "  - A type of deep learning model designed for processing structured grid data, such as images.\n",
        "  - Utilizes convolutional layers to automatically and adaptively learn spatial hierarchies of features.\n",
        "\n",
        "--- \n",
        "\n",
        "- **Key Components**:\n",
        "  - **Convolutional Layers**: Apply filters to input data to create feature maps.\n",
        "  - **Pooling Layers**: Reduce the dimensionality of feature maps while retaining important information.\n",
        "  - **Fully Connected Layers**: Perform classification based on the features extracted by convolutional and pooling layers.\n",
        "\n",
        "---\n",
        "\n",
        "- **Advantages**:\n",
        "  - **Parameter Sharing**: Reduces the number of parameters, making the network more efficient.\n",
        "  - **Translation Invariance**: Recognizes patterns regardless of their position in the input.\n",
        "\n",
        "## Convolutional Network Applications\n",
        "\n",
        "<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n",
        "\n",
        "![](figs/NN-figs/L25-img-class.svg){width=\"75%\" fig-align=\"center\"}\n",
        "\n",
        "* Multi-class classification problem ( >2 possible classes)\n",
        "* Convolutional network with classification output\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Convolutional Network Applications\n",
        ":::\n",
        "\n",
        "<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n",
        "\n",
        "![](figs/NN-figs/L25-obj-det.png){width=\"75%\" fig-align=\"center\"}\n",
        "\n",
        "* Localize and classify objects in an image\n",
        "* Convolutional network with classification _and_ regression output\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Convolutional Network Applications\n",
        ":::\n",
        "\n",
        "<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n",
        "\n",
        "![](figs/NN-figs/L25-img-seg.png){width=\"75%\" fig-align=\"center\"}\n",
        "\n",
        "* Classify each pixel in an image to 2 or more classes\n",
        "* Convolutional encoder-decoder network with a classification values for each pixel\n",
        "\n",
        "\n",
        "\n",
        "## Classification Invariant to Shift\n",
        "\n",
        "<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n",
        "\n",
        "![](figs/NN-figs/L25-shift-img-class.png){width=\"75%\" fig-align=\"center\"}\n",
        "\n",
        "* Let's say we want to do classification on these two images.\n",
        "* If you look carefully, one image is shifted w.r.t. the other.\n",
        "* An FCN would have to learn a new set of weights for each shift.\n",
        "\n",
        "\n",
        "## Image Segmentation Invariant to Shift\n",
        "\n",
        "<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n",
        "\n",
        "![](figs/NN-figs/L25-shift-seg.png){.r-stretch width=\"65%\" fig-align=\"center\"}\n",
        "\n",
        "* Same thing for image segmentation.\n",
        "* An FCN would have to learn a new set of weights for each shift.\n",
        "\n",
        ":::: {.fragment}\n",
        "Solution: Convolutional Neural Networks\n",
        "\n",
        "* Parameters only look at local data regions\n",
        "* Shares parameters across image or signal\n",
        "::::\n",
        "\n",
        "## 1-D Convolution\n",
        "\n",
        "<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n",
        "\n",
        "![](figs/NN-figs/L25-conv06.png){width=\"65%\" fig-align=\"center\"}\n",
        "\n",
        "In CNNs, we define a set of weights that are moved across\n",
        "the input data.\n",
        "\n",
        "Here is an example with 3 weights and input of length 6.\n",
        "\n",
        "In Figure (a), we calculate \n",
        "\n",
        "$$\n",
        "z_2 = \\omega_1 x_1 + \\omega_2 x_2 + \\omega_3 x_3.\n",
        "$$\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 1-D Convolution\n",
        "![](figs/NN-figs/L25-conv06.png){width=\"65%\" fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "To calculate $z_3$, we shift the weights over 1 place (figure (b)) and then\n",
        "weight and sum the inputs. We can generalize the equation slightly.\n",
        "\n",
        "$$\n",
        "z_i = \\omega_1 x_{i - 1} + \\omega_2 x_i + \\omega_3 x_{i+1}.\n",
        "$$\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 1-D Convolution -- Edge Cases\n",
        "![](figs/NN-figs/L25-conv06.png){width=\"65%\" fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "What do we do about $z_1$?\n",
        "\n",
        "We calculate $z_1$ by _padding_ our input data. In figure (c), we\n",
        "simply add (pad with) $0$. This allows us to calculate $z_1$.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 1-D Convolution -- Edge Cases\n",
        ":::\n",
        "\n",
        "<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n",
        "\n",
        "![](figs/NN-figs/L25-conv07.png){width=\"85%\" fig-align=\"center\"}\n",
        "\n",
        "Alternatively, we can just reduce the size of the output, by only calculating where\n",
        "we have _valid_ input data, as in figure (d).\n",
        "\n",
        "For 1-D data, this reduces the output size by 1 at the beginning and end of the\n",
        "data. This means that for a length-3 filter, the size of the output is reduced by 2.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 1-D Convolution -- Parameters\n",
        ":::\n",
        "\n",
        "<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n",
        "\n",
        "![](figs/NN-figs/L25-conv-fig10-3.png){width=\"75%\" fig-align=\"center\"}\n",
        "\n",
        "There are a few design choices one can make with convolution layers, such as:\n",
        "\n",
        "1. __filter length__, e.g., size 3 in figures (a) and (b) and 5 in (c).\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 1-D Convolution -- Parameters\n",
        "![](figs/NN-figs/L25-conv-fig10-3.png){width=\"75%\" fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "2. __stride__, the shift of the weights to calculate the next output. Common values are\n",
        "    1. _stride 1_ as we saw in the previous examples and in figures (c) and (d),\n",
        "    2. _stride 2_, which effectively halves the size of the output as in figures (a) and (b).\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 1-D Convolution -- Parameters\n",
        "![](figs/NN-figs/L25-conv-fig10-3.png){width=\"75%\" fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "3. __dilation__, the spacing between elements in the filter. There is an example of dilation=2 in the filter  in figure (d)\n",
        "\n",
        "## 2D Convolution\n",
        "\n",
        "For images and video frames we use a two-dimensional convolution\n",
        "(called `conv2d` in PyTorch) which is an extension of the 1-D\n",
        "convolution.\n",
        "From [cs231n](https://cs231n.github.io/convolutional-networks/).\n",
        "\n",
        "Let's look at a 2D convolution layer: $7 \\times 7 \\times 3 \\rightarrow 3 \\times 3 \\times 2$\n",
        "\n",
        "<!-- Image Credit \"https://cs231n.github.io/convolutional-networks/\"-->\n",
        "\n",
        "<iframe src=\"figs/NN-figs/conv-demo/index.html\" width=\"100%\" height=\"800px\"></iframe>\n",
        "\n",
        "<!-- ![](figs/NN-figs/L25-conv-2d.png){width=\"75%\" fig-align=\"center\"} -->\n",
        "\n",
        "\n",
        "## Define a CNN in PyTorch\n",
        "\n",
        "We will do the following steps in order:\n",
        "\n",
        "1. Load and scale the MNIST training and test datasets using\n",
        "   ``torchvision`` (already done)\n",
        "2. Define a Convolutional Neural Network architecture\n",
        "3. Define a loss function\n",
        "4. Train the network on the training data\n",
        "5. Test the network on the test data\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Define a CNN in PyTorch\n",
        ":::\n",
        "\n",
        "Define and instantiate a CNN for MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# network for MNIST\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = nn.functional.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = nn.functional.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        output = nn.functional.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `Conv2d` layer is defined as:\n",
        "\n",
        "```python\n",
        "CLASS torch.nn.Conv2d(in_channels, out_channels, kernel_size, \n",
        "                      stride=1, padding_mode='valid', ...)\n",
        "```\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Define a CNN in PyTorch\n",
        ":::\n",
        "\n",
        "We can see the layers and shapes of the data as it passes through the network.\n",
        "\n",
        "\n",
        "| Layer   | Kernel Size | Stride | Input Shape | Input Channels | Output Channels | Output Shape |\n",
        "| ------- | ----------- | ------ | ----------- | -------------- | --------------- | ------------ |\n",
        "| Conv2D/ReLU  | (3x3)       | 1      |  28x28      |    1           |    32           |  26x26       |\n",
        "| Conv2D/ReLU  | (3x3)       | 1      |  26x26      |    32          |    64           |  24x24       |\n",
        "| Max_pool2d | (2x2)    | 2      |  24x24      |    64          |    64           |  12x12       |\n",
        "| Flatten |             |        |  12x12      |    64          |    1            |  9216x1      |\n",
        "| FC/ReLU |             |        |  9216x1     |    1           |    1            |  128x1       |\n",
        "| FC Linear |           |        |  128x1      |    1           |    1            |  10x1        |\n",
        "| Soft Max |            |        |  10x1      |    1           |    1            |  10x1        |\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Define a CNN in PyTorch\n",
        ":::\n",
        "\n",
        "Here's a common way to visualize a CNN architecture.\n",
        "\n",
        "<!-- Image Credit \"https://alexlenail.me/NN-SVG/AlexNet.html\"-->\n",
        "\n",
        "![](figs/NN-figs/L25-mnist-cnn2.svg){width=\"75%\" fig-align=\"center\"}\n",
        "\n",
        "[NN-SVG](https://alexlenail.me/NN-SVG/AlexNet.html)\n",
        "\n",
        "## 3. Define a Loss function and optimizer\n",
        "\n",
        "We'll use a Classification Cross-Entropy loss and SGD with momentum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross Entropy Loss\n",
        "\n",
        "* Popular loss function for multi-class classification that measures the \n",
        "  _dissimilarity_ between the predicted class log probability $\\log(p_i)$ and\n",
        "  the true class $y_i$.\n",
        "\n",
        "$$\n",
        "- \\sum_i y_i \\log(p_i).\n",
        "$$\n",
        "\n",
        "See this\n",
        "[link](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) \n",
        "for more information.\n",
        "\n",
        "## Momentum\n",
        "\n",
        "Momentum is a useful technique in optimization. It accelerates\n",
        "gradients vectors in the right directions, which can lead to faster convergence. \n",
        "\n",
        "It is inspired by physical laws of motion. The optimizer uses 'momentum' to push\n",
        "over hilly terrains and valleys to find the global minimum.\n",
        "\n",
        "In gradient descent, the weight update rule with momentum is given by:\n",
        "\n",
        "$$ \n",
        "m_{t+1} = \\beta m_t + (1 - \\beta) \\nabla J(w),\n",
        "$$\n",
        "\n",
        "$$\n",
        "w_{t+1} = w_t - \\alpha m_{t+1},\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        "* $m_t$ is the momentum (which drives the update at iteration $t$), \n",
        "* $\\beta \\in [0, 1)$, typically 0.9, controls the degree to which the gradient is smoothed over time, and \n",
        "* $\\alpha$ is the learning rate.\n",
        "\n",
        "See _Understanding Deep Learning_, Section 6.3 to learn more.\n",
        "\n",
        "## 4. Train the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "print(f\"[Epoch #, Iteration #] loss\")\n",
        "\n",
        "# loop over the dataset multiple times\n",
        "# change this value to 2\n",
        "for epoch in range(1):  \n",
        "    \n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 4. Train the network\n",
        ":::\n",
        "\n",
        "Display some of the images from the test set with the ground truth labels.\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"40%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: {.column width=\"60%\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "# Assuming batch_size is 64 and images are displayed in an 8x8 grid\n",
        "labels_grid = [testset.classes[labels[j]] for j in range(64)]\n",
        "labels_grid = np.array(labels_grid).reshape(8, 8)\n",
        "\n",
        "df = pd.DataFrame(labels_grid)\n",
        "\n",
        "# Generate HTML representation of DataFrame with border and smaller font size\n",
        "html = df.to_html(border=1)\n",
        "\n",
        "# Add CSS to shrink the size of the table\n",
        "html = f\"\"\"\n",
        "<style>\n",
        "    table {{\n",
        "        font-size: 14px;\n",
        "    }}\n",
        "</style>\n",
        "{html}\n",
        "\"\"\"\n",
        "\n",
        "# Display the DataFrame\n",
        "display(HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 4. Train the network\n",
        ":::\n",
        "\n",
        "Let's run inference (forward pass) on the model to get numeric outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "outputs = net(images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the index of the element with highest value and print the label \n",
        "associated with that index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "_, predicted = torch.max(outputs, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 4. Train the network\n",
        ":::\n",
        "\n",
        "We can display the predicted labels for the images.\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"40%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: {.column width=\"60%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Assuming batch_size is 64 and images are displayed in an 8x8 grid\n",
        "labels_grid = [testset.classes[predicted[j]] for j in range(64)]\n",
        "labels_grid = np.array(labels_grid).reshape(8, 8)\n",
        "\n",
        "df = pd.DataFrame(labels_grid)\n",
        "\n",
        "# Generate HTML representation of DataFrame with border\n",
        "html = df.to_html(border=1)\n",
        "\n",
        "# Add CSS to shrink the size of the table\n",
        "html = f\"\"\"\n",
        "<style>\n",
        "    table {{\n",
        "        font-size: 14px;\n",
        "    }}\n",
        "</style>\n",
        "{html}\n",
        "\"\"\"\n",
        "\n",
        "# Display the DataFrame\n",
        "display(HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 4. Train the network\n",
        ":::\n",
        "\n",
        "Evaluate over the entire test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 4. Train the network\n",
        ":::\n",
        "\n",
        "Evaluate the performance per class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in testset.classes}\n",
        "total_pred = {classname: 0 for classname in testset.classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[testset.classes[label]] += 1\n",
        "            total_pred[testset.classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## To Dig Deeper\n",
        "\n",
        "Try working with common CNN network architectures. \n",
        "\n",
        "For example see [_Understanding Deep Learning_](https://udlbook.github.io/udlbook/)\n",
        "section 10.5 or \n",
        "[PyTorch models and pre-trained weights](https://pytorch.org/vision/stable/models.html).\n",
        "\n",
        "## Recap\n",
        "\n",
        "We covered the following topics:\n",
        "\n",
        "* Convolutional Neural Networks\n",
        "* 1-D and 2-D convolutions\n",
        "* Common CNN architectures\n",
        "* Training a CNN in PyTorch\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tomg/Source/courses/tools4ds/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}