{
  "hash": "c12e16dfc1bf643b71925d073adddbce",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Recommender Systems -- II\njupyter: python3\nbibliography: references.bib\n---\n\n## Introduction\n\nNext\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\nIn Part II, we will:\n\n* And one modern method:\n    * Deep Learning Recommender Model (DLRM)\n* Reflect on the impact of recommender systems\n\n:::\n::: {.column width=\"50%\"}\n\n\nThis section draws heavily on\n\n* _Deep Learning Recommender Model for Personalization and Recommendation Systems_, [@naumov2019deep]\n\n:::\n::::\n\n# Deep Learning for Recommender Systems\n\n## Deep Learning for Recommender Systems\n\nBesides the Collaborative Filtering and Matrix Factorization models, other popular\napproaches to building recommender systems use Deep Learning.\n\nWe'll look at the Deep Learning Recommender Model (DLRM) proposed by Facebook in\n2019 [@naumov2019deep] with [GitHub repository](https://github.com/facebookresearch/dlrm).\n\n## DLRM Architecture\n\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n- Components (@fig-dlrm-model):\n  1. **Embeddings**: Dense representations for categorical data.\n  2. **Bottom MLP**: Transforms dense continuous features.\n  3. **Feature Interaction**: Dot-product of embeddings and dense features.\n  4. **Top MLP**: Processes interactions and outputs probabilities.\n\n:::\n::: {.column width=\"50%\"}\n\n![DLRM Architecture](figs/RecSys-figs/dlrm-model.png){.lightbox width=80% fig-align=\"center\" #fig-dlrm-model}\n\n:::\n::::\n\nLet's look at each of these components in turn.\n\n## Embeddings\n\n**Embeddings**: Map categorical inputs to latent factor space.\n\n:::: {.columns}\n::: {.column width=\"65%\"}\n- A learned embedding matrix $W \\in \\mathbb{R}^{m \\times d}$ for each category of input\n- One-hot vector $e_i$ with $i\\text{-th}$ entry 1 and rest are 0s\n- Embedding of $e_i$ is $i\\text{-th}$ row of $W$, i.e., $w_i^T = e_i^T W$\n\nWe can also use weighted combination of multiple items with a multi-hot vector\nof weights $a^T = [0, ..., a_{i_1}, ..., a_{i_k}, ..., 0]$.\n\nThe embedding of this multi-hot vector is then $a^T W$.\n\n:::\n::: {.column width=\"35%\"}\n![DLRM Architecture](figs/RecSys-figs/dlrm-model01.png){.lightbox width=100% fig-align=\"center\"}\n:::\n::::\n\n---\n\nPyTorch has a convenient way to do this using `EmbeddingBag`, which besides summing\ncan combine embeddings via mean or max pooling.\n\nHere's an example with 5 embeddings of dimension 3:\n\n::: {#4ca484d7 .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\n\n# Example embedding matrix: 5 embeddings, each of dimension 3\nembedding_matrix = nn.EmbeddingBag(num_embeddings=5, embedding_dim=3, mode='mean')\n\n# Input: Indices into the embedding matrix\ninput_indices = torch.tensor([1, 2, 3, 4])  # Flat list of indices\noffsets = torch.tensor([0, 2])  # Start new bag at position 0 and 2 in input_indices\n\n# Forward pass\noutput = embedding_matrix(input_indices, offsets)\n\nprint(\"Embedding Matrix:\\n\", embedding_matrix.weight)\nprint(\"Output:\\n\", output)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEmbedding Matrix:\n Parameter containing:\ntensor([[ 0.7801,  0.6730,  0.1572],\n        [ 1.3169,  0.7998,  0.3681],\n        [-1.8945,  0.6570, -0.5110],\n        [ 0.4633,  0.0072, -0.7963],\n        [ 0.0663, -0.4947,  0.9090]], requires_grad=True)\nOutput:\n tensor([[-0.2888,  0.7284, -0.0715],\n        [ 0.2648, -0.2438,  0.0563]], grad_fn=<EmbeddingBagBackward0>)\n```\n:::\n:::\n\n\n## Dense Features\n\n:::: {.columns}\n::: {.column width=\"65%\"}\nThe advantage of the DLRM architecture is that it can take continuous features\nas input such as the user's age, time of day, etc.\n\nThere is a bottom MLP that transforms these dense features into a latent space of\nthe same dimension $d$.\n:::\n::: {.column width=\"35%\"}\n![DLRM Architecture](figs/RecSys-figs/dlrm-model02.png){.lightbox width=100% fig-align=\"center\"}\n:::\n::::\n\n## Optional Sparse Feature MLPs\n\n:::: {.columns}\n::: {.column width=\"65%\"}\nOptionally, one can add MLPs to transform the sparse features as well.\n\n:::\n::: {.column width=\"35%\"}\n![DLRM Architecture](figs/RecSys-figs/dlrm-model03.png){.lightbox width=100% fig-align=\"center\"}\n:::\n::::\n\n## Feature Interactions\n\n:::: {.columns}\n::: {.column width=\"65%\"}\nThe 2nd order interactions are modeled via dot-products of all pairs from the\ncollections of embedding vectors and processed dense features.\n\nThe results of the dot-product interactions are concatenated with the processed\ndense vectors.\n\n:::\n::: {.column width=\"35%\"}\n![DLRM Architecture](figs/RecSys-figs/dlrm-model04.png){.lightbox width=100% fig-align=\"center\"}\n:::\n::::\n\n## Top MLP\n\n:::: {.columns}\n::: {.column width=\"65%\"}\nThe concatenated vector is then passed to a final MLP and then to a sigmoid\nfunction to produce the final prediction (e.g., probability score of recommendation)\n\nThis entire model is trained end-to-end using standard deep learning techniques.\n\n:::\n::: {.column width=\"35%\"}\n![DLRM Architecture](figs/RecSys-figs/dlrm-model05.png){.lightbox width=100% fig-align=\"center\"}\n:::\n::::\n\n## Training Results\n\n![DLRM Training Results](figs/RecSys-figs/dlrm-training-results.png){width=\"70%\" fig-align=\"center\" #fig-dlrm-training-results}\n\n@fig-dlrm-training-results shows the training (solid) and validation (dashed)\naccuracies of DLRM on the [Criteo Ad Kaggle dataset](https://www.kaggle.com/competitions/criteo-display-ad-challenge/overview).\n\nAccuracy is compared with Deep and Cross network (DCN) [@wang2017deep].\n\n::: {style=\"font-size: 70%\"}\n## Other Modern Approaches\n\nThere are many other modern approaches to recommender systems for example:\n\n::: {.columns}\n::: {.column}\n\n1. **Graph-Based Recommender Systems**:\n   - Leverage graph structures to capture relationships between users and items.\n   - Use techniques like Graph Neural Networks (GNNs) to enhance recommendation accuracy.\n\n2. **Context-Aware Recommender Systems**:\n   - Incorporate contextual information such as time, location, and user mood to provide more personalized recommendations.\n   - Contextual data can be integrated using various machine learning models.\n\n:::\n::: {.column}\n\n3. **Hybrid Recommender Systems**:\n   - Combine multiple recommendation techniques, such as collaborative filtering and content-based filtering, to improve performance.\n   - Aim to leverage the strengths of different methods while mitigating their weaknesses.\n\n4. **Reinforcement Learning-Based Recommender Systems**:\n   - Use reinforcement learning to optimize long-term user engagement and satisfaction.\n   - Models learn to make sequential recommendations by interacting with users and receiving feedback.\n\n:::\n:::\n\nThese approaches often leverage advancements in machine learning and data processing to provide more accurate and personalized recommendations.\n\nSee [@ricci2022recommender] for a comprehensive overview of recommender systems.\n\n:::\n\n# Impact of Recommender Systems\n\n## Filter Bubbles\n\nThere are a number of concerns with the widespread use of recommender systems and personalization in society.\n\nFirst, recommender systems are accused of creating __filter bubbles.__ \n\nA filter bubble is the tendency for recommender systems to limit the variety of information presented to the user.\n\nThe concern is that a user's past expression of interests will guide the algorithm in continuing to provide \"more of the same.\"\n\nThis is believed to increase polarization in society, and to reinforce confirmation bias.\n\n## Maximizing Engagement\n\nSecond, recommender systems in modern usage are often tuned to __maximize engagement.__\n\nIn other words, the objective function of the system is not to present the user's most favored content, but rather the content that will be most likely to keep the user on the site.\n\nThe incentive to maximize engagement arises on sites that are supported by advertising revenue.   \n\nMore engagement time means more revenue for the site.\n\n## Extreme Content\n\nHowever, many studies have shown that sites that strive to __maximize \nengagement__ do so in large part by guiding users toward __extreme content:__\n\n* content that is shocking, \n* or feeds conspiracy theories, \n* or presents extreme views on popular topics.\n\nGiven this tendency of modern recommender systems, \nfor a third party to create \"clickbait\" content such as this, one of the easiest\nways is to present false claims.\n\nMethods for addressing these issues are being very actively studied at present.\n\nWays of addressing these issues can be:\n\n* via technology\n* via public policy\n\n# Recap and References\n\n## BU CS/CDS Research\n\nYou can read about some of the work done in Professor Mark Crovella's group on\nthis topic:\n\n* _How YouTube Leads Privacy-Seeking Users Away from Reliable Information_, [@spinelli2020youtube] \n* _Closed-Loop Opinion Formation_, [@spinelli2017closed] \n* _Fighting Fire with Fire: Using Antidote Data to Improve Polarization and Fairness of Recommender Systems_, [@rastegarpanah2019fighting] \n\n## Recap\n\n* Introduction to recommender systems and their importance in modern society.\n* Explanation of collaborative filtering (CF) and its two main approaches: user-user similarity and item-item similarity.\n* Discussion on the challenges of recommender systems, including scalability and data sparsity.\n* Introduction to matrix factorization (MF) as an improvement over CF, using latent vectors and alternating least squares (ALS) for optimization.\n* Practical implementation of ALS for matrix factorization on a subset of Amazon movie reviews.\n* Review of Deep Learning Recommender Model (DLRM) architecture and its components.\n* Discussion on the societal impact of recommender systems, including filter bubbles and engagement maximization.\n\n## References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "20-Recommender-Systems-II_files"
    ],
    "filters": [],
    "includes": {}
  }
}