{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Logistic Regression\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/18-Regression-II-Logistic.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mp\n",
        "import sklearn\n",
        "from IPython.display import Image, HTML\n",
        "import statsmodels.api as sm\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "\n",
        "import laUtilities as ut\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far we have seen linear regression: \n",
        "\n",
        "* a continuous valued observation is estimated as a linear (or affine) function\n",
        " of the independent variables.\n",
        "\n",
        "Now we will look at the following situation.\n",
        "\n",
        "## Estimating a Probability\n",
        "\n",
        "::: {.incremental}\n",
        "\n",
        "* Imagine that you are observing a binary variable -- value 0 or 1.\n",
        "\n",
        "* That is, these could be pass/fail, admit/reject, Democrat/Republican, etc.\n",
        "\n",
        "* Assume there is some __probability__ of observing a 1, and that probability is a\n",
        "function of certain independent variables.\n",
        "\n",
        "* So the key properties of a problem that make it appropriate for logistic\n",
        "regression are:\n",
        "\n",
        "    * You are trying to predict a __categorical__ variable\n",
        "    * You want to estimate a __probability__ of seeing a particular value of the\n",
        "      categorical variable.\n",
        "\n",
        ":::\n",
        "\n",
        "## Example: Grad School Admission\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "\n",
        "::: {.callout-note}\n",
        "The following example was adapted from this \n",
        "[URL](http://www.ats.ucla.edu/stat/r/dae/logit.htm) which seems to be no longer\n",
        "available. There is an \n",
        "[archive of the page](https://web.archive.org/web/20161118221128/http://www.ats.ucla.edu/stat/r/dae/logit.htm)\n",
        "and an [archive of the dataset](https://web.archive.org/web/20161022051727/http://www.ats.ucla.edu/stat/data/binary.csv).\n",
        ":::\n",
        ":::\n",
        "\n",
        "Let's consider this question:\n",
        "\n",
        "> What is the probability I will be admitted to Grad School?\n",
        "\n",
        "Let's see how variables, such as,\n",
        "\n",
        "* _GRE_ (Graduate Record Exam scores), \n",
        "* _GPA_ (grade point average), and \n",
        "* prestige of the undergraduate institution\n",
        "\n",
        "affect admission into graduate school. \n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Example continued\n",
        ":::\n",
        "\n",
        "The response variable, admit/don't admit, is a binary variable.\n",
        "\n",
        "So there are three predictor variables: __gre,__ __gpa__ and __rank.__ \n",
        "\n",
        "* We will treat the variables _gre_ and _gpa_ as continuous. \n",
        "* The variable _rank_ takes on the values 1 through 4 with 1 being the highest prestige.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Example continued\n",
        ":::\n",
        "\n",
        "Let's look at 10 lines of the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "# original data source: http://www.ats.ucla.edu/stat/data/binary.csv\n",
        "df = pd.read_csv('data/ats-admissions.csv') \n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Example continued\n",
        ":::\n",
        "\n",
        "and some summary statistics:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Example continued\n",
        ":::\n",
        "\n",
        "We can also plot histograms of the variables:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "df.hist(figsize = (10, 4));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Note how `df.hist()` automatically plots a histogram for each column in the\n",
        "> dataframe as a subplot.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Example continued\n",
        ":::\n",
        "\n",
        "Let's look at how each independent variable affects admission probability by\n",
        "plotting the mean admission probability as a function of the independent variable.\n",
        "\n",
        "> Note that there's a greatly expanded [`groupby`](https://tools4ds.github.io/DS701-Course-Notes/02B-Pandas.html#understanding-pandas-dataframe.groupby) section in the Pandas refresher.\n",
        "\n",
        "We add error bars to the means to indicate the standard error of the mean.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Example continued\n",
        ":::\n",
        "\n",
        "First, __rank__:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import numpy as np\n",
        "\n",
        "# Calculate mean and standard error\n",
        "grouped = df.groupby('rank')['admit']\n",
        "means = grouped.mean()\n",
        "\n",
        "# Compute 'standard error of the mean'\n",
        "errors = grouped.std() / np.sqrt(grouped.count())\n",
        "\n",
        "# Plot with error bars\n",
        "ax = means.plot(marker='o', yerr=errors, fontsize=12, capsize=5)\n",
        "ax.set_ylabel('P[admit]', fontsize=16)\n",
        "ax.set_xlabel('Rank', fontsize=16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Example continued\n",
        ":::\n",
        "\n",
        "Next, __GRE__:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "grouped_gre = df.groupby('gre')['admit']\n",
        "means_gre = grouped_gre.mean()\n",
        "errors_gre = grouped_gre.std() / np.sqrt(grouped_gre.count())\n",
        "\n",
        "ax = means_gre.plot(marker='o', yerr=errors_gre, fontsize=12, capsize=5)\n",
        "ax.set_ylabel('P[admit]', fontsize=16)\n",
        "ax.set_xlabel('GRE', fontsize=16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Example continued\n",
        ":::\n",
        "\n",
        "Finally, __GPA__ (for this visualization, we aggregate GPA into 8 bins):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "bins = np.linspace(df.gpa.min(), df.gpa.max(), 8)\n",
        "bin_centers = (bins[:-1] + bins[1:]) / 2\n",
        "grouped_gpa = df.groupby(np.digitize(df.gpa, bins)).mean()['admit']\n",
        "ax = grouped_gpa.plot(marker='o', fontsize=12)\n",
        "ax.set_ylabel('P[admit]', fontsize=16)\n",
        "ax.set_xlabel('GPA', fontsize=16)\n",
        "ax.set_xticks(range(1, len(bin_centers) + 1))\n",
        "ax.set_xticklabels([f'{center:.2f}' for center in bin_centers], rotation=45);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Example continued\n",
        ":::\n",
        "\n",
        "Finally, we plot admission status versus GRE score for each data point for each of the four ranks:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "df1 = df[df['rank']==1]\n",
        "df2 = df[df['rank']==2]\n",
        "df3 = df[df['rank']==3]\n",
        "df4 = df[df['rank']==4]\n",
        "\n",
        "fig = plt.figure(figsize = (10, 5))\n",
        "\n",
        "ax1 = fig.add_subplot(221)\n",
        "df1.plot.scatter('gre','admit', ax = ax1)\n",
        "plt.title('Rank 1 Institutions')\n",
        "\n",
        "ax2 = fig.add_subplot(222)\n",
        "df2.plot.scatter('gre','admit', ax = ax2)\n",
        "plt.title('Rank 2 Institutions')\n",
        "\n",
        "ax3 = fig.add_subplot(223, sharex = ax1)\n",
        "df3.plot.scatter('gre','admit', ax = ax3)\n",
        "plt.title('Rank 3 Institutions')\n",
        "\n",
        "ax4 = fig.add_subplot(224, sharex = ax2)\n",
        "plt.title('Rank 4 Institutions')\n",
        "df4.plot.scatter('gre','admit', ax = ax4);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What we want to do is to fit a model that predicts the probability of admission\n",
        "as a function of these independent variables.\n",
        "\n",
        "## Logistic Regression\n",
        "\n",
        "::: {.incremental}\n",
        "* Logistic regression is concerned with estimating a __probability.__\n",
        "\n",
        "* However, all that is available are categorical observations, which we will code as 0/1.\n",
        "\n",
        "* That is, these could be pass/fail, admit/reject, Democrat/Republican, etc.\n",
        "\n",
        "* Now, a linear function like $\\beta_0 + \\beta_1 x$ cannot be used to predict\n",
        "probability directly, because \n",
        "    * the linear function takes on all values (from -$\\infty$ to +$\\infty$), \n",
        "    * and probability only ranges over $[0, 1]$.\n",
        "\n",
        ":::\n",
        "\n",
        "## Odds and Log-Odds\n",
        "\n",
        "However, there is a transformation of probability that works: it is called\n",
        "__log-odds__.\n",
        "\n",
        "For any probabilty $p$, the __odds__ is defined as $p/(1-p)$, which is the ratio\n",
        "of the probability of an event to the probability of the non-event.\n",
        "\n",
        "Notice that odds vary from 0 to $\\infty$, and odds < 1 indicates that $p < 1/2$.\n",
        "\n",
        "Now, there is a good argument that to fit a linear function, instead of using\n",
        "odds, we should use log-odds. \n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Odds and Log-Odds\n",
        ":::\n",
        "\n",
        "That is simply $\\log p/(1-p)$ which is also called the __logit__ function, which\n",
        "is an abbreviation for **log**istic un**it**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "pvec = np.linspace(0.01, 0.99, 100)\n",
        "ax = plt.figure(figsize = (6, 4)).add_subplot()\n",
        "ax.plot(pvec, np.log(pvec / (1-pvec)))\n",
        "ax.tick_params(labelsize=12)\n",
        "ax.set_xlabel('Probability', fontsize = 14)\n",
        "ax.set_ylabel('Log-Odds', fontsize = 14)\n",
        "ax.set_title('Logit Function: $\\log (p/1-p)$', fontsize = 16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Odds and Log-Odds\n",
        ":::\n",
        "\n",
        "So, logistic regression does the following: it does a linear regression of\n",
        "$\\beta_0 + \\beta_1 x$ against $\\log p/(1-p)$.\n",
        "\n",
        "That is, it fits:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\beta_0 + \\beta_1 x &= \\log \\frac{p(x)}{1-p(x)} \\\\\n",
        "e^{\\beta_0 + \\beta_1 x} &= \\frac{p(x)}{1-p(x)} \\quad \\text{(exponentiate both sides)} \\\\\n",
        "e^{\\beta_0 + \\beta_1 x} (1-p(x)) &= p(x) \\quad \\text{(multiply both sides by $1-p(x)$)} \\\\\n",
        "e^{\\beta_0 + \\beta_1 x}  &= p(x) + p(x)e^{\\beta_0 + \\beta_1 x} \\quad \\text{(distribute $p(x)$)} \\\\\n",
        "\\frac{e^{\\beta_0 + \\beta_1 x}}{1 +e^{\\beta_0 + \\beta_1 x}} &= p(x)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Odds and Log-Odds\n",
        ":::\n",
        "\n",
        "So, logistic regression fits a probability of the following form:\n",
        "\n",
        "$$\n",
        "p(x) = P(y=1\\mid x) = \\frac{e^{\\beta_0+\\beta_1 x}}{1+e^{\\beta_0+\\beta_1 x}}.\n",
        "$$\n",
        "\n",
        "This is a **sigmoid function**; when $\\beta_1 > 0$, \n",
        "\n",
        "* as $x\\rightarrow \\infty$, then $p(x)\\rightarrow 1$ and \n",
        "* as $x\\rightarrow -\\infty$, then $p(x)\\rightarrow 0$.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Odds and Log-Odds\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "alphas = [-4, -8,-12,-20]\n",
        "betas = [0.4,0.4,0.6,1]\n",
        "x = np.arange(40)\n",
        "fig = plt.figure(figsize=(8, 6)) \n",
        "ax = plt.subplot(111)\n",
        "\n",
        "for i in range(len(alphas)):\n",
        "    a = alphas[i]\n",
        "    b = betas[i]\n",
        "    y = np.exp(a+b*x)/(1+np.exp(a+b*x))\n",
        "#     plt.plot(x,y,label=r\"$\\frac{e^{%d + %3.1fx}}{1+e^{%d + %3.1fx}}\\;\\beta_0=%d, \\beta_1=%3.1f$\" % (a,b,a,b,a,b))\n",
        "    ax.plot(x,y,label=r\"$\\beta_0=%d,$    $\\beta_1=%3.1f$\" % (a,b))\n",
        "ax.tick_params(labelsize=12)\n",
        "ax.set_xlabel('x', fontsize = 14)\n",
        "ax.set_ylabel('$p(x)$', fontsize = 14)\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size': 16})\n",
        "ax.set_title('Logistic Functions', fontsize = 16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parameter $\\beta_1$ controls how fast $p(x)$ raises from $0$ to $1$\n",
        "\n",
        "The value of -$\\beta_0$/$\\beta_1$ shows the value of $x$ for which $p(x)=0.5$\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Odds and Log-Odds\n",
        ":::\n",
        "\n",
        "Another interpretation of $\\beta_0$ is that it gives the __base rate__ -- the\n",
        "unconditional probability of a 1.   \n",
        "\n",
        "That is, if you knew nothing about a\n",
        "particular data item, then $p(x) = 1/(1+e^{-\\beta_0})$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "\n",
        "# plot the base rate as a function of beta_0\n",
        "beta_0_values = np.linspace(-10, 10, 100)\n",
        "base_rate = 1 / (1 + np.exp(-beta_0_values))\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(beta_0_values, base_rate)\n",
        "plt.xlabel('beta_0')\n",
        "plt.ylabel('Base Rate')\n",
        "plt.title('Base Rate as a function of beta_0')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Odds and Log-Odds\n",
        ":::\n",
        "\n",
        "The function $f(x) = \\log (x/(1-x))$ is called the __logit__ function.\n",
        "\n",
        "So a compact way to describe logistic regression is that it finds regression\n",
        "coefficients $\\beta_0, \\beta_1$ to fit:\n",
        "\n",
        "$$\n",
        "\\text{logit}\\left(p(x)\\right)=\\log\\left(\\frac{p(x)}{1-p(x)} \\right) = \\beta_0 + \\beta_1 x.\n",
        "$$\n",
        "\n",
        "Note also that the __inverse__ logit function is:\n",
        "\n",
        "$$\n",
        "\\text{logit}^{-1}(x) = \\frac{e^x}{1 + e^x}.\n",
        "$$\n",
        "\n",
        "Somewhat confusingly, this is called the __logistic__ function.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Odds and Log-Odds\n",
        ":::\n",
        "\n",
        "So, the best way to think of logistic regression is that we compute a linear function:\n",
        "    \n",
        "$$\n",
        "\\beta_0 + \\beta_1 x,\n",
        "$$\n",
        "    \n",
        "and then *map* that to a probability using the inverse $\\text{logit}$ function:\n",
        "\n",
        "$$\n",
        "\\frac{e^{\\beta_0+\\beta_1 x}}{1+e^{\\beta_0+\\beta_1 x}}.\n",
        "$$\n",
        "\n",
        "## Logistic vs Linear Regression\n",
        "\n",
        "Let's take a moment to compare linear and logistic regression.\n",
        "\n",
        "In __Linear regression__ we fit \n",
        "\n",
        "$$\n",
        "y_i = \\beta_0 +\\beta_1 x_i + \\epsilon_i.\n",
        "$$\n",
        "\n",
        "We do the fitting by minimizing the sum of squared errors $\\Vert\\epsilon\\Vert$.\n",
        "This can be done in closed form using either geometric arguments or by calculus.\n",
        "\n",
        "Now, if $\\epsilon_i$ comes from a normal distribution with mean zero and some\n",
        "fixed variance, then minimizing the sum of squared errors is exactly the same as finding the\n",
        "maximum likelihood of the data with respect to the probability of the errors.\n",
        "\n",
        "So, in the case of linear regression, it is a lucky fact that the __MLE__ of\n",
        "$\\beta_0$ and $\\beta_1$ can be found by a __closed-form__ calculation.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Logistic vs Linear Regression\n",
        ":::\n",
        "\n",
        "In __Logistic regression__ we fit \n",
        "\n",
        "$$\n",
        "\\text{logit}(p(x_i)) = \\beta_0 + \\beta_1 x_i.\n",
        "$$\n",
        "\n",
        "\n",
        "with $\\text{P}(y_i=1\\mid x_i)=p(x_i).$\n",
        "\n",
        "How should we choose parameters?   \n",
        "\n",
        "Here too, we use Maximum Likelihood Estimation of the parameters.\n",
        "\n",
        "That is, we choose the parameter values that maximize the likelihood of the data given the model.\n",
        "\n",
        "$$\n",
        "\\text{P}(y_i \\mid x_i) = \n",
        "\\left\\{\\begin{array}{lr}\\text{logit}^{-1}(\\beta_0 + \\beta_1 x_i)& \\text{if } y_i = 1\\\\\n",
        "1 - \\text{logit}^{-1}(\\beta_0 + \\beta_1 x_i)& \\text{if } y_i = 0\\end{array}\\right.\n",
        "$$\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Logistic vs Linear Regression\n",
        ":::\n",
        "\n",
        "We can write this as a single expression:\n",
        "\n",
        "$$\n",
        "\\text{P}(y_i \\mid x_i) = \\text{logit}^{-1}(\\beta_0 + \\beta_1 x_i)^{y_i} (1-\\text{logit}^{-1}(\\beta_0 + \\beta_1 x_i))^{1-y_i}.\n",
        "$$\n",
        "\n",
        "We then use this to compute the __likelihood__ of parameters $\\beta_0$, $\\beta_1$:\n",
        "\n",
        "$$\n",
        "L(\\beta_0, \\beta_1 \\mid x_i, y_i) = \\text{logit}^{-1}(\\beta_0 + \\beta_1 x_i)^{y_i} (1-\\text{logit}^{-1}(\\beta_0 + \\beta_1 x_i))^{1-y_i},\n",
        "$$\n",
        "\n",
        "which is a function that we can maximize, for example, with gradient descent.\n",
        "\n",
        "## Logistic Regression In Practice\n",
        "\n",
        "So, in summary, we have:\n",
        "\n",
        "**Input** pairs $(x_i,y_i)$\n",
        "\n",
        "**Output** parameters $\\widehat{\\beta_0}$ and $\\widehat{\\beta_1}$ that maximize the\n",
        "likelihood of the data given these parameters for the logistic regression model.\n",
        "\n",
        "**Method** Maximum likelihood estimation, obtained by gradient descent.\n",
        "\n",
        "The standard package will give us a coefficient $\\beta_i$ for each\n",
        "independent variable (feature).\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Logistic Regression in Practice\n",
        ":::\n",
        "\n",
        "If we want to include a constant (i.e., $\\beta_0$) we need to add a column of 1s (just\n",
        "like in linear regression)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "df['intercept'] = 1.0\n",
        "train_cols = df.columns[1:]\n",
        "train_cols"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "logit = sm.Logit(df['admit'], df[train_cols])\n",
        " \n",
        "# fit the model\n",
        "result = logit.fit() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Logistic Regression in Practice\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "result.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice that all of our independent variables are considered significant (no\n",
        "confidence intervals contain zero).\n",
        "\n",
        "## Using the Model\n",
        "\n",
        "Note that by fitting a model to the data, we can make predictions for inputs that\n",
        "were not in the training data.  \n",
        "\n",
        "Furthermore, we can make a prediction of a probability for cases where we don't\n",
        "have enough data to estimate the probability directly -- e.g., for specific\n",
        "parameter values.\n",
        "\n",
        "Let's see how well the model fits the data.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Using the Model\n",
        ":::\n",
        "\n",
        "We have three independent variables, so in each case we'll use average values\n",
        "for the two that we aren't evaluating.\n",
        "\n",
        "GPA (GRE = 600, Rank = 2.5):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "bins = np.linspace(df.gpa.min(), df.gpa.max(), 10)\n",
        "groups = df.groupby(np.digitize(df.gpa, bins))\n",
        "prob = [result.predict([600, b, 2.5, 1.0]) for b in bins]\n",
        "ax = plt.figure(figsize = (7, 4)).add_subplot()\n",
        "ax.plot(bins, prob)\n",
        "ax.plot(bins,groups.admit.mean(),'o')\n",
        "ax.tick_params(labelsize=12)\n",
        "ax.set_xlabel('gpa', fontsize = 14)\n",
        "ax.set_ylabel('P[admit]', fontsize = 14)\n",
        "ax.set_title('Marginal Effect of GPA', fontsize = 16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Logistic Regression in Practice\n",
        ":::\n",
        "\n",
        "GRE Score (GPA = 3.4, Rank = 2.5):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "prob = [result.predict([b, 3.4, 2.5, 1.0]) for b in sorted(df.gre.unique())]\n",
        "ax = plt.figure(figsize = (7, 4)).add_subplot()\n",
        "ax.plot(sorted(df.gre.unique()), prob)\n",
        "ax.plot(df.groupby('gre').mean()['admit'],'o')\n",
        "ax.tick_params(labelsize=12)\n",
        "ax.set_xlabel('gre', fontsize = 14)\n",
        "ax.set_ylabel('P[admit]', fontsize = 14)\n",
        "ax.set_title('Marginal Effect of GRE', fontsize = 16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Logistic Regression in Practice\n",
        ":::\n",
        "\n",
        "Institution Rank (GRE = 600, GPA = 3.4):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "prob = [result.predict([600, 3.4, b, 1.0]) for b in range(1,5)]\n",
        "ax = plt.figure(figsize = (7, 4)).add_subplot()\n",
        "ax.plot(range(1,5), prob)\n",
        "ax.plot(df.groupby('rank').mean()['admit'],'o')\n",
        "ax.tick_params(labelsize=12)\n",
        "ax.set_xlabel('Rank', fontsize = 14)\n",
        "ax.set_xlim([0.5,4.5])\n",
        "ax.set_ylabel('P[admit]', fontsize = 14)\n",
        "ax.set_title('Marginal Effect of Rank', fontsize = 16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression in Perspective\n",
        "\n",
        "At the start of lecture we emphasized that logistic regression is concerned with\n",
        "estimating a __probability__ model for __discrete__ (0/1) data. \n",
        "\n",
        "However, it may well be the case that we want to do something with the\n",
        "probability that amounts to __classification.__\n",
        "\n",
        "For example, we may classify data items using a rule such as \"Assign item $x_i$\n",
        "to Class 1 if $p(x_i) > 0.5$\".\n",
        "\n",
        "For this reason, logistic regression could be considered a classification method.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Logistic Regression in Perspective\n",
        ":::\n",
        "\n",
        "Let's use our logistic regression as a classifier.\n",
        "\n",
        "We want to ask whether we can correctly predict whether a student gets admitted\n",
        "to graduate school.\n",
        "\n",
        "Let's separate our training and test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "        df[train_cols], df['admit'],\n",
        "        test_size=0.4, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Logistic Regression in Perspective\n",
        ":::\n",
        "\n",
        "Now, there are some standard metrics used when evaluating a binary classifier.\n",
        "\n",
        "Let's say our classifier is outputting \"yes\" when it thinks the student will be admitted.\n",
        "\n",
        "There are four cases:\n",
        "\n",
        "* Classifier says \"yes\", and student __is__ admitted:  __True Positive.__\n",
        "* Classifier says \"yes\", and student __is not__ admitted:  __False Positive.__\n",
        "* Classifier says \"no\", and student __is__ admitted:  __False Negative.__\n",
        "* Classifier says \"no\", and student __is not__ admitted:  __True Negative.__\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Logistic Regression in Perspective\n",
        ":::\n",
        "\n",
        "__Precision__ is the fraction of \"yes\" classifications that are correct:\n",
        "\n",
        "$$\n",
        "\\mbox{Precision} = \\frac{\\mbox{True Positives}}{\\mbox{True Positives + False Positives}}.\n",
        "$$\n",
        "    \n",
        "__Recall__ is the fraction of admits that we say \"yes\" to:\n",
        "\n",
        "$$\n",
        "\\mbox{Recall} = \\frac{\\mbox{True Positives}}{\\mbox{True Positives + False Negatives}}.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "def evaluate(y_train, X_train, y_test, X_test, threshold):\n",
        "\n",
        "    # learn model on training data\n",
        "    logit = sm.Logit(y_train, X_train)\n",
        "    result = logit.fit(disp=False)\n",
        "    \n",
        "    # make probability predictions on test data\n",
        "    y_pred = result.predict(X_test)\n",
        "    \n",
        "    # threshold probabilities to create classifications\n",
        "    y_pred = y_pred > threshold\n",
        "    \n",
        "    # report metrics\n",
        "    precision = metrics.precision_score(y_test, y_pred)\n",
        "    recall = metrics.recall_score(y_test, y_pred)\n",
        "    return precision, recall\n",
        "\n",
        "precision, recall = evaluate(y_train, X_train, y_test, X_test, 0.5)\n",
        "\n",
        "print(f'Precision: {precision:0.3f}, Recall: {recall:0.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Logistic Regression in Perspective\n",
        ":::\n",
        "\n",
        "Now, let's get a sense of average accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "PR = []\n",
        "for i in range(20):\n",
        "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "            df[train_cols], df['admit'],\n",
        "            test_size=0.4)\n",
        "    PR.append(evaluate(y_train, X_train, y_test, X_test, 0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "avgPrec = np.mean([f[0] for f in PR])\n",
        "avgRec = np.mean([f[1] for f in PR])\n",
        "print(f'Average Precision: {avgPrec:0.3f}, Average Recall: {avgRec:0.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Logistic Regression in Perspective\n",
        ":::\n",
        "\n",
        "Sometimes we would like a single value that describes the overall performance of\n",
        "the classifier.\n",
        "\n",
        "For this, we take the harmonic mean of precision and recall, called __F1 Score__:\n",
        "\n",
        "$$\n",
        "\\mbox{F1 Score} = 2 \\;\\;\\frac{\\mbox{Precision} \\cdot \\mbox{Recall}}{\\mbox{Precision} + \\mbox{Recall}}.\n",
        "$$\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Logistic Regression in Perspective\n",
        ":::\n",
        "\n",
        "Using this, we can evaluate other settings for the threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "def evalThresh(df, thresh):\n",
        "    PR = []\n",
        "    for i in range(20):\n",
        "        X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
        "                df[train_cols], df['admit'],\n",
        "                test_size=0.4)\n",
        "        PR.append(evaluate(y_train, X_train, y_test, X_test, thresh))\n",
        "    avgPrec = np.mean([f[0] for f in PR])\n",
        "    avgRec = np.mean([f[1] for f in PR])\n",
        "    return 2 * (avgPrec * avgRec) / (avgPrec + avgRec), avgPrec, avgRec\n",
        "\n",
        "tvals = np.linspace(0.05, 0.8, 50)\n",
        "f1vals = [evalThresh(df, tval)[0] for tval in tvals]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(tvals,f1vals)\n",
        "plt.ylabel('F1 Score')\n",
        "plt.xlabel('Threshold for Classification')\n",
        "plt.title('F1 as a function of Threshold');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on this plot, we can say that the best classification threshold appears to\n",
        "be around 0.3, where precision and recall are:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "F1, Prec, Rec = evalThresh(df, 0.3)\n",
        "print('Best Precision: {:0.3f}, Best Recall: {:0.3f}'.format(Prec, Rec))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"web\"}\n",
        "\n",
        "The example here is based on\n",
        "http://blog.yhathq.com/posts/logistic-regression-and-python.html\n",
        "where you can find additional details.\n",
        "\n",
        ":::\n",
        "\n",
        "## Recap\n",
        "\n",
        "* Logistic regression is used to predict a probability.\n",
        "* It is a linear model for the log-odds.\n",
        "* It is fit by maximum likelihood.\n",
        "* It can be evaluated as a classifier."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}