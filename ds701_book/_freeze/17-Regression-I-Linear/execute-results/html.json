{
  "hash": "cd8ab609ed3a87d92bed2b81a9b7c40b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Linear Regression\njupyter: python3\n---\n\n# Introduction\n\n## Introduction\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/17-Regression-I-Linear.ipynb)\n\n\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n![](figs/Sir_Francis_Galton_by_Charles_Wellington_Furse.jpg){width=70%}\n\n[Sir Francis Galton by Charles Wellington Furse](https://commons.wikimedia.org/wiki/File:Sir_Francis_Galton_by_Charles_Wellington_Furse.jpg#/media/File:Sir_Francis_Galton_by_Charles_Wellington_Furse.jpg)\n\n<!--\n[National Portrait Gallery](https://en.wikipedia.org/wiki/National_Portrait_Gallery,_London)\n[NPG 3916](http://www.npg.org.uk/collections/search/portrait.php?search=ap&amp;npgno=3916&amp;eDate=&amp;lDate=)\n-->\n\n:::\n::: {.column width=\"60%\"}\n\n![](figs/galton-title.png){width=70%}\n\nIn 1886 Francis Galton published his observations about the height of children\ncompared to their parents.\n\n:::\n::::\n\n## Regression to the Mean\n\n:::: {.columns}\n::: {.column width=\"%\"}\n\n\n![](figs/galton-regression.png){width=100%}\n\n\n:::\n::: {.column width=\"%\"}\n\n\nDefined mid-parent as the average of the heights of the parents.\n\n:::: {.fragment}\nWhen parents are taller or shorter than average, their children are more likely\nto be closer to the average height than their parents. \n::::\n\n:::: {.fragment}\nSo called **\"regression to the mean.\"**\n\nGalton fit a straight line to this effect, and the fitting of lines or curves to\ndata has come to be called regression as well.\n::::\n\n:::\n::::\n\n## Praise versus Punishment\n\n:::: {.columns}\n::: {.column width=\"30%\"}\n\n\n::: {layout-ncol=\"1\"}\n\n![](figs/L17-Israeli-Airforce-Pirate.png){width=\"100px\"}\n\n<!-- From https://airpowerasia.com/2020/08/30/israels-giora-hawkeye-epstein-ace-of-aces-of-supersonic-fighter-jets-17-aerial-victories/ -->\n\n![Daniel Kahneman](figs/L17-Daniel-Kahneman--NYT.png){height=\"200px\"}\n\n:::\n\n\n:::\n::: {.column width=\"70%\"}\n\n:::: {.fragment}\nIn the 60s, psychologist Daniel Kahneman studied the effect of praise versus\npunishment on performance of Israeli Air Force trainees. He was led to believe\nthat praise was more effective than punishment.\n::::\n\n:::: {.fragment}\nAir force trainers disagreed and said that when they praised pilots that did well,\nthey usually did worse the next time, and when they punished pilots that did poorly,\nthey usually did better the next time.\n::::\n\n:::: {.fragment}\nWhat Khaneman later realised was that this phenomenon could be explained by\nthe idea of regression to the mean.\n\nIf a pilot does worse than average on a task, they are more likely to do better\nthe next time, and if they do better than average, they are more likely to do\nworse the next time.\n::::\n\n:::\n::::\n\n\n## Firing Coaches\n\n![NFL Coaches](figs/L17-nfl-coaching-candidates_0jpg.jpg){width=\"300px\"}\n\n[Reference](https://faculty.mccombs.utexas.edu/carlos.carvalho/teaching/regression_to_the_mean.pdf)\n\nIs it a good strategy to fire a coach after a bad season?\n\n> bad season -> fire and replace coach -> better season next year\n\n:::: {.fragment}\nWhen this could be interpreted as:\n\n> bad season (outlier) -> regression to the mean -> better season next year\n::::\n\n## This applies to many situations\n\n> \"I had so many colds last year, and then I started herbal remedies, and I have much\n> fewer colds this year.\"\n\n> \"My investments were doing quite bad last year, so switched investment managers, and\n> now it is doing much better.\"\n\n\n:::: {.fragment}\n<br>\nWhen in fact, they could be (at least partially) explained by: \n\n::: {.text-center}\n**Regression to the mean**\n:::\n\n::::\n\n## Model Types\n\nThe most common form of machine learning is __regression__, which means\nconstructing an equation that describes the relationships among variables.\n\nIt is a form of supervised learning: \n\n* whereas __classification__ deals with predicting categorical features (labels or classes), \n* __regression__ deals with predicting continuous features (real values).\n\n## Fit to Linear Models\n\nFor example, we may look at these points and decide to model them using a line.\n\n::: {#28665244 .cell execution_count=3}\n``` {.python .cell-code}\nax = plt.figure(figsize = (5, 5)).add_subplot()\ncenterAxes(ax)\nline = np.array([1, 0.5])\nxlin = -10.0 + 20.0 * np.random.random(100)\nylin = line[0] + (line[1] * xlin) + np.random.randn(100)\nax.plot(xlin, ylin, 'ro', markersize = 4)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](17-Regression-I-Linear_files/figure-revealjs/cell-3-output-1.png){width=397 height=396 fig-align='center'}\n:::\n:::\n\n\n## Fit to Quadratic Models\n\nWe may look at these points and decide to model them using a quadratic function.\n\n::: {#568b3256 .cell execution_count=4}\n``` {.python .cell-code}\nax = plt.figure(figsize = (5, 5)).add_subplot()\ncenterAxes(ax)\nquad = np.array([1, 3, 0.5])\nstd = 8.0\nxquad = -10.0 + 20.0 * np.random.random(100)\nyquad = quad[0] + (quad[1] * xquad) + (quad[2] * xquad * xquad) + std * np.random.randn(100)\nax.plot(xquad, yquad, 'ro', markersize = 4)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](17-Regression-I-Linear_files/figure-revealjs/cell-4-output-1.png){width=397 height=389 fig-align='center'}\n:::\n:::\n\n\n## Fit to Logarithmic Models\n\nAnd we may look at these points and decide to model them using a logarithmic function.\n\n::: {#a9d959f3 .cell execution_count=5}\n``` {.python .cell-code}\nax = plt.figure(figsize = (5, 5)).add_subplot()\ncenterAxes(ax)\nlog = np.array([1, 4])  \nstd = 1.5\nxlog = 10.0 * np.random.random(100)\nylog = log[0] + log[1] * np.log(xlog) + std * np.random.randn(100)\nax.plot(xlog, ylog, 'ro', markersize=4)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](17-Regression-I-Linear_files/figure-revealjs/cell-5-output-1.png){width=416 height=389 fig-align='center'}\n:::\n:::\n\n\n## Approach\n\nClearly, none of these datasets agrees perfectly with the proposed model.   \n\nSo the question arises:\n\nHow do we find the __best__ linear function (or quadratic function, or logarithmic function) given the data?\n\n# Framework\n\n## Framework\n\nThis problem has been studied extensively in the field of statistics.   \n\nCertain terminology is used:\n\n* Some values are referred to as \"independent,\" and\n* Some values are referred to as \"dependent.\"\n\n::: {.content-visible when-profile=\"slides\"}\n## Framework cont.\n:::\n\nThe basic regression task is:\n\n* given a set of independent variables \n* and the associated dependent variables, \n* estimate the parameters of a model (such as a line, parabola, etc) that describes\n  how the dependent variables are related to the independent variables.\n\n::: {.content-visible when-profile=\"slides\"}\n## Framework cont.\n:::\n\n::: {.columns}\n::: {.column width=\"50%\"}\n\n<br>\n\nThe **independent variables** are collected into a matrix $X,$ sometimes called the\n__design matrix.__\n\nThe dependent variables are collected into an __observation__ vector $\\mathbf{y}.$\n\nThe parameters of the model (for any kind of model) are collected into a \n__parameter__ vector $\\mathbf{\\beta}.$\n:::\n::: {.column width=\"50%\"}\n\n::: {#805ebc07 .cell execution_count=6}\n``` {.python .cell-code}\nax = plt.figure(figsize = (5, 5)).add_subplot()\ncenterAxes(ax)\nline = np.array([1, 0.5])\nxlin = -10.0 + 20.0 * np.random.random(100)\nylin = line[0] + (line[1] * xlin) + np.random.randn(100)\nax.plot(xlin, ylin, 'ro', markersize = 4)\nax.plot(xlin, line[0] + line[1] * xlin, 'b-')\nplt.text(-9, 3, r'$y = \\beta_0 + \\beta_1x$', size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](17-Regression-I-Linear_files/figure-revealjs/cell-6-output-1.png){width=401 height=389 fig-align='center'}\n:::\n:::\n\n\n:::\n::: \n\n## Least-Squares Lines\n\n::: {.incremental}\n* The first kind of model we'll study is a linear equation, $y = \\beta_0 + \\beta_1 x.$\n\n* Experimental data often produce points $(x_1, y_1), \\dots, (x_n, y_n)$ that seem\nto lie close to a line.   \n\n* We want to determine the parameters $\\beta_0, \\beta_1$ that define a line that is\nas \"close\" to the points as possible.\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Least-Squares Lines cont.\n:::\n\nSuppose we have a line $y = \\beta_0 + \\beta_1 x$.   \n\nFor each data point $(x_j, y_j),$ there is a point $(x_j, \\beta_0 + \\beta_1 x_j)$\nthat is the point on the line with the same $x$-coordinate.\n\n::: {#fcaf27ca .cell execution_count=7}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate some data points\nx = np.array([1, 2, 3, 4, 5])\ny = np.array([3.7, 2.0, 2.1, 0.1, 1.5])\n\n# Linear regression parameters (intercept and slope)\nbeta_0 = 4  # intercept\nbeta_1 = -0.8  # slope\n\n# Regression line (y = beta_0 + beta_1 * x)\ny_line = beta_0 + beta_1 * x\n\n# Create the plot\nfig, ax = plt.subplots()\nfig.set_size_inches(7, 4)\n\n# Plot the data points\nax.scatter(x, y, color='blue', label='Data points', zorder=5)\nax.scatter(x, y_line, color='red', label='Predicted values', zorder=5)\n\n# Plot the regression line\nax.plot(x, y_line, color='cyan', label='Regression line (y = β0 + β1x)', zorder=4)\n\n# Add residuals (vertical lines from points to regression line)\nfor i in range(len(x)):\n    ax.vlines(x[i], y_line[i], y[i], color='blue', linestyles='dashed', label='Residual' if i == 0 else \"\", zorder=2)\n\ny_offset = [0.2, -0.2, 0.2, -0.2, 0.2]\ny_line_offset = [-0.3, 0.3, -0.3, 0.3, -0.2]\n\n# Annotate points\nfor i in range(len(x)):\n    ax.text(x[i], y[i] + y_offset[i], f'({x[i]}, {y[i]})', fontsize=9, ha='center')\n    ax.text(x[i], y_line[i] + y_line_offset[i], f'({x[i]}, {y_line[i]:.1f})', fontsize=9, ha='center')\n\n# Remove the box around the plot and show only x and y axis with no tics and numbers\nax.spines['top'].set_color('none')\nax.spines['right'].set_color('none')\n#ax.spines['left'].set_position('zero')\n#ax.spines['bottom'].set_position('zero')\nax.xaxis.set_ticks([])\nax.yaxis.set_ticks([])\n\n# Title\nax.set_title('Linear Regression with Residuals')\n\n# Add legend\nax.legend()\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](17-Regression-I-Linear_files/figure-revealjs/cell-7-output-1.png){width=540 height=338 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Least-Squares Lines cont.\n:::\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {#8053f466 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display}\n![](17-Regression-I-Linear_files/figure-revealjs/cell-8-output-1.png){width=334 height=255}\n:::\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\n\nWe call \n\n* $y_j$ the __observed__ value of $y$ and \n* $\\beta_0 + \\beta_1 x_j$ the __predicted__ $y$-value.   \n\n:::\n::::\n\n:::: {.fragment}\nThe difference between an observed $y$-value and a predicted $y$-value is called\na __residual__.\n::::\n\n:::: {.fragment}\nThere are several ways to measure how \"close\" the line is to the data. \n::::\n\n:::: {.fragment}\nThe usual choice is to sum the squares of the residuals.  \n::::\n\n:::: {.fragment}\nThe __least-squares line__ is the line $y = \\beta_0 + \\beta_1x$ that minimizes\nthe sum of squares of the residuals. \n\nThe coefficients $\\beta_0, \\beta_1$ of the line are called\n__regression coefficients.__\n::::\n\n## A least-squares problem\n\nIf the data points were on the line, the parameters $\\beta_0$ and $\\beta_1$ would satisfy the equations\n\n$$\n\\begin{align*}\n\\beta_0 + \\beta_1 x_1 &= y_1, \\\\\n\\beta_0 + \\beta_1 x_2 &= y_2, \\\\\n\\beta_0 + \\beta_1 x_3 &= y_3, \\\\\n&\\vdots \\\\\n\\beta_0 + \\beta_1 x_n &= y_n.\n\\end{align*}\n$$\n\n--- \n\nWe can write this system as \n\n$$\nX\\mathbf{\\beta} = \\mathbf{y},\n$$\n\nwhere \n\n$$\nX=\n\\begin{bmatrix}\n1 & x_1 \\\\\n1 & x_2 \\\\\n\\vdots & \\vdots \\\\\n1 & x_n\n\\end{bmatrix},\n\\;\\;\\mathbf{\\beta} = \\begin{bmatrix}\\beta_0\\\\\\beta_1\\end{bmatrix},\n\\;\\;\\mathbf{y}=\\begin{bmatrix}y_1\\\\y_2\\\\\\vdots\\\\y_n\\end{bmatrix}.\n$$\n\n---\n\nOf course, if the data points don't actually lie exactly on a line, \n\n... then there are no parameters $\\beta_0, \\beta_1$ for which the predicted\n$y$-values in $X\\mathbf{\\beta}$ equal the observed $y$-values in $\\mathbf{y}$, \n\n... and $X\\mathbf{\\beta}=\\mathbf{y}$ has no solution.\n\n---\n\nNow, since the data doesn't fall exactly on a line, we have decided to seek the\n$\\beta$ that minimizes the sum of squared residuals, i.e.,\n\n$$\n\\sum_i (\\beta_0 + \\beta_1 x_i - y_i)^2\n=\\Vert X\\beta -\\mathbf{y}\\Vert^2.\n$$\n\nThis is key: \n\n::: {.callout-important}\n__The sum of squares of the residuals__ is __exactly__ the \n__square of the distance between the vectors $X\\mathbf{\\beta}$ and $\\mathbf{y}.$__\n:::\n\n---\n\nComputing the least-squares solution of $X\\beta = \\mathbf{y}$ is equivalent to\nfinding the $\\mathbf{\\beta}$ that determines the least-squares line.\n\n::: {#6d4bdc9b .cell execution_count=9}\n``` {.python .cell-code}\nax = plt.figure(figsize = (5, 5)).add_subplot()\ncenterAxes(ax)\nline = np.array([1, 0.5])\nxlin = -10.0 + 20.0 * np.random.random(100)\nylin = line[0] + (line[1] * xlin) + np.random.randn(100)\nax.plot(xlin, ylin, 'ro', markersize = 4)\nax.plot(xlin, line[0] + line[1] * xlin, 'b-')\nplt.text(-9, 3, r'$y = \\beta_0 + \\beta_1x$', size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](17-Regression-I-Linear_files/figure-revealjs/cell-9-output-1.png){width=395 height=389 fig-align='center'}\n:::\n:::\n\n\n---\n\nNow, to obtain the least-squares line, find the least-squares solution to\n$X\\mathbf{\\beta} = \\mathbf{y}.$\n\nFrom linear algebra we know that the least squares solution of\n$X\\mathbf{\\beta} = \\mathbf{y}$ is given by the solution of the __normal equations__:\n\n$$\nX^TX\\mathbf{\\beta} = X^T\\mathbf{y},\n$$\n\nbecause $X\\mathbf{\\beta} - \\mathbf{y}$ is normal (e.g. orthogonal) to the column\nspace of $X.$    \n\n:::: {.fragment}\nWe also know that the normal equations __always__ have at least one solution.\n\nAnd if $X^TX$ is invertible, there is a unique solution that is given by:\n    \n$$\n\\hat{\\mathbf{\\beta}} = (X^TX)^{-1} X^T\\mathbf{y}.\n$$\n\n::::\n\n:::: {.fragment}\nFor $X^T X$ to be invertible, where $X$ is an $n \\times p$ matrix (n observations, p features):\n\n1. $X$ must have full column rank - i.e., $\\text{rank}(X) = p$\n2. We need $n \\geq p$ - at least as many observations as features\n3. The columns of $X$ must be linearly independent - no column can be written as a linear combination of the others\n\n::::\n\n# The General Linear Model\n\n## The General Linear Model\n\nAnother way that the inconsistent (.i.e. no exact solution) linear system is often written is to collect\nall the residuals into a __residual vector.__ \n\nThen an exact equation is\n\n$$\ny = X\\mathbf{\\beta} + {\\mathbf\\epsilon},\n$$\n\nwhere $\\mathbf{\\epsilon}$ is the __residual vector.__\n\nAny equation of this form is referred to as a __linear model.__ \n\nIn this formulation, the goal is to find the $\\beta$ so as to minimize the\n__norm__ of $\\epsilon$, i.e., $\\Vert\\epsilon\\Vert.$\n\n:::: {.fragment}\nIn some cases, one would like to fit data points with something other than a\nstraight line.  \n\nIn cases like this, the matrix equation is still $X\\mathbf{\\beta} = \\mathbf{y}$,\nbut the specific form of $X$ changes from one problem to the next.\n::::\n\n## Least-Squares Fitting of Other Models\n\nIn model fitting, the parameters of the model are what is unknown.  \n\nA central question for us is whether the model is _linear_ in its parameters.\n\nFor example, is this model linear in its parameters?\n\n$$\ny = \\beta_0 e^{-\\beta_1 x}\n$$\n\n:::: {.fragment}\nIt __is not__ linear in its parameters.  \n::::\n\n:::: {.fragment}\nIs this model linear in its parameters?\n\n$$\ny = \\beta_0 e^{-2 x}\n$$\n::::\n\n:::: {.fragment}\nIt __is__ linear in its parameters.\n::::\n\n---\n\nFor a model that is linear in its parameters, an observation is a linear\ncombination of (arbitrary) known functions.\n\nIn other words, a model that is linear in its parameters is\n\n$$\ny = \\beta_0f_0(x) + \\beta_1f_1(x) + \\dots + \\beta_nf_n(x)\n$$\n\nwhere $f_0, \\dots, f_n$ are known functions and $\\beta_0,\\dots,\\beta_n$ are\nparameters.\n\n## Example\n\nSuppose data points $(x_1, y_1), \\dots, (x_n, y_n)$ appear to lie along some\nsort of parabola instead of a straight line.  \n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {#d1047daf .cell execution_count=10}\n``` {.python .cell-code}\nax = plt.figure(figsize = (5, 5)).add_subplot()\ncenterAxes(ax)\nquad = np.array([1, 3, 0.5])\nxquad = -10.0 + 20.0 * np.random.random(100)\nyquad = quad[0] + (quad[1] * xquad) + (quad[2] * xquad * xquad) + 2*np.random.randn(100)\nax.plot(xquad, yquad, 'ro', markersize = 4)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](17-Regression-I-Linear_files/figure-revealjs/cell-10-output-1.png){width=397 height=389}\n:::\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\nAs a result, we wish to approximate the data by an equation of the form\n\n$$\ny = \\beta_0 + \\beta_1x + \\beta_2x^2.\n$$\n\nLet's describe the linear model that produces a \"least squares fit\" of the data by the equation.\n\n:::\n::::\n\n## Solution\n  \nThe ideal relationship is $y = \\beta_0 + \\beta_1x + \\beta_2x^2.$\n\nSuppose the actual values of the parameters are $\\beta_0, \\beta_1, \\beta_2.$  \nThen the coordinates of the first data point satisfy the equation\n\n$$\ny_1 = \\beta_0 + \\beta_1x_1 + \\beta_2x_1^2 + \\epsilon_1,\n$$\n\nwhere $\\epsilon_1$ is the residual error between the observed value $y_1$ and\nthe predicted $y$-value.\n\nEach data point determines a similar equation:\n\n$$\n\\begin{align*}\ny_1 &= \\beta_0 + \\beta_1x_1 + \\beta_2x_1^2 + \\epsilon_1, \\\\\ny_2 &= \\beta_0 + \\beta_1x_2 + \\beta_2x_2^2 + \\epsilon_2, \\\\\n&\\vdots \\\\\ny_n &= \\beta_0 + \\beta_1x_n + \\beta_2x_n^2 + \\epsilon_n.\n\\end{align*}\n$$\n\n---\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\nClearly, this system can be written as $\\mathbf{y} = X\\mathbf{\\beta} + \\mathbf{\\epsilon}.$\n\n$$\n\\begin{bmatrix}\ny_1\\\\\ny_2\\\\\n\\vdots\\\\\ny_n\n\\end{bmatrix} = \n\\begin{bmatrix}1&x_1&x_1^2\\\\\n1&x_2&x_2^2\\\\\n\\vdots&\\vdots&\\vdots\\\\\n1&x_n&x_n^2\n\\end{bmatrix} \n\\begin{bmatrix}\n\\beta_0\\\\\\\n\\beta_1\\\\\\\n\\beta_2\n\\end{bmatrix}\n + \n \\begin{bmatrix}\n \\epsilon_1\\\\\n \\epsilon_2\\\\\n \\vdots\\\\\n \\epsilon_n\n \\end{bmatrix}\n $$\n\nLet's solve for $\\beta.$\n\n::: {#6d88e4f6 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"false\"}\nm = np.shape(xquad)[0]\nX = np.array([np.ones(m), xquad, xquad**2]).T\n\n# Solve for beta\nbeta = np.linalg.inv(X.T @ X) @ X.T @ yquad\n```\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\nAnd plot the results.\n\n::: {#560b61e5 .cell execution_count=12}\n``` {.python .cell-code}\nax = ut.plotSetup(-10, 10, -10, 20)\nut.centerAxes(ax)\nxplot = np.linspace(-10, 10, 50)\nyestplot = beta[0] + beta[1] * xplot + beta[2] * xplot**2\nax.plot(xplot, yestplot, 'b-', lw=2)\nax.plot(xquad, yquad, 'ro', markersize=4)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](17-Regression-I-Linear_files/figure-revealjs/cell-12-output-1.png){width=501 height=328}\n:::\n:::\n\n\n:::\n::::\n\n\n---\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\nNow let's try a different model.\n\n$$\n\\begin{bmatrix}\ny_1\\\\\ny_2\\\\\n\\vdots\\\\\ny_n\n\\end{bmatrix} = \n\\begin{bmatrix}1&log(x_1)\\\\\n1&log(x_2)\\\\\n\\vdots&\\vdots\\\\\n1&log(x_n)\n\\end{bmatrix} \n\\begin{bmatrix}\n\\beta_0\\\\\\\n\\beta_1\n\\end{bmatrix}\n+ \n\\begin{bmatrix}\n\\epsilon_1\\\\\n\\epsilon_2\\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n $$\n\n::: {#3acd3ee2 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"false\"}\nm = np.shape(xlog)[0]\nX = np.array([np.ones(m), np.log(xlog)]).T\n\n# Solve for beta\nbeta = np.linalg.inv(X.T @ X) @ X.T @ ylog\n```\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\nAnd plot the results.\n\n::: {#6ea1e445 .cell execution_count=14}\n``` {.python .cell-code}\n# \n# plot the results\n#\nax = ut.plotSetup(-10,10,-10,15)\nut.centerAxes(ax)\nxplot = np.logspace(np.log10(0.0001),1,100)\nyestplot = beta[0]+beta[1]*np.log(xplot)\nax.plot(xplot,yestplot,'b-',lw=2)\nax.plot(xlog,ylog,'ro',markersize=4)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](17-Regression-I-Linear_files/figure-revealjs/cell-14-output-1.png){width=501 height=328}\n:::\n:::\n\n\n:::\n::::\n\n## Multiple Regression\n\nSuppose an experiment involves two independent variables -- say, $u$ and $v$, --\nand one dependent variable, $y$.  \n\nA simple equation for predicting $y$ from $u$ and $v$ has the form\n\n$$\ny = \\beta_0 + \\beta_1 u + \\beta_2 v.\n$$\n\nSince there is more than one independent variable, this is called\n__multiple regression.__\n\n::: {.content-visible when-profile=\"slides\"}\n## Multiple Regression, cont.\n:::\n\nA more general prediction equation might have the form\n\n$$\ny = \\beta_0 + \\beta_1 u + \\beta_2 v + \\beta_3u^2 + \\beta_4 uv + \\beta_5 v^2.\n$$\n\nA least squares fit to equations like this is called a __trend surface.__\n\nIn general, a linear model will arise whenever $y$ is to be predicted by an equation of the form\n\n$$\ny = \\beta_0f_0(u,v) + \\beta_1f_1(u,v) + \\cdots + \\beta_nf_n(u,v),\n$$\n\nwith $f_0,\\dots,f_n$ any sort of known functions and $\\beta_0,...,\\beta_n$ unknown weights.\n\n## Example\n\nLet's take an example. Here are a set of points in $\\mathbb{R}^3$:\n\n::: {#0180e5e5 .cell execution_count=15}\n``` {.python .cell-code}\nax = ut.plotSetup3d(-7, 7, -7, 7, -10, 10, figsize = (5, 5))\nv = [4.0, 4.0, 2.0]\nu = [-4.0, 3.0, 1.0]\nnpts = 50\n\n# set locations of points that fall within x,y\nxc = -7.0 + 14.0 * np.random.random(npts)\nyc = -7.0 + 14.0 * np.random.random(npts)\nA = np.array([u,v]).T\n\n# project these points onto the plane\nP = A @ np.linalg.inv(A.T @ A) @ A.T\ncoords = P @ np.array([xc, yc, np.zeros(npts)])\ncoords[2] += np.random.randn(npts)\nax.plot(coords[0], coords[1], 'ro', zs=coords[2], markersize = 6)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](17-Regression-I-Linear_files/figure-revealjs/cell-15-output-1.png){width=412 height=396 fig-align='center'}\n:::\n:::\n\n\n## Example\n\nIn geography, local models of terrain are constructed from data\n\n$$\n(u_1, v_1, z_1), \\dots, (u_n, v_n, z_n),\n$$\n\nwhere $u_j, v_j$, and $z_j$ are latitude, longitude, and altitude, respectively.\n\n:::: {.fragment}\nLet's describe the linear models that gives a least-squares fit to such data.  \nThe solution is called the least-squares _plane._\n::::\n\n## Solution\n\nWe expect the data to satisfy these equations:\n\n$$\n\\begin{align*}\ny_1 &= \\beta_0 + \\beta_1 u_1 + \\beta_2 v_1 + \\beta_3 z_1 + \\epsilon_1, \\\\\ny_2 &= \\beta_0 + \\beta_1 u_2 + \\beta_2 v_2 + \\beta_3 z_2 + \\epsilon_2, \\\\\n\\vdots \\\\\ny_n &= \\beta_0 + \\beta_1 u_n + \\beta_2 v_n + \\beta_3 z_n + \\epsilon_n.\n\\end{align*}\n$$\n\nThis system has the matrix for $\\mathbf{y} = X\\mathbf{\\beta} + \\epsilon,$ where\n\n$$\n\\mathbf{y} = \n\\begin{bmatrix}y_1\\\\\ny_1\\\\\n\\vdots\\\\\ny_n\n\\end{bmatrix},\\;\\;\nX = \n\\begin{bmatrix}1&u_1&v_1&z_1\\\\\n1&u_2&v_2&z_2\\\\\n\\vdots&\\vdots&\\vdots&\\vdots\\\\\n1&u_n&v_n&z_n\n\\end{bmatrix},\\;\\;\n\\mathbf{\\beta}=\n\\begin{bmatrix}\n\\beta_0\\\\\n\\beta_1\\\\\n\\beta_2\\\\\n\\beta_3\n\\end{bmatrix},\\;\\;\n\\epsilon = \n\\begin{bmatrix}\n\\epsilon_1\\\\\n\\epsilon_2\\\\\n\\vdots\\\\ \n\\epsilon_n\n\\end{bmatrix}.\n$$\n\n---\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {#79b4ff42 .cell execution_count=16}\n``` {.python .cell-code}\nax = ut.plotSetup3d(-7, 7, -7, 7, -10, 10, figsize = (7, 7))\nv = [4.0, 4.0, 2.0]\nu = [-4.0, 3.0, 1.0]\n\n# plotting the span of v\nut.plotSpan3d(ax,u,v,'Green')\nnpts = 50\nax.plot(coords[0], coords[1], 'ro', zs = coords[2], markersize=6)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](17-Regression-I-Linear_files/figure-revealjs/cell-16-output-1.png){width=553 height=537}\n:::\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\nThis example shows that the linear model for multiple regression has the same\nform as the model for the simple regression in the earlier examples.\n\nWe can see that the general principle is the same across all the different\nkinds of linear models.\n\n:::: {.fragment}\nOnce $X$ is defined properly, the normal equations for $\\mathbf{\\beta}$ have the\nsame matrix form, no matter how many variables are involved.\n\nThus, for any linear model where $X^TX$ is invertible, the least squares estimate is\n\n$$\n\\hat{\\mathbf{\\beta}} = (X^TX)^{-1}X^T\\mathbf{y}.\n$$\n::::\n\n:::\n::::\n\n## Measuring the fit of a regression model: $R^2$\n\nGiven any $X$ and $\\mathbf{y}$, the above algorithm will produce an output\n$\\hat{\\beta}$.\n\nBut how do we know whether the data is in fact well described by the model?\n\n:::: {.fragment}\nThe most common measure of fit is $R^2$.\n\n$R^2$ measures the __fraction of the variance__ of $\\mathbf{y}$ that can be\nexplained by the model $X\\hat{\\beta}$.\n\nThe variance of $\\mathbf{y}$ is \n\n$$\n\\text{Var}(\\mathbf{y}) =\\frac{1}{n} \\sum_{i=1}^n \\left(y_i-\\overline{y}\\right)^2,\n$$\n\nwhere $\\overline{y}=\\frac{1}{n}\\sum_{i=1}^ny_i$.\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## Measuring the fit, cont.\n:::\n\nFor any given $n$, we can equally work with just \n$$\n\\sum_{i=1}^n \\left(y_i-\\overline{y}\\right)^2,\n$$\n\nwhich is called the __Total Sum of Squares__ (TSS).\n\n## Total Sum of Squares\n\nNow to measure the quality of fit of a model, we break TSS down into two components. \n\nFor any given $\\mathbf{x}_i$, the prediction made by the model is \n$\\hat{y_i} = \\mathbf{x}_i^T\\beta$.\n\nWe can break the total sum of squares into two parts:\n\n* the residual $\\epsilon$ is $y_i - \\hat{y_i}$, and \n* the part that the model \"explains\" is $\\hat{y_i} - \\overline{y}.$\n\n## RSS and ESS\n\nSe we define the Residual Sum of Squares (RSS) as:\n\n$$\n\\text{RSS} = \\sum_{i=1}^n \\left(y_i-\\hat{y_i}\\right)^2,\n$$\n\nand the Explained Sum of Squares (ESS) as:\n\n$$\n\\text{ESS} = \\sum_{i=1}^n \\left(\\hat{y_i}-\\overline{y}\\right)^2,\n$$\n\nOne can show that the total sum of squares is exactly equal to the sum of\nsquares of the residuals plus the sum of squares of the explained part.\n\nIn other words:\n\n$$\n\\text{TSS} = \\text{RSS} + \\text{ESS}.\n$$\n\n::: {.content-visible when-profile=\"web\"}\n\n\n## Proof that TSS = RSS + ESS\n\n**Given:**\n\n- TSS = $\\sum_{i=1}^n (y_i - \\bar{y})^2$ (Total Sum of Squares)\n- RSS = $\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$ (Residual Sum of Squares)\n- ESS = $\\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2$ (Explained Sum of Squares)\n\n**Proof:**\n\nStart by decomposing the deviation from the mean:\n$$y_i - \\bar{y} = (y_i - \\hat{y}_i) + (\\hat{y}_i - \\bar{y})$$\n\nSquare both sides:\n$$\\begin{align}\n(y_i - \\bar{y})^2 &= [(y_i - \\hat{y}_i) + (\\hat{y}_i - \\bar{y})]^2\\\\\n&= (y_i - \\hat{y}_i)^2 + 2(y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) + (\\hat{y}_i - \\bar{y})^2\n\\end{align}$$\n\nSum over all $i$:\n$$\\sum_{i=1}^n (y_i - \\bar{y})^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + 2\\sum_{i=1}^n (y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) + \\sum_{i=1}^n (\\hat{y}_i - \\bar{y})^2$$\n\nWhich gives us:\n$$\\text{TSS} = \\text{RSS} + 2\\sum_{i=1}^n (y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) + \\text{ESS}$$\n\n**The key step:** We need to show that the cross-term equals zero:\n$$\\sum_{i=1}^n (y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) = 0$$\n\n**Why is this zero?**\n\nFor OLS regression **with an intercept**, the residuals $(y_i - \\hat{y}_i)$ satisfy two orthogonality conditions:\n\n1. $\\sum_{i=1}^n (y_i - \\hat{y}_i) = 0$ (residuals sum to zero)\n2. $\\sum_{i=1}^n (y_i - \\hat{y}_i)\\hat{y}_i = 0$ (residuals are orthogonal to predictions)\n\nUsing condition (2):\n$$\\sum_{i=1}^n (y_i - \\hat{y}_i)(\\hat{y}_i - \\bar{y}) = \\sum_{i=1}^n (y_i - \\hat{y}_i)\\hat{y}_i - \\bar{y}\\sum_{i=1}^n (y_i - \\hat{y}_i) = 0 - \\bar{y} \\cdot 0 = 0$$\n\nTherefore: **TSS = RSS + ESS** ✓\n\n**Geometric Intuition:**\n\nThis decomposition is a Pythagorean theorem in high-dimensional space! The vectors $(y_i - \\bar{y})$, $(y_i - \\hat{y}_i)$, and $(\\hat{y}_i - \\bar{y})$ form a right triangle because the residuals are orthogonal to the fitted values.\n\n\n\n#### Why residuals sum to exactly zero (with intercept)\n\nCondition 1 is satisfied **exactly** (not just approximately), but this is **only true when the model includes an intercept term**.\n\nWhen you include an intercept $\\beta_0$, the normal equations $X^TX\\beta = X^Ty$ include a row corresponding to the column of 1s in the design matrix.\n\n\n\n#### Proof\n\nConsider a linear model with an intercept. The design matrix has a column of 1s:\n\n$$X = \\begin{bmatrix}\n1 & x_{11} & x_{12} & \\cdots & x_{1p} \\\\\n1 & x_{21} & x_{22} & \\cdots & x_{2p} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_{n1} & x_{n2} & \\cdots & x_{np}\n\\end{bmatrix}, \\quad\n\\beta = \\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_p\n\\end{bmatrix}, \\quad\ny = \\begin{bmatrix}\ny_1 \\\\\ny_2 \\\\\n\\vdots \\\\\ny_n\n\\end{bmatrix}$$\n\n#### The Normal Equations\n\nThe OLS solution satisfies the normal equations:\n$$X^T X \\beta = X^T y$$\n\n#### Looking at $X^T$\n\nThe transpose of $X$ is:\n$$X^T = \\begin{bmatrix}\n1 & 1 & 1 & \\cdots & 1 \\\\\nx_{11} & x_{21} & x_{31} & \\cdots & x_{n1} \\\\\nx_{12} & x_{22} & x_{32} & \\cdots & x_{n2} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{1p} & x_{2p} & x_{3p} & \\cdots & x_{np}\n\\end{bmatrix}$$\n\n**The first row is all 1s!**\n\n#### The First Normal Equation\n\nThe normal equations give us a system of $(p+1)$ equations. The **first equation** comes from multiplying the first row of $X^T$ with both sides:\n\nLeft side: First row of $X^T$ times $X\\beta$\n$$[1, 1, 1, \\ldots, 1] \\cdot X\\beta = \\sum_{i=1}^n (X\\beta)_i = \\sum_{i=1}^n \\hat{y}_i$$\n\nRight side: First row of $X^T$ times $y$\n$$[1, 1, 1, \\ldots, 1] \\cdot y = \\sum_{i=1}^n y_i$$\n\n#### The Result\n\nTherefore, the first normal equation is:\n$$\\boxed{\\sum_{i=1}^n \\hat{y}_i = \\sum_{i=1}^n y_i}$$\n\nThis is **exact**, not approximate. It's a direct consequence of having the column of 1s in $X$.\n\n#### Interpretation\n\nDividing by $n$:\n$$\\bar{\\hat{y}} = \\bar{y}$$\n\nThe mean of the fitted values equals the mean of the observed values. This is why, with an intercept, the regression line always passes through the point $(\\bar{x}, \\bar{y})$.\n\n**End of proof**\n\n\n\n\n#### Without an intercept\n\nIf you force the regression through the origin (no $\\beta_0$ term), then:\n\n- Condition 1 is **NOT** satisfied - residuals don't sum to zero\n- The TSS = RSS + ESS decomposition **doesn't hold**\n- $R^2$ can even be negative!\n\nThe centered version assumes an intercept; the uncentered version doesn't.\n\n#### Key takeaway\n\nFor standard OLS with intercept:\n- $\\sum (y_i - \\hat{y}_i) = 0$ ✓ (exactly)\n- $\\sum (y_i - \\hat{y}_i)\\hat{y}_i = 0$ ✓ (exactly, from normal equations)\n\nBoth are mathematical properties, not approximations!\n\n:::\n\n## $R^2$, cont.\n\nNow, a good fit is one in which the model explains a large part of the variance\nof $\\mathbf{y}$.  \n\nSo the measure of fit $R^2$ is defined as:\n\n$$\nR^2 = \\frac{\\text{ESS}}{\\text{TSS}} \n= 1-\\frac{\\text{RSS}}{\\text{TSS}}\n= 1 - \\frac{\\sum_{i=1}^n \\left(y_i-\\hat{y_i}\\right)^2}{\\sum_{i=1}^n \\left(y_i-\\overline{y}\\right)^2}.\n$$\n\nAs a result, $0\\leq R^2\\leq 1$.\n\n:::: {.fragment}\nThis is more specifically called $R^2$ __centered__.\n::::\n\n\n:::: {.fragment}\nThere is also an __uncentered__ version of $R^2$ defined as:\n\n$$\nR^2 (\\text{uncentered}) = 1 - \\frac{\\sum_{i=1}^n \\left(y_i-\\hat{y_i}\\right)^2}{\\sum_{i=1}^n y_i^2}.\n$$\n\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## $R^2$, cont.\n:::\n\nThe closer the value of $R^2$ is to $1$ the better the fit of the regression:\n\n* small values of RSS imply that the residuals are small and therefore we have a better fit.\n\n$R^2$ is called the __coefficient of determination.__  \n\nIt tells us \"how well does the model predict the data?\"\n\n## Warning\n\nDo __not__ confuse $R^2$ with Pearson's $r$, which is the\n__correlation coefficient.__\n\n(To make matters worse, sometimes people talk about $r^2$... very confusing!)\n\n:::: {.fragment}\nThe correlation coefficient tells us whether two variables are __correlated__.   \n\nHowever, just because two variables are correlated does not mean that one is a\ngood __predictor__ of the other!\n\nTo compare ground truth with predictions, we always use $R^2$.\n::::\n\n# OLS in Practice\n\n## OLS in Practice\n\nFirst, we'll look at a test case on synthetic data. We'll use Scikit-Learn's\n`make_regression` function.\n\n::: {#c65f540f .cell execution_count=17}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn import datasets\nX, y = datasets.make_regression(n_samples=100, n_features=20, n_informative=5, bias=0.1, noise=30, random_state=1)\n```\n:::\n\n\nAnd then use the `statsmodels` package to fit the model.\n\n::: {#8549423f .cell execution_count=18}\n``` {.python .cell-code code-fold=\"false\"}\nimport statsmodels.api as sm\n\n# Add a constant (intercept) to the design matrix\n#X_with_intercept = sm.add_constant(X)\n#model = sm.OLS(y, X_with_intercept)\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\n```\n:::\n\n\n---\n\nAnd print the summary of the results.\n\n::: {#8934d530 .cell execution_count=19}\n``` {.python .cell-code}\nprint(results.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:                      y   R-squared (uncentered):                   0.969\nModel:                            OLS   Adj. R-squared (uncentered):              0.961\nMethod:                 Least Squares   F-statistic:                              123.8\nDate:                Wed, 22 Oct 2025   Prob (F-statistic):                    1.03e-51\nTime:                        22:31:23   Log-Likelihood:                         -468.30\nNo. Observations:                 100   AIC:                                      976.6\nDf Residuals:                      80   BIC:                                      1029.\nDf Model:                          20                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1            12.5673      3.471      3.620      0.001       5.659      19.476\nx2            -3.8321      2.818     -1.360      0.178      -9.440       1.776\nx3            -2.4197      3.466     -0.698      0.487      -9.316       4.477\nx4             2.0143      3.086      0.653      0.516      -4.127       8.155\nx5            -2.6256      3.445     -0.762      0.448      -9.481       4.230\nx6             0.7894      3.159      0.250      0.803      -5.497       7.076\nx7            -3.0684      3.595     -0.853      0.396     -10.224       4.087\nx8            90.1383      3.211     28.068      0.000      83.747      96.529\nx9            -0.0133      3.400     -0.004      0.997      -6.779       6.752\nx10           15.2675      3.248      4.701      0.000       8.804      21.731\nx11           -0.2247      3.339     -0.067      0.947      -6.869       6.419\nx12            0.0773      3.546      0.022      0.983      -6.979       7.133\nx13           -0.2452      3.250     -0.075      0.940      -6.712       6.222\nx14           90.0179      3.544     25.402      0.000      82.966      97.070\nx15            1.6684      3.727      0.448      0.656      -5.748       9.085\nx16            4.3945      2.742      1.603      0.113      -1.062       9.851\nx17            8.7918      3.399      2.587      0.012       2.028      15.556\nx18           73.3771      3.425     21.426      0.000      66.562      80.193\nx19           -1.9139      3.515     -0.545      0.588      -8.908       5.080\nx20           -1.3206      3.284     -0.402      0.689      -7.855       5.214\n==============================================================================\nOmnibus:                        5.248   Durbin-Watson:                   2.018\nProb(Omnibus):                  0.073   Jarque-Bera (JB):                4.580\nSkew:                           0.467   Prob(JB):                        0.101\nKurtosis:                       3.475   Cond. No.                         2.53\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nThe $R^2$ value is very good.   We can see that the linear model does a very good\njob of predicting the observations $y_i$.\n\n::: {.callout-note}\nThe summary uses the __uncentered__ version of $R^2$ because, as the footnote says,\nthe model does not include a constant to account for an intercept term. You can try\nuncommenting the line `X_with_intercept = sm.add_constant(X)` and see that the\nsummary then uses the centered version of $R^2$.\n:::\n\n## Aside: Summary Statistics\n\nHere is what all the statistics in the summary table mean:\n\n1. **R-squared (Uncentered):**\n\n* Definition: Measures the proportion of the variation in the dependent variable\n  that is explained by the model, without subtracting the mean of the dependent variable.\n* Formula:\n  $R^2 (\\text{uncentered}) = 1 - \\frac{\\sum (y_i - \\hat{y}_i)^2}{\\sum y_i^2}$\n\nwhere $y_i$ is the actual value, and $\\hat{y}_i$ is the predicted value from the regression.\n\n* Interpretation: Higher values (closer to 1) indicate a better fit, but this\n  version of $R^2$ can sometimes be misleading because it doesn’t account for the mean.\n\n2. **Adjusted R-squared (Uncentered):**\n\n* Definition: Adjusts the uncentered $R^2$ to account for the number of predictors\n  in the model, preventing overfitting by penalizing for adding more variables.\n* Formula:\n    $R^2 (\\text{adj, uncentered}) = 1 - \\left( \\frac{(1 - R^2 (\\text{uncentered}))(n - 1)}{n - p} \\right)$\n\nwhere $n$ is the number of observations, and $p$ is the number of predictors in the model.\n\n* Interpretation: It is a more conservative measure of fit than the regular $R^2$,\n  particularly useful when comparing models with different numbers of predictors.\n\n3. **F-statistic:**\n\n* Definition: The F-statistic tests whether the overall regression model is\n  significant, i.e., whether at least one of the predictors explains a significant\n  amount of the variance in the dependent variable.\n* Formula:\n    $F = \\frac{R^2 (\\text{uncentered}) / (p - 1)}{(1 - R^2 (\\text{uncentered})) / (n - p)}$\n\nwhere $p$ is the number of predictors, and $n$ is the number of observations.\n\n* Interpretation: A high F-statistic suggests that the model is statistically significant.\n\n4. **Prob (F-statistic):**\n\n* Definition: This is the p-value associated with the F-statistic. It indicates the\n  probability that the observed F-statistic would occur if the null hypothesis\n  (that none of the predictors are significant) were true.\n* Interpretation: A small p-value (typically less than 0.05) suggests that the\n  model is statistically significant, meaning that at least one predictor is important.\n\n5. **Log-Likelihood:**\n\n* Definition: The log-likelihood measures the likelihood that the model could\n  have produced the observed data. It is a measure of the fit of the model to the data.\n* Formula:\n    $\\mathcal{L} = -\\frac{n}{2} \\log(2\\pi \\sigma^2) - \\frac{1}{2\\sigma^2} \\sum (y_i - \\hat{y}_i)^2$\n\nwhere $\\sigma^2$ is the variance of the residuals.\n\n* Interpretation: A higher log-likelihood indicates a better fit of the model.\n\n6. **Akaike Information Criterion (AIC):**\n\n* Definition: AIC is a measure of the relative quality of a statistical model for a\n  given set of data. It penalizes the likelihood function for adding too many\n  parameters.\n* Formula:\n    $\\text{AIC} = 2p - 2 \\log(\\mathcal{L})$\n\nwhere $p$ is the number of parameters, and $\\mathcal{L}$ is the log-likelihood.\n\n* Interpretation: Lower AIC values suggest a better model fit, but it penalizes for overfitting.\n\n7. **Bayesian Information Criterion (BIC):**\n\n* Definition: Similar to AIC, BIC penalizes models with more parameters, but more\n  strongly than AIC. It is used to select among models.\n\n* Formula:\n    $\\text{BIC} = p \\log(n) - 2 \\log(\\mathcal{L})$\n\nwhere $p$ is the number of parameters, $n$ is the number of observations, and\n$\\mathcal{L}$ is the log-likelihood.\n\n* Interpretation: A lower BIC indicates a better model, with stronger penalties for\n  models with more parameters.\n\n8. **Coefficient Table:**\n\n* The first two columns are the independent variables and their coefficients. It\n  is the $m$ in $y = mx + b$. One unit of change in the variable will affect the\n  variable’s coefficient’s worth of change in the dependent variable. If the\n  coefficient is negative, they have an inverse relationship. As one rises, the\n  other falls.\n\n* The `std error` is an estimate of the standard deviation of the coefficient, a\n  measurement of the amount of variation in the coefficient throughout its data\n  points. \n  \n* The `t` is related and is a measurement of the precision with which the coefficient\n  was measured. A low std error compared to a high coefficient produces a\n  high t statistic, which signifies a high significance for your coefficient.\n\n* `P>|t|` is one of the most important statistics in the summary. It uses the\n  `t` statistic to produce the `P` value, a measurement of how likely your\n   coefficient is measured through our model by chance. For example, a `P` value\n   of 0.378 is saying there is a 37.8% chance the variable has no affect on the\n   dependent variable and the results are produced by chance. Proper model analysis\n   will compare the `P` value to a previously established alpha value, or a\n   threshold with which we can apply significance to our coefficient. A common\n   alpha is 0.05, which few of our variables pass in this instance.\n\n* `[0.025 and 0.975]` are both measurements of values of our coefficients within\n  95% of our data, or within two standard deviations. Outside of these values can\n  generally be considered outliers.\n\nThe coefficient description came from this [blog post](https://medium.com/swlh/interpreting-linear-regression-through-statsmodels-summary-4796d359035a)\nwhich also describes the other statistics in the summary table.\n\n---\n\nBack to the example.\n\nSome of the independent variables may not contribute to the accuracy of\nthe prediction. \n\n:::: {.columns}\n::: {.column width=\"%\"}\n\n::: {#6c34bbec .cell execution_count=20}\n``` {.python .cell-code}\nax = ut.plotSetup3d(-2, 2, -2, 2, -200, 200)\n\n# try columns of X with large coefficients, or not\nax.plot(X[:, 1], X[:, 2], 'ro', zs=y, markersize = 4)\n```\n\n::: {.cell-output .cell-output-display}\n![](17-Regression-I-Linear_files/figure-revealjs/cell-20-output-1.png){width=340 height=331}\n:::\n:::\n\n\n:::\n::: {.column width=\"%\"}\n\nNote that each parameter of an independent variable has an associated confidence\ninterval. \n\nIf a coefficient is not distinguishable from zero, then we cannot assume that\nthere is any relationship between the independent variable and the observations.\n\n:::\n::::\n\n\n\n::: {.content-visible when-profile=\"web}\n\n\n## Confidence Interval Formula\n\nFor coefficient $\\beta_j$, the 95% confidence interval is:\n\n$$\\hat{\\beta}_j \\pm t_{\\alpha/2, n-p} \\cdot \\text{SE}(\\hat{\\beta}_j)$$\n\nwhere:\n- $\\hat{\\beta}_j$ is the estimated coefficient\n- $t_{\\alpha/2, n-p}$ is the critical value from the t-distribution with $n-p$ degrees of freedom (for 95% CI, $\\alpha = 0.05$)\n- $\\text{SE}(\\hat{\\beta}_j)$ is the standard error of $\\hat{\\beta}_j$\n\n#### Calculating the Standard Error\n\nThe standard errors come from the **variance-covariance matrix** of the coefficient estimates:\n\n#### Step 1: Estimate residual variance\n$$\\hat{\\sigma}^2 = \\frac{\\text{RSS}}{n-p} = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{n-p}$$\n\nwhere $n$ = number of observations, $p$ = number of parameters (including intercept)\n\n#### Step 2: Compute variance-covariance matrix\n$$\\text{Var}(\\hat{\\beta}) = \\hat{\\sigma}^2 (X^T X)^{-1}$$\n\nThis is a $p \\times p$ matrix where:\n- Diagonal elements are variances of each coefficient\n- Off-diagonal elements are covariances between coefficients\n\n##### Step 3: Extract standard errors\n$$\\text{SE}(\\hat{\\beta}_j) = \\sqrt{[(X^T X)^{-1}]_{jj} \\cdot \\hat{\\sigma}^2}$$\n\nThe standard error is the square root of the $j$-th diagonal element of the variance-covariance matrix.\n\n#### Interpretation\n\nFrom lines 1427-1429, the `[0.025, 0.975]` columns in the statsmodels output show the 95% confidence interval bounds:\n- Lower bound: $\\hat{\\beta}_j - t_{0.025, n-p} \\cdot \\text{SE}(\\hat{\\beta}_j)$\n- Upper bound: $\\hat{\\beta}_j + t_{0.025, n-p} \\cdot \\text{SE}(\\hat{\\beta}_j)$\n\nIf this interval contains zero, the coefficient is not statistically significant - we can't distinguish it from no effect.\n\n#### Why $(X^T X)^{-1}$?\n\nThis matrix captures the geometry of your design:\n- Larger values → more uncertainty (wider CIs)\n- Happens when: features are correlated (multicollinearity) or you have fewer data points\n\n\n:::\n\n\n\n---\n\nIn other words, if the confidence interval for the parameter includes zero, the\nassociated independent variable may not have any predictive value.\n\n:::: {.columns}\n::: {.column width=\"%\"}\n\n::: {#4171df94 .cell execution_count=21}\n``` {.python .cell-code}\nprint('Confidence Intervals: {}'.format(results.conf_int()))\nprint('Parameters: {}'.format(results.params))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfidence Intervals: [[  5.65891465  19.47559281]\n [ -9.44032559   1.77614877]\n [ -9.31636359   4.47701749]\n [ -4.12661379   8.15524508]\n [ -9.4808662    4.22965424]\n [ -5.49698033   7.07574692]\n [-10.22359973   4.08684835]\n [ 83.74738375  96.52928603]\n [ -6.77896356   6.75226985]\n [  8.80365396  21.73126149]\n [ -6.86882065   6.4194618 ]\n [ -6.97868351   7.1332267 ]\n [ -6.71228582   6.2218515 ]\n [ 82.96557061  97.07028228]\n [ -5.74782503   9.08465366]\n [ -1.06173893   9.85081724]\n [  2.02753258  15.5561241 ]\n [ 66.56165458  80.19256546]\n [ -8.90825108   5.0804296 ]\n [ -7.85545335   5.21424811]]\nParameters: [ 1.25672537e+01 -3.83208841e+00 -2.41967305e+00  2.01431564e+00\n -2.62560598e+00  7.89383294e-01 -3.06837569e+00  9.01383349e+01\n -1.33468527e-02  1.52674577e+01 -2.24679428e-01  7.72715974e-02\n -2.45217158e-01  9.00179264e+01  1.66841432e+00  4.39453916e+00\n  8.79182834e+00  7.33771100e+01 -1.91391074e+00 -1.32060262e+00]\n```\n:::\n:::\n\n\n:::\n::: {.column width=\"%\"}\n\nFind the independent variables that are not significant.\n\n::: {#bf6f07be .cell execution_count=22}\n``` {.python .cell-code}\nCIs = results.conf_int()\nnotSignificant = (CIs[:,0] < 0) & (CIs[:,1] > 0)\nnotSignificant\n```\n\n::: {.cell-output .cell-output-display execution_count=340}\n```\narray([False,  True,  True,  True,  True,  True,  True, False,  True,\n       False,  True,  True,  True, False,  True,  True, False, False,\n        True,  True])\n```\n:::\n:::\n\n\nLet's look at the shape of significant variables.\n\n::: {#ecdb927c .cell execution_count=23}\n``` {.python .cell-code}\nXsignif = X[:,~notSignificant]\nXsignif.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=341}\n```\n(100, 6)\n```\n:::\n:::\n\n\n:::\n::::\n\n---\n\nBy eliminating independent variables that are not significant, we help avoid\noverfitting.\n\n::: {#b12a21f2 .cell execution_count=24}\n``` {.python .cell-code}\nmodel = sm.OLS(y, Xsignif)\nresults = model.fit()\nprint(results.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:                      y   R-squared (uncentered):                   0.965\nModel:                            OLS   Adj. R-squared (uncentered):              0.963\nMethod:                 Least Squares   F-statistic:                              437.1\nDate:                Wed, 22 Oct 2025   Prob (F-statistic):                    2.38e-66\nTime:                        22:31:23   Log-Likelihood:                         -473.32\nNo. Observations:                 100   AIC:                                      958.6\nDf Residuals:                      94   BIC:                                      974.3\nDf Model:                           6                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1            11.9350      3.162      3.775      0.000       5.657      18.213\nx2            90.5841      2.705     33.486      0.000      85.213      95.955\nx3            14.3652      2.924      4.913      0.000       8.560      20.170\nx4            90.5586      3.289     27.535      0.000      84.028      97.089\nx5             8.3185      3.028      2.747      0.007       2.307      14.330\nx6            71.9119      3.104     23.169      0.000      65.749      78.075\n==============================================================================\nOmnibus:                        9.915   Durbin-Watson:                   2.056\nProb(Omnibus):                  0.007   Jarque-Bera (JB):               11.608\nSkew:                           0.551   Prob(JB):                      0.00302\nKurtosis:                       4.254   Cond. No.                         1.54\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n## Real Data: House Prices in Ames, Iowa\n\nLet's see how powerful multiple regression can be on a real-world example.\n\nA typical application of linear models is predicting house prices.\n\nLinear models have been used for this problem for decades, and when a municipality\ndoes a value assessment on your house, they typically use a linear model.\n\nWe can consider various measurable attributes of a house (its \"features\") as the\nindependent variables, and the most recent sale price of the house as the\ndependent variable.\n\n---\n\nFor our case study, we will use the features:\n\n* Lot Area (sq ft), \n* Gross Living Area (sq ft), \n* Number of Fireplaces, \n* Number of Full Baths, \n* Number of Half Baths, \n* Garage Area (sq ft), \n* Basement Area (sq ft)\n\nSo our design matrix will have 8 columns (including the constant for the intercept):\n\n$$\nX\\beta = \\mathbf{y},\n$$\n\nand it will have one row for each house in the data set, with $y$ the sale price\nof the house.\n\n::: {.content-visible when-profile=\"slides\"}\n## Ames Housing Data\n:::\n\nWe will use data from housing sales in Ames, Iowa from 2006 to 2009:\n\n![Ames Iowa](figs/ames-iowa-downtown.jpeg){width=45%}\n\n[Tim Kiser (w:User:Malepheasant)](https://commons.wikimedia.org/wiki/File:Ames_Iowa_Main_Street.jpg)\n[CC BY-SA 2.5](https://creativecommons.org/licenses/by-sa/2.5)\nvia Wikimedia Commons\n\n---\n\n::: {#056dd53d .cell execution_count=25}\n``` {.python .cell-code}\ndf = pd.read_csv('data/ames-housing-data/train.csv')\n```\n:::\n\n\n::: {#81c98301 .cell execution_count=26}\n``` {.python .cell-code}\ndf[['LotArea', 'GrLivArea', 'Fireplaces', 'FullBath', 'HalfBath', 'GarageArea', 'TotalBsmtSF', 'SalePrice']].head()\n```\n\n::: {.cell-output .cell-output-display execution_count=344}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>LotArea</th>\n      <th>GrLivArea</th>\n      <th>Fireplaces</th>\n      <th>FullBath</th>\n      <th>HalfBath</th>\n      <th>GarageArea</th>\n      <th>TotalBsmtSF</th>\n      <th>SalePrice</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8450</td>\n      <td>1710</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>548</td>\n      <td>856</td>\n      <td>208500</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>9600</td>\n      <td>1262</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>460</td>\n      <td>1262</td>\n      <td>181500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>11250</td>\n      <td>1786</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>608</td>\n      <td>920</td>\n      <td>223500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9550</td>\n      <td>1717</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>642</td>\n      <td>756</td>\n      <td>140000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>14260</td>\n      <td>2198</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>836</td>\n      <td>1145</td>\n      <td>250000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\nSome things to note here:\n\n* House prices are in dollars\n* Areas are in square feet\n* Rooms are in counts\n\nDo we have scaling concerns here?  \n\nNo, because each feature will get its own $\\beta$, which will correct for the\nscaling differences between different units of measure.\n\n::: {#6a45f58e .cell execution_count=27}\n``` {.python .cell-code}\nX_no_intercept = df[['LotArea', 'GrLivArea', 'Fireplaces', 'FullBath', 'HalfBath', 'GarageArea', 'TotalBsmtSF']]\nX_intercept = sm.add_constant(X_no_intercept)\ny = df['SalePrice'].values\n```\n:::\n\n\n::: {.callout-note}\nNote that removing the intercept will cause the $R^2$ to go up, which is \ncounter-intuitive.  The reason is explained \n[here](https://stats.stackexchange.com/questions/26176/removal-of-statistically-significant-intercept-term-increases-r2-in-linear-mo/26205#26205)] \n-- but amounts to the fact that\nthe formula for R2 with/without an intercept is different.\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Ames Housing Data, cont.\n:::\n\nLet's split the data into training and test sets.\n\n::: {#8dc9ca4d .cell execution_count=28}\n``` {.python .cell-code}\nfrom sklearn import utils, model_selection\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n    X_intercept, y, test_size = 0.5, random_state = 0)\n```\n:::\n\n\nFit the model to the training data.\n\n::: {#aab75619 .cell execution_count=29}\n``` {.python .cell-code}\nmodel = sm.OLS(y_train, X_train)\nresults = model.fit()\nprint(results.summary())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      y   R-squared:                       0.759\nModel:                            OLS   Adj. R-squared:                  0.757\nMethod:                 Least Squares   F-statistic:                     325.5\nDate:                Wed, 22 Oct 2025   Prob (F-statistic):          1.74e-218\nTime:                        22:31:23   Log-Likelihood:                -8746.5\nNo. Observations:                 730   AIC:                         1.751e+04\nDf Residuals:                     722   BIC:                         1.755e+04\nDf Model:                           7                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P>|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nconst       -4.285e+04   5350.784     -8.007      0.000   -5.34e+04   -3.23e+04\nLotArea         0.2361      0.131      1.798      0.073      -0.022       0.494\nGrLivArea      48.0865      4.459     10.783      0.000      39.332      56.841\nFireplaces   1.089e+04   2596.751      4.192      0.000    5787.515     1.6e+04\nFullBath      1.49e+04   3528.456      4.224      0.000    7977.691    2.18e+04\nHalfBath      1.56e+04   3421.558      4.559      0.000    8882.381    2.23e+04\nGarageArea     98.9856      8.815     11.229      0.000      81.680     116.291\nTotalBsmtSF    62.6392      4.318     14.508      0.000      54.163      71.116\n==============================================================================\nOmnibus:                      144.283   Durbin-Watson:                   1.937\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              917.665\nSkew:                           0.718   Prob(JB):                    5.39e-200\nKurtosis:                       8.302   Cond. No.                     6.08e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 6.08e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\n---\n\nWe see that we have:\n\n* $\\beta_0$: Intercept of -\\$42,850\n* $\\beta_1$: Marginal value of one square foot of Lot Area: \\$0.23 \n    * but __NOTICE__ - this coefficient is not statistically different from zero!\n* $\\beta_2$: Marginal value of one square foot of Gross Living Area: \\$48 \n* $\\beta_3$: Marginal value of one additional fireplace: \\$10,890\n* $\\beta_4$: Marginal value of one additional full bath: \\$14,900\n* $\\beta_5$: Marginal value of one additional half bath: \\$15,600\n* $\\beta_6$: Marginal value of one square foot of Garage Area: \\$99\n* $\\beta_7$: Marginal value of one square foot of Basement Area: \\$62\n\n---\n\nIs our model doing a good job?  \n\nThere are many statistics for testing this question, but we'll just look at the\npredictions versus the ground truth.\n\nFor each house we compute its predicted sale value according to our model:\n\n$$\n\\hat{\\mathbf{y}} = X\\hat{\\beta}\n$$\n\n::: {#4c8f7095 .cell execution_count=30}\n``` {.python .cell-code}\n%matplotlib inline\nfrom sklearn.metrics import r2_score\n\nfig, (ax1, ax2) = plt.subplots(1,2,sharey = 'row', figsize=(12, 5))\ny_oos_predict = results.predict(X_test)\nr2_test = r2_score(y_test, y_oos_predict)\nax1.scatter(y_test, y_oos_predict, s = 8)\nax1.set_xlabel('True Price')\nax1.set_ylabel('Predicted Price')\nax1.plot([0,500000], [0,500000], 'r-')\nax1.axis('equal')\nax1.set_ylim([0, 500000])\nax1.set_xlim([0, 500000])\nax1.set_title(f'Out of Sample Prediction, $R^2$ is {r2_test:0.3f}')\n#\ny_is_predict = results.predict(X_train)\nax2.scatter(y_train, y_is_predict, s = 8)\nr2_train = r2_score(y_train, y_is_predict)\nax2.set_xlabel('True Price')\nax2.plot([0,500000],[0,500000],'r-')\nax2.axis('equal')\nax2.set_ylim([0,500000])\nax2.set_xlim([0,500000])\nax2.set_title(f'In Sample Prediction, $R^2$ is {r2_train:0.3f}')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](17-Regression-I-Linear_files/figure-revealjs/cell-30-output-1.png){width=998 height=453}\n:::\n:::\n\n\nWe see that the model does a reasonable job for house values less than about \n\\$250,000. \n\nIt tends to underestimate at both ends of the price range.\n\nNote that the $R^2$ on the (held out) test data is 0.610.   \n\nWe are not doing as well on test data as on training data (somewhat to be expected).\n\nFor a better model, we'd want to consider more features of each house, and perhaps\nsome additional functions such as polynomials as components of our model.\n\n## Recap\n\n* Linear models are a powerful tool for prediction and inference\n* The normal equations provide a simple way to compute the model coefficients\n* The $R^2$ statistic provides a simple measure of fit\n* Careful use of the model is required to avoid overfitting\n\n",
    "supporting": [
      "17-Regression-I-Linear_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}