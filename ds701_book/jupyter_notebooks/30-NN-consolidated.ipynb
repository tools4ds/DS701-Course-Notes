{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Neural Networks -- From Theory to Practice'\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/30-NN-consolidated.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.display import Image, HTML\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this lecture, we'll build an understanding of neural networks, starting from the foundations and moving to practical implementation using scikit-learn.\n",
        "\n",
        "\n",
        "We'll cover:\n",
        "\n",
        "* How neural networks extend linear and logistic regression\n",
        "* The Multi-Layer Perceptron (MLP) architecture\n",
        "* Gradient descent and optimization\n",
        "* Practical implementation with scikit-learn's `MLPClassifier` and `MLPRegressor`\n",
        "\n",
        "\n",
        "## Effectiveness of Neural Networks\n",
        "\n",
        "\n",
        "![](figs/NN-figs/IntroModels.svg){width=\"50%\" fig-align=\"center\"}\n",
        "\n",
        "<!--\n",
        "From [Understanding Deep Learning, Simon J.D. Prince, MIT Press, 2023](http://udlbook.com)\n",
        "-->\n",
        "\n",
        "## Applications Across Domains\n",
        "\n",
        "![](figs/NN-figs/IntroModels2a.svg){width=\"50%\" fig-align=\"center\"}\n",
        "\n",
        "# From Regression to Neural Networks\n",
        "\n",
        "## Linear Regression Revisited\n",
        "\n",
        "Recall linear regression predicts a continuous output:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p = \\mathbf{x}^T\\boldsymbol{\\beta}\n",
        "$$\n",
        "\n",
        "Or in matrix form for multiple samples:\n",
        "\n",
        "$$\n",
        "\\hat{\\mathbf{y}} = \\mathbf{X}\\boldsymbol{\\beta}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "<details>\n",
        "<summary><b>Question:</b> What's the main limitation of linear regression?</summary>\n",
        "<b>Answer:</b> It can only model linear relationships between inputs and outputs!\n",
        "</details>\n",
        "\n",
        "## Logistic Regression \n",
        "\n",
        "* Adds Non-linearity\n",
        "\n",
        "* For binary classification, logistic regression applies a **sigmoid function**:\n",
        "\n",
        "$$\n",
        "P(y=1|\\mathbf{x}) = \\sigma(\\mathbf{x}^T\\boldsymbol{\\beta}) = \\frac{1}{1 + e^{-\\mathbf{x}^T\\boldsymbol{\\beta}}}\n",
        "$$\n",
        "\n",
        "## Logistic Regression, cont.\n",
        "\n",
        "The sigmoid function introduces non-linearity:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "y = sigmoid(x)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(x, y)\n",
        "plt.title('Sigmoid function')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('sigmoid(x)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Key Insight\n",
        "\n",
        "> A single neuron with a sigmoid activation is essentially logistic regression!\n",
        "\n",
        "Neural networks extend this by:\n",
        "\n",
        "::: {.incremental}\n",
        "1. **Multiple neurons** in parallel (learning different features)\n",
        "    - Universal Approximation Theorem guarantees that a network with a single hidden layer can approximate any continuous function to any desired accuracy.\n",
        "2. **Multiple layers** in sequence (learning hierarchical representations)\n",
        "    - Representational capacity is more efficiient\n",
        "3. **Various activation functions** (ReLU, tanh, etc.)\n",
        "    - Required to not collapse to a single linear transformation\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "This allows neural networks to learn complex, non-linear decision boundaries.\n",
        ":::\n",
        "\n",
        "# Artificial Neurons\n",
        "\n",
        "## The Artificial Neuron\n",
        "\n",
        "An artificial neuron is loosely modeled on biological neurons:\n",
        "\n",
        "![](figs/NN-figs/neuron_model.jpeg){width=\"75%\" fig-align=\"center\"}\n",
        "\n",
        "From [cs231n](https://cs231n.github.io/neural-networks-1/)\n",
        "\n",
        "## Neuron Components\n",
        "\n",
        "A neuron performs the following operation:\n",
        "\n",
        "$$\n",
        "\\text{output} = f\\left(\\sum_{i=1}^n w_i x_i + b\\right)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $x_i$ are the **inputs**\n",
        "* $w_i$ are the **weights** (parameters to learn)\n",
        "* $b$ is the **bias** (another parameter)\n",
        "* $f$ is the **activation function** (introduces non-linearity)\n",
        "\n",
        "## Activation Functions\n",
        "\n",
        "**ReLU (Rectified Linear Unit)** - most popular today:\n",
        "$$\n",
        "\\text{ReLU}(x) = \\max(0, x)\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "plt.figure(figsize=(5,3))\n",
        "plt.plot(np.arange(-5,5,0.2), np.maximum(0,np.arange(-5,5,0.2)))\n",
        "plt.title('ReLU(x)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary><b>Why activation functions?</b></summary>\n",
        "<b>Answer:</b> Without non-linearity, multiple layers collapse to a single linear transformation!\n",
        "\n",
        "</details>\n",
        "\n",
        "# Multi-Layer Perceptron (MLP)\n",
        "\n",
        "## MLP Architecture\n",
        "\n",
        "A Multi-Layer Perceptron stacks multiple layers of neurons:\n",
        "\n",
        "![](figs/NN-figs/neural_net2.jpeg){width=\"55%\" fig-align=\"center\"}\n",
        "\n",
        "From [cs231n](https://cs231n.github.io/convolutional-networks/)\n",
        "\n",
        "* **Input layer**: Raw features\n",
        "* **Hidden layers**: Learn intermediate representations\n",
        "* **Output layer**: Final prediction\n",
        "\n",
        "## Matrix Formulation\n",
        "\n",
        "![FCN from UDL.](figs/NN-figs/L24-fcn-dag.png){width=\"75%\" fig-align=\"center\"}\n",
        "\n",
        "**Key property:** Every neuron in layer $i$ connects to every neuron in layer $i+1$.\n",
        "\n",
        "This is also called a **Fully Connected Network (FCN)** or **Dense Network**.\n",
        "\n",
        "## MLP Mathematical Formulation\n",
        "\n",
        "For a network with $K$ hidden layers:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{h}_1 &= f(\\boldsymbol{\\beta}_0 + \\boldsymbol{\\Omega}_0 \\mathbf{x}) \\\\\n",
        "\\mathbf{h}_2 &= f(\\boldsymbol{\\beta}_1 + \\boldsymbol{\\Omega}_1 \\mathbf{h}_1) \\\\\n",
        "&\\vdots \\\\\n",
        "\\mathbf{h}_K &= f(\\boldsymbol{\\beta}_{K-1} + \\boldsymbol{\\Omega}_{K-1} \\mathbf{h}_{K-1}) \\\\\n",
        "\\mathbf{\\hat{y}} &= \\boldsymbol{\\beta}_K + \\boldsymbol{\\Omega}_K \\mathbf{h}_K\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\mathbf{h}_k$ = hidden layer activations\n",
        "* $\\boldsymbol{\\Omega}_k$ = weight matrices\n",
        "* $\\boldsymbol{\\beta}_k$ = bias vectors\n",
        "* $f$ = activation function (e.g., ReLU)\n",
        "\n",
        "# Training Neural Networks\n",
        "\n",
        "## The Loss Function\n",
        "\n",
        "Training means finding weights that minimize a **loss function**:\n",
        "\n",
        "**For regression** (e.g., predicting house prices):\n",
        "$$\n",
        "L = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2 \\quad \\text{(Mean Squared Error)}\n",
        "$$\n",
        "\n",
        "**For classification** (e.g., digit recognition):\n",
        "$$\n",
        "L = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{c=1}^C y_{ic} \\log(\\hat{y}_{ic}) \\quad \\text{(Cross-Entropy)}\n",
        "$$\n",
        "\n",
        "**Goal:** Find parameters $\\theta = \\{\\boldsymbol{\\Omega}_k, \\boldsymbol{\\beta}_k\\}$ that minimize $L$.\n",
        "\n",
        "## Visualizing the Loss Surface\n",
        "\n",
        "The loss function creates a surface over the parameter space:\n",
        "\n",
        "![](figs/L23-convex_cost_function.jpeg){width=\"55%\" fig-align=\"center\"}\n",
        "\n",
        "* Left: **Convex** loss surface (e.g., linear regression)\n",
        "* Right: **Non-convex** loss surface (e.g., neural networks)\n",
        "\n",
        "For neural networks, we can't solve analytically—we need **gradient descent**!\n",
        "\n",
        "# Gradient Descent\n",
        "\n",
        "## The Gradient Descent Intuition\n",
        "\n",
        "Imagine you're lost in foggy mountains and want to reach the valley:\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"30%\"}\n",
        "![](figs/L23-fog-in-the-mountains.jpeg){width=\"100%\"}\n",
        ":::\n",
        "::: {.column width=\"70%\"}\n",
        "What would you do?\n",
        "\n",
        "1. Look around 360 degrees\n",
        "2. Find the direction sloping **downward most steeply**\n",
        "3. Take a few steps in that direction\n",
        "4. Repeat until the ground is level\n",
        "\n",
        "This is **gradient descent**!\n",
        ":::\n",
        "::::\n",
        "\n",
        "## The Gradient\n",
        "\n",
        "For a function $L(\\mathbf{w})$ where $\\mathbf{w} = (w_1, \\ldots, w_n)$, the **gradient** is:\n",
        "\n",
        "$$\n",
        "\\nabla_\\mathbf{w} L(\\mathbf{w}) = \n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial L}{\\partial w_1}\\\\\n",
        "\\frac{\\partial L}{\\partial w_2}\\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\partial L}{\\partial w_n}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "* The gradient points in the direction of **steepest increase**\n",
        "* The negative gradient points toward **steepest decrease**\n",
        "\n",
        "## Gradient Descent Algorithm\n",
        "\n",
        "Start with random weights $\\mathbf{w}^{(0)}$, then iterate:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\nabla_\\mathbf{w} L(\\mathbf{w}^{(t)})\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\eta$ is the **learning rate** (step size)\n",
        "* $\\nabla_\\mathbf{w} L$ is the **gradient** of the loss\n",
        "\n",
        "**Stop when:**\n",
        "\n",
        "* Loss stops decreasing (convergence)\n",
        "* Maximum iterations reached\n",
        "\n",
        "## Learning Rate Matters\n",
        "\n",
        "The learning rate $\\eta$ is crucial:\n",
        "\n",
        "**Too small:** Slow convergence\n",
        "\n",
        "**Too large:** May fail to converge or even diverge!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "# Simulate gradient descent with different learning rates\n",
        "def f(x):\n",
        "    return 3*x**2 - 4*x + 5\n",
        "\n",
        "def df(x):\n",
        "    return 6*x - 4\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Good learning rate\n",
        "x_good = -3.0\n",
        "trajectory_good = [x_good]\n",
        "for _ in range(20):\n",
        "    x_good = x_good - 0.1 * df(x_good)\n",
        "    trajectory_good.append(x_good)\n",
        "\n",
        "xs = np.linspace(-4, 3, 100)\n",
        "axes[0].plot(xs, f(xs), 'b-')\n",
        "axes[0].plot(trajectory_good, [f(x) for x in trajectory_good], 'ro-', markersize=4)\n",
        "axes[0].set_title('Good Learning Rate (η=0.1)')\n",
        "axes[0].set_xlabel('Parameter w')\n",
        "axes[0].set_ylabel('Loss L(w)')\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Too large learning rate\n",
        "x_bad = -3.0\n",
        "trajectory_bad = [x_bad]\n",
        "for _ in range(20):\n",
        "    x_bad = x_bad - 0.34 * df(x_bad)\n",
        "    trajectory_bad.append(x_bad)\n",
        "    if abs(x_bad) > 10:\n",
        "        break\n",
        "\n",
        "xs = np.linspace(-5, 8, 100)\n",
        "axes[1].plot(xs, f(xs), 'b-')\n",
        "axes[1].plot(trajectory_bad[:min(8, len(trajectory_bad))], \n",
        "             [f(x) for x in trajectory_bad[:min(8, len(trajectory_bad))]], \n",
        "             'ro-', markersize=4)\n",
        "axes[1].set_title('Learning Rate Too Large (η=0.4)')\n",
        "axes[1].set_xlabel('Parameter w')\n",
        "axes[1].set_ylabel('Loss L(w)')\n",
        "axes[1].grid(True)\n",
        "axes[1].set_ylim([0, 100])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stochastic Gradient Descent\n",
        "\n",
        "## Full Batch vs Stochastic GD\n",
        "\n",
        "**Full Batch Gradient Descent:** Compute gradient using ALL training samples:\n",
        "\n",
        "$$\n",
        "\\nabla_\\mathbf{w} L = \\frac{1}{N}\\sum_{i=1}^N \\nabla_\\mathbf{w} \\ell_i(\\mathbf{w})\n",
        "$$\n",
        "\n",
        "**Problems:**\n",
        "\n",
        "* Slow for large datasets (millions of samples!)\n",
        "* Memory intensive\n",
        "* Can get stuck in local minima\n",
        "\n",
        "## Stochastic Gradient Descent (SGD)\n",
        "\n",
        "**Stochastic Gradient Descent:** Historically meant using ONE random sample at a time:\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\nabla_\\mathbf{w} \\ell_i(\\mathbf{w}^{(t)})\n",
        "$$\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Much faster per iteration\n",
        "* Can escape local minima (due to noise)\n",
        "* Enables online learning\n",
        "\n",
        "**Disadvantage:**\n",
        "\n",
        "* _Extremely_ noisy gradient estimates\n",
        "* May not converge exactly to minimum\n",
        "\n",
        "## Mini-Batch Gradient Descent\n",
        "\n",
        "**Mini-Batch GD:** Best of both worlds—use a small batch of samples:\n",
        "\n",
        "$$\n",
        "\\nabla_\\mathbf{w} L \\approx \\frac{1}{B}\\sum_{i \\in \\text{batch}} \\nabla_\\mathbf{w} \\ell_i(\\mathbf{w})\n",
        "$$\n",
        "\n",
        "Typical batch sizes: 32, 64, 128, 256\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Balances speed and stability\n",
        "* Efficient GPU parallelization\n",
        "* Better gradient estimates than pure SGD\n",
        "\n",
        "**This is what most modern neural network training uses!**\n",
        "\n",
        "## Visualizing Batch Strategies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "# Create a simple visualization\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "\n",
        "# Sample trajectory data (simulated)\n",
        "np.random.seed(42)\n",
        "iterations = np.arange(0, 50, 1)\n",
        "\n",
        "# Full batch - smooth\n",
        "full_batch = 100 * np.exp(-iterations/15) + 2\n",
        "\n",
        "# Mini-batch - some oscillation\n",
        "mini_batch = 100 * np.exp(-iterations/15) + 5 * np.random.randn(len(iterations)) * np.exp(-iterations/20) + 2\n",
        "\n",
        "# SGD - more noise\n",
        "sgd = 100 * np.exp(-iterations/18) + 15 * np.random.randn(len(iterations)) * np.exp(-iterations/25) + 2\n",
        "\n",
        "ax.plot(iterations, full_batch, 'b-', linewidth=2, label='Full Batch GD', alpha=0.8)\n",
        "ax.plot(iterations, mini_batch, 'g-', linewidth=2, label='Mini-Batch GD (B=32)', alpha=0.8)\n",
        "ax.plot(iterations, sgd, 'r-', linewidth=1, label='Stochastic GD (B=1)', alpha=0.6)\n",
        "\n",
        "ax.set_xlabel('Iteration', fontsize=12)\n",
        "ax.set_ylabel('Loss', fontsize=12)\n",
        "ax.set_title('Convergence Comparison: Different Batch Sizes', fontsize=14)\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_ylim([0, 120])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(For illustration purposes only -- not a real training curve.)\n",
        "\n",
        "# Neural Networks in Scikit-Learn\n",
        "\n",
        "## MLPClassifier and MLPRegressor\n",
        "\n",
        "Scikit-learn provides simple, high-level interfaces:\n",
        "\n",
        "* **`MLPClassifier`**: Multi-layer Perceptron classifier\n",
        "* **`MLPRegressor`**: Multi-layer Perceptron regressor\n",
        "\n",
        "**Key features:**\n",
        "\n",
        "* Multiple hidden layers with various activation functions\n",
        "* Multiple solvers: `'adam'`, `'sgd'`, `'lbfgs'`\n",
        "* Built-in regularization (L2 penalty)\n",
        "* Early stopping support\n",
        "* Easy integration with scikit-learn pipelines\n",
        "\n",
        "## Architecture Specification\n",
        "\n",
        "Specify architecture as a tuple:\n",
        "\n",
        "```python\n",
        "# Single hidden layer with 100 neurons\n",
        "hidden_layer_sizes=(100,)\n",
        "\n",
        "# Two hidden layers: 100 and 50 neurons\n",
        "hidden_layer_sizes=(100, 50)\n",
        "\n",
        "# Three hidden layers\n",
        "hidden_layer_sizes=(128, 64, 32)\n",
        "```\n",
        "\n",
        "Input and output layers are automatically determined from your data!\n",
        "\n",
        "## Scikit-Learn vs PyTorch/TensorFlow\n",
        "\n",
        "<br>\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "### Use Scikit-Learn for:\n",
        "\n",
        "* Small to medium datasets (< 100K samples)\n",
        "* Standard feedforward architectures\n",
        "* Rapid prototyping needed\n",
        "* Integration with scikit-learn pipelines\n",
        "* CPU training is sufficient\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "### Use PT/TF for:\n",
        "\n",
        "* Large datasets (> 100K samples)\n",
        "* Complex architectures (CNNs, RNNs)\n",
        "* GPU acceleration required\n",
        "* Production deployment\n",
        "* Research and experimentation\n",
        ":::\n",
        "::::\n",
        "\n",
        "# Classification Example: MNIST\n",
        "\n",
        "## Load the MNIST Dataset\n",
        "\n",
        "Let's classify handwritten digits (0-9):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load MNIST data\n",
        "print(\"Loading MNIST dataset...\")\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, \n",
        "                    as_frame=False, parser='auto')\n",
        "\n",
        "# Convert labels to integers\n",
        "y = y.astype(int)\n",
        "\n",
        "# Use subset for faster demo\n",
        "X, _, y, _ = train_test_split(X, y, train_size=10000, \n",
        "                               stratify=y, random_state=42)\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y))}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 4))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(X[i].reshape(28, 28), cmap='gray')\n",
        "    ax.set_title(f'Label: {y[i]}')\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each image is 28×28 pixels = 784 features\n",
        "\n",
        "## Prepare the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features to [0, 1]\n",
        "X_train_scaled = X_train / 255.0\n",
        "X_test_scaled = X_test / 255.0\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Feature range: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-important}\n",
        "**Always scale/normalize your features for neural networks!** This helps gradient descent converge faster.\n",
        ":::\n",
        "\n",
        "## Create the MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Create MLP with 2 hidden layers\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(128, 64),  # Architecture\n",
        "    activation='relu',              # Activation function\n",
        "    solver='adam',                  # Optimizer (uses mini-batches)\n",
        "    alpha=0.0001,                   # L2 regularization\n",
        "    batch_size=64,                  # Mini-batch size\n",
        "    learning_rate_init=0.001,       # Initial learning rate\n",
        "    max_iter=20,                    # Number of epochs\n",
        "    random_state=42,\n",
        "    verbose=True                    # Show progress\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "print(\"Training MLP...\")\n",
        "mlp.fit(X_train_scaled, y_train)\n",
        "print(f\"Training completed in {mlp.n_iter_} iterations\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loss Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(mlp.loss_curve_)\n",
        "plt.xlabel('Iteration (Epoch)')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Curve')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The loss decreases smoothly—our model is learning.\n",
        "\n",
        "## Evaluate Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Make predictions\n",
        "y_pred = mlp.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Detailed report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=mlp.classes_)\n",
        "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
        "plt.title('Confusion Matrix for MNIST Classification')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "# Get prediction probabilities\n",
        "y_pred_proba = mlp.predict_proba(X_test_scaled)\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
        "    pred_label = y_pred[i]\n",
        "    true_label = y_test[i]\n",
        "    confidence = y_pred_proba[i].max()\n",
        "    \n",
        "    color = 'green' if pred_label == true_label else 'red'\n",
        "    ax.set_title(f'True: {true_label}, Pred: {pred_label}\\nConf: {confidence:.2f}', \n",
        "                 color=color)\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Regression Example\n",
        "\n",
        "## California Housing Dataset\n",
        "\n",
        "Let's predict house prices using MLP regression:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Load housing data\n",
        "housing = fetch_california_housing()\n",
        "X_housing = housing.data\n",
        "y_housing = housing.target\n",
        "\n",
        "print(f\"Dataset shape: {X_housing.shape}\")\n",
        "print(f\"Features: {housing.feature_names}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Target:** Median house value (in $100,000s)\n",
        "\n",
        "## Prepare Data and Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Split and scale\n",
        "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
        "    X_housing, y_housing, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_h_scaled = scaler.fit_transform(X_train_h)\n",
        "X_test_h_scaled = scaler.transform(X_test_h)\n",
        "\n",
        "# Train MLP Regressor with early stopping\n",
        "mlp_reg = MLPRegressor(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    alpha=0.001,\n",
        "    batch_size=32,\n",
        "    max_iter=100,\n",
        "    early_stopping=True,\n",
        "    validation_fraction=0.1,\n",
        "    n_iter_no_change=10,\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "print(\"Training MLP Regressor...\")\n",
        "mlp_reg.fit(X_train_h_scaled, y_train_h)\n",
        "print(f\"Training stopped at iteration: {mlp_reg.n_iter_}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Loss Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(mlp_reg.loss_curve_)\n",
        "plt.xlabel('Iteration (Epoch)')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Curve')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Make predictions\n",
        "y_pred_h = mlp_reg.predict(X_test_h_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test_h, y_pred_h)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test_h, y_pred_h)\n",
        "r2 = r2_score(y_test_h, y_pred_h)\n",
        "\n",
        "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
        "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "An $R^2$ of ~0.8 means our model explains 80% of the variance in house prices!\n",
        "\n",
        "## Predictions vs Actual"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Scatter plot\n",
        "axes[0].scatter(y_test_h, y_pred_h, alpha=0.5)\n",
        "axes[0].plot([y_test_h.min(), y_test_h.max()], \n",
        "             [y_test_h.min(), y_test_h.max()], 'r--', lw=2)\n",
        "axes[0].set_xlabel('Actual Values')\n",
        "axes[0].set_ylabel('Predicted Values')\n",
        "axes[0].set_title('Predicted vs Actual House Prices')\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Residual plot\n",
        "residuals = y_test_h - y_pred_h\n",
        "axes[1].scatter(y_pred_h, residuals, alpha=0.5)\n",
        "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "axes[1].set_xlabel('Predicted Values')\n",
        "axes[1].set_ylabel('Residuals')\n",
        "axes[1].set_title('Residual Plot')\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "## Grid Search for Optimal Architecture\n",
        "\n",
        "scikit-learn provides [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) for hyperparameter tuning:\n",
        "\n",
        "Create a model without certain hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Create MLP\n",
        "mlp_grid = MLPClassifier(\n",
        "    max_iter=20,\n",
        "    random_state=42,\n",
        "    early_stopping=True,\n",
        "    validation_fraction=0.1,\n",
        "    n_iter_no_change=5,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Use subset for faster demo\n",
        "X_grid = X_train_scaled[:1500]\n",
        "y_grid = y_train[:1500]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grid Search, cont.\n",
        "\n",
        "Define the parameter grid and run the grid search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "#| warning: false\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "    'activation': ['relu'],\n",
        "    'alpha': [0.0001, 0.001]\n",
        "}\n",
        "\n",
        "print(\"Running Grid Search...\")\n",
        "grid_search = GridSearchCV(mlp_grid, param_grid, cv=3, n_jobs=2, verbose=0)\n",
        "grid_search.fit(X_grid, y_grid)\n",
        "\n",
        "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV score: {grid_search.best_score_:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grid Search Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "\n",
        "\n",
        "n_configs = min(10, len(results_df))\n",
        "top_results = results_df.nlargest(n_configs, 'mean_test_score')\n",
        "\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# plt.barh(range(len(top_results)), top_results['mean_test_score'])\n",
        "# plt.yticks(range(len(top_results)), \n",
        "#            [f\"Config {i+1}\" for i in range(len(top_results))])\n",
        "# plt.xlabel('Mean CV Score')\n",
        "# plt.title('Top Hyperparameter Configurations')\n",
        "# plt.grid(True, axis='x')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "print(\"\\nTop configurations:\")\n",
        "for idx, row in top_results.iterrows():\n",
        "    print(f\"\\nConfiguration {idx + 1}:\")\n",
        "    for key, value in row['params'].items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\nMean CV Score:\")\n",
        "print(top_results[['mean_test_score', 'std_test_score']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Best Practices\n",
        "\n",
        "## Data Preprocessing\n",
        "\n",
        "::: {.callout-tip}\n",
        "### Always preprocess your data!\n",
        "\n",
        "1. **Scale features:** Use `StandardScaler` or normalize to [0, 1]\n",
        "2. **Handle missing values:** Impute or remove\n",
        "3. **Encode categorical variables:** One-hot encoding\n",
        "4. **Use pipelines:** Ensures consistent preprocessing\n",
        "\n",
        "```python\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('mlp', MLPClassifier(hidden_layer_sizes=(100,)))\n",
        "])\n",
        "\n",
        "pipeline.fit(X_train, y_train)\n",
        "```\n",
        ":::\n",
        "\n",
        "## Architecture Selection\n",
        "\n",
        "::: {.callout-tip}\n",
        "### Rules of thumb for architecture:\n",
        "\n",
        "1. **Start simple:** Try single hidden layer first\n",
        "2. **Layer sizes:** Between input and output dimensions\n",
        "3. **Depth vs width:** \n",
        "   - More layers → learn complex patterns\n",
        "   - But risk overfitting on small data\n",
        "4. **Typical architectures:**\n",
        "   - Small data: `(100,)` or `(50, 50)`\n",
        "   - Medium data: `(100, 50)` or `(128, 64, 32)`\n",
        "   - Large data: Consider PyTorch\n",
        ":::\n",
        "\n",
        "## Preventing Overfitting\n",
        "\n",
        "Three key techniques:\n",
        "\n",
        "**1. Regularization:** Add L2 penalty (`alpha` parameter)\n",
        "\n",
        "```python\n",
        "mlp = MLPClassifier(alpha=0.01)  # Stronger regularization\n",
        "```\n",
        "\n",
        "**2. Early Stopping:** Stop when validation performance plateaus\n",
        "\n",
        "```python\n",
        "mlp = MLPClassifier(early_stopping=True, \n",
        "                    validation_fraction=0.2,\n",
        "                    n_iter_no_change=10)\n",
        "```\n",
        "\n",
        "**3. Cross-Validation:** Get robust performance estimates\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(mlp, X_train, y_train, cv=5)\n",
        "```\n",
        "\n",
        "## Solver Selection\n",
        "\n",
        "Different solvers for different scenarios:\n",
        "\n",
        "| Solver | Best For | Notes |\n",
        "|--------|----------|-------|\n",
        "| `'adam'` | Most cases | Good default, adaptive learning rate |\n",
        "| `'sgd'` | Large datasets | Classic mini-batch SGD |\n",
        "| `'lbfgs'` | Small datasets | Faster for small data, more memory |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "# Compare solvers\n",
        "solvers = ['adam', 'sgd', 'lbfgs']\n",
        "results = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    mlp = MLPClassifier(\n",
        "        hidden_layer_sizes=(50,),\n",
        "        solver=solver,\n",
        "        max_iter=50,\n",
        "        random_state=42,\n",
        "        verbose=False\n",
        "    )\n",
        "    mlp.fit(X_train_scaled[:2000], y_train[:2000])\n",
        "    score = mlp.score(X_test_scaled, y_test)\n",
        "    results[solver] = score\n",
        "    print(f\"{solver:10s}: {score:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Issues\n",
        "\n",
        "::: {.callout-warning}\n",
        "### Convergence Warnings\n",
        "\n",
        "If you see `ConvergenceWarning`:\n",
        "\n",
        "1. **Increase** `max_iter`\n",
        "2. **Decrease** `learning_rate_init`\n",
        "3. **Enable** `early_stopping=True`\n",
        "4. **Check** if data is properly scaled\n",
        ":::\n",
        "\n",
        "::: {.callout-warning}\n",
        "### Poor Performance\n",
        "\n",
        "If accuracy is low:\n",
        "\n",
        "1. Is data scaled/normalized?\n",
        "2. Is architecture appropriate?\n",
        "3. Is learning rate too high/low?\n",
        "4. Do you need more iterations?\n",
        "5. Is regularization too strong?\n",
        ":::\n",
        "\n",
        "# Summary\n",
        "\n",
        "## Summary\n",
        "\n",
        "**Theory:**\n",
        "\n",
        "* Neural networks extend linear/logistic regression with multiple layers and non-linearity\n",
        "* MLPs learn hierarchical representations through hidden layers\n",
        "* Gradient descent optimizes the loss function\n",
        "* Mini-batch GD balances speed and stability\n",
        "\n",
        "**Practice:**\n",
        "\n",
        "* Scikit-learn's `MLPClassifier` and `MLPRegressor` for easy implementation\n",
        "* Always preprocess/scale your data\n",
        "* Use early stopping and regularization to prevent overfitting\n",
        "* Grid search helps find optimal hyperparameters\n",
        "\n",
        "## To Dig Deeper\n",
        "\n",
        "Other modules in the course notes:\n",
        "\n",
        "* [NN I -- Gradient Descent](./23-NN-I-Gradient-Descent.qmd)\n",
        "* [NN II -- Compute Graph and Backpropagation](./24-NN-II-Backprop.qmd)\n",
        "* [NN III -- SGD and CNNs](./25-NN-III-CNNs.qmd)\n",
        "* [NN IV -- NNs with Scikit-Learn](./29-NN-IV-Scikit-Learn.qmd)\n",
        "\n",
        "Additional resources:\n",
        "\n",
        "* [Understanding Deep Learning, Simon J.D. Prince, MIT Press, 2023](http://udlbook.com)\n",
        "* DS542, Deep Learning for Data Science\n",
        "\n",
        "\n",
        "<!--\n",
        "## When to Use What"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "ax.set_xlim(0, 10)\n",
        "ax.set_ylim(0, 10)\n",
        "ax.axis('off')\n",
        "\n",
        "# Title\n",
        "ax.text(5, 9, 'Choosing Your Neural Network Framework', \n",
        "        ha='center', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Scikit-learn box\n",
        "sklearn_box = mpatches.FancyBboxPatch((0.5, 5), 4, 3, \n",
        "                                      boxstyle=\"round,pad=0.1\", \n",
        "                                      edgecolor='blue', facecolor='lightblue', linewidth=2)\n",
        "ax.add_patch(sklearn_box)\n",
        "ax.text(2.5, 7.2, 'Scikit-Learn MLP', ha='center', fontsize=12, fontweight='bold')\n",
        "ax.text(2.5, 6.5, '• Dataset < 100K', ha='center', fontsize=9)\n",
        "ax.text(2.5, 6.1, '• Standard architectures', ha='center', fontsize=9)\n",
        "ax.text(2.5, 5.7, '• CPU training', ha='center', fontsize=9)\n",
        "ax.text(2.5, 5.3, '• Quick prototyping', ha='center', fontsize=9)\n",
        "\n",
        "# PyTorch box\n",
        "pytorch_box = mpatches.FancyBboxPatch((5.5, 5), 4, 3, \n",
        "                                       boxstyle=\"round,pad=0.1\", \n",
        "                                       edgecolor='red', facecolor='lightcoral', linewidth=2)\n",
        "ax.add_patch(pytorch_box)\n",
        "ax.text(7.5, 7.2, 'PyTorch / TensorFlow', ha='center', fontsize=12, fontweight='bold')\n",
        "ax.text(7.5, 6.5, '• Dataset > 100K', ha='center', fontsize=9)\n",
        "ax.text(7.5, 6.1, '• Complex architectures', ha='center', fontsize=9)\n",
        "ax.text(7.5, 5.7, '• GPU acceleration', ha='center', fontsize=9)\n",
        "ax.text(7.5, 5.3, '• Production systems', ha='center', fontsize=9)\n",
        "\n",
        "# Key principles box\n",
        "principles_box = mpatches.FancyBboxPatch((1, 1), 8, 2.5, \n",
        "                                          boxstyle=\"round,pad=0.1\", \n",
        "                                          edgecolor='green', facecolor='lightgreen', linewidth=2)\n",
        "ax.add_patch(principles_box)\n",
        "ax.text(5, 3, 'Key Principles (Apply to Both)', ha='center', fontsize=12, fontweight='bold')\n",
        "ax.text(5, 2.4, '✓ Always scale your features', ha='center', fontsize=9)\n",
        "ax.text(5, 2.0, '✓ Start simple, then increase complexity', ha='center', fontsize=9)\n",
        "ax.text(5, 1.6, '✓ Use validation sets and early stopping', ha='center', fontsize=9)\n",
        "ax.text(5, 1.2, '✓ Monitor training curves to diagnose issues', ha='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-->\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}