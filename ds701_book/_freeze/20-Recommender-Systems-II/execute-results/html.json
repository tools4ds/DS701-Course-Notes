{
  "hash": "c058af89c8572072b936ea4ecd0d70ef",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Recommender Systems -- II\njupyter: python3\nbibliography: references.bib\n---\n\n## Introduction\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/20-Recommender-Systems-II.ipynb)\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\nIn Part II, we will:\n\n* Study a modern deep learning approach:\n    * Deep Learning Recommender Model (DLRM)\n    * Connection to Matrix Factorization\n    * Architecture and components\n    * System challenges and optimizations\n* Reflect on the societal impact of recommender systems\n\n:::\n::: {.column width=\"50%\"}\n\n\nThis section draws heavily on\n\n* _Deep Learning Recommender Model for Personalization and Recommendation Systems_, [@naumov2019deep]\n\n:::\n::::\n\n# Deep Learning for Recommender Systems\n\n## Why Deep Learning for Recommendations?\n\nModern recommender systems face unique challenges:\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n**Scale:**\n\n* Billions of users and items\n* Massive embedding tables (100s of GB to TB)\n* Over 79% of AI inference cycles in production data centers [@naumov2019deep]\n\n:::\n::: {.column width=\"50%\"}\n\n**Data Types:**\n\n* **Dense features**: continuous (age, time, etc.)\n* **Sparse features**: categorical (user ID, item ID)\n* Need to handle both efficiently\n\n:::\n::::\n\n## The Economic Impact\n\nRecommender systems drive substantial business value:\n\n* **Amazon**: Up to 35% of revenue attributed to recommendations\n* **Netflix**: 75% of movies watched come from recommendations\n* **Meta/Facebook**: Core infrastructure for content ranking and ads\n\nThis economic importance motivates sophisticated deep learning approaches.\n\n## DLRM: Unifying Two Traditions\n\nThe Deep Learning Recommender Model (DLRM) [@naumov2019deep] synthesizes two \nhistorical approaches:\n\n| Tradition | Key Concept | DLRM Component |\n|:----------|:------------|:---------------|\n| **RecSys/Collaborative Filtering** | Latent factors, Matrix Factorization | **Embeddings** & **Dot Products** |\n| **Predictive Analytics** | Statistical models ‚Üí Deep Networks | **MLPs** for feature processing |\n\nThis fusion creates a model that efficiently handles both sparse and dense features.\n\n## Connection to Matrix Factorization\n\nRecall from Part I that Matrix Factorization approximates $R \\approx WV^T$:\n\n* User matrix $W$ and item matrix $V$ can be viewed as **embedding tables**\n* The dot product $w_i^T v_j$ predicts the rating\n* **DLRM generalizes this**: embeddings for many categorical features, not just users/items\n\nThis is why DLRM uses dot products in the interaction layer‚Äîit's a principled way to \nmodel feature interactions based on collaborative filtering theory.\n\n## Training and Evaluation Dataset\n\nCriteo Ad Click-Through Rate (CTR) challenge: \n[Kaggle Challenge](https://www.kaggle.com/c/criteo-display-ad-challenge), \n[Dataset on HF](https://huggingface.co/datasets/criteo/CriteoClickLogs)\n\n### üèóÔ∏è Dataset Construction\n\n- Each row represents a **display ad** served by Criteo covering 24 days\n- The **first column** indicates whether the ad was **clicked (1)** or **not clicked (0)**.\n- Both **positive (clicked)** and **negative (non-clicked)** examples have been **subsampled**, though at **different rates** to keep business confidentiality.\n\n### üß± Features\n\n- **13 integer features**  \n  Mostly count-based; represent numerical properties of the ad, user, or context.\n  \n- **26 categorical features**  \n  Values are **hashed into 32-bit integers** for anonymization.  \n  The **semantic meaning** of these features is **undisclosed**.\n\n## DLRM Architecture Overview\n\nDLRM is a **dual-path architecture** combining sparse and dense feature processing:\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n**Components** (@fig-dlrm-model):\n\n1. **Embeddings**: Map sparse categorical features to dense vectors\n2. **Bottom MLP**: Transform continuous features\n3. **Feature Interaction**: Explicit 2nd-order interactions via dot products\n4. **Top MLP**: Final prediction from combined features\n\n:::\n::: {.column width=\"50%\"}\n\n![DLRM Architecture](figs/RecSys-figs/dlrm-model.png){.lightbox width=80% fig-align=\"center\" #fig-dlrm-model}\n\n:::\n::::\n\n**Key Insight**: Parallel processing paths that merge at the interaction layer.\n\n---\n\nLet's examine each component to build intuition.\n\n## Embeddings\n\n**Embeddings**: Map categorical inputs to latent factor space.\n\n:::: {.columns}\n::: {.column width=\"65%\"}\n- A learned embedding matrix $W \\in \\mathbb{R}^{m \\times d}$ for each category of input\n- One-hot vector $e_i$ with $i\\text{-th}$ entry 1 and rest are 0s\n- Embedding of $e_i$ is $i\\text{-th}$ row of $W$, i.e., $w_i^T = e_i^T W$\n\n<!--\nWe can also use weighted combination of multiple items with a multi-hot vector\nof weights $a^T = [0, ..., a_{i_1}, ..., a_{i_k}, ..., 0]$.\n\nThe embedding of this multi-hot vector is then $a^T W$.\n-->\n\n**Criteo Dataset:** 26 categorical features, each embedded to dimension $d=128$.\n\n:::\n::: {.column width=\"35%\"}\n![DLRM Architecture](figs/RecSys-figs/dlrm-model01.png){.lightbox width=100% fig-align=\"center\"}\n:::\n::::\n\n<!--\n---\n\nPyTorch has a convenient way to do this using `EmbeddingBag`, which besides summing\ncan combine embeddings via mean or max pooling.\n\nHere's an example with 5 embeddings of dimension 3:\n\n::: {#4ca484d7 .cell execution_count=2}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\n\n# Example embedding matrix: 5 embeddings, each of dimension 3\nembedding_matrix = nn.EmbeddingBag(num_embeddings=5, embedding_dim=3, mode='mean')\n\n# Input: Indices into the embedding matrix\ninput_indices = torch.tensor([1, 2, 3, 4])  # Flat list of indices\noffsets = torch.tensor([0, 2])  # Start new bag at position 0 and 2 in input_indices\n\n# Forward pass\noutput = embedding_matrix(input_indices, offsets)\n\nprint(\"Embedding Matrix:\\n\", embedding_matrix.weight)\nprint(\"Output:\\n\", output)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEmbedding Matrix:\n Parameter containing:\ntensor([[ 0.7801,  0.6730,  0.1572],\n        [ 1.3169,  0.7998,  0.3681],\n        [-1.8945,  0.6570, -0.5110],\n        [ 0.4633,  0.0072, -0.7963],\n        [ 0.0663, -0.4947,  0.9090]], requires_grad=True)\nOutput:\n tensor([[-0.2888,  0.7284, -0.0715],\n        [ 0.2648, -0.2438,  0.0563]], grad_fn=<EmbeddingBagBackward0>)\n```\n:::\n:::\n\n\n-->\n\n## Dense Features\n\n:::: {.columns}\n::: {.column width=\"65%\"}\nThe advantage of the DLRM architecture is that it can take continuous features\nas input such as the user's age, time of day, etc.\n\nThere is a bottom MLP that transforms these dense features into a latent space of\nthe same dimension $d$ as the embeddings.\n\n**Criteo Dataset Configuration:**\n\n* **Input:** 13 continuous (dense) features\n* **Bottom MLP layers:** 512 ‚Üí 256 ‚Üí 128\n* **Output:** 128-dimensional vector (matches embedding dimension)\n\n:::\n::: {.column width=\"35%\"}\n![DLRM Architecture](figs/RecSys-figs/dlrm-model02.png){.lightbox width=100% fig-align=\"center\"}\n:::\n::::\n\n## Optional Sparse Feature MLPs\n\n:::: {.columns}\n::: {.column width=\"65%\"}\nOptionally, one can add MLPs to transform the sparse features as well.\n\n:::\n::: {.column width=\"35%\"}\n![DLRM Architecture](figs/RecSys-figs/dlrm-model03.png){.lightbox width=100% fig-align=\"center\"}\n:::\n::::\n\n## Feature Interactions\n\n:::: {.columns}\n::: {.column width=\"65%\"}\n\n**Why explicit interactions?**\n\n* Simply concatenating features lets the MLP learn interactions implicitly\n* But explicitly computing 2nd-order interactions is more efficient and interpretable\n* Inspired by Factorization Machines from predictive analytics\n\n**How:** Compute dot products of **all pairs** of embedding vectors and processed dense features.\n\nThen concatenate dot products with the original dense features.\n\n:::\n::: {.column width=\"35%\"}\n![DLRM Architecture](figs/RecSys-figs/dlrm-model04.png){.lightbox width=100% fig-align=\"center\"}\n:::\n::::\n\n## Feature Interactions, Continued\n\n<br>\n\n**Dimensionality reduction**: Using dot products between $d$-dimensional vectors yields \nscalars, avoiding the explosion of treating each element separately.\n\n**Criteo Example:** 27 total vectors (1 from Bottom MLP + 26 embeddings)\n\n* Pairwise interactions: $\\binom{27}{2} = \\frac{27 \\times 26}{2} = 351$ dot products\n* Each dot product is a scalar (interaction strength)\n\n## Top MLP\n\n:::: {.columns}\n::: {.column width=\"65%\"}\nThe concatenated vector is then passed to a final MLP and then to a sigmoid\nfunction to produce the final prediction (e.g., probability score of recommendation)\n\nThis entire model is trained end-to-end using standard deep learning techniques.\n\n**Criteo Configuration:**\n\n* **Input:** 506-dimensional concatenated vector\n  - 128d processed dense features\n  - 351 pairwise dot products\n  - 27 embedding vectors (27 √ó 1 = 27 after pooling)\n* **Top MLP layers:** 512 ‚Üí 256 ‚Üí 1\n* **Output:** Sigmoid activation ‚Üí click probability\n\n:::\n::: {.column width=\"35%\"}\n![DLRM Architecture](figs/RecSys-figs/dlrm-model05.png){.lightbox width=100% fig-align=\"center\"}\n:::\n::::\n\n## DLRM Dimensions: Criteo Dataset Summary\n\nPutting it all together for the Criteo Ad Kaggle dataset configuration:\n\n| Component | Details |\n|:----------|:--------|\n| **Input Features** | 13 dense (continuous) + 26 sparse (categorical) |\n| **Bottom MLP** | 13 ‚Üí 512 ‚Üí 256 ‚Üí **128** |\n| **Embeddings** | 26 categorical features √ó 128d each |\n| **Interaction Layer** | 27 vectors ‚Üí $\\binom{27}{2} = 351$ dot products |\n| **Concatenation** | 128 (dense) + 351 (interactions) + 27 (pooled embeddings) = **506** |\n| **Top MLP** | 506 ‚Üí 512 ‚Üí 256 ‚Üí **1** |\n| **Output** | Sigmoid(1) ‚Üí Click probability |\n\n**Key observation:** All vectors in interaction layer are 128-dimensional, enabling \nefficient pairwise dot products.\n\n## The Memory Challenge\n\nProduction-scale DLRM models face unique bottlenecks:\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n**Memory Intensive:**\n\n* Embedding tables can be **>99.9% of model memory**\n* Real datasets have millions of unique IDs\n* Example: Criteo has 26 categorical features, some with 10M+ unique values\n* Total size: 100s of GB to TB\n\n:::\n::: {.column width=\"50%\"}\n\n**Irregular Access Patterns:**\n\n* Embedding lookups (`SparseLengthsSum`) have low compute intensity\n* High cache miss rates (vs. dense operations)\n* Memory bandwidth becomes the bottleneck\n* Different from typical DNN workloads (CNNs, RNNs)\n\n:::\n::::\n\nThis is why DLRM requires specialized system optimizations.\n\n## Parallelization Strategy\n\nDLRM's size prevents simple data parallelism (can't replicate massive embedding tables).\n\n**Solution: Hybrid Model + Data Parallelism**\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n**Model Parallelism** for embeddings:\n\n* Distribute embedding tables across devices\n* Each device stores subset of embeddings\n* Reduces memory per device\n\n:::\n::: {.column width=\"50%\"}\n\n**Data Parallelism** for MLPs:\n\n* MLPs have fewer parameters\n* Can replicate across devices\n* Process different samples in parallel\n\n:::\n::::\n\n**Communication**: Use \"butterfly shuffle\" (personalized all-to-all) to gather \nembeddings for the interaction layer.\n\n## Training Results\n\n![DLRM Training Results](figs/RecSys-figs/dlrm-training-results.png){width=\"70%\" fig-align=\"center\" #fig-dlrm-training-results}\n\n@fig-dlrm-training-results shows the training (solid) and validation (dashed)\naccuracies of DLRM on the [Criteo Ad Kaggle dataset](https://www.kaggle.com/competitions/criteo-display-ad-challenge/overview).\n\nDLRM achieves comparable or better accuracy than Deep and Cross Network (DCN) [@wang2017deep],\nwhile being more efficient for sparse feature interactions.\n\n::: {style=\"font-size: 70%\"}\n## Other Modern Approaches\n\nThere are many other modern approaches to recommender systems for example:\n\n::: {.columns}\n::: {.column}\n\n1. **Graph-Based Recommender Systems**:\n   - Leverage graph structures to capture relationships between users and items.\n   - Use techniques like Graph Neural Networks (GNNs) to enhance recommendation accuracy.\n\n2. **Context-Aware Recommender Systems**:\n   - Incorporate contextual information such as time, location, and user mood to provide more personalized recommendations.\n   - Contextual data can be integrated using various machine learning models.\n\n:::\n::: {.column}\n\n3. **Hybrid Recommender Systems**:\n   - Combine multiple recommendation techniques, such as collaborative filtering and content-based filtering, to improve performance.\n   - Aim to leverage the strengths of different methods while mitigating their weaknesses.\n\n4. **Reinforcement Learning-Based Recommender Systems**:\n   - Use reinforcement learning to optimize long-term user engagement and satisfaction.\n   - Models learn to make sequential recommendations by interacting with users and receiving feedback.\n\n:::\n:::\n\nThese approaches often leverage advancements in machine learning and data processing to provide more accurate and personalized recommendations.\n\nSee [@ricci2022recommender] for a comprehensive overview of recommender systems.\n\n:::\n\n# Impact of Recommender Systems\n\n## Filter Bubbles\n\nThere are a number of concerns with the widespread use of recommender systems and personalization in society.\n\nFirst, recommender systems are accused of creating __filter bubbles.__ \n\nA filter bubble is the tendency for recommender systems to limit the variety of information presented to the user.\n\nThe concern is that a user's past expression of interests will guide the algorithm in continuing to provide \"more of the same.\"\n\nThis is believed to increase polarization in society, and to reinforce confirmation bias.\n\n## Maximizing Engagement\n\nSecond, recommender systems in modern usage are often tuned to __maximize engagement.__\n\nIn other words, the objective function of the system is not to present the user's most favored content, \nbut rather the content that will be most likely to keep the user on the site.\n\n**How this works in practice:**\n\n* **Objective Functions**: Models optimize for metrics like click-through rate, watch time, or session duration\n* **A/B Testing**: Continuous experimentation to find which content keeps users engaged longer\n* **Feedback Loops**: User interactions train the model to predict and serve \"sticky\" content\n\n**The Incentive:** Sites supported by advertising revenue directly benefit from more engagement time.\nMore engagement means more ad impressions and more revenue.\n\n## Extreme Content\n\nHowever, many studies have shown that sites that strive to __maximize \nengagement__ do so in large part by guiding users toward __extreme content:__\n\n* content that is shocking, \n* or feeds conspiracy theories, \n* or presents extreme views on popular topics.\n\nGiven this tendency of modern recommender systems, \nfor a third party to create \"clickbait\" content such as this, one of the easiest\nways is to present false claims.\n\nMethods for addressing these issues are being very actively studied at present.\n\nWays of addressing these issues can be:\n\n* via technology\n* via public policy\n\n# Recap and References\n\n## BU CS/CDS Research\n\nYou can read about some of the work done in Professor Mark Crovella's group on\nthis topic:\n\n* _How YouTube Leads Privacy-Seeking Users Away from Reliable Information_, [@spinelli2020youtube] \n* _Closed-Loop Opinion Formation_, [@spinelli2017closed] \n* _Fighting Fire with Fire: Using Antidote Data to Improve Polarization and Fairness of Recommender Systems_, [@rastegarpanah2019fighting] \n\n## Recap\n\n**Part I (Collaborative Filtering & Matrix Factorization):**\n\n* Collaborative filtering (CF): user-user and item-item similarity approaches\n* Matrix factorization (MF): latent vectors and ALS optimization\n* Practical implementation of ALS on Amazon movie reviews\n\n**Part II (Deep Learning & Impact):**\n\n* DLRM: A production-scale deep learning approach unifying RecSys and predictive analytics traditions\n* Connection between embeddings and matrix factorization latent factors\n* DLRM architecture: dual-path design with explicit feature interactions\n* System challenges: massive embedding tables (>99.9% of model memory) and irregular memory access\n* Parallelization: hybrid model + data parallelism with butterfly shuffle communication\n* Societal impact: filter bubbles, engagement maximization, and extreme content concerns\n\n## References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "20-Recommender-Systems-II_files"
    ],
    "filters": [],
    "includes": {}
  }
}