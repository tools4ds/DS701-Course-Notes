{
  "hash": "fd65f4df0f0a6f8656a2deb1acbf89ea",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"GMM Appendix D: Deriving the E-Step: From Indicators to Responsibilities\"\n---\n\n## The Complete Data Log-Likelihood^[Courtesy of Claude.ai]\n\nRecall that if we knew the cluster assignments $z_1, z_2, \\ldots, z_n$, the complete data log-likelihood would be:\n\n$$\n\\log p(X, Z \\mid \\theta) = \\sum_{i=1}^{n} \\log p(x_i, z_i \\mid \\theta)\n$$\n\nFor a GMM, each data point $x_i$ is generated by:\n\n1. Choosing cluster $z_i = k$ with probability $\\pi_k$\n2. Drawing $x_i$ from $\\mathcal{N}(\\mu_k, \\Sigma_k)$\n\nSo the joint probability is:\n\n$$\np(x_i, z_i \\mid \\theta) = p(z_i \\mid \\theta) \\cdot p(x_i \\mid z_i, \\theta)\n$$\n\n---\n\n## Using Indicator Variables\n\nWe can write $z_i = k$ using an indicator variable. Let:\n\n$$\n\\mathbb{1}(z_i = k) = \\begin{cases} 1 & \\text{if } z_i = k \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\n\nThen we can express the complete data log-likelihood as:\n\n$$\n\\log p(X, Z \\mid \\theta) = \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\mathbb{1}(z_i = k) \\log [\\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)]\n$$\n\n**Why?** For each point $i$, only one value of $k$ has $\\mathbb{1}(z_i = k) = 1$, so we pick out exactly the right term.\n\nExpanding:\n\n$$\n\\log p(X, Z \\mid \\theta) = \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\mathbb{1}(z_i = k) [\\log \\pi_k + \\log \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)]\n$$\n\n---\n\n## The E-Step: Taking Expectations\n\nThe problem is that we don't observe $Z$, so we can't compute the complete data log-likelihood directly. The EM algorithm solves this by taking the **expected value** of the complete data log-likelihood with respect to the distribution of $Z$ given $X$ and the current parameters $\\theta^{(t)}$.\n\n### The Q-Function\n\n$$\nQ(\\theta \\mid \\theta^{(t)}) = \\mathbb{E}_{Z \\mid X, \\theta^{(t)}} [\\log p(X, Z \\mid \\theta)]\n$$\n\nSubstituting our expression:\n\n$$\nQ(\\theta \\mid \\theta^{(t)}) = \\mathbb{E}_{Z \\mid X, \\theta^{(t)}} \\left[ \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\mathbb{1}(z_i = k) [\\log \\pi_k + \\log \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)] \\right]\n$$\n\nSince expectation is linear, we can move it inside the sum:\n\n$$\nQ(\\theta \\mid \\theta^{(t)}) = \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\mathbb{E}_{Z \\mid X, \\theta^{(t)}} [\\mathbb{1}(z_i = k)] \\cdot [\\log \\pi_k + \\log \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)]\n$$\n\n---\n\n## Computing the Expected Value of the Indicator\n\nNow we need to compute:\n\n$$\n\\mathbb{E}_{Z \\mid X, \\theta^{(t)}} [\\mathbb{1}(z_i = k)]\n$$\n\n### What is this expectation?\n\nThe expectation of an indicator variable is just the probability that the indicator equals 1:\n\n$$\n\\mathbb{E}[\\mathbb{1}(z_i = k)] = 1 \\cdot P(z_i = k) + 0 \\cdot P(z_i \\neq k) = P(z_i = k)\n$$\n\nBut we need the **conditional** expectation given $X$ and $\\theta^{(t)}$:\n\n$$\n\\mathbb{E}_{Z \\mid X, \\theta^{(t)}} [\\mathbb{1}(z_i = k)] = P(z_i = k \\mid X, \\theta^{(t)})\n$$\n\n### Factorization of the conditional distribution\n\nSince the $z_i$ are conditionally independent given the data and parameters (each point's cluster assignment depends only on that point):\n\n$$\nP(z_i = k \\mid X, \\theta^{(t)}) = P(z_i = k \\mid x_i, \\theta^{(t)})\n$$\n\nThis is the **posterior probability** that point $i$ belongs to cluster $k$, given the observed data and current parameter estimates.\n\n---\n\n## Defining the Responsibility\n\nWe define the **responsibility** of cluster $k$ for point $i$ as:\n\n$$\n\\gamma_{ik} = P(z_i = k \\mid x_i, \\theta^{(t)})\n$$\n\nSo we've shown that:\n\n$$\n\\boxed{\\mathbb{E}_{Z \\mid X, \\theta^{(t)}} [\\mathbb{1}(z_i = k)] = \\gamma_{ik}}\n$$\n\n---\n\n## Computing the Responsibility Using Bayes' Rule\n\nNow we need to actually compute $\\gamma_{ik} = P(z_i = k \\mid x_i, \\theta^{(t)})$.\n\n### Bayes' Rule\n\n$$\nP(z_i = k \\mid x_i, \\theta^{(t)}) = \\frac{P(x_i \\mid z_i = k, \\theta^{(t)}) \\cdot P(z_i = k \\mid \\theta^{(t)})}{P(x_i \\mid \\theta^{(t)})}\n$$\n\n### Breaking down each term:\n\n1. **Prior**: $P(z_i = k \\mid \\theta^{(t)}) = \\pi_k^{(t)}$ (the mixing weight)\n\n2. **Likelihood**: $P(x_i \\mid z_i = k, \\theta^{(t)}) = \\mathcal{N}(x_i \\mid \\mu_k^{(t)}, \\Sigma_k^{(t)})$\n\n3. **Evidence** (marginal likelihood): \n   $$\n   \\begin{aligned}\n   P(x_i \\mid \\theta^{(t)}) & = \\sum_{j=1}^{K} P(x_i \\mid z_i = j, \\theta^{(t)}) \\cdot P(z_i = j \\mid \\theta^{(t)}) \\\\\n     & = \\sum_{j=1}^{K} \\pi_j^{(t)} \\mathcal{N}(x_i \\mid \\mu_j^{(t)}, \\Sigma_j^{(t)})\n   \\end{aligned}\n   $$\n\n### Final formula for responsibility:\n\n$$\n\\boxed{\\gamma_{ik} = \\frac{\\pi_k^{(t)} \\mathcal{N}(x_i \\mid \\mu_k^{(t)}, \\Sigma_k^{(t)})}{\\sum_{j=1}^{K} \\pi_j^{(t)} \\mathcal{N}(x_i \\mid \\mu_j^{(t)}, \\Sigma_j^{(t)})}}\n$$\n\n---\n\n## The Complete E-Step\n\nPutting it all together, the Q-function becomes:\n\n$$\nQ(\\theta \\mid \\theta^{(t)}) = \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\gamma_{ik} [\\log \\pi_k + \\log \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)]\n$$\n\nwhere:\n\n$$\n\\gamma_{ik} = \\frac{\\pi_k^{(t)} \\mathcal{N}(x_i \\mid \\mu_k^{(t)}, \\Sigma_k^{(t)})}{\\sum_{j=1}^{K} \\pi_j^{(t)} \\mathcal{N}(x_i \\mid \\mu_j^{(t)}, \\Sigma_j^{(t)})}\n$$\n\n---\n\n## Intuitive Interpretation\n\n### The Indicator Variable Perspective\n\n- If we knew $z_i = k$, then $\\mathbb{1}(z_i = k) = 1$ and this term contributes fully to the log-likelihood\n- If we knew $z_i \\neq k$, then $\\mathbb{1}(z_i = k) = 0$ and this term contributes nothing\n\n### The Responsibility Perspective\n\n- Since we don't know $z_i$, we use a **soft assignment** $\\gamma_{ik} \\in [0, 1]$\n- $\\gamma_{ik}$ is the probability that point $i$ came from cluster $k$\n- If $\\gamma_{ik} = 0.8$, then point $i$ contributes 80% to cluster $k$'s statistics\n- The responsibilities for each point sum to 1: $\\sum_{k=1}^{K} \\gamma_{ik} = 1$\n\n### Mathematical Equivalence\n\n$$\n\\text{Hard assignment: } \\mathbb{1}(z_i = k) \\in \\{0, 1\\}\n$$\n\n$$\n\\text{Soft assignment: } \\gamma_{ik} = \\mathbb{E}[\\mathbb{1}(z_i = k)] \\in [0, 1]\n$$\n\n---\n\n## Numerical Example\n\n::: {#392a7267 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.stats import multivariate_normal\n\n# Current parameters (iteration t)\npi = np.array([0.3, 0.5, 0.2])  # mixing weights\nmu = np.array([[0, 0], [3, 3], [0, 5]])  # means\nSigma = [np.eye(2), np.eye(2), np.eye(2)]  # covariances\n\n# A single data point\nx_i = np.array([2, 2])\n\n# Compute likelihood for each component\nlikelihoods = np.array([\n    multivariate_normal.pdf(x_i, mean=mu[k], cov=Sigma[k])\n    for k in range(3)\n])\n\nprint(\"Likelihoods p(x_i | z_i=k, θ):\")\nfor k in range(3):\n    print(f\"  Component {k+1}: {likelihoods[k]:.6f}\")\n\n# Compute numerators: π_k * p(x_i | z_i=k, θ)\nnumerators = pi * likelihoods\nprint(f\"\\nNumerators π_k * p(x_i | z_i=k, θ):\")\nfor k in range(3):\n    print(f\"  Component {k+1}: {numerators[k]:.6f}\")\n\n# Compute denominator: sum over all components\ndenominator = np.sum(numerators)\nprint(f\"\\nDenominator (marginal): {denominator:.6f}\")\n\n# Compute responsibilities\ngamma_i = numerators / denominator\nprint(f\"\\nResponsibilities γ_ik:\")\nfor k in range(3):\n    print(f\"  Component {k+1}: {gamma_i[k]:.6f} ({gamma_i[k]*100:.1f}%)\")\n\nprint(f\"\\nSum of responsibilities: {np.sum(gamma_i):.6f}\")\n\n# Interpretation\nprint(f\"\\nInterpretation:\")\nprint(f\"Point x_i = {x_i} is:\")\nprint(f\"  {gamma_i[0]*100:.1f}% likely from cluster 1 (mean {mu[0]})\")\nprint(f\"  {gamma_i[1]*100:.1f}% likely from cluster 2 (mean {mu[1]})\")\nprint(f\"  {gamma_i[2]*100:.1f}% likely from cluster 3 (mean {mu[2]})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLikelihoods p(x_i | z_i=k, θ):\n  Component 1: 0.002915\n  Component 2: 0.058550\n  Component 3: 0.000239\n\nNumerators π_k * p(x_i | z_i=k, θ):\n  Component 1: 0.000875\n  Component 2: 0.029275\n  Component 3: 0.000048\n\nDenominator (marginal): 0.030197\n\nResponsibilities γ_ik:\n  Component 1: 0.028960 (2.9%)\n  Component 2: 0.969455 (96.9%)\n  Component 3: 0.001585 (0.2%)\n\nSum of responsibilities: 1.000000\n\nInterpretation:\nPoint x_i = [2 2] is:\n  2.9% likely from cluster 1 (mean [0 0])\n  96.9% likely from cluster 2 (mean [3 3])\n  0.2% likely from cluster 3 (mean [0 5])\n```\n:::\n:::\n\n\n---\n\n## Visualization of Responsibilities\n\n::: {#4a45b95e .cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Generate a grid of points\nx_range = np.linspace(-3, 6, 100)\ny_range = np.linspace(-3, 8, 100)\nX_grid, Y_grid = np.meshgrid(x_range, y_range)\ngrid_points = np.column_stack([X_grid.ravel(), Y_grid.ravel()])\n\n# Compute responsibilities for each point on the grid\nresponsibilities = np.zeros((len(grid_points), 3))\n\nfor i, point in enumerate(grid_points):\n    likelihoods = np.array([\n        multivariate_normal.pdf(point, mean=mu[k], cov=Sigma[k])\n        for k in range(3)\n    ])\n    numerators = pi * likelihoods\n    denominator = np.sum(numerators)\n    responsibilities[i] = numerators / denominator\n\n# Reshape for plotting\ngamma_1 = responsibilities[:, 0].reshape(X_grid.shape)\ngamma_2 = responsibilities[:, 1].reshape(X_grid.shape)\ngamma_3 = responsibilities[:, 2].reshape(X_grid.shape)\n\n# Create RGB image where each color represents a cluster\nrgb_image = np.zeros((X_grid.shape[0], X_grid.shape[1], 3))\nrgb_image[:, :, 0] = gamma_1  # Red for cluster 1\nrgb_image[:, :, 1] = gamma_2  # Green for cluster 2\nrgb_image[:, :, 2] = gamma_3  # Blue for cluster 3\n\n# Plot\nfig, axes = plt.subplots(1, 4, figsize=(20, 5))\n\n# RGB combined view\naxes[0].imshow(rgb_image, extent=[x_range[0], x_range[-1], y_range[0], y_range[-1]], \n               origin='lower', aspect='auto')\naxes[0].scatter(mu[:, 0], mu[:, 1], c='white', s=200, marker='X', \n                edgecolors='black', linewidths=2)\naxes[0].set_title('Combined Responsibilities\\n(Red=C1, Green=C2, Blue=C3)', fontsize=12)\naxes[0].set_xlabel('x₁')\naxes[0].set_ylabel('x₂')\n\n# Individual responsibility maps\nfor k in range(3):\n    gamma_k = responsibilities[:, k].reshape(X_grid.shape)\n    im = axes[k+1].contourf(X_grid, Y_grid, gamma_k, levels=20, cmap='viridis')\n    axes[k+1].scatter(mu[k, 0], mu[k, 1], c='red', s=200, marker='X', \n                      edgecolors='black', linewidths=2)\n    axes[k+1].set_title(f'γ_i{k+1}: Responsibility of Component {k+1}', fontsize=12)\n    axes[k+1].set_xlabel('x₁')\n    axes[k+1].set_ylabel('x₂')\n    plt.colorbar(im, ax=axes[k+1])\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09D-Appendex-Derive-the-E-step_files/figure-html/cell-3-output-1.png){width=1898 height=471}\n:::\n:::\n\n\n---\n\n## Key Properties of Responsibilities\n\n### 1. Normalization\nFor each point $i$:\n$$\n\\sum_{k=1}^{K} \\gamma_{ik} = 1\n$$\n\nThis follows from the law of total probability.\n\n::: {#4315e429 .cell execution_count=4}\n``` {.python .cell-code}\n# Verify normalization\nprint(\"Verification that responsibilities sum to 1:\")\nfor i in range(5):\n    point = grid_points[i]\n    likelihoods = np.array([\n        multivariate_normal.pdf(point, mean=mu[k], cov=Sigma[k])\n        for k in range(3)\n    ])\n    numerators = pi * likelihoods\n    gamma = numerators / np.sum(numerators)\n    print(f\"Point {i}: sum of γ = {np.sum(gamma):.10f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nVerification that responsibilities sum to 1:\nPoint 0: sum of γ = 1.0000000000\nPoint 1: sum of γ = 1.0000000000\nPoint 2: sum of γ = 1.0000000000\nPoint 3: sum of γ = 1.0000000000\nPoint 4: sum of γ = 1.0000000000\n```\n:::\n:::\n\n\n### 2. Soft Assignment\n- $\\gamma_{ik} = 1$: point $i$ definitely belongs to cluster $k$\n- $\\gamma_{ik} = 0$: point $i$ definitely does not belong to cluster $k$\n- $0 < \\gamma_{ik} < 1$: partial membership (uncertainty)\n\n### 3. Relationship to Hard K-Means\nK-means uses hard assignments:\n$$\nz_i = \\arg\\max_k \\gamma_{ik}\n$$\n\nEM uses the full probability distribution over assignments.\n\n::: {#6e1a5fc9 .cell execution_count=5}\n``` {.python .cell-code}\n# Compare soft vs hard assignments\ntest_point = np.array([1.5, 1.5])\n\nlikelihoods = np.array([\n    multivariate_normal.pdf(test_point, mean=mu[k], cov=Sigma[k])\n    for k in range(3)\n])\nnumerators = pi * likelihoods\ngamma = numerators / np.sum(numerators)\n\nprint(f\"Test point: {test_point}\")\nprint(f\"\\nSoft assignment (EM):\")\nfor k in range(3):\n    print(f\"  Cluster {k+1}: {gamma[k]:.4f}\")\n\nprint(f\"\\nHard assignment (K-means):\")\nhard_assignment = np.argmax(gamma) + 1\nprint(f\"  Cluster {hard_assignment}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest point: [1.5 1.5]\n\nSoft assignment (EM):\n  Cluster 1: 0.3744\n  Cluster 2: 0.6239\n  Cluster 3: 0.0017\n\nHard assignment (K-means):\n  Cluster 2\n```\n:::\n:::\n\n\n---\n\n## Summary\n\nWe've derived that:\n\n$$\n\\boxed{\\mathbb{E}_{Z \\mid X, \\theta^{(t)}} [\\mathbb{1}(z_i = k)] = \\gamma_{ik} = P(z_i = k \\mid x_i, \\theta^{(t)})}\n$$\n\nAnd computed it using Bayes' rule:\n\n$$\n\\boxed{\\gamma_{ik} = \\frac{\\pi_k^{(t)} \\mathcal{N}(x_i \\mid \\mu_k^{(t)}, \\Sigma_k^{(t)})}{\\sum_{j=1}^{K} \\pi_j^{(t)} \\mathcal{N}(x_i \\mid \\mu_j^{(t)}, \\Sigma_j^{(t)})}}\n$$\n\n**Key insight**: Since we don't know which cluster generated each point, we replace the binary indicator $\\mathbb{1}(z_i = k)$ with its expected value under the posterior distribution, which is the probability (responsibility) $\\gamma_{ik}$.\n\n",
    "supporting": [
      "09D-Appendex-Derive-the-E-step_files"
    ],
    "filters": [],
    "includes": {}
  }
}