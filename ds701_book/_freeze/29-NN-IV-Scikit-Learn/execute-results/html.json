{
  "hash": "0481f0efafc99139d659764cf01a5ddf",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'NN IV -- Neural Networks in Scikit-Learn'\njupyter: python3\n---\n\n## Introduction\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/29-NN-IV-Scikit-Learn.ipynb)\n\n::: {#e0927417 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import Image, HTML\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n```\n:::\n\n\nIn previous lectures, we built neural networks from scratch and used PyTorch. Now we'll explore scikit-learn's neural network capabilities, which provide a simpler, high-level interface for many common tasks.\n\n::: {.callout-note}\nWhile PyTorch and TensorFlow are more powerful for complex deep learning tasks, scikit-learn's `MLPClassifier` and `MLPRegressor` are excellent for:\n\n* Rapid prototyping\n* Small to medium-sized datasets\n* Integration with scikit-learn pipelines\n* Cases where you need a simple, fast neural network solution\n:::\n\n# Multi-Layer Perceptron in Scikit-Learn\n\n## MLPClassifier and MLPRegressor\n\nScikit-learn provides two main classes for neural networks:\n\n* **`MLPClassifier`**: Multi-layer Perceptron classifier\n* **`MLPRegressor`**: Multi-layer Perceptron regressor\n\nBoth use the same underlying architecture but differ in their output layer and loss function.\n\n::: {.content-visible when-profile=\"slides\"}\n## MLPClassifier and MLPRegressor\n:::\n\nKey features:\n\n* **Multiple hidden layers**: Specify architecture with a tuple\n* **Various activation functions**: `'relu'`, `'tanh'`, `'logistic'`, `'identity'`\n* **Multiple solvers**: `'adam'`, `'sgd'`, `'lbfgs'`\n* **Regularization**: L2 penalty parameter `alpha`\n* **Early stopping**: Automatic validation-based stopping\n\n## Basic Architecture\n\nThe architecture is specified as a tuple of hidden layer sizes:\n\n```python\n# Single hidden layer with 100 neurons\nhidden_layer_sizes=(100,)\n\n# Two hidden layers with 100 and 50 neurons\nhidden_layer_sizes=(100, 50)\n\n# Three hidden layers\nhidden_layer_sizes=(128, 64, 32)\n```\n\nThe input and output layers are automatically determined from the data.\n\n# Classification Example: MNIST Digits\n\n## Load and Explore the Data\n\nLet's classify handwritten digits using the MNIST dataset.\n\n::: {#f3536196 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Load MNIST data (this may take a moment)\nprint(\"Loading MNIST dataset...\")\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False, parser='auto')\n\n# Convert labels to integers (they come as strings from fetch_openml)\ny = y.astype(int)\n\n# Use a subset for faster training in this demo\n# Remove this line to use the full dataset\nX, _, y, _ = train_test_split(X, y, train_size=10000, stratify=y, random_state=42)\n\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Number of classes: {len(np.unique(y))}\")\nprint(f\"Label type: {y.dtype}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoading MNIST dataset...\nDataset shape: (10000, 784)\nNumber of classes: 10\nLabel type: int64\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Load and Explore the Data\n:::\n\nSplit into training and test sets:\n\n::: {#9bb68bc2 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(f\"Training set size: {X_train.shape[0]}\")\nprint(f\"Test set size: {X_test.shape[0]}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining set size: 8000\nTest set size: 2000\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Load and Explore the Data\n:::\n\nVisualize some examples:\n\n::: {#61d8b291 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(X_train[i].reshape(28, 28), cmap='gray')\n    ax.set_title(f'Label: {y_train[i]}')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](29-NN-IV-Scikit-Learn_files/figure-revealjs/cell-5-output-1.png){width=1138 height=477 fig-align='center'}\n:::\n:::\n\n\n## Preprocessing\n\nNeural networks work best with normalized data:\n\n::: {#19de4de9 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"false\"}\n# Scale features to [0, 1] range (pixels are already in [0, 255])\nX_train_scaled = X_train / 255.0\nX_test_scaled = X_test / 255.0\n\nprint(f\"Feature range: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFeature range: [0.00, 1.00]\n```\n:::\n:::\n\n\n::: {.callout-tip}\nAlternatively, you could use `StandardScaler()` to normalize to zero mean and unit variance, which is often preferred for neural networks.\n:::\n\n## Create and Train the MLP\n\n::: {#6f6fcc30 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.neural_network import MLPClassifier\n\n# Create MLP with 2 hidden layers\nmlp = MLPClassifier(\n    hidden_layer_sizes=(128, 64),  # Two hidden layers\n    activation='relu',              # ReLU activation\n    solver='adam',                  # Adam optimizer\n    alpha=0.0001,                   # L2 regularization\n    batch_size=64,                  # Mini-batch size\n    learning_rate_init=0.001,       # Initial learning rate\n    max_iter=20,                    # Number of epochs\n    random_state=42,\n    verbose=True                    # Print progress\n)\n\n# Train the model\nprint(\"Training MLP...\")\nmlp.fit(X_train_scaled, y_train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining MLP...\nIteration 1, loss = 0.71345206\nIteration 2, loss = 0.26439384\nIteration 3, loss = 0.18880844\nIteration 4, loss = 0.14070827\nIteration 5, loss = 0.10813206\nIteration 6, loss = 0.08241134\nIteration 7, loss = 0.06312883\nIteration 8, loss = 0.04742022\nIteration 9, loss = 0.03751731\nIteration 10, loss = 0.02680532\nIteration 11, loss = 0.02321482\nIteration 12, loss = 0.01516949\nIteration 13, loss = 0.01235069\nIteration 14, loss = 0.00823132\nIteration 15, loss = 0.00665425\nIteration 16, loss = 0.00515634\nIteration 17, loss = 0.00431783\nIteration 18, loss = 0.00336489\nIteration 19, loss = 0.00283319\nIteration 20, loss = 0.00253876\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-1 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(batch_size=64, hidden_layer_sizes=(128, 64), max_iter=20,\n              random_state=42, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>MLPClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(batch_size=64, hidden_layer_sizes=(128, 64), max_iter=20,\n              random_state=42, verbose=True)</pre></div> </div></div></div></div>\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Create and Train the MLP\n:::\n\nThe `verbose=True` parameter shows the loss at each iteration, similar to what we saw in our custom implementation and PyTorch.\n\n## Evaluate the Model\n\n::: {#6824ef6d .cell execution_count=8}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Make predictions\ny_pred = mlp.predict(X_test_scaled)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"\\nTest Accuracy: {accuracy:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTest Accuracy: 0.9555\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Evaluate the Model\n:::\n\nDetailed classification report:\n\n::: {#ff882495 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"false\"}\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.99      0.97       191\n           1       0.98      1.00      0.99       226\n           2       0.94      0.96      0.95       215\n           3       0.95      0.93      0.94       202\n           4       0.98      0.94      0.96       206\n           5       0.96      0.94      0.95       168\n           6       0.98      0.97      0.98       196\n           7       0.93      0.96      0.95       185\n           8       0.96      0.95      0.95       206\n           9       0.91      0.91      0.91       205\n\n    accuracy                           0.96      2000\n   macro avg       0.96      0.96      0.96      2000\nweighted avg       0.96      0.96      0.96      2000\n\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Evaluate the Model\n:::\n\nVisualize the confusion matrix:\n\n::: {#50cebad2 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\nfrom sklearn.metrics import ConfusionMatrixDisplay\n\nfig, ax = plt.subplots(figsize=(10, 8))\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=mlp.classes_)\ndisp.plot(ax=ax, cmap='Blues', values_format='d')\nplt.title('Confusion Matrix for MNIST Classification')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](29-NN-IV-Scikit-Learn_files/figure-revealjs/cell-10-output-1.png){width=748 height=671 fig-align='center'}\n:::\n:::\n\n\n## Visualize Training Progress\n\nScikit-learn's MLP stores the loss at each iteration:\n\n::: {#9b23d2a0 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"false\"}\nplt.figure(figsize=(10, 6))\nplt.plot(mlp.loss_curve_)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.title('Training Loss Curve')\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](29-NN-IV-Scikit-Learn_files/figure-revealjs/cell-11-output-1.png){width=812 height=523 fig-align='center'}\n:::\n:::\n\n\n::: {.callout-note}\nThe loss curve shows how the model's error decreases during training. A smooth decreasing curve indicates good convergence.\n:::\n\n## Visualize Predictions\n\nLet's look at some predictions and their confidence:\n\n::: {#7e0a3732 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\n# Get prediction probabilities\ny_pred_proba = mlp.predict_proba(X_test_scaled)\n\n# Visualize some predictions\nfig, axes = plt.subplots(2, 5, figsize=(15, 6))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(X_test[i].reshape(28, 28), cmap='gray')\n    pred_label = y_pred[i]\n    true_label = y_test[i]\n    confidence = y_pred_proba[i].max()\n    \n    color = 'green' if pred_label == true_label else 'red'\n    ax.set_title(f'True: {true_label}, Pred: {pred_label}\\nConf: {confidence:.2f}', \n                 color=color)\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](29-NN-IV-Scikit-Learn_files/figure-revealjs/cell-12-output-1.png){width=1396 height=571 fig-align='center'}\n:::\n:::\n\n\n# Regression Example: California Housing\n\n## Load and Prepare Data\n\nNow let's use `MLPRegressor` for a regression task:\n\n::: {#fea84885 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\n\n# Load California housing dataset\nhousing = fetch_california_housing()\nX_housing = housing.data\ny_housing = housing.target\n\nprint(f\"Dataset shape: {X_housing.shape}\")\nprint(f\"Features: {housing.feature_names}\")\nprint(f\"Target: Median house value (in $100,000s)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset shape: (20640, 8)\nFeatures: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\nTarget: Median house value (in $100,000s)\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Load and Prepare Data\n:::\n\n::: {#ef03dc76 .cell execution_count=14}\n``` {.python .cell-code code-fold=\"false\"}\n# Split the data\nX_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n    X_housing, y_housing, test_size=0.2, random_state=42\n)\n\n# Scale the features (important for neural networks!)\nscaler = StandardScaler()\nX_train_h_scaled = scaler.fit_transform(X_train_h)\nX_test_h_scaled = scaler.transform(X_test_h)\n```\n:::\n\n\n## Train MLP Regressor\n\n::: {#473082e1 .cell execution_count=15}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.neural_network import MLPRegressor\n\nmlp_reg = MLPRegressor(\n    hidden_layer_sizes=(100, 50),\n    activation='relu',\n    solver='adam',\n    alpha=0.001,\n    batch_size=32,\n    learning_rate_init=0.001,\n    max_iter=100,\n    random_state=42,\n    verbose=False,\n    early_stopping=True,        # Use validation set for early stopping\n    validation_fraction=0.1,    # 10% of training data for validation\n    n_iter_no_change=10         # Stop if no improvement for 10 iterations\n)\n\nprint(\"Training MLP Regressor...\")\nmlp_reg.fit(X_train_h_scaled, y_train_h)\nprint(\"Training complete!\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining MLP Regressor...\nTraining complete!\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Train MLP Regressor\n:::\n\nThe `early_stopping=True` parameter automatically reserves some training data for validation and stops training when the validation score stops improving.\n\n## Evaluate Regression Model\n\n::: {#f66f844f .cell execution_count=16}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# Make predictions\ny_pred_h = mlp_reg.predict(X_test_h_scaled)\n\n# Calculate metrics\nmse = mean_squared_error(y_test_h, y_pred_h)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test_h, y_pred_h)\nr2 = r2_score(y_test_h, y_pred_h)\n\nprint(f\"Mean Squared Error: {mse:.4f}\")\nprint(f\"Root Mean Squared Error: {rmse:.4f}\")\nprint(f\"Mean Absolute Error: {mae:.4f}\")\nprint(f\"R² Score: {r2:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMean Squared Error: 0.2640\nRoot Mean Squared Error: 0.5138\nMean Absolute Error: 0.3460\nR² Score: 0.7986\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Evaluate Regression Model\n:::\n\nVisualize predictions vs actual values:\n\n::: {#4ba30068 .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\"}\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Scatter plot\naxes[0].scatter(y_test_h, y_pred_h, alpha=0.5)\naxes[0].plot([y_test_h.min(), y_test_h.max()], \n             [y_test_h.min(), y_test_h.max()], 'r--', lw=2)\naxes[0].set_xlabel('Actual Values')\naxes[0].set_ylabel('Predicted Values')\naxes[0].set_title('Predicted vs Actual House Prices')\naxes[0].grid(True)\n\n# Residual plot\nresiduals = y_test_h - y_pred_h\naxes[1].scatter(y_pred_h, residuals, alpha=0.5)\naxes[1].axhline(y=0, color='r', linestyle='--', lw=2)\naxes[1].set_xlabel('Predicted Values')\naxes[1].set_ylabel('Residuals')\naxes[1].set_title('Residual Plot')\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](29-NN-IV-Scikit-Learn_files/figure-revealjs/cell-17-output-1.png){width=1334 height=470 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Evaluate Regression Model\n:::\n\nTraining and validation loss curves:\n\n::: {#934533d4 .cell execution_count=18}\n``` {.python .cell-code code-fold=\"false\"}\nplt.figure(figsize=(10, 6))\nplt.plot(mlp_reg.loss_curve_, label='Training Loss')\nplt.plot(mlp_reg.validation_scores_, label='Validation Score (R²)')\nplt.xlabel('Iteration')\nplt.ylabel('Value')\nplt.title('Training Progress with Early Stopping')\nplt.legend()\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](29-NN-IV-Scikit-Learn_files/figure-revealjs/cell-18-output-1.png){width=812 height=523 fig-align='center'}\n:::\n:::\n\n\n# Hyperparameter Tuning\n\n## Grid Search\n\nOne of the advantages of scikit-learn is easy integration with hyperparameter tuning tools:\n\n::: {#abfe43c5 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.model_selection import GridSearchCV\n\n# Define parameter grid (simplified for faster execution)\nparam_grid = {\n    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n    'activation': ['relu'],\n    'alpha': [0.0001, 0.001]\n}\n\n# Create MLP with fewer iterations for faster grid search\nmlp_grid = MLPClassifier(\n    max_iter=20,\n    random_state=42,\n    early_stopping=True,\n    validation_fraction=0.1,\n    n_iter_no_change=5,\n    verbose=False\n)\n\nprint(\"Running Grid Search (this may take a while)...\")\n# Use a smaller subset for the grid search demo\nX_grid = X_train_scaled[:1500]\ny_grid = y_train[:1500]\n\ngrid_search = GridSearchCV(\n    mlp_grid, \n    param_grid, \n    cv=3,           # 3-fold cross-validation\n    n_jobs=2,       # Limit parallel jobs for better stability\n    verbose=0\n)\n\ngrid_search.fit(X_grid, y_grid)\nprint(\"\\nBest parameters:\", grid_search.best_params_)\nprint(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning Grid Search (this may take a while)...\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nBest parameters: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50,)}\nBest cross-validation score: 0.8740\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Grid Search\n:::\n\nVisualize grid search results:\n\n::: {#293d5ca8 .cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\"}\nresults_df = pd.DataFrame(grid_search.cv_results_)\n\n# Get top configurations\nn_configs = min(10, len(results_df))\ntop_results = results_df.nlargest(n_configs, 'mean_test_score')\n\nplt.figure(figsize=(12, 6))\nplt.barh(range(len(top_results)), top_results['mean_test_score'])\nplt.yticks(range(len(top_results)), \n           [f\"Config {i+1}\" for i in range(len(top_results))])\nplt.xlabel('Mean CV Score')\nplt.title('Top Hyperparameter Configurations')\nplt.grid(True, axis='x')\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nTop configurations:\")\nprint(top_results[['params', 'mean_test_score', 'std_test_score']].head())\n```\n\n::: {.cell-output .cell-output-display}\n![](29-NN-IV-Scikit-Learn_files/figure-revealjs/cell-20-output-1.png){width=1142 height=566 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTop configurations:\n                                              params  mean_test_score  \\\n3  {'activation': 'relu', 'alpha': 0.001, 'hidden...         0.874000   \n0  {'activation': 'relu', 'alpha': 0.0001, 'hidde...         0.872000   \n1  {'activation': 'relu', 'alpha': 0.0001, 'hidde...         0.870667   \n4  {'activation': 'relu', 'alpha': 0.001, 'hidden...         0.869333   \n5  {'activation': 'relu', 'alpha': 0.001, 'hidden...         0.869333   \n\n   std_test_score  \n3        0.007483  \n0        0.008165  \n1        0.016357  \n4        0.015861  \n5        0.010625  \n```\n:::\n:::\n\n\n# Comparison: Scikit-Learn vs PyTorch\n\n## When to Use Each\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n### Scikit-Learn MLP\n\n**Advantages:**\n\n* Simple, high-level API\n* Easy integration with scikit-learn pipelines\n* Built-in cross-validation and grid search\n* Good for small to medium datasets\n* Minimal boilerplate code\n\n**Best for:**\n\n* Rapid prototyping\n* Standard ML workflows\n* Small to medium datasets (< 100K samples)\n* When you need scikit-learn compatibility\n:::\n\n::: {.column width=\"50%\"}\n### PyTorch\n\n**Advantages:**\n\n* Full control over architecture\n* GPU acceleration\n* Advanced architectures (CNNs, RNNs, Transformers)\n* Dynamic computation graphs\n* Production deployment tools\n\n**Best for:**\n\n* Large datasets (> 100K samples)\n* Complex architectures\n* GPU-accelerated training\n* Research and experimentation\n* Production deep learning systems\n:::\n::::\n\n## Code Comparison\n\nLet's compare the code for creating a simple MLP:\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n### Scikit-Learn\n\n```python\nfrom sklearn.neural_network import MLPClassifier\n\n# Define and train\nmlp = MLPClassifier(\n    hidden_layer_sizes=(128, 64),\n    activation='relu',\n    max_iter=100\n)\nmlp.fit(X_train, y_train)\n\n# Predict\npredictions = mlp.predict(X_test)\n```\n:::\n\n::: {.column width=\"50%\"}\n### PyTorch\n\n```python\nimport torch\nimport torch.nn as nn\n\nclass MLP(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(784, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 10)\n        )\n    \n    def forward(self, x):\n        return self.layers(x)\n\n# Training loop required...\n```\n:::\n::::\n\n::: {.callout-tip}\nFor most standard tasks with moderate-sized datasets, scikit-learn's MLP is perfectly adequate and much simpler to use. Save PyTorch for when you need more power and flexibility.\n:::\n\n# Advanced Features\n\n## Learning Rate Schedules\n\nScikit-learn supports adaptive learning rates:\n\n::: {#0485e596 .cell execution_count=21}\n``` {.python .cell-code code-fold=\"false\"}\n# Adaptive learning rate\nmlp_adaptive = MLPClassifier(\n    hidden_layer_sizes=(100, 50),\n    learning_rate='adaptive',      # Decrease learning rate when loss plateaus\n    learning_rate_init=0.01,\n    max_iter=50,\n    random_state=42,\n    verbose=False\n)\n\nmlp_adaptive.fit(X_train_scaled[:5000], y_train[:5000])\nprint(f\"Final accuracy: {mlp_adaptive.score(X_test_scaled, y_test):.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFinal accuracy: 0.9410\n```\n:::\n:::\n\n\n## Warm Start\n\nYou can continue training from where you left off:\n\n::: {#c1a7a166 .cell execution_count=22}\n``` {.python .cell-code code-fold=\"false\"}\n# Initial training\nmlp_warm = MLPClassifier(\n    hidden_layer_sizes=(100,),\n    max_iter=10,\n    warm_start=True,    # Allow continued training\n    random_state=42,\n    verbose=False\n)\n\nprint(\"Initial training (10 iterations)...\")\nmlp_warm.fit(X_train_scaled[:5000], y_train[:5000])\nprint(f\"Accuracy after 10 iterations: {mlp_warm.score(X_test_scaled, y_test):.4f}\")\n\n# Continue training\nprint(\"\\nContinued training (10 more iterations)...\")\nmlp_warm.set_params(max_iter=20)\nmlp_warm.fit(X_train_scaled[:5000], y_train[:5000])\nprint(f\"Accuracy after 20 iterations: {mlp_warm.score(X_test_scaled, y_test):.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nInitial training (10 iterations)...\nAccuracy after 10 iterations: 0.9115\n\nContinued training (10 more iterations)...\nAccuracy after 20 iterations: 0.9225\n```\n:::\n:::\n\n\n## Partial Fit for Online Learning\n\nFor large datasets that don't fit in memory, use `partial_fit`:\n\n::: {#2af345d0 .cell execution_count=23}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.neural_network import MLPClassifier\n\n# Create model\nmlp_online = MLPClassifier(\n    hidden_layer_sizes=(100,),\n    random_state=42,\n    warm_start=True\n)\n\n# Train in batches\nbatch_size = 1000\nn_batches = len(X_train_scaled) // batch_size\n\nprint(\"Training with partial_fit...\")\nfor i in range(min(n_batches, 5)):  # Just 5 batches for demo\n    start_idx = i * batch_size\n    end_idx = start_idx + batch_size\n    \n    X_batch = X_train_scaled[start_idx:end_idx]\n    y_batch = y_train[start_idx:end_idx]\n    \n    # For first batch, need to specify classes\n    if i == 0:\n        mlp_online.partial_fit(X_batch, y_batch, classes=np.unique(y_train))\n    else:\n        mlp_online.partial_fit(X_batch, y_batch)\n    \n    if (i + 1) % 2 == 0:\n        score = mlp_online.score(X_test_scaled, y_test)\n        print(f\"  Batch {i+1}/{n_batches}: Test accuracy = {score:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining with partial_fit...\n  Batch 2/8: Test accuracy = 0.6390\n  Batch 4/8: Test accuracy = 0.7980\n```\n:::\n:::\n\n\n# Best Practices\n\n## 1. Data Preprocessing\n\nAlways scale your features:\n\n::: {#7299b76f .cell execution_count=24}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.pipeline import Pipeline\n\n# Create a pipeline with scaling and MLP\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('mlp', MLPClassifier(hidden_layer_sizes=(100,), random_state=42))\n])\n\n# The pipeline handles scaling automatically\npipeline.fit(X_train[:1000], y_train[:1000])\naccuracy = pipeline.score(X_test, y_test)\nprint(f\"Pipeline accuracy: {accuracy:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPipeline accuracy: 0.8805\n```\n:::\n:::\n\n\n## 2. Cross-Validation\n\nUse cross-validation to get robust performance estimates:\n\n::: {#0c320e9c .cell execution_count=25}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.model_selection import cross_val_score\n\nmlp_cv = MLPClassifier(\n    hidden_layer_sizes=(50,),\n    max_iter=20,\n    random_state=42,\n    verbose=False\n)\n\n# 5-fold cross-validation\ncv_scores = cross_val_score(\n    mlp_cv, \n    X_train_scaled[:2000], \n    y_train[:2000],\n    cv=5,\n    n_jobs=-1\n)\n\nprint(f\"CV Scores: {cv_scores}\")\nprint(f\"Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCV Scores: [0.8825 0.8925 0.9025 0.88   0.92  ]\nMean CV Score: 0.8955 (+/- 0.0292)\n```\n:::\n:::\n\n\n## 3. Monitor for Overfitting\n\nUse early stopping and regularization:\n\n::: {#95881f0d .cell execution_count=26}\n``` {.python .cell-code code-fold=\"false\"}\n# With early stopping and regularization\nmlp_reg = MLPClassifier(\n    hidden_layer_sizes=(100, 50),\n    alpha=0.01,              # L2 regularization\n    early_stopping=True,\n    validation_fraction=0.2,\n    n_iter_no_change=10,\n    max_iter=100,\n    random_state=42,\n    verbose=False\n)\n\nmlp_reg.fit(X_train_scaled[:5000], y_train[:5000])\nprint(f\"Training stopped at iteration: {mlp_reg.n_iter_}\")\nprint(f\"Best validation score: {mlp_reg.best_validation_score_:.4f}\")\nprint(f\"Test accuracy: {mlp_reg.score(X_test_scaled, y_test):.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining stopped at iteration: 49\nBest validation score: 0.9260\nTest accuracy: 0.9305\n```\n:::\n:::\n\n\n# Practical Tips\n\n## Architecture Selection\n\n::: {.callout-tip}\n### Rules of Thumb\n\n1. **Start simple**: Try a single hidden layer first\n2. **Layer size**: Start with layer sizes between input and output dimensions\n3. **Deeper vs wider**: More layers can learn more complex patterns, but may overfit\n4. **Typical architectures**:\n   - Small datasets: (100,) or (50, 50)\n   - Medium datasets: (100, 50) or (128, 64, 32)\n   - Large datasets: Consider PyTorch instead\n:::\n\n## Solver Selection\n\nDifferent solvers work better in different scenarios:\n\n| Solver | Best For | Notes |\n|--------|----------|-------|\n| `'adam'` | Most cases | Good default, fast convergence |\n| `'sgd'` | Large datasets | Need to tune learning rate carefully |\n| `'lbfgs'` | Small datasets | Faster for small datasets, more memory |\n\n::: {#b0e8c9e6 .cell execution_count=27}\n``` {.python .cell-code code-fold=\"false\"}\n# Example comparing solvers\nsolvers = ['adam', 'sgd', 'lbfgs']\nresults = {}\n\nfor solver in solvers:\n    mlp = MLPClassifier(\n        hidden_layer_sizes=(50,),\n        solver=solver,\n        max_iter=50,\n        random_state=42,\n        verbose=False\n    )\n    mlp.fit(X_train_scaled[:2000], y_train[:2000])\n    score = mlp.score(X_test_scaled, y_test)\n    results[solver] = score\n    print(f\"{solver:10s}: {score:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nadam      : 0.9000\nsgd       : 0.8330\nlbfgs     : 0.8800\n```\n:::\n:::\n\n\n## Common Issues and Solutions\n\n::: {.callout-warning}\n### Convergence Warnings\n\nIf you see `ConvergenceWarning`, try:\n1. Increase `max_iter`\n2. Decrease `learning_rate_init`\n3. Enable `early_stopping=True`\n4. Check if data is properly scaled\n:::\n\n::: {.callout-warning}\n### Poor Performance\n\nIf accuracy is low, check:\n1. Is the data scaled/normalized?\n2. Is the architecture appropriate for the problem?\n3. Is the learning rate too high or low?\n4. Do you need more training iterations?\n5. Is regularization (`alpha`) too strong?\n:::\n\n# Summary\n\n## Recap\n\nWe covered:\n\n* Scikit-learn's `MLPClassifier` and `MLPRegressor`\n* Classification example with MNIST\n* Regression example with California Housing\n* Hyperparameter tuning with Grid Search\n* Comparison with PyTorch\n* Advanced features: adaptive learning, warm start, partial fit\n* Best practices and practical tips\n\n::: {.content-visible when-profile=\"slides\"}\n## Recap\n:::\n\n**Key Takeaways:**\n\n1. Scikit-learn's MLP is great for standard ML tasks with moderate data\n2. Always preprocess/scale your data\n3. Use cross-validation and early stopping to avoid overfitting\n4. Start with simple architectures and gradually increase complexity\n5. For large-scale or complex tasks, consider PyTorch\n\n## Resources\n\n* [Scikit-learn Neural Network Documentation](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n* [MLPClassifier API Reference](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n* [MLPRegressor API Reference](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n* [Neural Network Models in Scikit-learn](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n\n## Exercise\n\nTry the following on your own:\n\n1. Train an MLP on the Iris dataset and compare with other classifiers\n2. Experiment with different architectures on MNIST\n3. Use Grid Search to find optimal hyperparameters for a regression task\n4. Build a pipeline that includes feature engineering and MLP\n5. Compare training time and accuracy between scikit-learn and PyTorch on the same task\n\n",
    "supporting": [
      "29-NN-IV-Scikit-Learn_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}