{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Recommender Systems\n",
        "jupyter: python3\n",
        "bibliography: references.bib\n",
        "---\n",
        "\n",
        "# Introduction\n",
        "\n",
        "## Introduction\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/20-Recommender-Systems-I.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mp\n",
        "import sklearn\n",
        "from IPython.display import Image, HTML\n",
        "\n",
        "import laUtilities as ut\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Today, we look at a topic that has become enormously important: recommender systems.\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "\n",
        "In Part I, we will:\n",
        "\n",
        "* Define recommender systems\n",
        "* Review the challenges they pose\n",
        "* Discuss two classic methods:\n",
        "    * Collaborative Filtering\n",
        "    * Matrix Factorization\n",
        "\n",
        "In Part II, we will:\n",
        "\n",
        "* Delve into more recent deep learning methods.\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "\n",
        "This section draws heavily on\n",
        "\n",
        "* These [slides](http://alex.smola.org/teaching/berkeley2012/slides/8_Recommender.pdf) by Alex Smola\n",
        "* _Matrix Factorization Techniques for Recommender Systems_, [@koren2009matrix]\n",
        "* _Collaborative Filtering with Temporal Dynamics_, [@koren2009collaborative]\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "\n",
        "## What are Recommender Systems?\n",
        "\n",
        "The concept of recommender systems emerged in the late 1990s / early 2000s as social life moved online:\n",
        "\n",
        "* online purchasing and commerce\n",
        "* online discussions and ratings\n",
        "* social information sharing\n",
        "\n",
        "In these systems the amount of content was exploding and users were having a hard time finding things they were interested in.\n",
        "\n",
        "> Users wanted recommendations.\n",
        "\n",
        "---\n",
        "\n",
        "Over time, the problem has only gotten worse:\n",
        "\n",
        "![](figs/L20-netflix-options.png){fig-align=\"center\"}\n",
        "\n",
        "---\n",
        "\n",
        "![](figs/L20-amazon-options.png){fig-align=\"center\"}\n",
        "\n",
        "---\n",
        "\n",
        "An enormous need has emerged for systems to help sort through products, services, and content items.\n",
        "\n",
        "This often goes by the term __personalization.__\n",
        "\n",
        "Some examples (as of Fall 2024):\n",
        "    \n",
        "* Movie recommendation (Netflix ~6.5K movies and shows, YouTube ~14B videos)\n",
        "* Related product recommendation (Amazon ~600M products)\n",
        "* Web page ranking (Google >>100B pages)\n",
        "* Social content filtering (Facebook, Twitter)\n",
        "* Services (Airbnb, Uber, TripAdvisor)\n",
        "* News content recommendation (Apple News)\n",
        "* Priority inbox & spam filtering (Gmail)\n",
        "* Online dating (Match.com)\n",
        "\n",
        "---\n",
        "\n",
        "A more formal view:\n",
        "    \n",
        "* User - requests content\n",
        "* Objects - instances of content\n",
        "* Context - ratings, purchases, views, device, location, time, history\n",
        "* Interface - browser, mobile\n",
        "\n",
        "![](figs/L20-recsys-abstractly.png){fig-align=\"center\"}\n",
        "\n",
        "## Inferring Preferences\n",
        "\n",
        "Unfortunately, users generally have a hard time __explaining__ what types of\n",
        "content they prefer.   \n",
        "\n",
        "Some early systems worked by interviewing users to ask what they liked.  Those\n",
        "systems did not work very well.\n",
        "\n",
        "::: aside\n",
        "A very interesting article about the earliest personalization systems is [User Modeling via Stereotypes](https://www.cs.utexas.edu/users/ear/CogSci.pdf) by Elaine Rich, dating from 1979.\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "Instead, modern systems work by capturing user's opinions about __specific__ items.\n",
        "\n",
        "This can be done actively:\n",
        "\n",
        "* When a user is asked to **rate** a movie, product, or experience,\n",
        "\n",
        "or it can be done passively:\n",
        "\n",
        "* By noting which items a user **chooses** to purchase (for example).\n",
        "\n",
        "![](figs/L20-example-data.png){width=\"55%\" fig-align=\"center\"}\n",
        "\n",
        "\n",
        "## Challenges\n",
        "\n",
        "* The biggest issue is __scalability__: typical data for this problem is huge.\n",
        "  * Millions of objects\n",
        "  * 100s of millions of users\n",
        "* Changing user base\n",
        "* Changing inventory (movies, stories, goods)\n",
        "* Available features\n",
        "* Imbalanced dataset\n",
        "    * User activity / item reviews are power law distributed    \n",
        "\n",
        "::: aside\n",
        "This data is a subset of the data presented in: \"From amateurs to connoisseurs:\n",
        "modeling the evolution of user expertise through online reviews,\" by J. McAuley\n",
        "and J. Leskovec. WWW, 2013\n",
        ":::\n",
        "\n",
        "## Example\n",
        "\n",
        "Let's look at a dataset for testing recommender systems consisting of Amazon movie reviews:\n",
        "\n",
        "We'll download a compressed pickle file containing the data if it is not already present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# This is a 647 MB file, delete it after use\n",
        "import gdown\n",
        "url = \"https://drive.google.com/uc?id=14GakA7oOjbQp7nxcGApI86WlP3GrYTZI\"\n",
        "pickle_output = \"train.pkl.gz\"\n",
        "\n",
        "import os.path\n",
        "if not os.path.exists(pickle_output):\n",
        "    gdown.download(url, pickle_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll load the data into a pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import gzip\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "with gzip.open(pickle_output, 'rb') as f:\n",
        "    df = pd.read_pickle(f)\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed read time: {elapsed_time:.2f} seconds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Run `df.info()` to see the column names and data types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "where\n",
        "\n",
        "* HelpfulnessNumerator: The number of users who found the review helpful (the \"yes\" votes)\n",
        "* HelpfulnessDenominator: The total number of users who voted on whether the review was helpful (both \"yes\" and \"no\" votes)\n",
        "\n",
        "---\n",
        "\n",
        "Now we can count the number of users and movies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "n_users = df[\"UserId\"].unique().shape[0]\n",
        "n_movies = df[\"ProductId\"].unique().shape[0]\n",
        "n_reviews = len(df)\n",
        "display(Markdown(f'There are:\\n'))\n",
        "display(Markdown(f'* {n_reviews:,} reviews\\n* {n_movies:,} movies\\n* {n_users:,} users'))\n",
        "\n",
        "display(Markdown(f'There are {n_users * n_movies:,} potential reviews, meaning sparsity of {(n_reviews/(n_users * n_movies)):0.4%}'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "where\n",
        "\n",
        "$$\n",
        "\\text{sparsity} \n",
        "= \\frac{\\text{\\# of reviews}}{\\text{\\# of users} \\times \\text{\\# of movies}}\n",
        "= \\frac{\\text{\\# of reviews}}{\\text{\\# of potential reviews}}\n",
        "$$\n",
        "\n",
        "## Reviews are Sparse\n",
        "\n",
        "Only 0.02% of the reviews are available -- 99.98% of the reviews are missing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display(Markdown(f'There are on average {n_reviews/n_movies:0.1f} reviews per movie' +\n",
        "     f' and {n_reviews/n_users:0.1f} reviews per user'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sparseness is Skewed\n",
        "\n",
        "Although on average a movie receives 34 reviews, __almost all movies have even\n",
        "fewer reviews.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "plt.figure(figsize=(10, 4))  # Set the figure size\n",
        "reviews_per_movie = df.groupby('ProductId').count()['Id'].values\n",
        "frac_below_mean = np.sum(reviews_per_movie < (n_reviews/n_movies))/len(reviews_per_movie)\n",
        "plt.plot(sorted(reviews_per_movie, reverse=True), '.-')\n",
        "xmin, xmax, ymin, ymax = plt.axis()\n",
        "plt.hlines(n_reviews/n_movies, xmin, xmax, 'r', lw = 3)\n",
        "plt.ylabel('Number of Ratings', fontsize = 14)\n",
        "plt.xlabel('Movie', fontsize = 14)\n",
        "plt.legend(['Number of Ratings', 'Average Number of Ratings'], fontsize = 14)\n",
        "plt.title(f'Amazon Movie Reviews\\nNumber of Ratings Per Movie\\n' +\n",
        "          f'{frac_below_mean:0.0%} of Movies Below Average', fontsize = 16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Likewise, although the average user writes 14 reviews, almost all users write even fewer reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "plt.figure(figsize=(10, 4))  # Set the figure size\n",
        "reviews_per_user = df.groupby('UserId').count()['Id'].values\n",
        "frac_below_mean = np.sum(reviews_per_user < (n_reviews/n_users))/len(reviews_per_user)\n",
        "plt.plot(sorted(reviews_per_user, reverse=True), '.-')\n",
        "xmin, xmax, ymin, ymax = plt.axis()\n",
        "plt.hlines(n_reviews/n_users, xmin, xmax, 'r', lw = 3)\n",
        "plt.ylabel('Number of Ratings', fontsize = 14)\n",
        "plt.xlabel('User', fontsize = 14)\n",
        "plt.legend(['Number of Ratings', 'Average Number of Ratings'], fontsize = 14)\n",
        "plt.title(f'Amazon Movie Reviews\\nNumber of Ratings Per User\\n' +\n",
        "          f'{frac_below_mean:0.0%} of Users Below Average', fontsize = 16);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Objective Function\n",
        "\n",
        "Ultimately, our goal is to predict the rating that a user would give to an item.\n",
        "\n",
        "For that, we need to define a loss or objective function.\n",
        "\n",
        "A typical objective function is root mean square error (RMSE)\n",
        "\n",
        "$$ \n",
        "\\text{RMSE} = \\sqrt{\\frac{1}{|S|} \\sum_{(i,u)\\in S} (\\hat{r}_{ui} - r_{ui})^2},\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "* $r_{ui}$ is the rating that user $u$ gives to item $i$, and \n",
        "* $S$ is the subset of items that have ratings.\n",
        "\n",
        "OK, now we know the problem and the data available.   How can we address the problem?\n",
        "\n",
        "The earliest method developed is called __collaborative filtering.__\n",
        "\n",
        "# Colaborative Filtering\n",
        "\n",
        "## Collaborative Filtering\n",
        "\n",
        "The central idea of collaborative filtering is that the set of known \n",
        "recommendations can be considered to be a __bipartite graph.__\n",
        "\n",
        "![](figs/L20-bipartite.png){width=\"35%\" fig-align=\"center\"}\n",
        "\n",
        "The nodes of the bipartite graph are __users__ ($U$) and __items__ ($V$).   \n",
        "\n",
        "Each edge corresponds to a known rating $r_{ui}.$\n",
        "\n",
        "---\n",
        "\n",
        "Then recommendations are formed by traversing or processing the bipartite graph.\n",
        "\n",
        "![](figs/L20-cf-basic-idea.png){width=\"60%\" fig-align=\"center\"}\n",
        "\n",
        "There are at least two ways this graph can be used. \n",
        "\n",
        "---\n",
        "\n",
        "Two ways to form a rating for item $(u, i)$: \n",
        "\n",
        ":::: {.fragment}\n",
        "1. Using **user-user similarity**:\n",
        "      * look at users that have similar item preferences to user $u$\n",
        "      * look at how those users rated item $i$\n",
        "::::  \n",
        "\n",
        ":::: {.fragment}\n",
        "⟹ <span style=\"background-color: yellow;\">Good for many users, fewer items.</span><br>\n",
        "(e.g., NetFix had ~280M subscribers, ~6.5K movies/shows)\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "2. Using **item-item similarity**:\n",
        "      * look at other items that have been liked by similar users as item $i$\n",
        "      * look at how user $u$ rated those items\n",
        "::::\n",
        "      \n",
        ":::: {.fragment}\n",
        "⟹ <span style=\"background-color: yellow;\">Good for many items, fewer users</span><br>\n",
        "(e.g. Amazon had ~300M accounts, ~600M products)\n",
        "::::\n",
        "\n",
        "## Item-Item CF\n",
        "\n",
        "For item-item similarity, we'll look at **item-item Collaborative Filtering (CF).**\n",
        "\n",
        "The questions are:\n",
        "\n",
        "::: {.incremental}\n",
        "* How do we judge \"similarity\" of items?\n",
        "* How do we form a predicted rating?\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "Here is another view of the ratings graph, this time as a matrix that includes missing entries:\n",
        "\n",
        "![](figs/L20-u-u-cf-1.png){width=\"60%\" fig-align=\"center\"}\n",
        "\n",
        "---\n",
        "\n",
        "Let's say we want to predict the value of this unknown rating:\n",
        "\n",
        "![](figs/L20-u-u-cf-2.png){width=\"60%\" fig-align=\"center\"}\n",
        "\n",
        "---\n",
        "\n",
        "We'll consider two other items, namely items 3 and 6 (for example).\n",
        "\n",
        "Note that we are only interested in items that this user has rated.\n",
        "\n",
        "![](figs/L20-u-u-cf-3.png){width=\"60%\" fig-align=\"center\"}\n",
        "\n",
        "---\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/L20-u-u-cf-3.png){width=\"100%\" fig-align=\"center\"}\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "We will discuss strategies for assessing similarity shortly. \n",
        "\n",
        "How did we choose these two items?   \n",
        "\n",
        "We used __$k$-nearest neighbors__.   Here $k$ = 2.\n",
        "\n",
        "For now, let's just say we determine the similarities as:\n",
        "\n",
        "$$\n",
        "s_{13} = 0.2 \n",
        "$$\n",
        "\n",
        "$$\n",
        "s_{16} = 0.3 \n",
        "$$\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "These similarity scores tell us how much weight to put on the rating of the other items.\n",
        "\n",
        "![](figs/L20-u-u-cf-4.png){width=\"50%\" fig-align=\"center\"}\n",
        "\n",
        "So we can form a prediction of $\\hat{r}_{15}$ as:\n",
        "  \n",
        "$$\n",
        "\\hat{r}_{15} = \\frac{s_{13} \\cdot r_{35} + s_{16} \\cdot r_{65}}{s_{13} + s_{16}} = \\frac{0.2 \\cdot 2 + 0.3 \\cdot 3}{0.2 + 0.3} = 2.6 \n",
        "$$\n",
        "\n",
        "## Similarity\n",
        "\n",
        "How should we assess similarity of items?\n",
        "\n",
        "A reasonable approach is to consider items similar if their ratings are\n",
        "__correlated:__, i.e., \n",
        "\n",
        "$$\\rho(X,Y) = \\frac{E\\left[(X-\\mu_X)(Y-\\mu_Y)\\right]}{\\sigma_X \\sigma_Y}.$$\n",
        "\n",
        "However, note that two items will not have ratings in the same positions.\n",
        "\n",
        "![](figs/L20-corr-support.png){width=\"60%\" fig-align=\"center\"}\n",
        "\n",
        "So we want to compute correlation only over the users who rated both the items.\n",
        "\n",
        "## Example\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "![](figs/L20-corr-support.png){width=\"60%\" fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "Let's put the ratings in python lists:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "ratings_item_i = [1, np.nan, np.nan, 5, 5, 3, np.nan, np.nan, np.nan, 4, 2, np.nan, np.nan, np.nan, np.nan, 4, np.nan, 5, 4, 1, np.nan]\n",
        "ratings_item_j = [np.nan, np.nan, 4, 2, 5, np.nan, np.nan, 1, 2, 5, np.nan, np.nan, 2, np.nan, np.nan, 3, np.nan, np.nan, np.nan, 5, 4]\n",
        "\n",
        "display(Markdown(f'Ratings for item $i$:\\n\\n{ratings_item_i}'))\n",
        "display(Markdown(f'Ratings for item $j$:\\n\\n{ratings_item_j}'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example, continued\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "![](figs/L20-corr-support.png){width=\"60%\" fig-align=\"center\"}\n",
        ":::\n",
        "\n",
        "Let's drop the non-common ratings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create new lists where only numbers are kept that are not np.nan in both lists\n",
        "filtered_ratings_item_i = [rating_i for rating_i, rating_j in zip(ratings_item_i, ratings_item_j) if not np.isnan(rating_i) and not np.isnan(rating_j)]\n",
        "filtered_ratings_item_j = [rating_j for rating_i, rating_j in zip(ratings_item_i, ratings_item_j) if not np.isnan(rating_i) and not np.isnan(rating_j)]\n",
        "\n",
        "display(Markdown(f'Common ratings for item $i$: {filtered_ratings_item_i}'))\n",
        "display(Markdown(f'Common ratings for item $j$: {filtered_ratings_item_j}'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example, continued"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display(Markdown(f'Common ratings for item $i$: {filtered_ratings_item_i}'))\n",
        "display(Markdown(f'Common ratings for item $j$: {filtered_ratings_item_j}'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can compute the Pearson correlation coefficient:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rho = np.corrcoef(filtered_ratings_item_i, filtered_ratings_item_j)[0,1]\n",
        "display(Markdown(f'Pearson correlation coefficient: {rho:0.2f}'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Which is a _moderate negative correlation_, meaning that as item $i$ gets rated higher, item $j$ gets rated lower.\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "## Pearson Correlation Coefficient\n",
        "\n",
        "The Pearson correlation coefficient, often denoted as $r$, is a measure of the\n",
        "linear correlation between two variables $X$ and $Y$. It quantifies the degree\n",
        "to which a linear relationship exists between the variables. The value of $r$\n",
        "ranges from -1 to 1, where:\n",
        "\n",
        "- $r = 1$ indicates a perfect positive linear relationship,\n",
        "- $r = -1$ indicates a perfect negative linear relationship,\n",
        "- $r = 0$ indicates no linear relationship.\n",
        "\n",
        "The formula for the Pearson correlation coefficient is:\n",
        "\n",
        "$$\n",
        "r = \\frac{\\sum (X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum (X_i - \\bar{X})^2 \\sum (Y_i - \\bar{Y})^2}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $X_i$ and $Y_i$ are the individual sample points,\n",
        "- $\\bar{X}$ and $\\bar{Y}$ are the means of the $X$ and $Y$ samples, respectively.\n",
        "\n",
        "The Pearson correlation coefficient is sensitive to outliers and assumes that the\n",
        "relationship between the variables is linear and that the data is normally distributed.\n",
        ":::\n",
        "\n",
        "## Similarity for Binary Data\n",
        "\n",
        "In some cases we will need to work with binary $r_{ui}$.  \n",
        "\n",
        "For example, purchase histories on an e-commerce site, or clicks on an ad.\n",
        "\n",
        "In this case, an appropriate replacement for Pearson $r$ is the \n",
        "**Jaccard similarity coefficient** or **Intersection over Union**.\n",
        "\n",
        "$$\n",
        "J_{Sim}(\\mathbf{x}, \\mathbf{y}) = \\frac{|\\mathbf{x} \\cap \\mathbf{y}|}{|\\mathbf{x} \\cup \\mathbf{y}|}.\n",
        "$$\n",
        "\n",
        "See the lecture on [similarity measures](05-Distances-Timeseries.qmd#norms).\n",
        "\n",
        "## Improving CF in the presence of bias\n",
        "\n",
        "One problem with the story so far arises due to __bias__.\n",
        "\n",
        "* Some items are significantly higher or lower rated\n",
        "* Some users rate substantially higher or lower in general\n",
        "\n",
        "These properties interfere with similarity assessment.  \n",
        "\n",
        "Bias correction is crucial for CF recommender systems.\n",
        "\n",
        "We need to include\n",
        "\n",
        "* Per-user offset\n",
        "* Per-item offset\n",
        "* Global offset\n",
        "\n",
        "## Representing biases\n",
        "\n",
        "Hence we need to form a per-item bias of:\n",
        "    \n",
        "$$ \n",
        "b_{ui} = \\mu + \\alpha_u + \\beta_i \n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "* $b_{ui}$ is the bias of user $u$ for item $i$.\n",
        "* $\\mu$ is the global average rating across all items and users.\n",
        "* $\\alpha_u$ is the offset of user $u$ and \n",
        "* $\\beta_i$ is the offset of item $i$.\n",
        "\n",
        "If we gather all these elements together we can form: \n",
        "\n",
        "* $\\boldsymbol{\\alpha}$ an $n\\times 1$ vector of per-user offsets, and\n",
        "* $\\boldsymbol{\\beta}$ an $m\\times 1$ vector of per-item offsets.\n",
        "\n",
        "\n",
        "## Estimating biases\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "$$ \n",
        "b_{ui} = \\mu + \\alpha_u + \\beta_i \n",
        "$$\n",
        "\n",
        ":::\n",
        "\n",
        "How can we estimate the parameters $\\boldsymbol{\\alpha}$, $\\boldsymbol{\\beta}$, and $\\mu$?\n",
        "\n",
        "Let's assume for a minute that we had a fully-dense matrix of ratings $R$.\n",
        "\n",
        "Recall that each of the $m$ rows of $R$ represents an item and each of the $n$ columns represents a user.\n",
        "\n",
        "![](figs/L20-u-u-cf-1.png){width=\"40%\" fig-align=\"center\"}\n",
        "\n",
        "## Estimating biases, continued\n",
        "\n",
        "One way to do this is to minimize the squared error between the ratings and the biases:\n",
        "\n",
        "$$\n",
        "\\min_{\\boldsymbol{\\alpha},\\boldsymbol{\\beta},\\mu} \n",
        "\\Vert R - \\mathbf{1}\\boldsymbol{\\alpha}^T + \\boldsymbol{\\beta}\\mathbf{1}^T + \\mu1\\Vert^2 \n",
        "+ \\lambda(\\Vert\\boldsymbol{\\alpha}\\Vert^2 + \\Vert\\boldsymbol{\\beta}\\Vert^2).\n",
        "$$\n",
        "\n",
        "and include a regularization term to minimize the magnitude of the biases.\n",
        "\n",
        "* Here, bold-faced $\\mathbf{1}$ represents appropriately sized vectors of ones,\n",
        "and non-boldfaced $1$ is an $m\\times n$ matrix of ones.\n",
        "\n",
        "* So $\\mathbf{1}\\boldsymbol{\\alpha}^T$ is an $m\\times n$ matrix where each row is the bias for a user.\n",
        "\n",
        "* Similarly, $\\boldsymbol{\\beta}\\mathbf{1}^T$ is an $m\\times n$ matrix where each column is the bias for an item.\n",
        "\n",
        "* And $\\mu1$ is an $m\\times n$ matrix where each element is $\\mu$.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "\n",
        "## Estimating biases, continued\n",
        "\n",
        "$$\n",
        "\\min_{\\alpha,\\beta,\\mu} \n",
        "\\Vert R - \\mathbf{1}\\alpha^T + \\beta\\mathbf{1}^T + \\mu1\\Vert^2 \n",
        "+ \\lambda(\\Vert\\alpha\\Vert^2 + \\Vert\\beta\\Vert^2) \n",
        "$$\n",
        ":::\n",
        "\n",
        "While this is not a simple ordinary least squares problem, there is a strategy for solving it.\n",
        "\n",
        "Assume we hold $\\beta\\mathbf{1}^T$ and $\\mu1$ constant.  \n",
        "\n",
        "Then the remaining problem is \n",
        "\n",
        "$$\n",
        "\\min_{\\alpha} \\Vert R - \\mathbf{1}\\alpha^T \\Vert^2 + \\lambda \\Vert\\alpha\\Vert^2,\n",
        "$$\n",
        "\n",
        "which (for each column of $R$) is a standard regularized least squares problem\n",
        "solved via [Ridge regression](./19-Regression-III-More-Linear.qmd#ridge-regression).\n",
        "\n",
        "## Aside: Ridge Regression\n",
        "\n",
        "**Ridge Regression** is a regularized least squares method that adds a penalty term to prevent overfitting.\n",
        "\n",
        "**Standard Least Squares:**\n",
        "$$\n",
        "\\min_{\\boldsymbol{\\beta}} \\Vert X\\boldsymbol{\\beta} - \\mathbf{y}\\Vert_2^2\n",
        "$$\n",
        "\n",
        "**Ridge Regression:**\n",
        "$$\n",
        "\\min_{\\boldsymbol{\\beta}} \\Vert X\\boldsymbol{\\beta} - \\mathbf{y}\\Vert_2^2 + c\\Vert\\boldsymbol{\\beta}\\Vert_2^2\n",
        "$$\n",
        "\n",
        "The penalty term $c\\Vert\\boldsymbol{\\beta}\\Vert_2^2$ shrinks the coefficients $\\boldsymbol{\\beta}$ towards zero.\n",
        "\n",
        "---\n",
        "\n",
        "**Why Ridge Regression?**\n",
        "\n",
        "- Addresses **multicollinearity** (when features are highly correlated)\n",
        "- Prevents coefficient magnitudes from becoming too large\n",
        "- Always has a unique solution: $\\hat{\\boldsymbol{\\beta}} = (X^TX + cI)^{-1}X^T\\mathbf{y}$\n",
        "\n",
        "**The hyperparameter $c$:**\n",
        "\n",
        "- When $c = 0$: ordinary least squares\n",
        "- When $c \\rightarrow \\infty$: all coefficients shrink to zero\n",
        "- Choose $c$ via cross-validation to balance fitting vs. regularization\n",
        "\n",
        "## Back to Solving the problem\n",
        "\n",
        "This sort of problem is called __jointly convex__ in that it is convex in each of the variables $\\alpha$, $\\beta$, and $\\mu$.\n",
        "\n",
        "The strategy for solving is:\n",
        "    \n",
        "1. Hold $\\alpha$ and $\\beta$ constant, solve for $\\mu$.\n",
        "2. Hold $\\alpha$ and $\\mu$ constant, solve for $\\beta$.\n",
        "3. Hold $\\beta$ and $\\mu$ constant, solve for $\\alpha$.\n",
        "\n",
        "Each of the three steps will reduce the overall error.  As a result, we iterate over them until convergence.\n",
        "\n",
        "---\n",
        "\n",
        "The last issue is that the matrix $R$ is not dense - in reality we only have a small subset of its entries.\n",
        "\n",
        "We simply need to adapt the least-squares solution to only consider the entries in $R$ that we know.\n",
        "\n",
        "As a result, the actual calculation is as follows...\n",
        "\n",
        "---\n",
        "\n",
        "Step 1:\n",
        "\n",
        "$$\n",
        "\\mu = \\frac{\\sum_{(u, i) \\in R} (r_{ui} - \\alpha_u - \\beta_i)}{|R|} \n",
        "$$\n",
        "\n",
        "Step 2: \n",
        "\n",
        "$$ \n",
        "\\alpha_u = \\frac{\\sum_{i \\in R(u)}(r_{ui} - \\mu - \\beta_i)}{\\lambda + |R(u)|} \n",
        "$$\n",
        "\n",
        "Step 3:\n",
        "    \n",
        "$$ \n",
        "\\beta_i = \\frac{\\sum_{u \\in R(i)}(r_{ui} - \\mu - \\alpha_u)}{\\lambda + |R(i)|} \n",
        "$$\n",
        "\n",
        "Step 4: If not converged, go to Step 1.\n",
        "\n",
        "Here $i \\in R(u)$ means the set of items rated by user $u$ and $u \\in R(i)$ means\n",
        "the set of users who have rated item $i$ and $|R(u)|$ is the number of ratings.\n",
        "\n",
        "---\n",
        "\n",
        "Now that we have learned the biases, we can do a better job of estimating correlation:\n",
        "\n",
        "$$ \n",
        "\\hat{\\rho}_{ij} = \\frac{\\sum_{u\\in U(i,j)}(r_{ui} - b_{ui})(r_{uj}-b_{uj})} \n",
        "{\\sqrt{\\sum_{u\\in U(i,j)}(r_{ui} - b_{ui})^2\\sum_{u\\in U(i,j)}(r_{uj}-b_{uj})^2}},\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "* $b_{ui} = \\mu + \\alpha_u + \\beta_i$, and\n",
        "* $U(i,j)$ are the users who have rated both $i$ and $j$.\n",
        "\n",
        "---\n",
        "\n",
        "And using biases we can also do a better job of estimating ratings:\n",
        "\n",
        "$$ \n",
        "\\hat{r}_{ui} = b_{ui} + \\frac{\\sum_{j \\in n_k(i, u)} s_{ij}(r_{uj} - b_{uj})}{\\sum_{j \\in n_k(i, u)} s_{ij}},\n",
        "$$\n",
        "\n",
        "where \n",
        "\n",
        "* $b_{ui} = \\mu + \\alpha_u + \\beta_i$,\n",
        "* $n_k(i, u)$ are the $k$ nearest neighbors to $i$ that were rated by user $u$ and\n",
        "* $s_{ij}$ is the similarity between items $i$ and $j$, estimated as above.\n",
        "\n",
        "## Negative Similarity Scores?\n",
        "\n",
        "When using correlation coefficient as the similarity score, negative values have important implications for the weighted calculation.\n",
        "\n",
        "Looking at the weighted rating formula:\n",
        "\n",
        "$$ \n",
        "\\hat{r}_{ui} = b_{ui} + \\frac{\\sum_{j \\in n_k(i, u)} s_{ij}(r_{uj} - b_{uj})}{\\sum_{j \\in n_k(i, u)} s_{ij}},\n",
        "$$\n",
        "\n",
        "\n",
        "## How Negative Similarity Scores Work\n",
        "\n",
        "When $s_{ij}$ is **negative** (negative correlation), here's what happens:\n",
        "\n",
        "1. **Numerator**: The term $s_{ij}(r_{uj} - b_{uj})$ becomes negative when $s_{ij} < 0$. This means:\n",
        "   - If user $u$ rated item $j$ **above** its bias ($r_{uj} - b_{uj} > 0$), the negative similarity will push the prediction for item $i$ **down**\n",
        "   - If user $u$ rated item $j$ **below** its bias ($r_{uj} - b_{uj} < 0$), the negative similarity will push the prediction for item $i$ **up**\n",
        "   \n",
        "   This makes intuitive sense: if items $i$ and $j$ are negatively correlated, then liking one means you're likely to dislike the other.\n",
        "\n",
        "2. **Denominator**: When $s_{ij}$ is negative, it contributes a negative value to the sum in the denominator. This can potentially lead to issues:\n",
        "   - If all similarities are negative, the denominator becomes negative\n",
        "   - If there's a mix of positive and negative similarities, they might partially cancel out, potentially making the denominator close to zero or even zero\n",
        "\n",
        "## Common Solutions\n",
        "\n",
        "To address these issues, practitioners typically use one of these approaches:\n",
        "\n",
        "1. **Use only positive similarities**: Filter out items with negative correlation (only use $k$-NN where $s_{ij} > 0$)\n",
        "\n",
        "2. **Use absolute values in denominator**: $\\displaystyle\\hat{r}_{ui} = b_{ui} + \\frac{\\sum_{j \\in n_k(i, u)} s_{ij}(r_{uj} - b_{uj})}{\\sum_{j \\in n_k(i, u)} |s_{ij}|}$\n",
        "\n",
        "3. **Shift and scale similarities**: Transform correlation values from $[-1, 1]$ to $[0, 1]$ using $s'_{ij} = \\frac{s_{ij} + 1}{2}$\n",
        "\n",
        "The first approach (filtering to positive similarities) is most common because negatively correlated items are typically less informative for prediction than positively correlated items in most domains.\n",
        "\n",
        "\n",
        "## Assessing CF\n",
        "\n",
        "This completes the high level view of CF.\n",
        "\n",
        "Working with user-user similarities is analogous.\n",
        "\n",
        "Strengths:\n",
        "\n",
        "* Essentially no training.\n",
        "    * The reliance on $k$-nearest neighbors helps in this respect.\n",
        "* Easy to update with new users, items, and ratings.\n",
        "* Explainable: \n",
        "    * \"We recommend _Minority Report_ because you liked _Blade Runner_ and _Total Recall._\"\n",
        "\n",
        "Weaknesses:\n",
        "\n",
        "* Accuracy can be a problem -- resulting in poor recommendations\n",
        "* Scalability can be a problem -- compute grows (think $k$-NN)\n",
        "\n",
        "# Matrix Factorization Approaches\n",
        "\n",
        "## Matrix Factorization\n",
        "\n",
        "Note that standard CF forces us to consider similarity among items, __or__ among\n",
        "users, but does not take into account __both.__\n",
        "\n",
        "Can we use both kinds of similarity simultaneously?\n",
        "\n",
        "We can't use both the rows and columns of the ratings matrix $R$ at the same\n",
        "time -- the user and item vectors live in different vector spaces.\n",
        "\n",
        "---\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/L10-Movie-Latent-Space.png){width=\"100%\" fig-align=\"center\" #fig-movie-latent-space}\n",
        "\n",
        "[@koren2009matrix]\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "What we could try to do is find a __single__ vector space in which we represent\n",
        "__both__ users __and__ items, along with a similarity function, such that:\n",
        "\n",
        "* users who have similar item ratings are similar in the vector space\n",
        "* items who have similar user ratings are similar in the vector space\n",
        "* when a given user highly rates a given item, that user and item are similar in the vector space.\n",
        ":::\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "We saw this idea previously, in an SVD lecture.\n",
        "\n",
        "This new vector space is called a __latent__ space, and the user and item\n",
        "representations are called __latent vectors.__\n",
        "\n",
        "This notion of a shared latent space is also central to deep learning \n",
        "recommender approaches [@naumov2019deep] we will look at later.\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "Now, however, we are working with a matrix which is only __partially observed.__\n",
        "That is, we only know __some__ of the entries in the ratings matrix.\n",
        "\n",
        "Nonetheless, we can imagine a situation like this:\n",
        "\n",
        "![](figs/L20-mf-1.png){.lightbox width=\"50%\" fig-align=\"center\"}\n",
        "\n",
        "where we decompose the ratings matrix $R$ into two matrices.\n",
        "\n",
        "We want the product of the two matrices to be as close as possible \n",
        "__to the known values__ of the ratings matrix.\n",
        "\n",
        "---\n",
        "\n",
        "What this setup implies is that our similarity function is the __inner product.__\n",
        "\n",
        "Which means that to predict an unknown rating, we take the __inner product of \n",
        "latent vectors:__\n",
        "\n",
        "![](figs/L20-mf-2.png){width=\"60%\" fig-align=\"center\"}\n",
        "\n",
        "Taking, for example, the 2nd row of \"items\" and the 5th row of \"users\"...\n",
        "\n",
        "---\n",
        "\n",
        "We have\n",
        "\n",
        "$$\n",
        "(-0.5 \\cdot -2)+(0.6 \\cdot 0.3)+(0.5 \\cdot 2.4) = 2.43,\n",
        "$$\n",
        "\n",
        "so:\n",
        "\n",
        "![](figs/L20-mf-3.png){width=\"60%\" fig-align=\"center\"}\n",
        "\n",
        "## Solving Matrix Factorization\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"40%\"}\n",
        "![](figs/L20-mf-1.png){.lightbox width=\"100%\" fig-align=\"center\"}\n",
        ":::\n",
        "::: {.column width=\"60%\"}\n",
        "Notice that in this case we've decided that the factorization should be rank 3,\n",
        "i.e., low-rank.\n",
        "\n",
        "So we want something like an SVD.\n",
        "\n",
        "(Recall that SVD gives us the most-accurate-possible low-rank factorization of a matrix).\n",
        ":::\n",
        "::::\n",
        "\n",
        "However, we can't use the SVD algorithm directly, because we don't know all the entries in $R$. \n",
        "\n",
        "> Indeed, the unseen entries in $R$ are exactly what we want to predict.\n",
        "\n",
        "---\n",
        "\n",
        "Here is what we want to solve: \n",
        "    \n",
        "$$\n",
        "\\min_{U,V} \\Vert (R - UV^T)_S\\Vert^2 + \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2) \n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "* $R$ is $m\\times n$, \n",
        "* $U$ is the $m\\times k$ items matrix,\n",
        "* $V$ is the $n\\times k$ users matrix and\n",
        "* $k$ is the rank of the factorization and dimensionality of the latent space.\n",
        "\n",
        "The $(\\cdot)_S$ notation means that we are only considering the _subset_ of\n",
        "matrix entries that correspond to known reviews (the set $S$).\n",
        "\n",
        "Note that as usual, we add $\\ell_2$ penalization to avoid overfitting\n",
        "([Ridge regression](./19-Regression-III-More-Linear.qmd#ridge-regression)).\n",
        "\n",
        "---\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "$$\n",
        "\\min_{U,V} \\Vert (R - UV^T)_S\\Vert^2 + \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2) \n",
        "$$\n",
        ":::\n",
        "\n",
        "Once again, this problem is __jointly convex__ in that it is convex in each of the variables $U$ and $V$.\n",
        "\n",
        "In particular, if we hold either $U$ or $V$ constant, then the result is a simple\n",
        "ridge regression.\n",
        "\n",
        "---\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "$$\n",
        "\\min_{U,V} \\Vert (R - UV^T)_S\\Vert^2 + \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2) \n",
        "$$\n",
        ":::\n",
        "\n",
        "\n",
        "So one commonly used algorithm for this problem is called __alternating least squares (ALS):__\n",
        "    \n",
        "1. Hold $U$ constant, and solve for $V$\n",
        "2. Hold $V$ constant, and solve for $U$\n",
        "3. If not converged, go to Step 1.\n",
        "\n",
        "The only thing we've left out at this point is how to deal with the missing entries of $R$.  \n",
        "\n",
        "It's not hard, but the details aren't that interesting, so we'll give you code instead!\n",
        "\n",
        "## ALS in Practice\n",
        "\n",
        "The entire Amazon reviews dataset is too large to work with easily, and it is too sparse. \n",
        "\n",
        "Hence, we will take the densest rows and columns of the matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# print(df.shape)\n",
        "\n",
        "# The densest columns: products with more than 50 reviews\n",
        "pids = df.groupby('ProductId').count()['Id']\n",
        "hi_pids = pids[pids > 50].index\n",
        "\n",
        "# reviews that are for these products\n",
        "hi_pid_rec = [r in hi_pids for r in df['ProductId']]\n",
        "\n",
        "# the densest rows: users with more than 50 reviews\n",
        "uids = df.groupby('UserId').count()['Id']\n",
        "hi_uids = uids[uids > 50].index\n",
        "\n",
        "# reviews that are from these users\n",
        "hi_uid_rec = [r in hi_uids for r in df['UserId']]\n",
        "\n",
        "# The result is a list of booleans equal to the number of rewviews\n",
        "# that are from those dense users and movies\n",
        "goodrec = [a and b for a, b in zip(hi_uid_rec, hi_pid_rec)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we create a $\\textnormal{UserID} \\times \\textnormal{ProductID}$ matrix from these reviews.\n",
        "\n",
        "Missing entries will be filled with NaNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dense_df = df.loc[goodrec]\n",
        "good_df = dense_df.loc[~df['Score'].isnull()]\n",
        "R = good_df.pivot_table(columns = 'ProductId', index = 'UserId', values = 'Score')\n",
        "print(f\"Review matrix shape: {R.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "And we can look at a small part of the matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "R.iloc[900:905, 1000:1004]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "We'll use code from the [Antidote Data Framework](https://github.com/rastegarpanah/antidote-data-framework) to do the matrix factorization and ALS. We have local copies \n",
        "[recommender_MF.py](recommender_MF.py), [recommender_als.py](recommender_als.py)\n",
        "and [recommender_lmafit.py](recommender_lmafit.py) in our repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Import local python package MF.py\n",
        "import recommender_MF as MF\n",
        "\n",
        "# Instantiate the model\n",
        "# We are pulling these hyperparameters out of the air -- that's not the right way to do it!\n",
        "RS = MF.als_MF(rank = 20, lambda_ = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "%time pred, error = RS.fit_model(R)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "print(f'RMSE on visible entries (training data): {np.sqrt(error/R.count().sum()):0.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "And we can look at a small part of the predicted ratings matrix and see that it is a dense matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f'Shape of predicted ratings matrix: {pred.shape}')\n",
        "pred.iloc[900:905, 1000:1004]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## todo: hold out test data, compute oos error\n",
        "\n",
        "# We create a mask of the known entries, then calculate the indices of the known\n",
        "# entries, then split that data into training and test sets.\n",
        "\n",
        "# Create a mask for the known entries\n",
        "RN = ~R.isnull()\n",
        "\n",
        "# Get the indices of the known entries\n",
        "visible = np.where(RN)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "import sklearn.model_selection as model_selection\n",
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(visible[0], visible[1], test_size = 0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Just for comparison's sake, let's check the performance of $k$-NN on this dataset.\n",
        "\n",
        "Again, this is only on the training data -- so overly optimistic for sure.\n",
        "\n",
        "And note that this is a subset of the full dataset -- the subset that is \"easiest\" to predict due to density."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Drop the columns that are not features\n",
        "X_train = good_df.drop(columns=['Id', 'ProductId', 'UserId', 'Text', 'Summary'])\n",
        "\n",
        "# The target is the score\n",
        "y_train = good_df['Score']\n",
        "\n",
        "# Using k-NN on features HelpfulnessNumerator, HelpfulnessDenominator, Score, Time\n",
        "model = KNeighborsRegressor(n_neighbors=3).fit(X_train, y_train)\n",
        "%time y_hat = model.predict(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f'RMSE on visible entries (test set): {np.sqrt(mean_squared_error(y_train, y_hat)):.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Assessing Matrix Factorization\n",
        "\n",
        "Matrix Factorization per se is a good idea.    \n",
        "However, many of the improvements we've discussed for CF apply to MF as well.\n",
        "\n",
        "To illustrate, we'll look at some of the successive improvements used by the\n",
        "team that won the Netflix prize (\"BellKor's Pragmatic Chaos\").\n",
        "\n",
        "When the prize was announced, the Netflix supplied solution achieved an RMSE of 0.951.\n",
        "\n",
        "By the end of the competition (about 3 years), the winning team's solution achieved RMSE of 0.856.\n",
        "\n",
        "Let's restate our MF objective in a way that will make things clearer:\n",
        "\n",
        "$$\n",
        "\\min_{U, V} \\sum_{(u, i)\\in S}(r_{ui} - u_u^Tv_i)^2 + \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2) \n",
        "$$\n",
        "\n",
        "where we have written out the vector $\\ell_2$ norm as the summation.\n",
        "\n",
        "## 1. Adding Biases\n",
        "\n",
        "If we add biases:\n",
        "$$ \n",
        "\\min_{U, V} \\sum_{(u, i)\\in S}(r_{ui} - (\\mu + \\alpha_u + \\beta_i + u_u^Tv_i)^2 \n",
        "+ \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2 + \\Vert \\alpha\\Vert^2 + \\Vert \\beta \\Vert^2) \n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "we see improvements in accuracy:\n",
        "\n",
        "![Matrix factorization models’ accuracy. The plots show the root-mean-square error of each of four individual factor models (lower is better). Accuracy improves when the factor model’s dimensionality (denoted by numbers on the charts) increases. In addition, the more refined factor models, whose descriptions involve more distinct sets of parameters, are more accurate.](figs/L20-netflix-1.png){width=\"70%\" fig-align=\"center\" #fig-mf}\n",
        "\n",
        "## 2. Who Rated What?\n",
        "\n",
        "In reality, ratings are not provided __at random.__\n",
        "\n",
        "Take note of which users rated the same movies (ala CF) and use this information.\n",
        "\n",
        "![](figs/L20-netflix-2.png){width=\"70%\" fig-align=\"center\"}\n",
        "\n",
        "---\n",
        "\n",
        "![](figs/L20-netflix-3.png){width=\"70%\" fig-align=\"center\"}\n",
        "\n",
        "## 3. Ratings Change Over Time\n",
        "\n",
        "Older movies tend to get higher ratings!\n",
        "\n",
        "![](figs/L20-netflix-4.png){width=\"60%\" fig-align=\"center\"}\n",
        "\n",
        "---\n",
        "\n",
        "If we add time-varying biases:\n",
        "\n",
        "$$\n",
        "\\min_{U, V} \\sum_{(u, i)\\in S}(r_{ui} - (\\mu + \\alpha_u(t) + \\beta_i(t) + u_u^Tv_i(t))^2 \n",
        "+ \\lambda(\\Vert U\\Vert^2 + \\Vert V\\Vert^2 + \\Vert \\alpha\\Vert^2 + \\Vert \\beta \\Vert^2) \n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "we see further improvements in accuracy:\n",
        "\n",
        "![](figs/L20-netflix-5.png){width=\"50%\" fig-align=\"center\"}\n",
        "\n",
        "To estimate these billions of parameters, we cannot use alternating least squares or any linear algebraic method.\n",
        "\n",
        "We need to use gradient descent (which we covered previously).\n",
        "\n",
        "\n",
        "## Recap\n",
        "\n",
        "* Introduction to recommender systems and their importance in modern society.\n",
        "* Explanation of collaborative filtering (CF) and its two main approaches: user-user similarity and item-item similarity.\n",
        "* Discussion on the challenges of recommender systems, including scalability and data sparsity.\n",
        "* Introduction to matrix factorization (MF) as an improvement over CF, using latent vectors and alternating least squares (ALS) for optimization.\n",
        "* Practical implementation of ALS for matrix factorization on a subset of Amazon movie reviews.\n",
        "\n",
        "## References\n",
        "\n",
        "::: {#refs}\n",
        ":::\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}