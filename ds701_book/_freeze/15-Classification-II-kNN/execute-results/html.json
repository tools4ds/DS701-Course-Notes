{
  "hash": "879127298445ff4cbb4a0b99d2864452",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: $k$-Nearest Neighbors, Curse of Dimensionality, and Model Selection\njupyter: python3\nfig-align: center\n---\n\n## Introduction \n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/15-Classification-II-kNN.ipynb)\n\n\n\nIn this lecture we'll introduce another classification technique, $k$-Nearest Neighbors.\n\nFirst we'll introduce the notion of **parametric** and **nonparametric** models.\n\n## Parametric vs. Nonparametric Models\n\nThere are many ways to define models (whether supervised or unsupervised).\n\nHowever a key distinction is this: \n\n* does the model have a _fixed number_ of parameters or \n* does the number of parameters _grow with the training data_.\n\n::: {.incremental}\n* **Parametric:** If the model has a __fixed number__ of parameters, it is called __parametric.__\n* **Nonparametric:** If the number of parameters grows with the data, the model is called __nonparametric.__\n:::\n\n---\n\n**Parametric models** \n\n* have fixed parameters independent of the training data\n* have the advantage of (often) being faster to use\n* have the disadvantage of making strong assumptions about the nature of the underlying data distributions.\n\n**Nonparametric models**\n\n* have parameters that grow with the size of the training data\n* are more flexible\n* can be computationally intractable for large datasets\n\n\n## Parametric Models\n- **Linear Regression**: Assumes a linear relationship between the input variables and the output.\n- **Logistic Regression**: Used for binary classification problems.\n- **Polynomial Regression**: Extends linear regression by considering polynomial relationships.\n- **Support Vector Machines (SVM)**: With a linear kernel.\n- **Naive Bayes**: Based on Bayes' theorem with strong independence assumptions.\n- **Generalized Linear Models (GLM)**: Extends linear models to allow for response variables that have error distribution models other than a normal distribution.\n\n## Nonparametric Models\n- **K-Nearest Neighbors (KNN)**: Classifies a data point based on how its neighbors are classified.\n- **Decision Trees**: Splits the data into subsets based on the value of input features.\n- **Random Forest**: An ensemble of decision trees.\n- **Support Vector Machines (SVM)**: With non-linear kernels like RBF.\n\n\n\n## $k$-Nearest Neighbors\n\n> When I see a bird that walks like a duck and swims like a duck and quacks like a duck, I call that bird a duck.\n\n--James Whitcomb Riley (1849 - 1916)\n\n![](figs/L15-duck.jpg){width=\"100px\"}\n\n\nLike any classifier, $k$-Nearest Neighbors is trained by providing it a set of labeled data.\n\nHowever, at training time, the classifier does very little. It just stores away the training data.\n\n## Example: Training Points - 2 Classes\n\n::: {#479e0ea7 .cell execution_count=4}\n``` {.python .cell-code}\nplt.figure(figsize=(5, 5))\ndemo_y = [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\ndemo_X = np.array([[-3,1], [-2, 4], [-2, 2], [-1.5, 1], [-1, 3], [0, 0], [1, 1.5], [2, 0.5], [2, 3], [2, 0], [3, 1], [4, 4], [0, 1]])\ntest_X = [-0.3, 0.7]\n#\nplt.scatter(demo_X[:,0], demo_X[:,1], c=demo_y, cmap=cmap_bold, s=80)\nplt.axis('equal')\nplt.axis('off')\n# plt.title('Training Points: 2 Classes')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-4-output-1.png){width=391 height=389 fig-align='center'}\n:::\n:::\n\n\n--- \n\n::: {#1249833e .cell execution_count=5}\n``` {.python .cell-code}\nplt.figure(figsize=(5, 5))\nplt.scatter(demo_X[:, 0], demo_X[:, 1], c=demo_y, cmap=cmap_bold, s=80)\nplt.plot(test_X[0], test_X[1], 'ok', markersize=10)\nplt.annotate('Test Point', test_X, [75, 25], \n             textcoords = 'offset points', fontsize = 14, \n             arrowprops = {'arrowstyle': '->'})\nplt.axis('equal')\nplt.axis('off')\n# plt.title('Training Points: 2 Classes')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-5-output-1.png){width=391 height=389 fig-align='center'}\n:::\n:::\n\n\nAt test time, simply \n\n* \"look at\" the $k$ points in the training set that are nearest to the test input $x$, and \n* make a decision based on the labels on those points, e.g. majority vote\n\n## 1-Nearest Neighbor\n\n::: {#ece362b0 .cell execution_count=6}\n``` {.python .cell-code}\nplt.figure(figsize=(5, 5))\nplt.scatter(demo_X[:, 0], demo_X[:, 1], c=demo_y, cmap=cmap_bold, s=80)\nplt.plot(test_X[0], test_X[1], 'ok', markersize=10)\nax=plt.gcf().gca()\ncircle = mp.patches.Circle(test_X, 0.5, facecolor = 'red', alpha = 0.2)\nplt.axis('equal')\nplt.axis('off')\nax.add_artist(circle)\n# plt.title('1-Nearest-Neighbor: Classification: Red')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-6-output-1.png){width=391 height=389 fig-align='center'}\n:::\n:::\n\n\n## 2-Nearest Neighbors\n\n::: {#c2293154 .cell execution_count=7}\n``` {.python .cell-code}\nplt.figure(figsize=(5, 5))\nplt.scatter(demo_X[:,0], demo_X[:,1], c=demo_y, cmap=cmap_bold, s=80)\ntest_X = [-0.3, 0.7]\nplt.plot(test_X[0], test_X[1], 'ok', markersize=10)\nax=plt.gcf().gca()\ncircle = mp.patches.Circle(test_X, 0.9, facecolor = 'gray', alpha = 0.3)\nplt.axis('equal')\nplt.axis('off')\nax.add_artist(circle)\n# plt.title('2-Nearest-Neighbor')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-7-output-1.png){width=391 height=389 fig-align='center'}\n:::\n:::\n\n\n## 3-Nearest Neighbors\n\n::: {#687420aa .cell execution_count=8}\n``` {.python .cell-code}\nplt.figure(figsize=(5, 5))\nax=plt.gcf().gca()\ncircle = mp.patches.Circle(test_X, 1.4, facecolor = 'blue', alpha = 0.2)\nax.add_artist(circle)\nplt.scatter(demo_X[:,0], demo_X[:,1], c=demo_y, cmap=cmap_bold, s=80)\ntest_X = [-0.3, 0.7]\nplt.plot(test_X[0], test_X[1], 'ok', markersize=10)\nplt.axis('equal')\nplt.axis('off')\n# plt.title('3-Nearest-Neighbor: Classification: Blue')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-8-output-1.png){width=470 height=470 fig-align='center'}\n:::\n:::\n\n\n## Hard vs. Soft Classification\n\n$k$-Nearest Neighbors can do either __hard__ or __soft__ classification.\n\n* As a **hard classifier**, it returns the majority vote of the labels on the $k$ Nearest Neighbors.\n\nWhich may be indeterminate in the case of a tie.\n\n::: {.fragment}\n* As a **soft classifier**, it returns the fraction of points in the neighborhood with label $c$.\n    \n$$\np(x = c\\,|\\,\\mathbf{x}, k) = \\frac{\\text{number of points in neighborhood with label } c}{k} \n$$\n\n:::\n\n## Model Selection for $k$-NN\n\n::: {.incremental}\n* Each value of $k$ results in a different \"model\".\n\n* The complexity of the resulting model is therefore controlled by the hyperparameter $k$.\n\n* We will want to select $k$ using held-out data to avoid over-fitting.\n:::\n\n## Varying $k$\n\nConsider this dataset where items fall into three classes:\n\n::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {#247c1fa3 .cell execution_count=9}\n``` {.python .cell-code}\nimport sklearn.datasets as sk_data\nX, y = sk_data.make_blobs(n_samples=150, \n                          centers=[[-2, 0],[1, 5], [2.5, 1.5]],\n                          cluster_std = [2, 2, 3],\n                          n_features=2,\n                          center_box=(-10.0, 10.0),random_state=0)\nplt.figure(figsize = (5, 5))\nplt.axis('equal')\nplt.axis('off')\nplt.scatter(X[:, 0], X[:, 1], c = y, cmap = cmap_bold, s = 80)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-9-output-1.png){width=391 height=389}\n:::\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\n\n* Let's observe how the complexity of the resulting model changes as we vary $k$.\n\n* We'll do this by plotting the __decision regions__.   These show how the method would classify each potential test point in the space.\n\n:::\n:::\n\n---\n\n::: {#976b1df2 .cell execution_count=10}\n``` {.python .cell-code}\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nh = .1  # step size in the mesh\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                      np.arange(y_min, y_max, h))\n```\n:::\n\n\n::: {#738214d4 .cell execution_count=11}\n``` {.python .cell-code}\nf, axs = plt.subplots(1, 3, figsize=(15, 5))\nfor i, k in enumerate([1, 5, 25]):\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X, y)\n    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    axs[i].pcolormesh(xx, yy, Z, cmap = cmap_light, shading = 'auto')\n    axs[i].axis('equal')\n    axs[i].axis('off')\n    axs[i].set_title(f'Decision Regions for $k$ = {k}')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-11-output-1.png){width=1135 height=410}\n:::\n:::\n\n\nNotice how increasing $k$ results in smoother decision boundaries.\n\nThese are more likely to show good generalization ability.\n\n## Challenges for $k$-NN\n\nWorking with a $k$-NN classifier can involve some challenges.\n\n::: {.incremental}\n1. The computational cost of classification grows linearly with the size of the training data. Note that the training step is trivial, but the classification step can be prohibitively expensive.\n\n2. Euclidean distance is the most common distance function used in $k$-NN so data scaling is important. As previously\n   [discussed](https://tools4ds.github.io/DS701-Course-Notes/06-Clustering-I-kmeans.html#feature-scales), features\n   should be scaled to prevent distance measures from being dominated by a potentially small subset of features.\n\n3. The __curse of dimensionality.__ If the training data lives in a high dimensional space, Euclidean distance measures become less effective.\n:::\n\n::: {.fragment}\nThis last point is subtle but important, so we will now look at the curse of dimensionality more closely.\n:::\n\n# The Curse of Dimensionality\n\n## The Curse of Dimensionality\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n![](figs/L15-curse-frankenstein-edited.jpeg){width=\"300px\"}\n:::\n::: {.column width=\"60%\"}\n\n<br><br>\nThe **curse of dimensionality** is a somewhat tongue-in-cheek term for serious problems that arise when we use geometric algorithms in high dimensions.\n:::\n::::\n\nThere are various aspects of the curse that affect $k$-NN.\n\n::: aside\nThe phrase `curse of dimensionality' was coined by the mathematician Richard Bellman in 1957 in his book _Dynamic Programming._\n:::\n\n\n## Points are far apart in high D\n\n* $k$-NN relies on there being one or more \"close\" points to the test point $x$.   \n\n* In other words, we need the training data to be relatively dense, so there are \"close\" points everywhere.\n\n* Unfortunately, the amount of space we work in grows exponentially with the dimension $d$.\n\n::: {.content-visible when-profile=\"slides\"}\n## Points are far apart in high D\n:::\n\n::: {layout-ncol=\"2\"}\n\n![10x10 grid with $10^2$ bins](figs/L15-10x10-grid.png){width=\"170px\"}  \n\n![10x10x10 grid with $10^3$ bins](figs/L15-10x10x10-grid.png){width=\"170px\"}\n\n:::\n\n::: {.incremental}\n* Imagine we have 10,000 (e.g. $10^4$) data points each with 40 features (e.g. $d = 40$).\n* Consider quantizing each dimension into 10 bins.\n* So in 40-D we have $10^{40}$ possible bins.\n* On average, we will only have 1 data point per $10^{36}$ bins. Most will be empty!!\n* So the amount of data we need to maintain a given density also grows exponentially with dimension $d$.\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Points are far apart in high D\n:::\n\nOne very intuitive way to think about it is this.\n\n>In order for two points to be close in $\\mathbb{R}^d$, they must be close in __each__ of the $d$ dimensions.\n>As the number of dimensions grows, it becomes harder and harder for a pair of points to be close in __each__ dimension.\n\n## Distances shrink in high dimensions\n\n* Contrarily, distances shrink in high dimensions. An example:\n\n* Let's generate 1000 normally distributed points in 2D, 3D, 100D, and 1000D and\nlook at the ratio of the largest to smallest distance between points.\n\n::: {#35c86f81 .cell execution_count=12}\n``` {.python .cell-code}\nimport numpy as np\n\n# Fix the random seed so we all have the same random numbers\nnp.random.seed(0)\n\nn_data = 1000\n\n# Create 1000 data examples (columns) each with 2 dimensions (rows)\nn_dim = 2\nx_2D = np.random.normal(size=(n_data, n_dim))\n\n# Create 1000 data examples (columns) each with 3 dimensions (rows)\nn_dim = 3\nx_3D = np.random.normal(size=(n_data, n_dim))\n\n# Create 1000 data examples (columns) each with 100 dimensions (rows)\nn_dim = 100\nx_100D = np.random.normal(size=(n_data, n_dim))\n\n# Create 1000 data examples (columns) each with 1000 dimensions (rows)\nn_dim = 1000\nx_1000D = np.random.normal(size=(n_data, n_dim))\n```\n:::\n\n\n##  Scatter plot of 2D data\n\n::: {#27c81643 .cell execution_count=13}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# scatter plot of the 2D data\nplt.scatter(x_2D[:,0], x_2D[:,1])\nplt.title('2D data')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-13-output-1.png){width=792 height=431 fig-align='center'}\n:::\n:::\n\n\n## Scatter plot of 3D data\n\n::: {#abc083a9 .cell execution_count=14}\n``` {.python .cell-code}\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(x_3D[:,0], x_3D[:,1], x_3D[:,2])\nplt.title('3D data')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-14-output-1.png){width=402 height=417 fig-align='center'}\n:::\n:::\n\n\n## Closest/farthest distance ratios\n\nDefine a function to calculate the ratio of the largest to smallest distance between points in a dataset.\n\n::: {#9e4744a5 .cell execution_count=15}\n``` {.python .cell-code}\nfrom scipy.spatial import distance\ndef distance_ratio(x, metric='euclidean'):\n\n    if metric == 'euclidean':\n        ord = 2\n    elif metric == 'manhattan':\n        ord = 1\n    elif metric == 'cosine':\n        pass\n    else:\n        raise ValueError(f\"Metric {metric} not supported\")\n\n    smallest_dist = np.inf\n    largest_dist = 0\n    for i in range(x.shape[0]):\n        for j in range(i + 1, x.shape[0]): # start from i+1 to avoid redundant calcuations\n            if i != j:\n                if metric == 'euclidean' or metric == 'manhattan':\n                    dist = np.linalg.norm(x[i,:] - x[j,:], ord=ord) \n                elif metric == 'cosine':\n                    dist = distance.cosine(x[i,:].flatten(), x[j,:].flatten())\n            if dist < smallest_dist:\n              smallest_dist = dist\n            if dist > largest_dist:\n              largest_dist = dist\n\n    # print(f\"smallest_dist = {smallest_dist}, largest_dist = {largest_dist}\")\n\n    # Calculate the ratio and return\n    dist_ratio = largest_dist / smallest_dist\n    return dist_ratio\n```\n:::\n\n\nAnd then calculate the Euclidean distance ratio for each dataset.\n\n::: {#47218031 .cell execution_count=16}\n``` {.python .cell-code}\ndist_ratio_2d = distance_ratio(x_2D)\nprint('Ratio of largest to smallest distance 2D: %3.3f'%(dist_ratio_2d))\n\ndist_ratio_3d = distance_ratio(x_3D)\nprint('Ratio of largest to smallest distance 3D: %3.3f'%(dist_ratio_3d))\n\ndist_ratio_100d = distance_ratio(x_100D)\nprint('Ratio of largest to smallest distance 100D: %3.3f'%(dist_ratio_100d))\n\ndist_ratio_1000d = distance_ratio(x_1000D)\nprint('Ratio of largest to smallest distance 1000D: %3.3f'%(dist_ratio_1000d))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRatio of largest to smallest distance 2D: 2699.982\nRatio of largest to smallest distance 3D: 356.654\nRatio of largest to smallest distance 100D: 1.974\nRatio of largest to smallest distance 1000D: 1.228\n```\n:::\n:::\n\n\n## Manhattan distance ratio\n\n::: {#870bd806 .cell execution_count=17}\n``` {.python .cell-code}\ndist_ratio_2d_manhattan = distance_ratio(x_2D, metric='manhattan')\nprint('Ratio of largest to smallest distance 2D (Manhattan): %3.3f'%(dist_ratio_2d_manhattan))\n\ndist_ratio_3d_manhattan = distance_ratio(x_3D, metric='manhattan')\nprint('Ratio of largest to smallest distance 3D (Manhattan): %3.3f'%(dist_ratio_3d_manhattan))\n\ndist_ratio_100d_manhattan = distance_ratio(x_100D, metric='manhattan')\nprint('Ratio of largest to smallest distance 100D (Manhattan): %3.3f'%(dist_ratio_100d_manhattan))\n\ndist_ratio_1000d_manhattan = distance_ratio(x_1000D, metric='manhattan')\nprint('Ratio of largest to smallest distance 1000D (Manhattan): %3.3f'%(dist_ratio_1000d_manhattan))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRatio of largest to smallest distance 2D (Manhattan): 3694.569\nRatio of largest to smallest distance 3D (Manhattan): 315.760\nRatio of largest to smallest distance 100D (Manhattan): 2.032\nRatio of largest to smallest distance 1000D (Manhattan): 1.255\n```\n:::\n:::\n\n\n## Cosine distance ratio\n\n::: {#eaa35bda .cell execution_count=18}\n``` {.python .cell-code}\ndist_ratio_2d_cosine = distance_ratio(x_2D, metric='cosine')\nprint('Ratio of largest to smallest distance 2D (Cosine): %3.3f'%(dist_ratio_2d_cosine))\n\ndist_ratio_3d_cosine = distance_ratio(x_3D, metric='cosine')\nprint('Ratio of largest to smallest distance 3D (Cosine): %3.3f'%(dist_ratio_3d_cosine))\n\ndist_ratio_100d_cosine = distance_ratio(x_100D, metric='cosine')\nprint('Ratio of largest to smallest distance 100D (Cosine): %3.3f'%(dist_ratio_100d_cosine))\n\ndist_ratio_1000d_cosine = distance_ratio(x_1000D, metric='cosine')\nprint('Ratio of largest to smallest distance 1000D (Cosine): %3.3f'%(dist_ratio_1000d_cosine))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRatio of largest to smallest distance 2D (Cosine): 76853891711.879\nRatio of largest to smallest distance 3D (Cosine): 251995.519\nRatio of largest to smallest distance 100D (Cosine): 2.606\nRatio of largest to smallest distance 1000D (Cosine): 1.351\n```\n:::\n:::\n\n\n## Euclidean distance ratios\n\n::: {#c8e4e889 .cell execution_count=19}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 5))\nplt.scatter([2,3,100,1000], [dist_ratio_2d, dist_ratio_3d, dist_ratio_100d, dist_ratio_1000d])\nplt.plot([2,3,100,1000], [dist_ratio_2d, dist_ratio_3d, dist_ratio_100d, dist_ratio_1000d], '--', color='lightgray')\nplt.xscale('log')\nplt.title('Euclidean Distance ratio')\nplt.xlabel('Dimension')\nplt.ylabel('Distance ratio')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-19-output-1.png){width=676 height=451 fig-align='center'}\n:::\n:::\n\n\nYou see that the ratio of the largest to smallest distance drops dramatically as the dimension increases.\n\n## Compare distance ratios\n\nLet's compare the distance ratios for the Euclidean, Manhattan, and Cosine metrics in 100d and 1000d all in the same plot.\n\n::: {#9ec224cb .cell execution_count=20}\n``` {.python .cell-code}\nplt.figure(figsize=(8, 5))\nplt.scatter([100, 1000], [dist_ratio_100d, dist_ratio_1000d], color='blue', s=80, label='Euclidean')\nplt.scatter([100, 1000], [dist_ratio_100d_manhattan, dist_ratio_1000d_manhattan], color='red', s=80, label='Manhattan')\nplt.scatter([100, 1000], [dist_ratio_100d_cosine, dist_ratio_1000d_cosine], color='green', s=80, label='Cosine')\nplt.xscale('log')\nplt.title('Distance ratio for different metrics')\nplt.xlabel('Dimension')\nplt.ylabel('Distance ratio')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-20-output-1.png){width=663 height=451 fig-align='center'}\n:::\n:::\n\n\n## Volume of a hypersphere\n\nThe above phenomenon is related to the strange phenomenon of the volume of a\nhypersphere (or n-ball) in increasing dimensions.\n\n::: aside\nFor a fascinating exploration of the properties of unit spheres in high dimension,\nsee [An Adventure in the Nth Dimension](https://www.americanscientist.org/article/an-adventure-in-the-nth-dimension) \nby Brian Hayes in _American Scientist._\n:::\n\nNow, the volume of a _hypersphere_ in 2D and 3D is:\n\n* For $d = 2$, $\\pi r^2$,  which is $\\pi$ for a unit radius.\n* For $d = 3$, $\\frac{4\\pi}{3} r^3$, which is $\\frac{4}{3} \\pi$ for a unit radius.\n\nThe [general equation](https://en.wikipedia.org/wiki/Volume_of_an_n-ball#Formulas)\nfor a hypersphere of radius $R$ in $d$ dimensions is:\n\n$$\nV_d(R) = \\frac{\\pi^{d/2}}{\\Gamma\\bigl(\\tfrac d2 + 1\\bigr)}R^d,\n$$\n\nwhere $\\Gamma$  is Euler's gamma function and simplifies to $\\Gamma(d) = (d - 1)!$ for all positive integers $d$ and has a compact representation for postive half integers.\n\n## Hypersphere volume in increasing dimensions\n\nHere's a function to calculate the volume of a hypersphere of a given radius and dimension.\n\n::: {#e9e083b2 .cell execution_count=21}\n``` {.python .cell-code}\nimport scipy.special as sci\n\ndef volume_of_hypersphere(radius, dimensions):\n    pi = np.pi\n    volume = (pi ** (dimensions / 2)) / (sci.gamma(dimensions / 2 + 1)) * (radius ** dimensions)\n    return volume\n```\n:::\n\n\nAnd now let's calculate the volume of a hypersphere of unit radius for dimensions 1 to 20 and store it in a list.\n\n::: {#dd337f84 .cell execution_count=22}\n``` {.python .cell-code}\nradius = 1.0\nvols = []\nfor c_dim in range(1,21):\n  vols.append(volume_of_hypersphere(radius, c_dim))\n  print(f\" {c_dim}: {volume_of_hypersphere(radius, c_dim):.1f}\", end=', ')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n 1: 2.0,  2: 3.1,  3: 4.2,  4: 4.9,  5: 5.3,  6: 5.2,  7: 4.7,  8: 4.1,  9: 3.3,  10: 2.6,  11: 1.9,  12: 1.3,  13: 0.9,  14: 0.6,  15: 0.4,  16: 0.2,  17: 0.1,  18: 0.1,  19: 0.0,  20: 0.0, \n```\n:::\n:::\n\n\n## Plot volumes of hyperspheres\n\nAnd plot the results.\n\n::: {#4f449ebd .cell execution_count=23}\n``` {.python .cell-code}\n# plot vols\nplt.scatter(range(1,21), vols)\nplt.xlabel('Dimensions')\nplt.ylabel('Volume')\nplt.title('Volume of unit radius hypersphere')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-23-output-1.png){width=799 height=449}\n:::\n:::\n\n\nThe volume decreases to almost nothing in high dimensions.  \n\n## Ratio of unit hypersphere volume and enclosing hypercube volume\n\nAnother strange phenomenon to think about is via another example.\n\nAssume you have data _uniformly_ distributed in a hypercube with side length 2 which will exactly fit a unit hypersphere.\n\n![](figs/L15-circle-in-square.png){fig-align=\"center\"}\n\nAs the dimension grows, we want to know what fraction of the data is within unit distance of the center.\n\n::: {#da9faaa3 .cell execution_count=24}\n``` {.python .cell-code}\nside_length = 2\ncube_volumes = [(lambda side, dim: side ** dim)(side_length, dim) for dim in range(1, 21)]\n\nvol_ratios = [vols[i] / cube_volumes[i] for i in range(len(vols))]\n```\n:::\n\n\n## Plot volume ratios\n\n::: {#ace84590 .cell execution_count=25}\n``` {.python .cell-code}\nplt.scatter(range(1,21), vol_ratios)\nplt.xlabel('Dimensions')\nplt.ylabel('Volume ratio')\nplt.title('Volume of unit radius hypersphere / Volume of enclosing hypercube')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-25-output-1.png){width=812 height=449}\n:::\n:::\n\n\nThe fraction quickly goes to zero as the dimension increases.\n\nPerhaps an intuition is to think about the space in the \"corners\" of the hypercube.\nAs the dimension grows, there are more corners.\n\n## Proportion of hypersphere in outer shell\n\nAnother strange thing about high dimensions is that most of the volume of a hypersphere is in the outer shell.\n\nWe can compare the volumes of hyperspheres of radius $1$ and $1-\\epsilon$ for a small $\\epsilon$.\n\n::: {#c7203be0 .cell execution_count=26}\n``` {.python .cell-code}\nax = plt.figure(figsize = (4, 4)).add_subplot(projection = '3d')\n# coordinates of sphere surface\nu, v = np.mgrid[0:2*np.pi:50j, 0:np.pi:50j]\nx = np.cos(u)*np.sin(v)\ny = np.sin(u)*np.sin(v)\nz = np.cos(v)\n#\nax.plot_surface(x, y, z, color='r', alpha = 0.2)\ns3 = 1/np.sqrt(3)\nax.quiver(0, 0, 0, s3, s3, s3, color = 'b')\nax.text(s3/2, s3/2, s3/2-0.2, '1', size = 14)\n#\neps = 0.9\n#\nax.plot_surface(eps * x, eps * y, eps * z, color='b', alpha = 0.2)\nax.quiver(0, 0, 0, eps, 0, 0, color = 'k')\nax.text(1/2-0.2, 0, -0.4, r'$1-\\epsilon$', size = 14)\nax.set_axis_off()\nplt.title('Inner and Outer Hyperspheres')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-26-output-1.png){width=315 height=335 fig-align='center'}\n:::\n:::\n\n\n## Proportion of volume in outer shell\n\nLet's create a function to calculate the proportion of the volume of a hypersphere that is in the outer $\\epsilon$ of the radius.\n\n::: {#3971e0e8 .cell execution_count=27}\n``` {.python .cell-code}\ndef get_prop_of_volume_in_outer_epsilon(dimension, epsilon):\n  \n  outer_radius = 1.0\n  outer_volume = volume_of_hypersphere(outer_radius, dimension)\n\n  inner_radius = 1 - epsilon\n  inner_volume = volume_of_hypersphere(inner_radius, dimension)\n  proportion = (outer_volume - inner_volume) / outer_volume\n\n  # print(f\"Outer volume: {outer_volume}, Inner volume: {inner_volume}\")\n  return proportion\n```\n:::\n\n\nLet's calculate the proportion of the volume in the outer 1% of the radius for dimensions 1 to 300.\n\n::: {#8a022938 .cell execution_count=28}\n``` {.python .cell-code}\npropvols = []\nfor c_dim in [1,2,10,20,50,100,150,200,250,300]:\n  propvols.append(get_prop_of_volume_in_outer_epsilon(c_dim, 0.01))\n  print('Proportion of volume in outer 1 percent of radius in %d dimensions =%3.3f'%(c_dim, get_prop_of_volume_in_outer_epsilon(c_dim, 0.01)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProportion of volume in outer 1 percent of radius in 1 dimensions =0.010\nProportion of volume in outer 1 percent of radius in 2 dimensions =0.020\nProportion of volume in outer 1 percent of radius in 10 dimensions =0.096\nProportion of volume in outer 1 percent of radius in 20 dimensions =0.182\nProportion of volume in outer 1 percent of radius in 50 dimensions =0.395\nProportion of volume in outer 1 percent of radius in 100 dimensions =0.634\nProportion of volume in outer 1 percent of radius in 150 dimensions =0.779\nProportion of volume in outer 1 percent of radius in 200 dimensions =0.866\nProportion of volume in outer 1 percent of radius in 250 dimensions =0.919\nProportion of volume in outer 1 percent of radius in 300 dimensions =0.951\n```\n:::\n:::\n\n\n## Plot outer shell proportion\n\n::: {#1fed8198 .cell execution_count=29}\n``` {.python .cell-code}\n# plot propvols\nplt.scatter([1,2,10,20,50,100,150,200,250,300], propvols)\nplt.xlabel('Dimensions')\nplt.ylabel('Proportion of volume in outer 1%')\nplt.title('Proportion of volume in outer 1% of diameter of hypersphere')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-29-output-1.png){width=812 height=449}\n:::\n:::\n\n\nBy the time we get to 300 dimensions most of the volume is in the outer 1 percent.\n\n## Fraction of points in outer shell\n\nWhat is the fraction $f_d$ of all the points that are within a unit distance, but __not__ within a distance of $1-\\epsilon$?\n\nLet\n\n$$\nK_d = \\frac{\\pi^{d/2}}{\\Gamma\\bigl(\\tfrac d2 + 1\\bigr)}\n$$\n\nso that the volume of a hypersphere of radius $R$ is $K_d \\times R^d$.\n\n---\n\nThen for $R=1$ and $R=1-\\epsilon$ we have:\n\n$$ \n\\begin{align*}\nf_d &= \\frac{\\text{Volume of Shell}}{\\text{Volume of unit hypersphere}} \\\\\n&= \\frac{K_d\\times(1)^d - K_d\\times(1-\\epsilon)^d}{K_d\\times(1)^d} \\\\\n&= 1 - (1-\\epsilon)^d\n\\end{align*}\n$$\n\n:::: {.fragment}\nObserve that $(1-\\epsilon)^d$ goes to 0 as $d \\rightarrow \\infty$ and $f_d\\rightarrow 1$ as $d \\rightarrow \\infty$.\n\nThis means that as $d\\rightarrow \\infty$, all of the points that are __within__ 1 unit of our location, are almost __exactly__ 1 unit from our location. \n::::\n\n\n## Curse of Dimensionality Example\n\n::: aside\nThe following example is based on _Data Science from Scratch,_ Joel Grus, Second Edition, Chapter 12.  \n:::\n\nHere's another example.\n\nWe create 100 points, scattered at random within a $d$-dimensional space.\n\nWe will look at two quantities as we vary $d$.\n\n* The __minimum__ distance between any two points.\n* The __average__ distance between any two points.\n\n---\n\n::: {#54e4dbb5 .cell execution_count=30}\n``` {.python .cell-code}\nimport sklearn.metrics as metrics\n\nnsamples = 1000\nunif_X = np.random.default_rng().uniform(0, 1, nsamples).reshape(-1, 1)\neuclidean_dists = metrics.euclidean_distances(unif_X)\n# extract the values above the diagonal\ndists = euclidean_dists[np.triu_indices(nsamples, 1)]\nmean_dists = [np.mean(dists)]\nmin_dists = [np.min(dists)]\nfor d in range(2, 101):\n    unif_X = np.column_stack([unif_X, np.random.default_rng().uniform(0, 1, nsamples)])\n    euclidean_dists = metrics.euclidean_distances(unif_X)\n    dists = euclidean_dists[np.triu_indices(nsamples, 1)]\n    mean_dists.append(np.mean(dists))\n    min_dists.append(np.min(dists))\n```\n:::\n\n\n::: {#13c57da8 .cell execution_count=31}\n``` {.python .cell-code}\nplt.figure(figsize = (6, 3))\nplt.plot(min_dists, label = \"Minimum Distance\")\nplt.plot(mean_dists, label = \"Average Distance\")\nplt.xlabel(r'Number of dimensions ($d$)')\nplt.ylabel('Distance')\nplt.legend(loc = 'best')\nplt.title(f'Comparison of Minimum Versus Average Distance Between {nsamples} Points\\nAs Dimension Grows')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-31-output-1.png){width=593 height=321 fig-align='center'}\n:::\n:::\n\n\nThe average distance between points grows. However we also observe that the minimum distance between points grows at a similar rate.\n\n---\n\nLet's look at the ratio between the average distance between points and the minimum distance between points.\n\n::: {#8f253cd3 .cell execution_count=32}\n``` {.python .cell-code}\nplt.figure(figsize = (6, 3))\nplt.plot([a/b for a, b in zip(min_dists, mean_dists)])\nplt.xlabel(r'Number of dimensions ($d$)')\nplt.ylabel('Ratio')\nplt.title(f'Ratio of Minimum to Average Distance Between {nsamples} Points\\nAs Dimension Grows')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-32-output-1.png){width=532 height=321 fig-align='center'}\n:::\n:::\n\n\nThis shows that, for any test point $x$, the distance to the __closest__ point to $x$, gets closer and closer to the __average__ distance between points.\n\nIf we used a point at the average distance for classifying $x$ we'd get a very poor classifier.\n\n##  Implications of the Curse\n\nFor $k$-NN, the Curse of Dimensionality means that in high dimensions most points are nearly the same distance from the test point.\n\nThis makes $k$-NN ineffective. It cannot reliably tell which are the $k$ nearest neighbors and its performance degrades as $d$ increases.\n\n:::: {.fragment}\n__What Can be Done?__\n::::\n\n:::: {.fragment}\nThe problem is that you simply cannot have enough data to do a good job using $k$-NN in high dimensions.\n::::\n\n:::: {.fragment}\nAlternative approaches to mitigate this issue\n\n:::: {.incremental}\n- Use a different dissimilarity metric\n    - For example cosine distance behaves a little better, but you will have to decide if it is a good distance function for your problem.\n- __Reduce__ the dimension of your data.\n    - We will discuss dimensionality reduction techniques in lectures [SVD I](10-Low-Rank-and-SVD.qmd) and [SVD II](11-Dimensionality-Reduction-SVD-II.qmd).\n::::\n::::\n\n## More on Model Selection\n\nRecall that each value of $k$ provides a different model.\n\n:::: {.fragment}\nTo better understand __model selection__, we need a way to evaluate our different models.\n::::\n\n:::: {.fragment}\nHow do we evaluate a classifier?\n::::\n\n## Binary classification\n\nIn the simple case of a binary classifier, we can call\n\n* one class the 'Positive' class or index 1\n* the other class the 'Negative' class or index 0.\n\nThe most basic measure of success for a classifier is __accuracy__:\n\n* **Accuracy** is the fraction of test points that are correctly classified.\n\n::: {.incremental}\n* Accuracy is important, however it may not convey enough useful information.\n* For example, let's say we have a dataset showing __class imbalance__, e.g., 90% of the data are the Positive class and 10% are the Negative class.\n* For this dataset, consider a classifier that always predicts 'Positive'. \n* Its accuracy is 90%, but it is not an effective classifier.\n \n::: \n\n## Precision and Recall\n\n::: {.columns}\n::: {.column width=\"50%\"}\n\n![](figs/L15-confusion-matrix.png){fig-align=\"center\"}\n\n:::\n::: {.column width=\"50%\"}\nA better way to measure the classifier's performance is using a Confusion Matrix.\n\nDiagonal elements represent successes and off diagonals represent errors.\n\n:::\n:::\n\nUsing the confusion matrix we can define some more useful measures:\n\n* __Recall__ - defined as the fraction of actual positives correctly classified\n    * TP/(TP + FN)\n* __Precision__ - defined as the fraction of classified positives correctly classified\n    * TP/(TP + FP)\n\n## Precision and Recall Illustration\n\n::: {layout-ncol=\"2\"}\n\n![](figs/Precisionrecall.png){width=\"400px\"}\n\n![](figs/Precisionrecall2.png)\n\n:::\n\n<!-- By Walber - Own work, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=36926283 -->\n\n## Evaluating $k$- Nearest Neighbors\n\n::: {.columns}\n::: {.column width=\"50%\"}\n\nFirst we'll generate some synthetic data to work with.\n\n::: {#b2c9fe11 .cell execution_count=33}\n``` {.python .cell-code}\nX, y = datasets.make_circles(noise=.1, factor=.5, random_state=1)\nprint('Shape of data: {}'.format(X.shape))\nprint('Unique labels: {}'.format(np.unique(y)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nShape of data: (100, 2)\nUnique labels: [0 1]\n```\n:::\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\n\nHere is what the data looks like.\n\n::: {#ea203efb .cell execution_count=34}\n``` {.python .cell-code}\nplt.figure(figsize = (4, 4))\nplt.prism()  # this sets a nice color map\nplt.scatter(X[:, 0], X[:, 1], c=y, s = 80)\nplt.axis('off')\nplt.axis('equal')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-34-output-1.png){width=317 height=315 fig-align='center'}\n:::\n:::\n\n\n:::\n:::\n\n---\n\nRecall that we always want to test on data separate from our training data.\n\nFor now, we will do something very simple: take the first 50 examples for training and the rest for testing.\n\n::: {#57e14fcc .cell execution_count=35}\n``` {.python .cell-code}\nX_train = X[:50]\ny_train = y[:50]\nX_test = X[50:]\ny_test = y[50:]\n\nplt.figure(figsize = (10, 4))\nplt.subplot(1, 2, 1)\nplt.scatter(X_train[:, 0], X_train[:, 1], c = y_train, s = 80)\nplt.axis('equal')\nplt.axis('off')\nplt.title('Training Data')\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_test, s = 80)\nplt.title('Test Data')\nplt.axis('equal')\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-35-output-1.png){width=763 height=335 fig-align='center'}\n:::\n:::\n\n\n---\n\nFor our first example, we will classify the points (in the two classes) using a $k$-NN classifier.\n\nWe will specify that $k=5$, i.e., we will classify based on the majority vote of the 5 nearest neighbors.\n\n::: {#58e30983 .cell execution_count=36}\n``` {.python .cell-code}\nk = 5\nknn5 = KNeighborsClassifier(n_neighbors = k)    \n```\n:::\n\n\nAs we have seen previously, the `scikit-learn` `fit()` function corresponds to __training__ and the `predict()` function corresponds to __testing.__\n\n::: {#6238b9b2 .cell execution_count=37}\n``` {.python .cell-code}\nknn5.fit(X_train,y_train)\nprint(f'Accuracy on test data: {knn5.score(X_test, y_test)}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy on test data: 0.72\n```\n:::\n:::\n\n\nAccuracy of 72% sounds good -- but let's dig deeper. \n\n## Confusion Matrix\n\nWe'll call the red points the Positive class and the green points the Negative class.\n\nHere is the confusion matrix:\n\n::: {#19894556 .cell execution_count=38}\n``` {.python .cell-code}\ny_pred_test = knn5.predict(X_test)\npd.DataFrame(metrics.confusion_matrix(y_test, y_pred_test), \n             columns = ['Predicted 1', 'Predicted 0'], \n             index = ['Actual 1', 'Actual 0'])\n```\n\n::: {.cell-output .cell-output-display execution_count=37}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Predicted 1</th>\n      <th>Predicted 0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Actual 1</th>\n      <td>14</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>Actual 0</th>\n      <td>0</td>\n      <td>22</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nLooks like the classifier is getting all of the Negative class correct, but only achieving accuracy of 50% on the Positive class.\n\nThat is, its __precision__ is 100%, but its __recall__ is only 50%.\n\nLet's visualize the results.\n\n---\n\n::: {#f2586b00 .cell execution_count=39}\n``` {.python .cell-code}\nk = 5\n\nplt.figure(figsize = (10, 4))\nplt.subplot(1, 2, 1)\n\nplt.scatter(X_train[:, 0], X_train[:, 1], c = y_train, s = 80)\nplt.axis('equal')\nplt.title('Training')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_pred_test, s = 80)\nplt.title(f'Testing $k$={k}\\nAccuracy: {knn5.score(X_test, y_test)}')\nplt.axis('off')\nplt.axis('equal')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-39-output-1.png){width=763 height=355 fig-align='center'}\n:::\n:::\n\n\nIndeed, the Positive (red) points in the upper half of the test data are all classified incorrectly. \n\n---\n\nLet's look at one of the points that the classifier got wrong.\n\n::: {#f9f9464d .cell execution_count=40}\n``` {.python .cell-code}\nk=5 \ntest_point = np.argmax(X_test[:, 1])\nneighbors = knn5.kneighbors([X_test[test_point]])[1]\n\nplt.figure(figsize = (10, 4))\nplt.subplot(1, 2, 1)\nplt.scatter(X_train[:, 0], X_train[:, 1], c = y_train, s = 80)\nplt.scatter(X_train[neighbors,0], X_train[neighbors,1],\n            c = y_train[neighbors], marker='o', \n            facecolors='none', edgecolors='b', s = 80)\nradius = np.max(metrics.euclidean_distances(X_test[test_point].reshape(1, -1), X_train[neighbors][0]))\nax = plt.gcf().gca()\ncircle = mp.patches.Circle(X_test[test_point], radius, facecolor = 'blue', alpha = 0.2)\nax.add_artist(circle)\nplt.axis('equal')\nplt.axis('off')\nplt.tight_layout()\nplt.title(r'Training')\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_pred_test, s = 80)\nplt.scatter(X_test[test_point, 0], X_test[test_point, 1], marker='o', \n            facecolors='none', edgecolors='b', s = 80)\nplt.title('Testing $k$={}\\nAccuracy: {}'.format(k,knn5.score(X_test, y_test)))\nplt.axis('equal')\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-40-output-1.png){width=950 height=414 fig-align='center'}\n:::\n:::\n\n\n--- \n\nFor comparison purposes, let's try $k$ = 3.\n\n::: {#fb0836a0 .cell execution_count=41}\n``` {.python .cell-code}\nk = 3\nknn3 = KNeighborsClassifier(n_neighbors=k)    \nknn3.fit(X_train,y_train)\ny_pred_test = knn3.predict(X_test)\n\nplt.figure(figsize = (10, 4))\nplt.subplot(1, 2, 1)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s = 80)\nplt.axis('equal')\nplt.axis('off')\nplt.title(r'Training')\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_test, s = 80)\nplt.title(f'Testing $k$={k}\\nAccuracy: {knn3.score(X_test, y_test)}')\nplt.axis('off')\nplt.axis('equal')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-41-output-1.png){width=763 height=355 fig-align='center'}\n:::\n:::\n\n\n--- \n\nAnd let's look at the same individual point as before.\n\n::: {#5007f2b5 .cell execution_count=42}\n``` {.python .cell-code}\nk = 3\ntest_point = np.argmax(X_test[:,1])\nX_test[test_point]\nneighbors = knn3.kneighbors([X_test[test_point]])[1]\n\nplt.figure(figsize = (10, 4))\nplt.subplot(1, 2, 1)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s = 80)\nplt.scatter(X_train[neighbors, 0], X_train[neighbors, 1], marker = 'o', \n            facecolors = 'none', edgecolors = 'b', s = 80)\nradius = np.max(metrics.euclidean_distances(X_test[test_point].reshape(1, -1), \n                                            X_train[neighbors][0]))\nax = plt.gcf().gca()\ncircle = mp.patches.Circle(X_test[test_point], radius, facecolor = 'blue', alpha = 0.2)\nax.add_artist(circle)\nplt.axis('equal')\nplt.axis('off')\nplt.title(r'Training')\nplt.subplot(1, 2, 2)\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_pred_test, s = 80)\nplt.scatter(X_test[test_point,0], X_test[test_point,1], marker = 'o', \n            facecolors = 'none', edgecolors = 'b', s = 80)\nplt.title(f'Testing $k$={k}\\nAccuracy: {knn3.score(X_test, y_test)}')\nplt.axis('off')\nplt.axis('equal')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-42-output-1.png){width=763 height=355 fig-align='center'}\n:::\n:::\n\n\n## Train-Test Splitting\n\nSo how confident can we be that the test accuracy is 92% in general?\n\nWhat we really need to do is consider __many__ different train/test splits.\n\nThus, the proper way to evaluate generalization ability (accuracy on the test data) is:\n    \n1. Form a random train/test split\n2. Train the classifier on the training split\n3. Test the classifier on the testing split\n4. Accumulate statistics\n5. Repeat from step 1 until enough statistics have been collected.\n\n---\n\n::: {#6f3adb5a .cell execution_count=43}\n``` {.python .cell-code}\nimport sklearn.model_selection as model_selection\n\nnreps = 50\nkvals = range(1, 10)\nacc = []\nnp.random.seed(4)\nfor k in kvals:\n    test_rep = []\n    train_rep = []\n    for i in range(nreps):\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, \n                                                                            y, \n                                                                            test_size = 0.5)\n        knn = KNeighborsClassifier(n_neighbors = k)    \n        knn.fit(X_train, y_train)\n        train_rep.append(knn.score(X_train, y_train))\n        test_rep.append(knn.score(X_test, y_test))\n    acc.append([np.mean(np.array(test_rep)), np.mean(np.array(train_rep))])\naccy = np.array(acc)\n```\n:::\n\n\n::: {#1a6fab68 .cell execution_count=44}\n``` {.python .cell-code}\nplt.plot(kvals, accy[:, 0], '.-', label = 'Accuracy on Test Data')\nplt.plot(kvals, accy[:, 1], '.-', label = 'Accuracy on Training Data')\nplt.xlabel(r'$k$')\nplt.ylabel('Accuracy')\nplt.title('Train/Test Comparision of $k$-NN')\nplt.legend(loc = 'best')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-44-output-1.png){width=821 height=450 fig-align='center'}\n:::\n:::\n\n\n---\n\nBased on the generalization error, i.e., accuracy on the held-out test data, it looks like $k = 2$ is the best choice. \n\nHere is the __decision boundary__ for $k$-NN with $k = 2$.\n\n::: {#4cad1d44 .cell execution_count=45}\n``` {.python .cell-code}\nx_min, x_max = X[:, 0].min() - .1, X[:, 0].max() + .1\ny_min, y_max = X[:, 1].min() - .1, X[:, 1].max() + .1\nplot_step = 0.02\nxx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                     np.arange(y_min, y_max, plot_step))\n```\n:::\n\n\n::: {#a45d70a8 .cell execution_count=46}\n``` {.python .cell-code}\nnp.random.seed(1)\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.5)\n\nk = 2\nknn = KNeighborsClassifier(n_neighbors = k)  \nknn.fit(X_train, y_train)\ny_pred_train = knn.predict(X_train)\ny_pred_test = knn.predict(X_test)\n\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize = (12, 5))\nplt.subplot(1, 2, 1)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.3)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30)\nplt.axis('equal')\nplt.axis('off')\nplt.xlim((x_min, x_max))\nplt.ylim((y_min, y_max))\nplt.title(f'{k}-NN - Training Data\\nAccuracy: {knn.score(X_train, y_train)}')\n\nplt.subplot(1, 2, 2)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.3)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=30)\nplt.axis('equal')\nplt.axis('off')\nplt.xlim((x_min, x_max))\nplt.ylim((y_min, y_max))\nplt.title(f'{k}-NN - Test Data\\nAccuracy: {knn.score(X_test, y_test)}')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-46-output-1.png){width=912 height=427 fig-align='center'}\n:::\n:::\n\n\n<!--\n## Decision Tree\n\nNext, we'll use a decision tree on the same data set.\n\n::: {#2efefd85 .cell execution_count=47}\n``` {.python .cell-code}\nimport sklearn.tree as tree\ndtc = tree.DecisionTreeClassifier(max_leaf_nodes = 5)\n\ndtc.fit(X_train,y_train)\ny_pred_test = dtc.predict(X_test)\nprint('DT accuracy on test data: ', dtc.score(X_test, y_test))\ny_pred_train = dtc.predict(X_train)\nprint('DT accuracy on training data: ', dtc.score(X_train, y_train))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDT accuracy on test data:  0.94\nDT accuracy on training data:  0.98\n```\n:::\n:::\n\n\n::: {#67e65c0a .cell execution_count=48}\n``` {.python .cell-code}\nplt.figure(figsize = (4, 4))\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_pred_test, marker='^', s=80)\nplt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=80)\nplt.axis('equal')\nplt.axis('off')\nplt.title(F'Decision Tree\\n Triangles: Test Data, Circles: Training Data\\nAccuracy: {dtc.score(X_test, y_test)}')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-48-output-1.png){width=361 height=371 fig-align='center'}\n:::\n:::\n\n\n---\n\nLet's visualize the __decision boundary__ of the Decision Tree.\n\n::: {#dc239a79 .cell execution_count=49}\n``` {.python .cell-code}\nZ = dtc.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.figure(figsize = (10, 4))\nplt.subplot(1, 2, 1)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.3)\nplt.scatter(X_train[:, 0], X_train[:, 1], c = y_train, s = 80)\nplt.axis('equal')\nplt.axis('off')\nplt.xlim((x_min, x_max))\nplt.ylim((y_min, y_max))\nplt.title(f'Decision Tree - Training Data\\nAccuracy: {dtc.score(X_train, y_train)}');\n\nplt.subplot(1, 2, 2)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.3)\nplt.scatter(X_test[:, 0], X_test[:, 1], c = y_pred_test, marker = '^', s = 80)\nplt.axis('equal')\nplt.axis('off')\nplt.xlim((x_min, x_max))\nplt.ylim((y_min, y_max))\nplt.title(f'Decision Tree - Test Data\\nAccuracy: {dtc.score(X_test, y_test)}')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-49-output-1.png){width=763 height=353 fig-align='center'}\n:::\n:::\n\n\n## $k$-NN vs. Decision Tree\n\nIt appears that $k$-NN and the decision tree have approximately comparable performance on this dataset.\n\nHowever - there is a difference in __interpretability__, which means the ability to __explain__ why the classifier made the decision it did.\n\nIt can be somewhat difficult to __understand__ why $k$-NN is making a specific prediction. It depends on the data in the neighborhood of the test point.\n\nOn the other hand, the decision tree is easy to interpret.\n\n:::: {.fragment}\nWe sometimes use the terms \"black box\" for an uninterpretable classifier like $k$-NN, and \"white box\" for an interpretable classifier like decision tree.\n::::\n\n---\n\nLet's see an example of the interpretability of the decision tree.\n\n::: {#e66b4d5d .cell execution_count=50}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport graphviz\n\n# Export the decision tree to DOT format\ndot_data = tree.export_graphviz(dtc, out_file=None,\n                                feature_names=['X', 'Y'],\n                                class_names=['Red', 'Green'],\n                                filled=True, \n                                rounded=True,\n                                special_characters=True)\n\n# Use graphviz to create a graph from the DOT data\ngraph = graphviz.Source(dot_data)\n\n# Save the graph to a file\ngraph.format = 'png'\ngraph.render(\"figs/decision_tree\")\n\n# Read the image file and display it using matplotlib\nimg = mpimg.imread(\"figs/decision_tree.png\")\nplt.figure(figsize=(4, 12))\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-50-output-1.png){width=317 height=479 fig-align='center'}\n:::\n:::\n\n\n-->\n\n# Real Data\n\nTo explore a few more issues, we'll now turn to some famous datasets that have been extensively studied in the past.\n\n## The Iris Dataset\n\nThe Iris dataset is a famous dataset used by Ronald Fisher in a classic 1936 paper on classification.\n\n![R. A. Fisher](figs/R._A._Fisher.png){width=\"35%\"}\n\n---\n\nQuoting from Wikipedia:\n\n>The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimetres. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other.\n\n---\n\n:::: {.columns style=\"display: flex; align-items: flex-end\"} \n::: {.column width=\"33%\"}\n![I. setosa](figs/Iris_setosa.png)\n:::\n::: {.column width=\"33%\"}\n![I. versicolor](figs/Iris_versicolor.png)\n:::\n::: {.column width=\"33%\"}\n![I. virginica](figs/Iris_virginica.png)\n:::\n::::\n\n## Load Iris Dataset  \n\n::: {#3ab53439 .cell execution_count=51}\n``` {.python .cell-code code-fold=\"false\"}\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\nynames = iris.target_names\nprint(X.shape, y.shape)\nprint(X[1, :])\nprint(iris.target_names)\nprint(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(150, 4) (150,)\n[4.9 3.  1.4 0.2]\n['setosa' 'versicolor' 'virginica']\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n```\n:::\n:::\n\n\n## Hyperparameters: k-NN\n\nFirst, we'll explore setting the hyperparameters. We start with $k$-NN.\n\nTo set the hyperparameter $k$, we evaluate error on the test set for many train/test splits:\n\n::: {#c0f1f8ce .cell execution_count=52}\n``` {.python .cell-code}\nX = iris.data\ny = iris.target\n\nkvals = range(2, 20)\nnreps = 50\n\nacc = []\nstd = []\nnp.random.seed(0)\nfor k in kvals:\n    test_rep = []\n    train_rep = []\n    for i in range(nreps):\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(\n            X, y, test_size = 0.33)\n        knn = KNeighborsClassifier(n_neighbors = k)    \n        knn.fit(X_train, y_train)\n        train_rep.append(knn.score(X_train, y_train))\n        test_rep.append(knn.score(X_test, y_test))\n    acc.append([np.mean(np.array(test_rep)), np.mean(np.array(train_rep))])\n    std.append([np.std(np.array(test_rep)), np.std(np.array(train_rep))])\n```\n:::\n\n\n::: {#ee3c46af .cell execution_count=53}\n``` {.python .cell-code}\nplt.figure(figsize= (6, 4))\naccy = np.array(acc)\nstds = np.array(std)/np.sqrt(nreps)\nprint(f'Max Test Accuracy at k = {kvals[np.argmax(accy[:, 0])]} with accuracy {np.max(accy[:, 0]):.03f}')\nplt.plot(kvals, accy[:, 0], '.-', label = 'Accuracy on Test Data')\nplt.plot(kvals, accy[:, 1], '.-', label = 'Accuracy on Training Data')\nplt.xlabel('k')\nplt.ylabel('Accuracy')\nplt.ylim(0.9, None)\nplt.xticks(kvals)\nplt.legend(loc = 'best')\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMax Test Accuracy at k = 13 with accuracy 0.969\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-53-output-2.png){width=523 height=356 fig-align='center'}\n:::\n:::\n\n\n---\n\nIt looks like $k$ = 13 is the best-performing value of the hyperparameter.\n\n:::: {.fragment}\nCan we be sure?\n::::\n\n:::: {.fragment}\nBe careful! Each point in the above plot is the mean of 50 random train/test splits!\n::::\n\n:::: {.fragment}\nIf we are going to be __sure__ that $k$ = 13 is best, then it should be be statistically distinguishable from the other values.\n::::\n\n:::: {.fragment}\nTo make this call, let's plot $\\pm 1 \\sigma$ confidence intervals on the mean values.\n\nSee the [Probability Refresher](03-Probability-and-Statistics-Refresher.qmd) for details on the proper formula.\n::::\n\n---\n\n::: {#651822d3 .cell execution_count=54}\n``` {.python .cell-code}\nplt.errorbar(kvals, accy[:, 0], stds[:, 0], label = 'Accuracy on Test Data')\nplt.xlabel('k')\nplt.ylabel('Accuracy')\nplt.legend(loc = 'lower center')\nplt.xticks(kvals)\nplt.title(r'Test Accuracy with $\\pm 1\\sigma$ Errorbars')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-54-output-1.png){width=829 height=450 fig-align='center'}\n:::\n:::\n\n\n--- \n\nIt looks like $k$ = 13 is a reasonable value, although a case can be made that 9 and 11 are not statistically distinguishable from 13.\n\nTo gain insight into the complexity of the model for $k$ = 13, let's look at the decision boundary.\n\nWe will re-run the classifier using only two (of four) features for visualization purposes.\n\n---\n\n::: {#7d53d2bb .cell execution_count=55}\n``` {.python .cell-code}\n# Create color maps\nfrom matplotlib.colors import ListedColormap\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n# we will use only the first two (of four) features, so we can visualize\nX = X_train[:, :2] \nh = .02  # step size in the mesh\nk = 13\nknn = KNeighborsClassifier(n_neighbors=k)\nknn.fit(X, y_train)\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                      np.arange(y_min, y_max, h))\nZ = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(6, 4))\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y_train, cmap=cmap_bold)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(f\"3-Class $k$-NN classification ($k$ = {k})\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-55-output-1.png){width=496 height=358 fig-align='center'}\n:::\n:::\n\n\nThere are a few artifacts, though overall this looks like a reasonably smooth set of decision boundaries.\n\n<!--\n## Hyperparameters: Decision Tree\n\nHow do we control the complexity of a Decision Tree?\n\nThere are a variety of ways (see the `sklearn` documentation) but the simplest one is to control the number of leaf nodes in the tree.  \n\nA small number of leaf nodes is a low-complexity model, and a large number of nodes is a high-complexity model.\n\n---\n\n::: {#6e9d37e1 .cell execution_count=56}\n``` {.python .cell-code}\nX = iris.data\ny = iris.target\nleaf_vals = range(3, 20)\nnreps = 50\n\nacc = []\nstd = []\nnp.random.seed(0)\nfor leaf_count in leaf_vals:\n    test_rep = []\n    train_rep = []\n    for i in range(nreps):\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.10)\n        dtc = tree.DecisionTreeClassifier(max_leaf_nodes = leaf_count)   \n        dtc.fit(X_train, y_train)\n        train_rep.append(dtc.score(X_train, y_train))\n        test_rep.append(dtc.score(X_test, y_test))\n    acc.append([np.mean(np.array(test_rep)), np.mean(np.array(train_rep))])\n    std.append([np.std(np.array(test_rep)), np.std(np.array(train_rep))])\naccy = np.array(acc)\nstds = np.array(std)/np.sqrt(nreps)\n```\n:::\n\n\n---\n\n::: {#a4b702fa .cell execution_count=57}\n``` {.python .cell-code}\nplt.plot(leaf_vals, accy[:, 0], '.-', label = 'Accuracy on Test Data')\nplt.plot(leaf_vals, accy[:, 1], '.-', label = 'Accuracy on Training Data')\nplt.xlabel('Max Leaf Nodes')\nplt.ylabel('Accuracy')\nplt.legend(loc = 'best')\nplt.xticks(leaf_vals)\nbest_leaf = leaf_vals[np.argmax(accy[:, 0])]\nplt.title(f'Test/Train Error for Decision Tree\\nMax Test Accuracy at {best_leaf} leaf nodes with accuracy {np.max(accy[:, 0]):.03f}')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-57-output-1.png){width=820 height=467 fig-align='center'}\n:::\n:::\n\n\n---\n\n::: {#7c1b197c .cell execution_count=58}\n``` {.python .cell-code}\nplt.errorbar(leaf_vals, accy[:, 0], stds[:, 0], label = 'Accuracy on Test Data')\nplt.xlabel('Max Leaf Nodes')\nplt.ylabel('Accuracy')\nplt.legend(loc = 'lower center')\nplt.xticks(leaf_vals)\nplt.title(r'Test Accuracy with $\\pm 1\\sigma$ Errorbars')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-58-output-1.png){width=829 height=450 fig-align='center'}\n:::\n:::\n\n\nIt looks like 9 leaf nodes is appropriate, but we would be justified to choose 4 or 13 as well.\n\n---\n\nHere we visualize the decision boundary for the DT:\n\n::: {#bc260351 .cell execution_count=59}\n``` {.python .cell-code}\n# we will use only the first two (of four) features, so we can visualize\nX = X_train[:, :2] \nh = .02  # step size in the mesh\ndtc = tree.DecisionTreeClassifier(max_leaf_nodes = best_leaf) \ndtc.fit(X, y_train)\n# Plot the decision boundary. For that, we will assign a color to each\n# point in the mesh [x_min, x_max]x[y_min, y_max].\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                      np.arange(y_min, y_max, h))\nZ = dtc.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y_train, cmap=cmap_bold)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(f\"3-Class DT Classification\\n{best_leaf} leaf nodes\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-59-output-1.png){width=794 height=449 fig-align='center'}\n:::\n:::\n\n\n-->\n\n## MNIST dataset\n\nNIST used to be called the \"National Bureau of Standards.\" These are the folks who bring you the reference meter, reference kilogram, etc.\n\nNIST constructed datasets for machine learning of handwritten digits. These were collected from Census Bureau employees and also from high-school students.\n\nThis dataset has been used repeatedly for many years to evaluate classifiers. For a peek at some of the work done with this dataset you can visit this [link](http://yann.lecun.com/exdb/mnist/). \n\n\n## MNIST Digits\n\n::: {#f543a080 .cell execution_count=60}\n``` {.python .cell-code}\nimport sklearn.utils as utils\n\ndigits = datasets.load_digits()\nX, y = utils.shuffle(digits.data, digits.target, random_state = 1)\n\nprint ('Data shape: {}'.format(X.shape))\nprint ('Data labels: {}'.format(y))\nprint ('Unique labels: {}'.format(digits.target_names))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nData shape: (1797, 64)\nData labels: [1 5 0 ... 9 1 5]\nUnique labels: [0 1 2 3 4 5 6 7 8 9]\n```\n:::\n:::\n\n\nAn individual item is an $8 \\times 8$ image, encoded as a matrix:\n\n::: {#ff370aae .cell execution_count=61}\n``` {.python .cell-code}\ndigits.images[3]\n```\n\n::: {.cell-output .cell-output-display execution_count=60}\n```\narray([[ 0.,  0.,  7., 15., 13.,  1.,  0.,  0.],\n       [ 0.,  8., 13.,  6., 15.,  4.,  0.,  0.],\n       [ 0.,  2.,  1., 13., 13.,  0.,  0.,  0.],\n       [ 0.,  0.,  2., 15., 11.,  1.,  0.,  0.],\n       [ 0.,  0.,  0.,  1., 12., 12.,  1.,  0.],\n       [ 0.,  0.,  0.,  0.,  1., 10.,  8.,  0.],\n       [ 0.,  0.,  8.,  4.,  5., 14.,  9.,  0.],\n       [ 0.,  0.,  7., 13., 13.,  9.,  0.,  0.]])\n```\n:::\n:::\n\n\n--- \n\nHere we show the matrix as an image.\n\n::: {#4b50147c .cell execution_count=62}\n``` {.python .cell-code}\nfig, ax = plt.subplots()\nax.matshow(digits.images[3], cmap='gray_r')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-62-output-1.png){width=407 height=416 fig-align='center'}\n:::\n:::\n\n\n---\n\nIt is easier to visualize if we blur the pixels a little bit.\n\n::: {#e9599881 .cell execution_count=63}\n``` {.python .cell-code}\nplt.rc('image', cmap = 'binary', interpolation = 'bilinear')\nplt.figure(figsize = (5, 5))\nplt.axis('off')\nplt.imshow(digits.images[3])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-63-output-1.png){width=389 height=389 fig-align='center'}\n:::\n:::\n\n\n---\n\nHere are some more samples from the dataset.\n\n::: {#743d9dfc .cell execution_count=64}\n``` {.python .cell-code}\nfor t in range(3):\n    plt.figure(figsize = (8, 2))\n    for j in range(4):\n        plt.subplot(1, 4, 1 + j)\n        plt.imshow(X[4*t + j].reshape(8, 8))\n        plt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-64-output-1.png){width=614 height=149 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-64-output-2.png){width=614 height=149 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-64-output-3.png){width=614 height=149 fig-align='center'}\n:::\n:::\n\n\n--- \n\nAlthough this is an 8 $\\times$ 8 image, we can reshape it to a vector of length 64.\n\nTo do model selection, we will again average over many train/test splits.\n\nHowever, since the performance of $k$-NN degrades on large sets of data we may decide to limit the size of our testing set to speed up testing.\n\n:::: {.fragment}\nHow does the train/test split affect results?\n::::\n\n:::: {.fragment}\nLet's consider two cases:  \n\n:::: {.incremental}\n1. Train: 90% of data, Test: 10% of data\n2. Train: 67% of data, Test: 33% of data\n::::\n::::\n\n---\n\n::: {#8788a8b2 .cell execution_count=65}\n``` {.python .cell-code}\ndef test_knn(kvals, test_fraction, nreps):\n    acc = []\n    std = []\n    np.random.seed(0)\n    #\n    for k in kvals:\n        test_rep = []\n        train_rep = []\n        for i in range(nreps):\n            X_train, X_test, y_train, y_test = model_selection.train_test_split(\n                X, y, test_size = test_fraction)\n            knn = KNeighborsClassifier(n_neighbors = k)    \n            knn.fit(X_train, y_train)\n            test_rep.append(knn.score(X_test, y_test))\n        acc.append(np.mean(np.array(test_rep)))\n        std.append(np.std(np.array(test_rep)))\n    return(np.array(acc), np.array(std)/np.sqrt(nreps))\n\ntest_fraction1 = 0.33\naccy1, stds1 = test_knn(range(2, 20), test_fraction1, 50)\ntest_fraction2 = 0.10\naccy2, stds2 = test_knn(range(2, 20), test_fraction2, 50)\n```\n:::\n\n\n::: {#032eac71 .cell execution_count=66}\n``` {.python .cell-code}\nplt.figure(figsize = (6, 4))\nplt.errorbar(kvals, accy1, stds1, \n             label = f'{test_fraction1:.0%} Used for Testing; accuracy {np.max(accy1):.03f}')\nplt.errorbar(kvals, accy2, stds2, \n             label = f'{test_fraction2:.0%} Used for Testing; accuracy {np.max(accy2):.03f}')\nplt.xlabel('k')\nplt.ylabel('Accuracy')\nplt.legend(loc = 'best')\nplt.xticks(kvals)\nbest_k = kvals[np.argmax(accy1)]\nplt.title(f'Test Accuracy with $\\\\pm 1\\\\sigma$ Error Bars')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-66-output-1.png){width=531 height=377 fig-align='center'}\n:::\n:::\n\n\n---\n\nThese plots illustrate show two important principles.\n    \n* With more training data the classifier performs better.\n* With less testing data, the testing results are more variable. Though testing is faster.\n    \nThe key decision here is what value to choose for $k$. So it makes sense to use the 33% test split, because the smaller error bars give us better confidence in our decision.\n   \n---\n\nWe can get a sense of why $k$-NN can succeed at this task by looking at the nearest neighbors of some points.\n\n::: {#6f00a1dd .cell execution_count=67}\n``` {.python .cell-code}\nknn = KNeighborsClassifier(n_neighbors = 3)    \nknn.fit(X, y)\nneighbors = knn.kneighbors(X[:3,:], n_neighbors=3, return_distance=False)\n```\n:::\n\n\n::: {#1e62afad .cell execution_count=68}\n``` {.python .cell-code}\nplt.rc(\"image\", cmap=\"binary\")  # this sets a black on white colormap\nfor t in range(3):\n    plt.figure(figsize=(8, 2))\n    plt.subplot(1, 4, 1)\n    plt.imshow(X[t].reshape(8, 8))\n    plt.axis('off')\n    plt.title(\"Query\")\n    # plot three nearest neighbors from the training set\n    for i in [0, 1, 2]:\n        plt.subplot(1, 4, 2 + i)\n        plt.title(\"neighbor {}\".format(i))\n        plt.imshow(X[neighbors[t, i]].reshape(8, 8))\n        plt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-68-output-1.png){width=614 height=169 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-68-output-2.png){width=614 height=169 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](15-Classification-II-kNN_files/figure-revealjs/cell-68-output-3.png){width=614 height=169 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Summary\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\nWe covered the following topics:\n\n- $k$-NN algorithm\n- Curse of Dimensionality\n- Worked with real datasets to perform model selection\n- Metrics like precision and recall for evaluating classifier performance\n\n:::\n::: {.column width=\"60%\"}\n\n![](figs/qrcode_course_feedback.png)\n\n[https://bit.ly/3NeAwgx](https://bit.ly/3NeAwgx)\n\n:::\n::::\n\n:::\n\n",
    "supporting": [
      "15-Classification-II-kNN_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}