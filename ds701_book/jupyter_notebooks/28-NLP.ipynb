{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Natural Language Processing'\n",
        "jupyter: python3\n",
        "bibliography: references.bib\n",
        "---\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/27-RNN.ipynb)\n",
        "\n",
        "Natural language processing (NLP) is a subfield of artificial intelligence that allows computers to understand, process, and manipulate human language.\n",
        "\n",
        "# Brief History of NLP and Lecture Outline\n",
        "\n",
        "## NLP Evolution Timeline\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "**1950s-1970s: Rule-Based Era**\n",
        "\n",
        "- Turing Test (1950), formal grammars, hand-crafted rules\n",
        "- Systems like ELIZA (1966) - pattern matching \"Rogerian Therapy\"chatbot\n",
        "- Limitations: brittle, didn't scale, couldn't handle ambiguity\n",
        "\n",
        "**1980s-2000s: Statistical Revolution**\n",
        "\n",
        "- Shift from rules to learning from data\n",
        "- N-grams, Hidden Markov Models, machine learning\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "**2006-2017: Deep Learning**\n",
        "\n",
        "- Neural networks applied to NLP\n",
        "- Word2Vec (2013), RNNs, LSTMs for sequence modeling\n",
        "- Attention mechanism (2014) - precursor to Transformers\n",
        "\n",
        "**2017-Present: Transformer Era**\n",
        "\n",
        "- Attention is All You Need (2017) - Transformer architecture\n",
        "- BERT (2018), GPT-2/3 (2019/2020), ChatGPT (2022)\n",
        "- LLMs with billions of parameters, human-level performance\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Lecture Outline\n",
        "\n",
        "For the rest of this lecture we will cover:\n",
        "\n",
        "- Text preprocessing and tokenization\n",
        "- Numerical representations of words\n",
        "- Language models overview\n",
        "- NLP Packages Overview\n",
        "- Named Entity Recognition (NER) with spaCy\n",
        "- Topic Modeling with BERTopic\n",
        "\n",
        "\n",
        "# Numerical Representation of Words\n",
        "\n",
        "## Numerical Representations of Words\n",
        "\n",
        "* Machine learning models for NLP are not able to process text in the form of characters and strings. \n",
        "* Characters and strings must be converted to numbers in order to train our language models.\n",
        "\n",
        "There are a number of ways to do this. These include\n",
        "\n",
        "- sparse representations, like one-hot encodings and TF-IDF encodings\n",
        "- word embeddings.\n",
        "\n",
        "However, prior to creating a numerical representation of text, we need to **tokenize** the text.\n",
        "\n",
        "## Tokenization\n",
        "\n",
        "* Tokenization is the process of splitting raw text into smaller pieces, called (drum-roll please), *tokens*. \n",
        "\n",
        "* Tokens can be individual characters, words, subwords, or sentences.\n",
        "\n",
        "<br>\n",
        "\n",
        "Examples of character and word tokenization:\n",
        "\n",
        "```Show me the money```\n",
        "\n",
        "Character tokenization:\n",
        "\n",
        "```['S', 'h', 'o', 'w', 'm', 'e', 't', 'h', 'e', 'm', 'o', 'n', 'e', 'y']```.\n",
        "\n",
        "Word tokenization:\n",
        "\n",
        "```['Show', 'me', 'the', 'money'] ```\n",
        "\n",
        "## Simple Tokenization in Python\n",
        "\n",
        "Simple python implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Character and word tokenization\n",
        "\n",
        "sentence = \"Show me the money\"\n",
        "word_tokens = sentence.split()\n",
        "print(word_tokens)\n",
        "\n",
        "character_tokens = [char for char in sentence if char != ' ']\n",
        "print(character_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization Methods\n",
        "\n",
        "* There are advantages and disadvantages to different tokenization methods. \n",
        "* We showed two very simple strategies. \n",
        "\n",
        "However, there are other strategies, such as subword and sentence tokenization,\n",
        "see for example:\n",
        "\n",
        " - [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte-pair_encoding),\n",
        " - [SentencePiece](https://github.com/google/sentencepiece).\n",
        "\n",
        "See Andrej Karpathy's [Let's build a GPT tokenizer](https://www.youtube.com/watch?v=zduSFxRajkE) video for a deep dive.\n",
        "\n",
        "* With tokenization, our goal is to not lose meaning with the tokens. With character based tokenization, especially for English (non-character based languages) we certainly lose meaning. \n",
        "\n",
        "## Huggingface Tokenization\n",
        "\n",
        "Here is a demo of how to tokenize using the [transformers](https://huggingface.co/docs/transformers/en/index) package from Huggingface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from transformers import AutoTokenizer, logging\n",
        "\n",
        "logging.set_verbosity_warning()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)\n",
        "\n",
        "# Try a more advanced sentence\n",
        "sentence2 = \"Let's try to see if we can get this transformer to tokenize.\"\n",
        "tokens2 = tokenizer.tokenize(sentence2)\n",
        "print(tokens2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokens, Token IDs, and Vocabulary\n",
        "\n",
        "* Associated to each token is a unique token ID. \n",
        "\n",
        "* The total number of unique tokens that a model can recognize and process is the *vocabulary size*. \n",
        "\n",
        "  * The *vocabulary* is the collection of all the unique tokens.\n",
        "\n",
        "* The tokens (and token ids) alone hold no (semantic) information. What is needed is a numerical representation that *encodes* this information. \n",
        "\n",
        "* There are different ways to achieve this:\n",
        "\n",
        "  - One encoding technique that we already considered is one-hot encodings. \n",
        "\n",
        "  - Another more powerful encoding method, is the creation of word embeddings.\n",
        "\n",
        "## Sparse Representations\n",
        "\n",
        "We have previously considered the following sparse representations of textual data.\n",
        "\n",
        "### One-Hot Encoding\n",
        "- Each word is represented as a vector of zeros and a single one.\n",
        "- Simple but inefficient for large vocabularies.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Given the words cat, dog, and emu here are sample one-hot encodings\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\text{cat} &= [1, 0, 0]^{T}, \\\\\n",
        "\\text{dog} &= [0, 1, 0]^{T}, \\\\\n",
        "\\text{emu} &= [0, 0, 1]^{T}. \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Bag of Words (BoW)\n",
        "- Represents text as a collection of word counts.\n",
        "- Ignores grammar and word order.\n",
        "\n",
        "**Example**\n",
        "\n",
        "Suppose we have the following sentences\n",
        "\n",
        "1. The cat sat on the mat.\n",
        "1. The dog sat on the log.\n",
        "1. The emu sat on the mat.\n",
        "\n",
        "| Sentence                              | the | cat | sat | on | mat | dog | log | emu |\n",
        "|---------------------------------------|-----|-----|-----|----|-----|-----|-----|----------|\n",
        "| \"The cat sat on the mat.\"             |  2  |  1  |  1  | 1  |  1  |  0  |  0  |    0     |\n",
        "| \"The dog sat on the log.\"             |  2  |  0  |  1  | 1  |  0  |  1  |  1  |    0     |\n",
        "| \"The emu sat on the mat.\"        |  2  |  0  |  1  | 1  |  1  |  0  |  0  |    1     |\n",
        "\n",
        "---\n",
        "\n",
        "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "\n",
        "- Adjusts word frequency by its importance across documents.\n",
        "- Highlights unique words in a document.\n",
        "- See [Clustering in Practice](https://tools4ds.github.io/DS701-Course-Notes/07-Clustering-II-in-practice.html#tf-idf) \n",
        "  for more details.\n",
        "\n",
        "**Example**\n",
        "\n",
        "The TF-IDF representations corresponding to the previous sentences.\n",
        "\n",
        "|       | cat       | dog       | log       | mat       | on        | emu  | sat       | the       |\n",
        "|-------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n",
        "| Sentence 1 | 0.4698    | 0.0000    | 0.0000    | 0.4698    | 0.3546    | 0.0000    | 0.3546    | 0.7093    |\n",
        "| Sentence 2 | 0.0000    | 0.4698    | 0.4698    | 0.0000    | 0.3546    | 0.0000    | 0.3546    | 0.7093    |\n",
        "| Sentence 3 | 0.0000    | 0.0000    | 0.0000    | 0.4698    | 0.3546    | 0.4698    | 0.3546    | 0.7093    |\n",
        "\n",
        "## Word Embeddings\n",
        "\n",
        "Word embeddings represent words as **dense vectors in high-dimensional spaces**.\n",
        "\n",
        "The individual values of the vector may be difficult to interpret, but the \n",
        "overall pattern is that _words with similar meanings are close to each other_, \n",
        "in the sense that their vectors have small angles with each other.\n",
        "\n",
        "<details>\n",
        "<summary><b>Question:</b> Is that true with one-hot encodings?</summary>\n",
        "\n",
        "<b>Answer:</b> No, with one-hot encodings, the similarity of two words is 0 if they are different, and 1 if they are the same.\n",
        "</details>\n",
        "\n",
        "\n",
        "The similarity of two word embeddings is the cosine of the angle between the two\n",
        "vectors. Recall that for two vectors $v_1, v_2\\in\\mathbb{R}^{n}$, the formula \n",
        "for the cosine of the angle between them is\n",
        "\n",
        "$$ \n",
        "\\cos{(\\theta)} = \\frac{v_1 \\cdot v_2}{\\Vert v_1 \\Vert_2 \\Vert v_2 \\Vert_2}.\n",
        "$$\n",
        "\n",
        "## Static vs Contextual Embeddings\n",
        "\n",
        "Word embeddings can be **static** or **contextual**. \n",
        "\n",
        "**Static:**<br>\n",
        "A static embedding is when each word has a single embedding, e.g., Word2Vec. \n",
        "\n",
        "**Contextual:**<br>\n",
        "A contextual embedding (used by more complex language model embedding algorithms) allows the embedding for a word to change depending on its context in a sentence.\n",
        "\n",
        "## Word2Vec: Static Embeddings\n",
        "\n",
        "**Word2Vec** [@mikolov2013efficient] is a technique to learn static word embeddings from large text corpora.\n",
        "\n",
        "**Key characteristics:**\n",
        "\n",
        "- Each word has a **single fixed vector** representation\n",
        "- Trained to predict context (CBOW) or predict word from context (Skip-gram)\n",
        "- Captures semantic relationships: *king - man + woman ≈ queen*\n",
        "- Popular pre-trained models: Google News (300d), GloVe\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "**Advantages:**\n",
        "\n",
        "- Fast to train and use \n",
        "- Efficient for large vocabularies\n",
        "- Good for similarity tasks, clustering\n",
        "- Pre-trained embeddings available\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "**Disadvantages:**\n",
        "\n",
        "- Limited context and no word order information.\n",
        "- For phrases and sentences, the embedding is the average of the word embeddings.\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "## Contextual vs Static Embeddings\n",
        "\n",
        "| Feature | Static (Word2Vec, GloVe) | Contextual (BERT, GPT) |\n",
        "|---------|--------------------------|------------------------|\n",
        "| **Representation** | One vector per word | Different vectors per context |\n",
        "| **Example** | \"bank\" always same vector | \"bank\" differs in \"river bank\" vs \"bank account\" |\n",
        "| **Model Type** | Shallow neural network | Deep transformer model |\n",
        "| **Training** | Fast, lightweight | Slow, resource-intensive |\n",
        "| **Best For** | Similarity, clustering, simple tasks | Complex understanding, ambiguity resolution |\n",
        "| **Dimensionality** | 100-300 dimensions | 768-1024+ dimensions |\n",
        "\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column}\n",
        "\n",
        "**When to use Word2Vec:**\n",
        "\n",
        "- Limited computational resources\n",
        "- Task doesn't require deep context understanding\n",
        "- Working with domain-specific corpora (train custom embeddings)\n",
        "\n",
        ":::\n",
        "::: {.column}\n",
        "\n",
        "**When to use Contextual Embeddings:**\n",
        "\n",
        "- Need to handle polysemy (words with multiple meanings)\n",
        "- Complex NLP tasks (NER, question answering, translation)\n",
        "- Have access to GPU resources\n",
        ":::\n",
        "::: \n",
        " \n",
        "## tiktokenizer\n",
        "\n",
        "![tiktokenizer](figs/tiktokenizer.png){fig-align=\"center\" width=\"50%\"}\n",
        "\n",
        "[Tiktokenizer demo](https://tiktokenizer.vercel.app/)\n",
        "\n",
        "# Language Models\n",
        "\n",
        "\n",
        "## Language Models\n",
        "\n",
        "A language model is a statistical tool that predicts the probability of a sequence of words. It helps in understanding and generating human language by learning patterns and structures from large text corpora.\n",
        "\n",
        "1. **N-gram Models**:\n",
        "   - Predict the next word based on the previous  $n-1$ words.\n",
        "   - Simple and effective for many tasks but limited by fixed context size.\n",
        "\n",
        "1. **Neural Language Models**:\n",
        "   - Use neural networks to capture more complex patterns.\n",
        "   - Examples include *RNNs*, *LSTMs*, and **Transformers**.\n",
        "\n",
        "See [RNNs and LSTMs](#27-RNN.qmd) for more details.\n",
        "\n",
        "We'll skip N-grams and focus on Transformers.\n",
        "\n",
        "\n",
        "# Transformers\n",
        "\n",
        "\n",
        "## Transformers: High-Level Overview\n",
        "\n",
        "Transformers [@vaswani2017attention] are the foundation of modern NLP systems.\n",
        "\n",
        "**Key innovations:**\n",
        "\n",
        "- Use **attention mechanism** to process entire sequences in parallel\n",
        "- Revolutionized NLP and enabled LLMs (ChatGPT, BERT, GPT-4)\n",
        "- Scalable across GPUs for massive models\n",
        "\n",
        "## Transformer Architecture\n",
        "\n",
        "![](drawio/Transformer_Enc_Dec.png){fig-align=\"center\"}\n",
        "\n",
        "**Components:**\n",
        "\n",
        "- **Encoder**: Processes input text, creates rich representations\n",
        "- **Decoder**: Generates output text using encoder representations\n",
        "- **Attention**: Allows model to focus on relevant parts of input\n",
        "\n",
        "> Other variants of the transformer architecture include the encoder-only and decoder-only architectures.\n",
        "\n",
        "\n",
        "## Encoder-Decoder Building Blocks\n",
        "\n",
        "![](drawio/TransformerOriginal.png){fig-align=\"center\"}\n",
        "\n",
        "> Can grow the model size by increasing the number of layers and the number of attention heads.\n",
        "\n",
        "## Transformer Building Block\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "**Components:**\n",
        "\n",
        "- Attention mechanism -- gathers information from neighboring tokens in the input sequence.\n",
        "- Feed-forward neural network -- processes the information gathered by the attention mechanism.\n",
        "- Layer normalization -- normalizes the output of the feed-forward neural network.\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "![](drawio/Encoder.png)\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "## The Attention Mechanism\n",
        "\n",
        "Attention allows models to understand which words are most relevant to each other.\n",
        "\n",
        "**Example sentence:**\n",
        "\n",
        "> \"The elephant didn't cross the river because **it** was tired.\"\n",
        "\n",
        "**Question:** What does \"it\" refer to?\n",
        "\n",
        "- Attention helps the model determine that **\"it\" = \"elephant\"** (not \"river\")\n",
        "- The model \"attends\" more strongly to \"elephant\" when processing \"it\"\n",
        "\n",
        "## How Attention Works (Simplified)\n",
        "\n",
        "For each word in a sentence:\n",
        "\n",
        "1. **Calculate relevance scores** with all other words\n",
        "2. **Apply [softmax](https://en.wikipedia.org/wiki/Softmax_function)** to get attention weights (sum to 1)\n",
        "3. **Compute weighted sum** of word representations\n",
        "4. Result: each word's representation includes context from relevant words\n",
        "\n",
        "**Multi-head attention**: Run multiple attention operations in parallel to capture different types of relationships\n",
        "\n",
        "> It's a form of adaptive network where some weights are derived from inputs.\n",
        "\n",
        "## Queries, Keys, and Values\n",
        "\n",
        "The attention mechanism uses three types of vectors for each word:\n",
        "\n",
        "- **Query (Q)**: \"What am I looking for?\"\n",
        "- **Key (K)**: \"What do I contain?\"\n",
        "- **Value (V)**: \"What information do I have?\"\n",
        "\n",
        "**Attention formula:**\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "- Compute similarity between queries and keys\n",
        "- Normalize with softmax\n",
        "- Weight the values by attention scores\n",
        "\n",
        "## Attention Visualized\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"60%\"}\n",
        "Darker connections show stronger attention.\n",
        "\n",
        "When processing \"it\", the model attends most to \"elephant\".\n",
        "\n",
        "This is how transformers handle long-range dependencies and ambiguity.\n",
        ":::\n",
        "::: {.column width=\"40%\"}\n",
        "![](drawio/VisualizedAttention.png)\n",
        ":::\n",
        "::::\n",
        "\n",
        "# Transformer Architectures\n",
        "\n",
        "## 3 Types of Transformer Models\n",
        "\n",
        "1. **Encoder-Decoder** – Sequence-to-sequence tasks (e.g., machine translation)\n",
        "   - Example: Original Transformer, T5\n",
        "\n",
        "2. **Encoder-Only** – Transforms text embeddings into latent representations for understanding and classification tasks\n",
        "   - Example: **BERT** (sentiment analysis, NER, classification)\n",
        "\n",
        "3. **Decoder-Only** – Predicts next token used for text generation and completion\n",
        "   - Example: **GPT** (ChatGPT, text generation, AI assistants)\n",
        "\n",
        "## BERT: Encoder Model\n",
        "\n",
        "**BERT** (Bidirectional Encoder Representations from Transformers) [@devlin2019bert]\n",
        "\n",
        "**Key features:**\n",
        "\n",
        "- Reads text **bidirectionally** (considers context from both directions)\n",
        "- Pre-trained on masked language modeling (predict hidden words)\n",
        "- 340M parameters (BERT-Large), 110M (BERT-Base)\n",
        "- Fine-tuned for specific tasks with one additional layer\n",
        "\n",
        "**Common uses:** Text classification, NER, question answering, sentiment analysis\n",
        "\n",
        "## GPT: Decoder Model\n",
        "\n",
        "**GPT** (Generative Pre-trained Transformer) [@brown2020language]\n",
        "\n",
        "**Key features:**\n",
        "\n",
        "- Reads text **left-to-right** (autoregressive)\n",
        "- Predicts next token: $P(t_1, t_2, \\ldots, t_N) = P(t_1)\\prod_{n=2}^{N} P(t_n | t_1, \\ldots t_{n-1})$\n",
        "- GPT-3: 175 billion parameters\n",
        "- Excellent at text generation\n",
        "\n",
        "**Common uses:** Text completion, creative writing, chatbots, code generation\n",
        "\n",
        "## Transformer Applications\n",
        "\n",
        "Transformers enable powerful NLP applications:\n",
        "\n",
        "- **Machine Translation**: Translate between languages\n",
        "- **Text Summarization**: Generate concise summaries\n",
        "- **Question Answering**: Answer questions from context\n",
        "- **Named Entity Recognition**: Identify entities in text\n",
        "- **Sentiment Analysis**: Determine sentiment/emotion\n",
        "- **Text Generation**: Create human-like text\n",
        "\n",
        "# NLP Packages Overview\n",
        "\n",
        "## NLTK: Natural Language Toolkit\n",
        "\n",
        "[NLTK](https://www.nltk.org/) is one of the most comprehensive Python libraries for NLP, created for teaching and research.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "- Extensive text processing tools (tokenization, stemming, tagging, parsing)\n",
        "- Access to 50+ corpora and lexical resources (WordNet, TreeBank)\n",
        "- Sentiment analysis, text classification, and chunking\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column}\n",
        "**Best For:**\n",
        "\n",
        "- Learning _classical_ NLP concepts and fundamentals\n",
        "- **Academic research and prototyping**\n",
        "- Working with linguistic data structures\n",
        "- Access to curated corpora\n",
        ":::\n",
        "::: {.column}\n",
        "**Limitations:**\n",
        "\n",
        "- Slower than modern libraries\n",
        "- Less suited for production\n",
        "- Requires more manual pipeline construction\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "## spaCy: Production-Ready NLP\n",
        "\n",
        "[spaCy](https://spacy.io/usage/spacy-101) is a modern, industrial-strength NLP library designed for production use.\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column}\n",
        "**Key Features:**\n",
        "\n",
        "- Fast and efficient (Cython-optimized)\n",
        "- Pre-trained models for 70+ languages\n",
        "- Built-in NER, POS tagging, dependency parsing\n",
        "- Word vectors and document similarity\n",
        "- Beautiful visualization tools (displaCy)\n",
        "- Easy integration with deep learning (PyTorch, TensorFlow)\n",
        ":::\n",
        "::: {.column}\n",
        "**Best For:**\n",
        "\n",
        "- Production NLP pipelines\n",
        "- Real-time text processing\n",
        "- Information extraction at scale\n",
        "- Named Entity Recognition\n",
        "- Document analysis and classification\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "## BERTopic: Overview\n",
        "\n",
        "[BERTopic](https://maartengr.github.io/BERTopic/index.html) [@grootendorst2022bertopic] is a modern topic modeling technique using transformer embeddings.\n",
        "\n",
        "**More later...**\n",
        "\n",
        "## LangChain: Overview\n",
        "\n",
        "[LangChain](https://docs.langchain.com/oss/python/langchain/overview) [@langchain2023] is a framework for building applications that use language models.\n",
        "\n",
        "**Key advantages:**\n",
        "\n",
        "- Easy to use\n",
        "- Built-in support for many language models\n",
        "- Built-in support for many NLP tasks\n",
        "\n",
        "**Common uses:**\n",
        "\n",
        "- Building chatbots and virtual assistants\n",
        "- Building information extraction systems\n",
        "- Building summarization systems, ...\n",
        "\n",
        "# NLP Applications\n",
        "\n",
        "We'll dive a bit deeper into two of the most common NLP applications: \n",
        "\n",
        "* Named entity recognition\n",
        "* Topic modeling\n",
        "\n",
        "# Named Entity Recognition (NER)\n",
        "\n",
        "## What is Named Entity Recognition?\n",
        "\n",
        "**Named Entity Recognition (NER)** is the task of identifying and classifying named entities in text into predefined categories.\n",
        "\n",
        "**Common entity types:**\n",
        "\n",
        "- **PERSON**: Names of people (e.g., \"Barack Obama\")\n",
        "- **ORGANIZATION**: Companies, agencies (e.g., \"Google\", \"FBI\")\n",
        "- **LOCATION**: Cities, countries, landmarks (e.g., \"Paris\", \"Mount Everest\")\n",
        "- **DATE**: Dates and times (e.g., \"January 1, 2024\")\n",
        "- **MONEY**: Monetary values (e.g., \"$100\")\n",
        "- **GPE**: Geopolitical entities (countries, cities, states)\n",
        "\n",
        "**Applications:** Information extraction, content classification, question answering, knowledge graphs\n",
        "\n",
        "## NER with spaCy: Setup\n",
        "\n",
        "You need to download the model you want to use.\n",
        "\n",
        "**Installation:**\n",
        "```bash\n",
        "pip install spacy\n",
        "python -m spacy download en_core_web_sm  # Small model\n",
        "```\n",
        "\n",
        "**Available models:**\n",
        "\n",
        "- `en_core_web_sm`: Small (12 MB) - fast, good accuracy\n",
        "- `en_core_web_md`: Medium (40 MB) - word vectors included\n",
        "- `en_core_web_lg`: Large (560 MB) - best accuracy, full vectors\n",
        "\n",
        "## spaCy NER: Basic Usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "import spacy\n",
        "\n",
        "# Load pre-trained model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Apple Inc. was founded by Steve Jobs in Cupertino, California. In 2024, the company is worth over $3 trillion dollars.\"\n",
        "\n",
        "# Process text - creates Doc object\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract named entities\n",
        "print(\"Entities found:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"  {ent.text:20} -> {ent.label_:15} ({spacy.explain(ent.label_)})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## spaCy NER: Visualization\n",
        "\n",
        "Built-in visualization with `displacy`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from spacy import displacy\n",
        "\n",
        "# Visualize entities inline\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key features:**\n",
        "\n",
        "- Highlights entities with color coding\n",
        "- Shows entity labels on hover\n",
        "- Can export as HTML or SVG\n",
        "- Customizable styles\n",
        "\n",
        "## spaCy NER: Advanced Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Access entity properties\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text:20} | Label: {ent.label_:10} | Start: {ent.start_char:3} | End: {ent.end_char:3}\")\n",
        "\n",
        "# Filter by entity type\n",
        "orgs = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
        "print(f\"\\nOrganizations: {orgs}\")\n",
        "\n",
        "# Entity spans and context\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} → Sentence: {ent.sent}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## spaCy Pipeline Architecture\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"50%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# View pipeline components\n",
        "print(\"Pipeline components:\")\n",
        "for name, component in nlp.pipeline:\n",
        "    print(f\"  - {name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "\n",
        "\n",
        "**Processing steps:**\n",
        "\n",
        "1. **Tokenizer**: Split text into tokens\n",
        "2. **tok2vec**: Convert tokens to vectors\n",
        "3. **Tagger**: Part-of-speech tagging\n",
        "4. **Parser**: Dependency parsing\n",
        "5. **NER**: Named entity recognition\n",
        "6. **Lemmatizer**: Word lemmatization\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "## spaCy -- Dive Deeper\n",
        "\n",
        "![](figs/spacy-get-started.png){fig-align=\"center\" width=\"50%\"}\n",
        "\n",
        "<p align=\"center\">\n",
        "  <a href=\"https://spacy.io/usage/spacy-101\">https://spacy.io/usage/spacy-101</a>\n",
        "</p>\n",
        "\n",
        "# Topic Modeling\n",
        "\n",
        "## What is Topic Modeling?\n",
        "\n",
        "**Topic modeling** is an unsupervised learning technique that discovers abstract \"topics\" in a collection of documents.\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column}\n",
        "**Key concepts:**\n",
        "\n",
        "- **Topic**: A distribution over words (e.g., \"sports\" → {game, team, player, score, ...})\n",
        "- **Document**: A mixture of topics\n",
        "- **Goal**: Automatically discover topics and their distributions\n",
        ":::\n",
        "::: {.column}\n",
        "**Applications:**\n",
        "\n",
        "- Content recommendation and discovery\n",
        "- Document organization and clustering\n",
        "- Trend analysis over time\n",
        "- Search and information retrieval\n",
        "- Content summarization\n",
        ":::\n",
        ":::\n",
        "\n",
        "**Classical approaches** (LDA, NMF) use bag-of-words representations and require specifying the number of topics.\n",
        "\n",
        "**Modern approaches** (BERTopic) leverage transformer embeddings and can automatically determine topics.\n",
        "\n",
        "## BERTopic: Overview\n",
        "\n",
        "**BERTopic** [@grootendorst2022bertopic] is a modern topic modeling technique using transformer embeddings.\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column}\n",
        "**Key advantages:**\n",
        "\n",
        "- Uses **semantic embeddings** from BERT (captures context and meaning)\n",
        "- **Automatically determines** optimal number of topics\n",
        "- Excellent for **short documents** (tweets, reviews, titles)\n",
        "- Produces **highly coherent** and interpretable topics\n",
        "- Built-in **interactive visualizations**\n",
        ":::\n",
        "::: {.column}\n",
        "**When to use BERTopic:**\n",
        "\n",
        "- Need high-quality, coherent topics\n",
        "- Working with social media, news articles, reviews\n",
        "- Want to avoid hyperparameter tuning (number of topics)\n",
        "- Have GPU resources available\n",
        "- Need to track topics over time\n",
        ":::\n",
        ":::\n",
        "\n",
        "## BERTopic: How It Works\n",
        "\n",
        "BERTopic uses a modular pipeline with four main steps:\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column}\n",
        "\n",
        "\n",
        "**1. Document Embeddings**\n",
        "\n",
        "- Use pre-trained BERT/transformer models\n",
        "- Each document → 768-dimensional vector (or higher)\n",
        "- Captures semantic meaning and context\n",
        "\n",
        "**2. Dimensionality Reduction (UMAP)**\n",
        "\n",
        "- Reduce from 768 dimensions to ~5 dimensions\n",
        "- Preserves local and global structure\n",
        "- Enables efficient clustering\n",
        "\n",
        ":::\n",
        "::: {.column}\n",
        "\n",
        "**3. Clustering (HDBSCAN)**\n",
        "\n",
        "- Density-based clustering algorithm\n",
        "- Automatically determines number of clusters\n",
        "- Assigns outliers to -1 topic\n",
        "\n",
        "**4. Topic Representation (c-TF-IDF)**\n",
        "\n",
        "- Class-based TF-IDF weights words by topic\n",
        "- Extracts most representative words per topic\n",
        "- Creates interpretable topic labels\n",
        "\n",
        ":::\n",
        ":::\n",
        "\n",
        "## BERTopic: Installation and Setup\n",
        "\n",
        "**Installation:**\n",
        "```bash\n",
        "pip install bertopic\n",
        "pip install umap-learn hdbscan  # clustering dependencies\n",
        "```\n",
        "\n",
        "**Quick start:**\n",
        "```python\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Initialize with default settings\n",
        "topic_model = BERTopic(language=\"english\", calculate_probabilities=True)\n",
        "\n",
        "# Fit and transform\n",
        "topics, probs = topic_model.fit_transform(documents)\n",
        "```\n",
        "\n",
        "## BERTopic: Basic Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from bertopic import BERTopic\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "categories = ['sci.space', 'rec.sport.baseball', 'comp.graphics']\n",
        "docs = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))['data']\n",
        "\n",
        "topic_model = BERTopic()\n",
        "\n",
        "topics, probs = topic_model.fit_transform(docs)\n",
        "\n",
        "print(f\"Number of topics found: {len(set(topics)) - (1 if -1 in topics else 0)}\")\n",
        "print(f\"Outlier documents: {sum(1 for t in topics if t == -1)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERTopic: Visualize Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "fig = topic_model.visualize_topics()\n",
        "fig.update_layout(width=600, height=400)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERTopic: Visualize Topic Hierarchy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "fig = topic_model.visualize_hierarchy()\n",
        "fig.update_layout(width=800, height=550)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERTopic: Visualize Topic Bar Chart"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig = topic_model.visualize_barchart(top_n_topics=8)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERTopic: Visualize Topic Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig = topic_model.visualize_heatmap()\n",
        "fig.update_layout(width=800, height=600)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERTopic: Find Similar Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "similar_topics, similarity = topic_model.find_topics(\"space exploration and satellites\", top_n=3)\n",
        "print(f\"Topics similar to 'space exploration and satellites': {similar_topics}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERTopic: Exploring Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Get topic information\n",
        "topic_info = topic_model.get_topic_info()\n",
        "print(\"Topic Overview:\")\n",
        "print(topic_info[['Topic', 'Count', 'Name']].head(10))\n",
        "\n",
        "# Get representative documents for a topic\n",
        "topic_id = 0\n",
        "rep_docs = topic_model.get_representative_docs(topic_id)\n",
        "print(f\"\\nRepresentative documents for Topic {topic_id}:\")\n",
        "for i, doc in enumerate(rep_docs[:2], 1):\n",
        "    print(f\"{i}. {doc[:100]}...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERTopic: Topic Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Show top words for each topic\n",
        "print(\"Top words per topic:\\n\")\n",
        "for topic_num in sorted(set(topics)):\n",
        "    if topic_num == -1:  # Skip outlier topic\n",
        "        continue\n",
        "    words = topic_model.get_topic(topic_num)\n",
        "    if words:\n",
        "        # Get word and score\n",
        "        top_words = ', '.join([f\"{word}({score:.2f})\" for word, score in words[:8]])\n",
        "        print(f\"Topic {topic_num}: {top_words}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERTopic: Finding Similar Topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Find topics similar to a search query\n",
        "query = \"space exploration and satellites\"\n",
        "similar_topics, similarity = topic_model.find_topics(query, top_n=3)\n",
        "\n",
        "print(f\"Topics similar to '{query}':\")\n",
        "for topic_id, score in zip(similar_topics, similarity):\n",
        "    if topic_id != -1:\n",
        "        words = topic_model.get_topic(topic_id)\n",
        "        top_words = ', '.join([word for word, _ in words[:5]])\n",
        "        print(f\"  Topic {topic_id} (similarity: {score:.3f}): {top_words}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERTopic: Topic Over Time\n",
        "\n",
        "BERTopic can track how topics evolve over time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare timestamps\n",
        "import pandas as pd\n",
        "timestamps = pd.date_range('2020-01-01', periods=len(docs), freq='D')\n",
        "\n",
        "# Track topics over time\n",
        "topics_over_time = topic_model.topics_over_time(\n",
        "    docs=docs,\n",
        "    timestamps=timestamps,\n",
        "    nr_bins=10\n",
        ")\n",
        "\n",
        "# Visualize evolution\n",
        "fig = topic_model.visualize_topics_over_time(topics_over_time)\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Use cases:**\n",
        "\n",
        "- News trends over months/years\n",
        "- Social media topic evolution\n",
        "- Product review sentiment changes\n",
        "\n",
        "## BERTopic: Customization\n",
        "\n",
        "BERTopic is highly customizable:\n",
        "\n",
        "```python\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "# Custom embedding model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Custom UMAP settings\n",
        "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0)\n",
        "\n",
        "# Custom HDBSCAN settings\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=15, min_samples=10)\n",
        "\n",
        "# Create custom BERTopic model\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    verbose=False\n",
        ")\n",
        "```\n",
        "\n",
        "## BERTopic: Practical Tips\n",
        "\n",
        "**Best practices:**\n",
        "\n",
        "- Use at least 100-200 documents per expected topic\n",
        "- For short texts, use smaller `min_topic_size` (5-10)\n",
        "- For long documents, increase `min_topic_size` (20-50)\n",
        "- Experiment with different embedding models\n",
        "- Save models for reuse: `topic_model.save(\"my_model\")`\n",
        "\n",
        "**Common issues:**\n",
        "\n",
        "- Too many outliers? Lower `min_cluster_size`\n",
        "- Topics too broad? Increase `min_topic_size`\n",
        "- Slow performance? Use smaller embedding model\n",
        "- Poor topics? Try different embedding model or more documents\n",
        "\n",
        "# Summary\n",
        "\n",
        "## NLP Tools and Applications Recap\n",
        "\n",
        "**What we covered:**\n",
        "\n",
        "1. **Text representation**: Sparse (TF-IDF) vs embeddings (Word2Vec vs BERT)\n",
        "2. **Language models**: N-grams → Transformers\n",
        "3. **Transformers**: Attention mechanism (QKV), encoder-decoder architecture\n",
        "4. **Transformer architectures**: BERT (encoder), GPT (decoder)\n",
        "5. **NLP Tools**: NLTK (learning) vs spaCy (production)\n",
        "6. **Named Entity Recognition**: spaCy for production-ready NER\n",
        "7. **Topic Modeling**: BERTopic with transformer embeddings\n",
        "\n",
        "## References"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}