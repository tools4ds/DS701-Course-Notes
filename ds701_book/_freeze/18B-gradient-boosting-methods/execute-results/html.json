{
  "hash": "bcd64484058ae7c49812295bc9d4b978",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Gradient Boosting Methods\"\njupyter: python3\n---\n\n<!--\nAI-GENERATED CODE\nGenerated by: Cursor / Sonnet 4.5\nPrompt: \"As suggested at the end of @18A-decision-tree-regression.md , \"*Next steps*: Explore gradient boosting methods (XGBoost, LightGBM) which often outperform random forests, especially on structured/tabular data competitions.\"  Create a markdown file '18-B-gradient-boosting-methods.md' lecture.\"\nDate: October 29, 2025\nPurpose: Used to create the markdown file for the gradient boosting methods lecture.\nThe code was reviewed and adapted.\n-->\n\n# Gradient Boosting Methods: XGBoost, LightGBM, and CatBoost\n\n## Introduction\n\nYou've learned that random forests build many independent trees in parallel and average their predictions. But what if, instead of building trees independently, each new tree could learn from the mistakes of previous trees? This is the key insight behind **gradient boosting** - arguably the most powerful machine learning technique for structured/tabular data.\n\nIn Kaggle competitions and real-world applications, gradient boosting methods (especially XGBoost, LightGBM, and CatBoost) consistently win or place highly. Let's understand why.\n\n## The Core Idea: Learning from Mistakes\n\n### Random Forests vs Gradient Boosting\n\n**Random Forest**: \"Let's build 100 trees independently, each on different random samples, and average their predictions.\"\n\n**Gradient Boosting**: \"Let's build trees sequentially. Each new tree focuses on correcting the errors made by all previous trees.\"\n\n### Intuitive Example: Estimating House Prices\n\nImagine you're estimating house prices with a team of experts:\n\n**Random Forest Approach:**\n- 100 experts independently look at the data\n- Each gives their estimate\n- You average all 100 estimates\n- Experts don't learn from each other\n\n**Gradient Boosting Approach:**\n- Expert 1 makes predictions (simple rules)\n  - Prediction: \"All houses cost $300K\"\n  - Error: Off by a lot!\n  \n- Expert 2 looks at Expert 1's mistakes\n  - \"Okay, Expert 1 overestimated small houses and underestimated large ones\"\n  - Expert 2 adds corrections: \"-$100K for houses < 1500 sqft, +$200K for houses > 2500 sqft\"\n  \n- Expert 3 looks at remaining mistakes\n  - \"Still underestimating houses in downtown\"\n  - Expert 3 adds: \"+$50K for downtown location\"\n  \n- Continue for 100 experts...\n- Final prediction: Expert 1 + Expert 2 + Expert 3 + ... + Expert 100\n\nEach new expert (tree) specializes in correcting the mistakes of all previous experts!\n\n## How Gradient Boosting Works: The Mathematics\n\n### The Algorithm (Simplified)\n\n1. **Initialize**: Start with a simple prediction (often the mean of training targets)\n   $$F_0(x) = \\bar{y}$$\n\n2. **For each tree m = 1 to M:**\n   \n   a. Calculate **residuals** (errors from current model):\n   $$r_i = y_i - F_{m-1}(x_i)$$\n   \n   b. Fit a new tree $h_m(x)$ to predict these residuals\n   \n   c. Add this tree to the ensemble (with a learning rate $\\eta$):\n   $$F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)$$\n\n3. **Final prediction**: $F_M(x)$ = sum of all trees\n\n**Key Insight**: Each tree is trained to predict the **residuals** (mistakes) of the previous model, not the original targets!\n\n### Example 1: Simple Gradient Boosting by Hand\n\nLet's predict house prices with just 3 trees:\n\n**Training Data:**\n| House | Features | True Price |\n|-------|----------|------------|\n| 1     | Small    | $200K      |\n| 2     | Medium   | $300K      |\n| 3     | Large    | $400K      |\n\n**Step 1: Initialize**\n- $F_0$ = mean price = $(200 + 300 + 400)/3 = 300K$\n- Predictions: [300K, 300K, 300K]\n- Residuals: [-100K, 0K, +100K]\n\n**Step 2: Tree 1 (predict residuals)**\n- Tree 1 learns: \"Small houses → -100K, Large houses → +100K\"\n- After Tree 1: $F_1$ = 300K + Tree1\n  - House 1: 300K + (-100K) = 200K ✓\n  - House 2: 300K + 0K = 300K ✓\n  - House 3: 300K + 100K = 400K ✓\n- New residuals: [0, 0, 0] - Perfect!\n\nIn practice, trees are limited (max_depth, learning_rate), so we need many trees to gradually reduce errors.\n\n### Example 2: With Learning Rate\n\nLet's add a **learning rate** (η = 0.5) to prevent overfitting:\n\n**Step 1: Initialize**\n- $F_0$ = 300K\n- Residuals: [-100K, 0K, +100K]\n\n**Step 2: Tree 1**\n- Tree 1 predicts: [-100K, 0K, +100K]\n- But we only add **50%** of this (η = 0.5)\n- $F_1$ = 300K + 0.5 × Tree1\n  - House 1: 300K + 0.5×(-100K) = 250K\n  - House 2: 300K + 0.5×(0K) = 300K\n  - House 3: 300K + 0.5×(100K) = 350K\n- New residuals: [-50K, 0K, +50K]\n\n**Step 3: Tree 2**\n- Tree 2 predicts these new residuals: [-50K, 0K, +50K]\n- $F_2$ = 250K, 300K, 350K + 0.5 × Tree2\n  - House 1: 250K + 0.5×(-50K) = 225K\n  - House 2: 300K + 0.5×(0K) = 300K\n  - House 3: 350K + 0.5×(50K) = 375K\n- New residuals: [-25K, 0K, +25K]\n\n**Step 4: Tree 3...**\n- Continue, each time cutting the error in half\n\n**Why a learning rate?**\n- Slower learning (smaller steps) often generalizes better\n- More trees needed, but each tree is \"weaker\"\n- Prevents overfitting to training data\n\n## Example 3: Visualizing Sequential Learning\n\n::: {#3462fa13 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import GradientBoostingRegressor\n\n# Simple non-linear relationship\nnp.random.seed(42)\nX = np.linspace(0, 10, 50).reshape(-1, 1)\ny = np.sin(X).ravel() + np.random.normal(0, 0.2, X.shape[0])\n\n# Train with just 3 trees to see progression\ngb = GradientBoostingRegressor(\n    n_estimators=3,\n    max_depth=2,\n    learning_rate=1.0,\n    random_state=42\n)\n\n# Visualize each stage\nfig, axes = plt.subplots(1, 4, figsize=(16, 4))\nX_plot = np.linspace(0, 10, 200).reshape(-1, 1)\n\n# Initial prediction (mean)\naxes[0].scatter(X, y, alpha=0.5)\naxes[0].axhline(y.mean(), color='r', linewidth=2)\naxes[0].set_title('Step 0: Initialize with mean')\naxes[0].set_ylim(-1.5, 1.5)\n\n# After each tree\nfor i in range(1, 4):\n    gb_temp = GradientBoostingRegressor(\n        n_estimators=i,\n        max_depth=2,\n        learning_rate=1.0,\n        random_state=42\n    )\n    gb_temp.fit(X, y)\n    y_pred = gb_temp.predict(X_plot)\n    \n    axes[i].scatter(X, y, alpha=0.5)\n    axes[i].plot(X_plot, y_pred, 'r-', linewidth=2)\n    axes[i].plot(X_plot, np.sin(X_plot), 'g--', alpha=0.5)\n    axes[i].set_title(f'After Tree {i}')\n    axes[i].set_ylim(-1.5, 1.5)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](18B-gradient-boosting-methods_files/figure-html/cell-2-output-1.png){width=1526 height=374}\n:::\n:::\n\n\n**What you'll observe:**\n- Step 0: Flat line (mean)\n- After Tree 1: Rough approximation of the pattern\n- After Tree 2: Better fit, capturing more details\n- After Tree 3: Even closer to the true function\n\nEach tree adds a \"correction layer\" to improve the fit!\n\n## The Three Kings: XGBoost, LightGBM, and CatBoost\n\nAll three implement gradient boosting but with different optimizations and features.\n\n### XGBoost (eXtreme Gradient Boosting)\n\n**History**: Released 2014, dominated Kaggle competitions for years\n\n**Key Features:**\n- **Regularization**: Built-in L1/L2 penalties prevent overfitting\n- **Handling missing values**: Learns best direction for missing data\n- **Parallel processing**: Fast training through parallelized tree construction\n- **Tree pruning**: Grows deep then prunes back (not greedy)\n- **Cache awareness**: Optimized memory access patterns\n\n**When to use:**\n- ✓ You have structured/tabular data\n- ✓ You need high accuracy\n- ✓ You want automatic handling of missing values\n- ✓ Your dataset is medium-sized (10K-1M rows)\n\n### LightGBM (Light Gradient Boosting Machine)\n\n**History**: Released by Microsoft in 2016\n\n**Key Innovation**: **Leaf-wise** (best-first) tree growth instead of level-wise\n\n**Traditional (level-wise):**\n```\n        Root\n       /    \\\n      A      B     <- Grow entire level\n     / \\    / \\\n    C   D  E   F   <- Then next level\n```\n\n**LightGBM (leaf-wise):**\n```\n        Root\n       /    \\\n      A      B\n     / \\\n    C   D            <- Split the leaf with highest gain\n       / \\\n      E   F          <- Then next best leaf\n```\n\n**Key Features:**\n- **Faster training**: Especially on large datasets\n- **Lower memory usage**: Through histogram-based binning\n- **Better accuracy**: On deep trees\n- **Handles categorical features**: Native support without encoding\n\n**When to use:**\n- ✓ You have **large datasets** (>100K rows)\n- ✓ Training speed matters\n- ✓ You have categorical features\n- ✓ You need lower memory footprint\n\n### CatBoost (Categorical Boosting)\n\n**History**: Released by Yandex in 2017\n\n**Key Innovation**: Superior handling of **categorical variables**\n\n**Key Features:**\n- **Ordered boosting**: Reduces overfitting through clever target encoding\n- **Categorical features**: No need for one-hot or label encoding\n- **Symmetric trees**: Faster prediction, better regularization\n- **Minimal tuning**: Works well with default parameters\n- **GPU acceleration**: Excellent GPU support\n\n**When to use:**\n- ✓ You have many **categorical features**\n- ✓ You want minimal hyperparameter tuning\n- ✓ You need fast predictions (deployment)\n- ✓ You want to avoid target leakage from categorical encoding\n\n## Example 4: Comparing All Methods\n\n::: {#dd705199 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor\nimport time\n\n# Load California housing dataset\nfrom sklearn.datasets import fetch_california_housing\nhousing = fetch_california_housing()\nX, y = housing.data, housing.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Dictionary to store results\nresults = {}\n\n# 1. Random Forest (baseline)\nprint(\"Training Random Forest...\")\nstart = time.time()\nrf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\nrf.fit(X_train, y_train)\nrf_time = time.time() - start\nrf_pred = rf.predict(X_test)\nresults['Random Forest'] = {\n    'R2': r2_score(y_test, rf_pred),\n    'RMSE': np.sqrt(mean_squared_error(y_test, rf_pred)),\n    'Time': rf_time\n}\n\n# 2. Sklearn Gradient Boosting\nprint(\"Training Sklearn GradientBoosting...\")\nstart = time.time()\ngb = GradientBoostingRegressor(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=3,\n    random_state=42\n)\ngb.fit(X_train, y_train)\ngb_time = time.time() - start\ngb_pred = gb.predict(X_test)\nresults['GradientBoosting'] = {\n    'R2': r2_score(y_test, gb_pred),\n    'RMSE': np.sqrt(mean_squared_error(y_test, gb_pred)),\n    'Time': gb_time\n}\n\n# 3. XGBoost\nprint(\"Training XGBoost...\")\nstart = time.time()\nxgb_model = xgb.XGBRegressor(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=3,\n    random_state=42\n)\nxgb_model.fit(X_train, y_train)\nxgb_time = time.time() - start\nxgb_pred = xgb_model.predict(X_test)\nresults['XGBoost'] = {\n    'R2': r2_score(y_test, xgb_pred),\n    'RMSE': np.sqrt(mean_squared_error(y_test, xgb_pred)),\n    'Time': xgb_time\n}\n\n# 4. LightGBM\nprint(\"Training LightGBM...\")\nstart = time.time()\nlgb_model = lgb.LGBMRegressor(\n    n_estimators=100,\n    learning_rate=0.1,\n    max_depth=3,\n    random_state=42,\n    verbose=-1\n)\nlgb_model.fit(X_train, y_train)\nlgb_time = time.time() - start\nlgb_pred = lgb_model.predict(X_test)\nresults['LightGBM'] = {\n    'R2': r2_score(y_test, lgb_pred),\n    'RMSE': np.sqrt(mean_squared_error(y_test, lgb_pred)),\n    'Time': lgb_time\n}\n\n# 5. CatBoost\nprint(\"Training CatBoost...\")\nstart = time.time()\ncat_model = CatBoostRegressor(\n    iterations=100,\n    learning_rate=0.1,\n    depth=3,\n    random_state=42,\n    verbose=0\n)\ncat_model.fit(X_train, y_train)\ncat_time = time.time() - start\ncat_pred = cat_model.predict(X_test)\nresults['CatBoost'] = {\n    'R2': r2_score(y_test, cat_pred),\n    'RMSE': np.sqrt(mean_squared_error(y_test, cat_pred)),\n    'Time': cat_time\n}\n\n# Display results\nresults_df = pd.DataFrame(results).T\nresults_df = results_df.sort_values('R2', ascending=False)\nprint(\"\\n\" + \"=\"*60)\nprint(\"RESULTS COMPARISON\")\nprint(\"=\"*60)\nprint(results_df.to_string())\nprint(\"=\"*60)\n\n# Typical output:\n#                     R2      RMSE    Time\n# XGBoost          0.847     0.476   0.123\n# LightGBM         0.846     0.477   0.089\n# CatBoost         0.845     0.479   0.234\n# GradientBoosting 0.842     0.483   0.543\n# Random Forest    0.812     0.527   0.178\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining Random Forest...\nTraining Sklearn GradientBoosting...\nTraining XGBoost...\nTraining LightGBM...\nTraining CatBoost...\n\n============================================================\nRESULTS COMPARISON\n============================================================\n                        R2      RMSE      Time\nRandom Forest     0.804850  0.505694  0.532041\nLightGBM          0.779154  0.537958  0.164001\nGradientBoosting  0.775645  0.542215  2.156963\nXGBoost           0.774706  0.543348  0.111631\nCatBoost          0.751702  0.570414  0.142931\n============================================================\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning:\n\nX does not have valid feature names, but LGBMRegressor was fitted with feature names\n\n```\n:::\n:::\n\n\n**Typical Observations:**\n- **Accuracy**: Gradient boosting methods beat random forest\n- **Speed**: LightGBM is fastest, sklearn GB is slowest\n- **Consistency**: All three (XGB, LGB, CB) perform similarly well\n\n## Key Hyperparameters\n\n### Common to All Boosting Methods:\n\n1. **n_estimators / iterations**: Number of trees\n   - More trees → better fit, but risk overfitting and slower\n   - Start: 100, increase to 500-1000 with proper early stopping\n   - Example: `n_estimators=500`\n\n2. **learning_rate / eta**: How much each tree contributes\n   - Lower → better generalization, need more trees\n   - Higher → faster training, risk overfitting\n   - Typical: 0.01 to 0.3\n   - Rule: learning_rate × n_estimators ≈ constant\n   - Example: `learning_rate=0.05`\n\n3. **max_depth / depth**: Maximum depth of each tree\n   - Deeper → can capture complex patterns, risk overfitting\n   - Shallower → faster, better regularization\n   - Typical: 3-8\n   - Example: `max_depth=5`\n\n4. **subsample**: Fraction of samples used for each tree\n   - < 1.0 → adds randomness, prevents overfitting\n   - Typical: 0.8\n   - Example: `subsample=0.8`\n\n5. **colsample_bytree**: Fraction of features per tree\n   - < 1.0 → adds randomness, like random forest\n   - Typical: 0.8\n   - Example: `colsample_bytree=0.8`\n\n### XGBoost Specific:\n\n6. **reg_alpha**: L1 regularization\n   - Higher → more regularization\n   - Example: `reg_alpha=0.1`\n\n7. **reg_lambda**: L2 regularization\n   - Higher → more regularization\n   - Example: `reg_lambda=1.0`\n\n### LightGBM Specific:\n\n8. **num_leaves**: Number of leaves (instead of max_depth)\n   - Controls model complexity\n   - Rule: `num_leaves < 2^max_depth`\n   - Example: `num_leaves=31`\n\n9. **min_child_samples**: Minimum samples in a leaf\n   - Prevents overfitting\n   - Example: `min_child_samples=20`\n\n### CatBoost Specific:\n\n10. **cat_features**: List of categorical feature indices\n    - Automatic handling\n    - Example: `cat_features=[0, 3, 5]`\n\n## Example 5: Hyperparameter Tuning with Cross-Validation\n\n::: {#e171e8a5 .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\n\n# Define parameter grid\nparam_grid = {\n    'n_estimators': [100, 300, 500],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'max_depth': [3, 5, 7],\n    'subsample': [0.8, 1.0],\n    'colsample_bytree': [0.8, 1.0]\n}\n\n# XGBoost model\nxgb_model = xgb.XGBRegressor(random_state=42)\n\n# Grid search with cross-validation\ngrid_search = GridSearchCV(\n    xgb_model,\n    param_grid,\n    cv=5,\n    scoring='neg_mean_squared_error',\n    n_jobs=-1,\n    verbose=1\n)\n\n# Fit\ngrid_search.fit(X_train, y_train)\n\n# Best parameters\nprint(f\"Best parameters: {grid_search.best_params_}\")\nprint(f\"Best CV score: {np.sqrt(-grid_search.best_score_):.4f}\")\n\n# Test performance\nbest_model = grid_search.best_estimator_\ny_pred = best_model.predict(X_test)\nprint(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\nprint(f\"Test R²: {r2_score(y_test, y_pred):.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFitting 5 folds for each of 108 candidates, totalling 540 fits\nBest parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.05, 'max_depth': 7, 'n_estimators': 500, 'subsample': 0.8}\nBest CV score: 0.4489\nTest RMSE: 0.4365\nTest R²: 0.8546\n```\n:::\n:::\n\n\n## Example 6: Early Stopping (Crucial!)\n\nPrevent overfitting by stopping when validation performance stops improving:\n\n::: {#c502f562 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\n# Split into train and validation\nX_train_sub, X_val, y_train_sub, y_val = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=42\n)\n\n# XGBoost with early stopping\nxgb_model = xgb.XGBRegressor(\n    n_estimators=1000,  # Set high\n    learning_rate=0.05,\n    max_depth=5,\n    random_state=42,\n    early_stopping_rounds=50\n)\n\n# Fit with early stopping\nxgb_model.fit(\n    X_train_sub, y_train_sub,\n    eval_set=[(X_val, y_val)],\n    verbose=10\n)\n\nprint(f\"Best iteration: {xgb_model.best_iteration}\")\nprint(f\"Best score: {xgb_model.best_score}\")\n\n# LightGBM version\nimport lightgbm as lgb\n\nlgb_model = lgb.LGBMRegressor(\n    n_estimators=1000,\n    learning_rate=0.05,\n    max_depth=5,\n    random_state=42\n)\n\nlgb_model.fit(\n    X_train_sub, y_train_sub,\n    eval_set=[(X_val, y_val)],\n    callbacks=[lgb.early_stopping(stopping_rounds=50)]\n)\n\n# CatBoost version\nfrom catboost import CatBoostRegressor, Pool\n\ncat_model = CatBoostRegressor(\n    iterations=1000,\n    learning_rate=0.05,\n    depth=5,\n    random_state=42,\n    verbose=0\n)\n\ncat_model.fit(\n    X_train_sub, y_train_sub,\n    eval_set=(X_val, y_val),\n    early_stopping_rounds=50\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0]\tvalidation_0-rmse:1.13853\n[10]\tvalidation_0-rmse:0.88241\n[20]\tvalidation_0-rmse:0.74413\n[30]\tvalidation_0-rmse:0.66982\n[40]\tvalidation_0-rmse:0.62565\n[50]\tvalidation_0-rmse:0.59119\n[60]\tvalidation_0-rmse:0.56868\n[70]\tvalidation_0-rmse:0.55319\n[80]\tvalidation_0-rmse:0.54303\n[90]\tvalidation_0-rmse:0.53409\n[100]\tvalidation_0-rmse:0.52949\n[110]\tvalidation_0-rmse:0.52532\n[120]\tvalidation_0-rmse:0.52162\n[130]\tvalidation_0-rmse:0.51892\n[140]\tvalidation_0-rmse:0.51587\n[150]\tvalidation_0-rmse:0.51342\n[160]\tvalidation_0-rmse:0.51099\n[170]\tvalidation_0-rmse:0.50972\n[180]\tvalidation_0-rmse:0.50783\n[190]\tvalidation_0-rmse:0.50657\n[200]\tvalidation_0-rmse:0.50443\n[210]\tvalidation_0-rmse:0.50331\n[220]\tvalidation_0-rmse:0.50249\n[230]\tvalidation_0-rmse:0.50070\n[240]\tvalidation_0-rmse:0.49922\n[250]\tvalidation_0-rmse:0.49757\n[260]\tvalidation_0-rmse:0.49672\n[270]\tvalidation_0-rmse:0.49509\n[280]\tvalidation_0-rmse:0.49402\n[290]\tvalidation_0-rmse:0.49312\n[300]\tvalidation_0-rmse:0.49231\n[310]\tvalidation_0-rmse:0.49160\n[320]\tvalidation_0-rmse:0.49101\n[330]\tvalidation_0-rmse:0.49052\n[340]\tvalidation_0-rmse:0.49017\n[350]\tvalidation_0-rmse:0.49005\n[360]\tvalidation_0-rmse:0.48930\n[370]\tvalidation_0-rmse:0.48894\n[380]\tvalidation_0-rmse:0.48849\n[390]\tvalidation_0-rmse:0.48783\n[400]\tvalidation_0-rmse:0.48742\n[410]\tvalidation_0-rmse:0.48706\n[420]\tvalidation_0-rmse:0.48612\n[430]\tvalidation_0-rmse:0.48555\n[440]\tvalidation_0-rmse:0.48496\n[450]\tvalidation_0-rmse:0.48461\n[460]\tvalidation_0-rmse:0.48412\n[470]\tvalidation_0-rmse:0.48367\n[480]\tvalidation_0-rmse:0.48303\n[490]\tvalidation_0-rmse:0.48271\n[500]\tvalidation_0-rmse:0.48218\n[510]\tvalidation_0-rmse:0.48195\n[520]\tvalidation_0-rmse:0.48133\n[530]\tvalidation_0-rmse:0.48079\n[540]\tvalidation_0-rmse:0.48054\n[550]\tvalidation_0-rmse:0.48025\n[560]\tvalidation_0-rmse:0.47988\n[570]\tvalidation_0-rmse:0.47934\n[580]\tvalidation_0-rmse:0.47914\n[590]\tvalidation_0-rmse:0.47843\n[600]\tvalidation_0-rmse:0.47818\n[610]\tvalidation_0-rmse:0.47801\n[620]\tvalidation_0-rmse:0.47790\n[630]\tvalidation_0-rmse:0.47790\n[640]\tvalidation_0-rmse:0.47774\n[650]\tvalidation_0-rmse:0.47729\n[660]\tvalidation_0-rmse:0.47699\n[670]\tvalidation_0-rmse:0.47659\n[680]\tvalidation_0-rmse:0.47609\n[690]\tvalidation_0-rmse:0.47596\n[700]\tvalidation_0-rmse:0.47538\n[710]\tvalidation_0-rmse:0.47489\n[720]\tvalidation_0-rmse:0.47460\n[730]\tvalidation_0-rmse:0.47451\n[740]\tvalidation_0-rmse:0.47437\n[750]\tvalidation_0-rmse:0.47400\n[760]\tvalidation_0-rmse:0.47381\n[770]\tvalidation_0-rmse:0.47356\n[780]\tvalidation_0-rmse:0.47344\n[790]\tvalidation_0-rmse:0.47343\n[800]\tvalidation_0-rmse:0.47346\n[810]\tvalidation_0-rmse:0.47344\n[820]\tvalidation_0-rmse:0.47326\n[830]\tvalidation_0-rmse:0.47304\n[840]\tvalidation_0-rmse:0.47294\n[850]\tvalidation_0-rmse:0.47275\n[860]\tvalidation_0-rmse:0.47239\n[870]\tvalidation_0-rmse:0.47237\n[880]\tvalidation_0-rmse:0.47211\n[890]\tvalidation_0-rmse:0.47195\n[900]\tvalidation_0-rmse:0.47172\n[910]\tvalidation_0-rmse:0.47148\n[920]\tvalidation_0-rmse:0.47129\n[930]\tvalidation_0-rmse:0.47112\n[940]\tvalidation_0-rmse:0.47098\n[950]\tvalidation_0-rmse:0.47087\n[960]\tvalidation_0-rmse:0.47063\n[970]\tvalidation_0-rmse:0.47057\n[980]\tvalidation_0-rmse:0.47056\n[990]\tvalidation_0-rmse:0.47050\n[999]\tvalidation_0-rmse:0.47032\nBest iteration: 999\nBest score: 0.4703220254083186\nTraining until validation scores don't improve for 50 rounds\nDid not meet early stopping. Best iteration is:\n[1000]\tvalid_0's l2: 0.225795\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n<catboost.core.CatBoostRegressor at 0x124df8fd0>\n```\n:::\n:::\n\n\n**Best Practice**: Always use early stopping with a validation set!\n\n## Example 7: Feature Importance\n\n::: {#197e7068 .cell execution_count=6}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\n# Train XGBoost\nxgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)\nxgb_model.fit(X_train, y_train)\n\n# Get feature importance\nimportance = xgb_model.feature_importances_\nfeature_names = housing.feature_names\n\n# Create DataFrame for easy sorting\nimportance_df = pd.DataFrame({\n    'Feature': feature_names,\n    'Importance': importance\n}).sort_values('Importance', ascending=False)\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.barh(importance_df['Feature'], importance_df['Importance'])\nplt.xlabel('Importance')\nplt.title('XGBoost Feature Importance')\nplt.tight_layout()\nplt.show()\n\nprint(importance_df)\n\n# Multiple importance types in XGBoost\nxgb.plot_importance(xgb_model, importance_type='gain')  # Average gain\nxgb.plot_importance(xgb_model, importance_type='weight')  # Number of splits\nxgb.plot_importance(xgb_model, importance_type='cover')  # Coverage\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](18B-gradient-boosting-methods_files/figure-html/cell-6-output-1.png){width=950 height=566}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n      Feature  Importance\n0      MedInc    0.489623\n5    AveOccup    0.148580\n7   Longitude    0.107952\n6    Latitude    0.090290\n1    HouseAge    0.070058\n2    AveRooms    0.043091\n3   AveBedrms    0.025678\n4  Population    0.024728\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](18B-gradient-boosting-methods_files/figure-html/cell-6-output-3.png){width=701 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](18B-gradient-boosting-methods_files/figure-html/cell-6-output-4.png){width=587 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](18B-gradient-boosting-methods_files/figure-html/cell-6-output-5.png){width=657 height=449}\n:::\n:::\n\n\n## Example 8: Handling Categorical Features with CatBoost\n\n::: {#ed6c9357 .cell execution_count=7}\n``` {.python .cell-code}\nfrom catboost import CatBoostRegressor\nimport pandas as pd\n\n# Create sample data with categorical features\ndata = pd.DataFrame({\n    'size': [1200, 1500, 1800, 2000, 2200, 2500],\n    'neighborhood': ['A', 'B', 'A', 'C', 'B', 'C'],\n    'style': ['Modern', 'Victorian', 'Modern', 'Ranch', 'Victorian', 'Ranch'],\n    'price': [200, 250, 280, 320, 300, 380]\n})\n\nX = data[['size', 'neighborhood', 'style']]\ny = data['price']\n\n# Identify categorical columns\ncat_features = ['neighborhood', 'style']  # or indices [1, 2]\n\n# CatBoost handles categorical features natively!\nmodel = CatBoostRegressor(\n    iterations=100,\n    learning_rate=0.1,\n    depth=3,\n    cat_features=cat_features,\n    random_state=42,\n    verbose=0\n)\n\nmodel.fit(X, y)\n\n# Make predictions (can handle categorical inputs directly)\nnew_house = pd.DataFrame({\n    'size': [1600],\n    'neighborhood': ['A'],\n    'style': ['Modern']\n})\nprediction = model.predict(new_house)\nprint(f\"Predicted price: ${prediction[0]:.0f}K\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPredicted price: $214K\n```\n:::\n:::\n\n\n**Why this matters**: No need for label encoding or one-hot encoding, which can:\n- Create sparse high-dimensional data\n- Introduce order where none exists\n- Cause target leakage\n\n## Comparison Table: Which Method to Choose?\n\n| Criterion | Random Forest | XGBoost | LightGBM | CatBoost |\n|-----------|--------------|---------|----------|----------|\n| **Accuracy** | Good | Excellent | Excellent | Excellent |\n| **Training Speed** | Medium | Medium | **Fast** | Slow |\n| **Prediction Speed** | Slow | Medium | Medium | **Fast** |\n| **Memory Usage** | High | Medium | **Low** | Medium |\n| **Large Datasets (>1M)** | ✗ | ✓ | **✓✓** | ✓ |\n| **Categorical Features** | ✗ | △ | ✓ | **✓✓** |\n| **Handles Missing Data** | ✓ | **✓✓** | ✓ | ✓ |\n| **Overfitting Risk** | Low | Medium | **High** | Low |\n| **Hyperparameter Tuning** | Easy | Medium | **Hard** | Easy |\n| **GPU Support** | ✗ | ✓ | ✓ | **✓✓** |\n| **Interpretability** | Medium | Medium | Medium | Medium |\n\n### Decision Guide:\n\n**Use XGBoost if:**\n- You want excellent out-of-box performance\n- You have missing data\n- You need a balance of speed and accuracy\n- You're used to it (most tutorials use XGBoost)\n\n**Use LightGBM if:**\n- You have **large datasets** (>100K rows)\n- Training speed is critical\n- You need low memory footprint\n- You have some categorical features\n\n**Use CatBoost if:**\n- You have **many categorical features**\n- You want minimal hyperparameter tuning\n- Prediction speed (deployment) matters\n- You want to avoid target leakage\n\n**Use Random Forest if:**\n- You need an easy baseline\n- Interpretability is important\n- You have small data (<10K rows)\n- You want parallel training with zero dependencies between models\n\n## Common Pitfalls and Best Practices\n\n### Pitfall 1: Not Using Early Stopping\n\n::: {#a90e40d7 .cell execution_count=8}\n``` {.python .cell-code}\n# ❌ BAD: Fixed number of trees, might overfit\nmodel = xgb.XGBRegressor(n_estimators=1000)\nmodel.fit(X_train, y_train)\n\n# ✅ GOOD: Early stopping prevents overfitting\nmodel = xgb.XGBRegressor(n_estimators=1000, early_stopping_rounds=50)\nmodel.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)]\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0]\tvalidation_0-rmse:0.95317\n[1]\tvalidation_0-rmse:0.80177\n[2]\tvalidation_0-rmse:0.70985\n[3]\tvalidation_0-rmse:0.64199\n[4]\tvalidation_0-rmse:0.60027\n[5]\tvalidation_0-rmse:0.57285\n[6]\tvalidation_0-rmse:0.54831\n[7]\tvalidation_0-rmse:0.53231\n[8]\tvalidation_0-rmse:0.51933\n[9]\tvalidation_0-rmse:0.49908\n[10]\tvalidation_0-rmse:0.48941\n[11]\tvalidation_0-rmse:0.47708\n[12]\tvalidation_0-rmse:0.46951\n[13]\tvalidation_0-rmse:0.46614\n[14]\tvalidation_0-rmse:0.46100\n[15]\tvalidation_0-rmse:0.45707\n[16]\tvalidation_0-rmse:0.44993\n[17]\tvalidation_0-rmse:0.44495\n[18]\tvalidation_0-rmse:0.43626\n[19]\tvalidation_0-rmse:0.43149\n[20]\tvalidation_0-rmse:0.42782\n[21]\tvalidation_0-rmse:0.42510\n[22]\tvalidation_0-rmse:0.42209\n[23]\tvalidation_0-rmse:0.41971\n[24]\tvalidation_0-rmse:0.41469\n[25]\tvalidation_0-rmse:0.41016\n[26]\tvalidation_0-rmse:0.40601\n[27]\tvalidation_0-rmse:0.40295\n[28]\tvalidation_0-rmse:0.40003\n[29]\tvalidation_0-rmse:0.39857\n[30]\tvalidation_0-rmse:0.39418\n[31]\tvalidation_0-rmse:0.39134\n[32]\tvalidation_0-rmse:0.39051\n[33]\tvalidation_0-rmse:0.38747\n[34]\tvalidation_0-rmse:0.38382\n[35]\tvalidation_0-rmse:0.37948\n[36]\tvalidation_0-rmse:0.37602\n[37]\tvalidation_0-rmse:0.37429\n[38]\tvalidation_0-rmse:0.37144\n[39]\tvalidation_0-rmse:0.36995\n[40]\tvalidation_0-rmse:0.36850\n[41]\tvalidation_0-rmse:0.36609\n[42]\tvalidation_0-rmse:0.36355\n[43]\tvalidation_0-rmse:0.36255\n[44]\tvalidation_0-rmse:0.36032\n[45]\tvalidation_0-rmse:0.35911\n[46]\tvalidation_0-rmse:0.35819\n[47]\tvalidation_0-rmse:0.35659\n[48]\tvalidation_0-rmse:0.35312\n[49]\tvalidation_0-rmse:0.35102\n[50]\tvalidation_0-rmse:0.34910\n[51]\tvalidation_0-rmse:0.34751\n[52]\tvalidation_0-rmse:0.34682\n[53]\tvalidation_0-rmse:0.34401\n[54]\tvalidation_0-rmse:0.34256\n[55]\tvalidation_0-rmse:0.33970\n[56]\tvalidation_0-rmse:0.33808\n[57]\tvalidation_0-rmse:0.33663\n[58]\tvalidation_0-rmse:0.33499\n[59]\tvalidation_0-rmse:0.33419\n[60]\tvalidation_0-rmse:0.33231\n[61]\tvalidation_0-rmse:0.33139\n[62]\tvalidation_0-rmse:0.33023\n[63]\tvalidation_0-rmse:0.32863\n[64]\tvalidation_0-rmse:0.32731\n[65]\tvalidation_0-rmse:0.32682\n[66]\tvalidation_0-rmse:0.32479\n[67]\tvalidation_0-rmse:0.32234\n[68]\tvalidation_0-rmse:0.32056\n[69]\tvalidation_0-rmse:0.31921\n[70]\tvalidation_0-rmse:0.31772\n[71]\tvalidation_0-rmse:0.31718\n[72]\tvalidation_0-rmse:0.31667\n[73]\tvalidation_0-rmse:0.31555\n[74]\tvalidation_0-rmse:0.31282\n[75]\tvalidation_0-rmse:0.31076\n[76]\tvalidation_0-rmse:0.30932\n[77]\tvalidation_0-rmse:0.30773\n[78]\tvalidation_0-rmse:0.30607\n[79]\tvalidation_0-rmse:0.30556\n[80]\tvalidation_0-rmse:0.30505\n[81]\tvalidation_0-rmse:0.30388\n[82]\tvalidation_0-rmse:0.30232\n[83]\tvalidation_0-rmse:0.30157\n[84]\tvalidation_0-rmse:0.30031\n[85]\tvalidation_0-rmse:0.29808\n[86]\tvalidation_0-rmse:0.29673\n[87]\tvalidation_0-rmse:0.29531\n[88]\tvalidation_0-rmse:0.29405\n[89]\tvalidation_0-rmse:0.29227\n[90]\tvalidation_0-rmse:0.29065\n[91]\tvalidation_0-rmse:0.29000\n[92]\tvalidation_0-rmse:0.28906\n[93]\tvalidation_0-rmse:0.28814\n[94]\tvalidation_0-rmse:0.28685\n[95]\tvalidation_0-rmse:0.28599\n[96]\tvalidation_0-rmse:0.28455\n[97]\tvalidation_0-rmse:0.28388\n[98]\tvalidation_0-rmse:0.28222\n[99]\tvalidation_0-rmse:0.28020\n[100]\tvalidation_0-rmse:0.27923\n[101]\tvalidation_0-rmse:0.27788\n[102]\tvalidation_0-rmse:0.27714\n[103]\tvalidation_0-rmse:0.27629\n[104]\tvalidation_0-rmse:0.27459\n[105]\tvalidation_0-rmse:0.27361\n[106]\tvalidation_0-rmse:0.27276\n[107]\tvalidation_0-rmse:0.27155\n[108]\tvalidation_0-rmse:0.27074\n[109]\tvalidation_0-rmse:0.26991\n[110]\tvalidation_0-rmse:0.26828\n[111]\tvalidation_0-rmse:0.26765\n[112]\tvalidation_0-rmse:0.26717\n[113]\tvalidation_0-rmse:0.26651\n[114]\tvalidation_0-rmse:0.26589\n[115]\tvalidation_0-rmse:0.26432\n[116]\tvalidation_0-rmse:0.26378\n[117]\tvalidation_0-rmse:0.26292\n[118]\tvalidation_0-rmse:0.26150\n[119]\tvalidation_0-rmse:0.26002\n[120]\tvalidation_0-rmse:0.25968\n[121]\tvalidation_0-rmse:0.25846\n[122]\tvalidation_0-rmse:0.25708\n[123]\tvalidation_0-rmse:0.25591\n[124]\tvalidation_0-rmse:0.25483\n[125]\tvalidation_0-rmse:0.25401\n[126]\tvalidation_0-rmse:0.25348\n[127]\tvalidation_0-rmse:0.25239\n[128]\tvalidation_0-rmse:0.25154\n[129]\tvalidation_0-rmse:0.25048\n[130]\tvalidation_0-rmse:0.24883\n[131]\tvalidation_0-rmse:0.24809\n[132]\tvalidation_0-rmse:0.24796\n[133]\tvalidation_0-rmse:0.24758\n[134]\tvalidation_0-rmse:0.24623\n[135]\tvalidation_0-rmse:0.24548\n[136]\tvalidation_0-rmse:0.24412\n[137]\tvalidation_0-rmse:0.24362\n[138]\tvalidation_0-rmse:0.24287\n[139]\tvalidation_0-rmse:0.24203\n[140]\tvalidation_0-rmse:0.24130\n[141]\tvalidation_0-rmse:0.24026\n[142]\tvalidation_0-rmse:0.23950\n[143]\tvalidation_0-rmse:0.23862\n[144]\tvalidation_0-rmse:0.23778\n[145]\tvalidation_0-rmse:0.23685\n[146]\tvalidation_0-rmse:0.23582\n[147]\tvalidation_0-rmse:0.23518\n[148]\tvalidation_0-rmse:0.23457\n[149]\tvalidation_0-rmse:0.23361\n[150]\tvalidation_0-rmse:0.23306\n[151]\tvalidation_0-rmse:0.23293\n[152]\tvalidation_0-rmse:0.23164\n[153]\tvalidation_0-rmse:0.23144\n[154]\tvalidation_0-rmse:0.23085\n[155]\tvalidation_0-rmse:0.23006\n[156]\tvalidation_0-rmse:0.22922\n[157]\tvalidation_0-rmse:0.22863\n[158]\tvalidation_0-rmse:0.22767\n[159]\tvalidation_0-rmse:0.22702\n[160]\tvalidation_0-rmse:0.22566\n[161]\tvalidation_0-rmse:0.22511\n[162]\tvalidation_0-rmse:0.22393\n[163]\tvalidation_0-rmse:0.22302\n[164]\tvalidation_0-rmse:0.22180\n[165]\tvalidation_0-rmse:0.22054\n[166]\tvalidation_0-rmse:0.21994\n[167]\tvalidation_0-rmse:0.21897\n[168]\tvalidation_0-rmse:0.21824\n[169]\tvalidation_0-rmse:0.21724\n[170]\tvalidation_0-rmse:0.21652\n[171]\tvalidation_0-rmse:0.21570\n[172]\tvalidation_0-rmse:0.21491\n[173]\tvalidation_0-rmse:0.21431\n[174]\tvalidation_0-rmse:0.21359\n[175]\tvalidation_0-rmse:0.21270\n[176]\tvalidation_0-rmse:0.21171\n[177]\tvalidation_0-rmse:0.21087\n[178]\tvalidation_0-rmse:0.21038\n[179]\tvalidation_0-rmse:0.20990\n[180]\tvalidation_0-rmse:0.20910\n[181]\tvalidation_0-rmse:0.20864\n[182]\tvalidation_0-rmse:0.20717\n[183]\tvalidation_0-rmse:0.20635\n[184]\tvalidation_0-rmse:0.20533\n[185]\tvalidation_0-rmse:0.20496\n[186]\tvalidation_0-rmse:0.20450\n[187]\tvalidation_0-rmse:0.20414\n[188]\tvalidation_0-rmse:0.20343\n[189]\tvalidation_0-rmse:0.20287\n[190]\tvalidation_0-rmse:0.20261\n[191]\tvalidation_0-rmse:0.20207\n[192]\tvalidation_0-rmse:0.20146\n[193]\tvalidation_0-rmse:0.20119\n[194]\tvalidation_0-rmse:0.20068\n[195]\tvalidation_0-rmse:0.19998\n[196]\tvalidation_0-rmse:0.19887\n[197]\tvalidation_0-rmse:0.19837\n[198]\tvalidation_0-rmse:0.19730\n[199]\tvalidation_0-rmse:0.19698\n[200]\tvalidation_0-rmse:0.19637\n[201]\tvalidation_0-rmse:0.19594\n[202]\tvalidation_0-rmse:0.19532\n[203]\tvalidation_0-rmse:0.19481\n[204]\tvalidation_0-rmse:0.19432\n[205]\tvalidation_0-rmse:0.19376\n[206]\tvalidation_0-rmse:0.19328\n[207]\tvalidation_0-rmse:0.19273\n[208]\tvalidation_0-rmse:0.19171\n[209]\tvalidation_0-rmse:0.19123\n[210]\tvalidation_0-rmse:0.19085\n[211]\tvalidation_0-rmse:0.19037\n[212]\tvalidation_0-rmse:0.18977\n[213]\tvalidation_0-rmse:0.18937\n[214]\tvalidation_0-rmse:0.18833\n[215]\tvalidation_0-rmse:0.18827\n[216]\tvalidation_0-rmse:0.18777\n[217]\tvalidation_0-rmse:0.18747\n[218]\tvalidation_0-rmse:0.18692\n[219]\tvalidation_0-rmse:0.18628\n[220]\tvalidation_0-rmse:0.18503\n[221]\tvalidation_0-rmse:0.18487\n[222]\tvalidation_0-rmse:0.18381\n[223]\tvalidation_0-rmse:0.18311\n[224]\tvalidation_0-rmse:0.18225\n[225]\tvalidation_0-rmse:0.18179\n[226]\tvalidation_0-rmse:0.18130\n[227]\tvalidation_0-rmse:0.18092\n[228]\tvalidation_0-rmse:0.18070\n[229]\tvalidation_0-rmse:0.18035\n[230]\tvalidation_0-rmse:0.17990\n[231]\tvalidation_0-rmse:0.17940\n[232]\tvalidation_0-rmse:0.17917\n[233]\tvalidation_0-rmse:0.17849\n[234]\tvalidation_0-rmse:0.17817\n[235]\tvalidation_0-rmse:0.17705\n[236]\tvalidation_0-rmse:0.17675\n[237]\tvalidation_0-rmse:0.17654\n[238]\tvalidation_0-rmse:0.17630\n[239]\tvalidation_0-rmse:0.17611\n[240]\tvalidation_0-rmse:0.17553\n[241]\tvalidation_0-rmse:0.17508\n[242]\tvalidation_0-rmse:0.17447\n[243]\tvalidation_0-rmse:0.17377\n[244]\tvalidation_0-rmse:0.17341\n[245]\tvalidation_0-rmse:0.17276\n[246]\tvalidation_0-rmse:0.17258\n[247]\tvalidation_0-rmse:0.17213\n[248]\tvalidation_0-rmse:0.17174\n[249]\tvalidation_0-rmse:0.17159\n[250]\tvalidation_0-rmse:0.17128\n[251]\tvalidation_0-rmse:0.17038\n[252]\tvalidation_0-rmse:0.16988\n[253]\tvalidation_0-rmse:0.16963\n[254]\tvalidation_0-rmse:0.16911\n[255]\tvalidation_0-rmse:0.16878\n[256]\tvalidation_0-rmse:0.16824\n[257]\tvalidation_0-rmse:0.16800\n[258]\tvalidation_0-rmse:0.16794\n[259]\tvalidation_0-rmse:0.16768\n[260]\tvalidation_0-rmse:0.16734\n[261]\tvalidation_0-rmse:0.16714\n[262]\tvalidation_0-rmse:0.16703\n[263]\tvalidation_0-rmse:0.16686\n[264]\tvalidation_0-rmse:0.16620\n[265]\tvalidation_0-rmse:0.16573\n[266]\tvalidation_0-rmse:0.16519\n[267]\tvalidation_0-rmse:0.16447\n[268]\tvalidation_0-rmse:0.16389\n[269]\tvalidation_0-rmse:0.16353\n[270]\tvalidation_0-rmse:0.16327\n[271]\tvalidation_0-rmse:0.16284\n[272]\tvalidation_0-rmse:0.16260\n[273]\tvalidation_0-rmse:0.16255\n[274]\tvalidation_0-rmse:0.16203\n[275]\tvalidation_0-rmse:0.16160\n[276]\tvalidation_0-rmse:0.16087\n[277]\tvalidation_0-rmse:0.16014\n[278]\tvalidation_0-rmse:0.15928\n[279]\tvalidation_0-rmse:0.15894\n[280]\tvalidation_0-rmse:0.15837\n[281]\tvalidation_0-rmse:0.15811\n[282]\tvalidation_0-rmse:0.15766\n[283]\tvalidation_0-rmse:0.15645\n[284]\tvalidation_0-rmse:0.15587\n[285]\tvalidation_0-rmse:0.15484\n[286]\tvalidation_0-rmse:0.15467\n[287]\tvalidation_0-rmse:0.15410\n[288]\tvalidation_0-rmse:0.15380\n[289]\tvalidation_0-rmse:0.15303\n[290]\tvalidation_0-rmse:0.15256\n[291]\tvalidation_0-rmse:0.15193\n[292]\tvalidation_0-rmse:0.15162\n[293]\tvalidation_0-rmse:0.15106\n[294]\tvalidation_0-rmse:0.15080\n[295]\tvalidation_0-rmse:0.15058\n[296]\tvalidation_0-rmse:0.15013\n[297]\tvalidation_0-rmse:0.14977\n[298]\tvalidation_0-rmse:0.14935\n[299]\tvalidation_0-rmse:0.14921\n[300]\tvalidation_0-rmse:0.14908\n[301]\tvalidation_0-rmse:0.14864\n[302]\tvalidation_0-rmse:0.14830\n[303]\tvalidation_0-rmse:0.14780\n[304]\tvalidation_0-rmse:0.14712\n[305]\tvalidation_0-rmse:0.14662\n[306]\tvalidation_0-rmse:0.14640\n[307]\tvalidation_0-rmse:0.14587\n[308]\tvalidation_0-rmse:0.14541\n[309]\tvalidation_0-rmse:0.14473\n[310]\tvalidation_0-rmse:0.14399\n[311]\tvalidation_0-rmse:0.14380\n[312]\tvalidation_0-rmse:0.14361\n[313]\tvalidation_0-rmse:0.14324\n[314]\tvalidation_0-rmse:0.14272\n[315]\tvalidation_0-rmse:0.14228\n[316]\tvalidation_0-rmse:0.14222\n[317]\tvalidation_0-rmse:0.14178\n[318]\tvalidation_0-rmse:0.14122\n[319]\tvalidation_0-rmse:0.14087\n[320]\tvalidation_0-rmse:0.14036\n[321]\tvalidation_0-rmse:0.13996\n[322]\tvalidation_0-rmse:0.13950\n[323]\tvalidation_0-rmse:0.13921\n[324]\tvalidation_0-rmse:0.13881\n[325]\tvalidation_0-rmse:0.13831\n[326]\tvalidation_0-rmse:0.13802\n[327]\tvalidation_0-rmse:0.13779\n[328]\tvalidation_0-rmse:0.13723\n[329]\tvalidation_0-rmse:0.13678\n[330]\tvalidation_0-rmse:0.13623\n[331]\tvalidation_0-rmse:0.13607\n[332]\tvalidation_0-rmse:0.13545\n[333]\tvalidation_0-rmse:0.13482\n[334]\tvalidation_0-rmse:0.13420\n[335]\tvalidation_0-rmse:0.13411\n[336]\tvalidation_0-rmse:0.13355\n[337]\tvalidation_0-rmse:0.13297\n[338]\tvalidation_0-rmse:0.13267\n[339]\tvalidation_0-rmse:0.13251\n[340]\tvalidation_0-rmse:0.13218\n[341]\tvalidation_0-rmse:0.13192\n[342]\tvalidation_0-rmse:0.13145\n[343]\tvalidation_0-rmse:0.13113\n[344]\tvalidation_0-rmse:0.13018\n[345]\tvalidation_0-rmse:0.12940\n[346]\tvalidation_0-rmse:0.12893\n[347]\tvalidation_0-rmse:0.12865\n[348]\tvalidation_0-rmse:0.12822\n[349]\tvalidation_0-rmse:0.12787\n[350]\tvalidation_0-rmse:0.12757\n[351]\tvalidation_0-rmse:0.12714\n[352]\tvalidation_0-rmse:0.12670\n[353]\tvalidation_0-rmse:0.12617\n[354]\tvalidation_0-rmse:0.12594\n[355]\tvalidation_0-rmse:0.12575\n[356]\tvalidation_0-rmse:0.12534\n[357]\tvalidation_0-rmse:0.12485\n[358]\tvalidation_0-rmse:0.12476\n[359]\tvalidation_0-rmse:0.12448\n[360]\tvalidation_0-rmse:0.12399\n[361]\tvalidation_0-rmse:0.12360\n[362]\tvalidation_0-rmse:0.12314\n[363]\tvalidation_0-rmse:0.12273\n[364]\tvalidation_0-rmse:0.12244\n[365]\tvalidation_0-rmse:0.12207\n[366]\tvalidation_0-rmse:0.12177\n[367]\tvalidation_0-rmse:0.12159\n[368]\tvalidation_0-rmse:0.12127\n[369]\tvalidation_0-rmse:0.12119\n[370]\tvalidation_0-rmse:0.12100\n[371]\tvalidation_0-rmse:0.12062\n[372]\tvalidation_0-rmse:0.12024\n[373]\tvalidation_0-rmse:0.12001\n[374]\tvalidation_0-rmse:0.11965\n[375]\tvalidation_0-rmse:0.11936\n[376]\tvalidation_0-rmse:0.11910\n[377]\tvalidation_0-rmse:0.11868\n[378]\tvalidation_0-rmse:0.11862\n[379]\tvalidation_0-rmse:0.11840\n[380]\tvalidation_0-rmse:0.11816\n[381]\tvalidation_0-rmse:0.11762\n[382]\tvalidation_0-rmse:0.11717\n[383]\tvalidation_0-rmse:0.11672\n[384]\tvalidation_0-rmse:0.11628\n[385]\tvalidation_0-rmse:0.11592\n[386]\tvalidation_0-rmse:0.11552\n[387]\tvalidation_0-rmse:0.11501\n[388]\tvalidation_0-rmse:0.11448\n[389]\tvalidation_0-rmse:0.11411\n[390]\tvalidation_0-rmse:0.11393\n[391]\tvalidation_0-rmse:0.11380\n[392]\tvalidation_0-rmse:0.11350\n[393]\tvalidation_0-rmse:0.11311\n[394]\tvalidation_0-rmse:0.11275\n[395]\tvalidation_0-rmse:0.11229\n[396]\tvalidation_0-rmse:0.11210\n[397]\tvalidation_0-rmse:0.11179\n[398]\tvalidation_0-rmse:0.11167\n[399]\tvalidation_0-rmse:0.11145\n[400]\tvalidation_0-rmse:0.11122\n[401]\tvalidation_0-rmse:0.11097\n[402]\tvalidation_0-rmse:0.11065\n[403]\tvalidation_0-rmse:0.11007\n[404]\tvalidation_0-rmse:0.11001\n[405]\tvalidation_0-rmse:0.10985\n[406]\tvalidation_0-rmse:0.10937\n[407]\tvalidation_0-rmse:0.10917\n[408]\tvalidation_0-rmse:0.10909\n[409]\tvalidation_0-rmse:0.10880\n[410]\tvalidation_0-rmse:0.10848\n[411]\tvalidation_0-rmse:0.10821\n[412]\tvalidation_0-rmse:0.10797\n[413]\tvalidation_0-rmse:0.10789\n[414]\tvalidation_0-rmse:0.10758\n[415]\tvalidation_0-rmse:0.10723\n[416]\tvalidation_0-rmse:0.10706\n[417]\tvalidation_0-rmse:0.10688\n[418]\tvalidation_0-rmse:0.10659\n[419]\tvalidation_0-rmse:0.10640\n[420]\tvalidation_0-rmse:0.10615\n[421]\tvalidation_0-rmse:0.10566\n[422]\tvalidation_0-rmse:0.10538\n[423]\tvalidation_0-rmse:0.10527\n[424]\tvalidation_0-rmse:0.10506\n[425]\tvalidation_0-rmse:0.10486\n[426]\tvalidation_0-rmse:0.10470\n[427]\tvalidation_0-rmse:0.10447\n[428]\tvalidation_0-rmse:0.10409\n[429]\tvalidation_0-rmse:0.10353\n[430]\tvalidation_0-rmse:0.10335\n[431]\tvalidation_0-rmse:0.10317\n[432]\tvalidation_0-rmse:0.10309\n[433]\tvalidation_0-rmse:0.10280\n[434]\tvalidation_0-rmse:0.10272\n[435]\tvalidation_0-rmse:0.10267\n[436]\tvalidation_0-rmse:0.10234\n[437]\tvalidation_0-rmse:0.10199\n[438]\tvalidation_0-rmse:0.10168\n[439]\tvalidation_0-rmse:0.10161\n[440]\tvalidation_0-rmse:0.10135\n[441]\tvalidation_0-rmse:0.10096\n[442]\tvalidation_0-rmse:0.10065\n[443]\tvalidation_0-rmse:0.10037\n[444]\tvalidation_0-rmse:0.10017\n[445]\tvalidation_0-rmse:0.10006\n[446]\tvalidation_0-rmse:0.09987\n[447]\tvalidation_0-rmse:0.09970\n[448]\tvalidation_0-rmse:0.09938\n[449]\tvalidation_0-rmse:0.09911\n[450]\tvalidation_0-rmse:0.09884\n[451]\tvalidation_0-rmse:0.09856\n[452]\tvalidation_0-rmse:0.09849\n[453]\tvalidation_0-rmse:0.09811\n[454]\tvalidation_0-rmse:0.09765\n[455]\tvalidation_0-rmse:0.09737\n[456]\tvalidation_0-rmse:0.09719\n[457]\tvalidation_0-rmse:0.09685\n[458]\tvalidation_0-rmse:0.09652\n[459]\tvalidation_0-rmse:0.09639\n[460]\tvalidation_0-rmse:0.09616\n[461]\tvalidation_0-rmse:0.09592\n[462]\tvalidation_0-rmse:0.09573\n[463]\tvalidation_0-rmse:0.09547\n[464]\tvalidation_0-rmse:0.09527\n[465]\tvalidation_0-rmse:0.09505\n[466]\tvalidation_0-rmse:0.09491\n[467]\tvalidation_0-rmse:0.09453\n[468]\tvalidation_0-rmse:0.09432\n[469]\tvalidation_0-rmse:0.09371\n[470]\tvalidation_0-rmse:0.09349\n[471]\tvalidation_0-rmse:0.09318\n[472]\tvalidation_0-rmse:0.09312\n[473]\tvalidation_0-rmse:0.09308\n[474]\tvalidation_0-rmse:0.09276\n[475]\tvalidation_0-rmse:0.09217\n[476]\tvalidation_0-rmse:0.09195\n[477]\tvalidation_0-rmse:0.09168\n[478]\tvalidation_0-rmse:0.09131\n[479]\tvalidation_0-rmse:0.09120\n[480]\tvalidation_0-rmse:0.09047\n[481]\tvalidation_0-rmse:0.09002\n[482]\tvalidation_0-rmse:0.08955\n[483]\tvalidation_0-rmse:0.08932\n[484]\tvalidation_0-rmse:0.08863\n[485]\tvalidation_0-rmse:0.08841\n[486]\tvalidation_0-rmse:0.08810\n[487]\tvalidation_0-rmse:0.08790\n[488]\tvalidation_0-rmse:0.08773\n[489]\tvalidation_0-rmse:0.08746\n[490]\tvalidation_0-rmse:0.08712\n[491]\tvalidation_0-rmse:0.08704\n[492]\tvalidation_0-rmse:0.08663\n[493]\tvalidation_0-rmse:0.08657\n[494]\tvalidation_0-rmse:0.08647\n[495]\tvalidation_0-rmse:0.08591\n[496]\tvalidation_0-rmse:0.08574\n[497]\tvalidation_0-rmse:0.08544\n[498]\tvalidation_0-rmse:0.08529\n[499]\tvalidation_0-rmse:0.08511\n[500]\tvalidation_0-rmse:0.08495\n[501]\tvalidation_0-rmse:0.08482\n[502]\tvalidation_0-rmse:0.08459\n[503]\tvalidation_0-rmse:0.08435\n[504]\tvalidation_0-rmse:0.08422\n[505]\tvalidation_0-rmse:0.08393\n[506]\tvalidation_0-rmse:0.08375\n[507]\tvalidation_0-rmse:0.08360\n[508]\tvalidation_0-rmse:0.08356\n[509]\tvalidation_0-rmse:0.08339\n[510]\tvalidation_0-rmse:0.08315\n[511]\tvalidation_0-rmse:0.08279\n[512]\tvalidation_0-rmse:0.08272\n[513]\tvalidation_0-rmse:0.08243\n[514]\tvalidation_0-rmse:0.08227\n[515]\tvalidation_0-rmse:0.08222\n[516]\tvalidation_0-rmse:0.08210\n[517]\tvalidation_0-rmse:0.08205\n[518]\tvalidation_0-rmse:0.08188\n[519]\tvalidation_0-rmse:0.08179\n[520]\tvalidation_0-rmse:0.08160\n[521]\tvalidation_0-rmse:0.08134\n[522]\tvalidation_0-rmse:0.08082\n[523]\tvalidation_0-rmse:0.08051\n[524]\tvalidation_0-rmse:0.08025\n[525]\tvalidation_0-rmse:0.08009\n[526]\tvalidation_0-rmse:0.07991\n[527]\tvalidation_0-rmse:0.07959\n[528]\tvalidation_0-rmse:0.07940\n[529]\tvalidation_0-rmse:0.07922\n[530]\tvalidation_0-rmse:0.07894\n[531]\tvalidation_0-rmse:0.07851\n[532]\tvalidation_0-rmse:0.07831\n[533]\tvalidation_0-rmse:0.07807\n[534]\tvalidation_0-rmse:0.07804\n[535]\tvalidation_0-rmse:0.07788\n[536]\tvalidation_0-rmse:0.07771\n[537]\tvalidation_0-rmse:0.07744\n[538]\tvalidation_0-rmse:0.07726\n[539]\tvalidation_0-rmse:0.07718\n[540]\tvalidation_0-rmse:0.07700\n[541]\tvalidation_0-rmse:0.07694\n[542]\tvalidation_0-rmse:0.07686\n[543]\tvalidation_0-rmse:0.07672\n[544]\tvalidation_0-rmse:0.07657\n[545]\tvalidation_0-rmse:0.07638\n[546]\tvalidation_0-rmse:0.07615\n[547]\tvalidation_0-rmse:0.07592\n[548]\tvalidation_0-rmse:0.07567\n[549]\tvalidation_0-rmse:0.07554\n[550]\tvalidation_0-rmse:0.07547\n[551]\tvalidation_0-rmse:0.07542\n[552]\tvalidation_0-rmse:0.07522\n[553]\tvalidation_0-rmse:0.07501\n[554]\tvalidation_0-rmse:0.07494\n[555]\tvalidation_0-rmse:0.07466\n[556]\tvalidation_0-rmse:0.07444\n[557]\tvalidation_0-rmse:0.07422\n[558]\tvalidation_0-rmse:0.07390\n[559]\tvalidation_0-rmse:0.07366\n[560]\tvalidation_0-rmse:0.07353\n[561]\tvalidation_0-rmse:0.07336\n[562]\tvalidation_0-rmse:0.07313\n[563]\tvalidation_0-rmse:0.07280\n[564]\tvalidation_0-rmse:0.07255\n[565]\tvalidation_0-rmse:0.07233\n[566]\tvalidation_0-rmse:0.07219\n[567]\tvalidation_0-rmse:0.07200\n[568]\tvalidation_0-rmse:0.07178\n[569]\tvalidation_0-rmse:0.07165\n[570]\tvalidation_0-rmse:0.07131\n[571]\tvalidation_0-rmse:0.07120\n[572]\tvalidation_0-rmse:0.07082\n[573]\tvalidation_0-rmse:0.07076\n[574]\tvalidation_0-rmse:0.07072\n[575]\tvalidation_0-rmse:0.07060\n[576]\tvalidation_0-rmse:0.07046\n[577]\tvalidation_0-rmse:0.07040\n[578]\tvalidation_0-rmse:0.07020\n[579]\tvalidation_0-rmse:0.07007\n[580]\tvalidation_0-rmse:0.06991\n[581]\tvalidation_0-rmse:0.06973\n[582]\tvalidation_0-rmse:0.06963\n[583]\tvalidation_0-rmse:0.06955\n[584]\tvalidation_0-rmse:0.06945\n[585]\tvalidation_0-rmse:0.06918\n[586]\tvalidation_0-rmse:0.06899\n[587]\tvalidation_0-rmse:0.06865\n[588]\tvalidation_0-rmse:0.06836\n[589]\tvalidation_0-rmse:0.06815\n[590]\tvalidation_0-rmse:0.06795\n[591]\tvalidation_0-rmse:0.06768\n[592]\tvalidation_0-rmse:0.06759\n[593]\tvalidation_0-rmse:0.06738\n[594]\tvalidation_0-rmse:0.06734\n[595]\tvalidation_0-rmse:0.06699\n[596]\tvalidation_0-rmse:0.06693\n[597]\tvalidation_0-rmse:0.06663\n[598]\tvalidation_0-rmse:0.06646\n[599]\tvalidation_0-rmse:0.06627\n[600]\tvalidation_0-rmse:0.06620\n[601]\tvalidation_0-rmse:0.06611\n[602]\tvalidation_0-rmse:0.06605\n[603]\tvalidation_0-rmse:0.06576\n[604]\tvalidation_0-rmse:0.06572\n[605]\tvalidation_0-rmse:0.06560\n[606]\tvalidation_0-rmse:0.06542\n[607]\tvalidation_0-rmse:0.06531\n[608]\tvalidation_0-rmse:0.06515\n[609]\tvalidation_0-rmse:0.06496\n[610]\tvalidation_0-rmse:0.06467\n[611]\tvalidation_0-rmse:0.06461\n[612]\tvalidation_0-rmse:0.06457\n[613]\tvalidation_0-rmse:0.06409\n[614]\tvalidation_0-rmse:0.06397\n[615]\tvalidation_0-rmse:0.06385\n[616]\tvalidation_0-rmse:0.06367\n[617]\tvalidation_0-rmse:0.06342\n[618]\tvalidation_0-rmse:0.06327\n[619]\tvalidation_0-rmse:0.06297\n[620]\tvalidation_0-rmse:0.06280\n[621]\tvalidation_0-rmse:0.06269\n[622]\tvalidation_0-rmse:0.06255\n[623]\tvalidation_0-rmse:0.06238\n[624]\tvalidation_0-rmse:0.06220\n[625]\tvalidation_0-rmse:0.06204\n[626]\tvalidation_0-rmse:0.06193\n[627]\tvalidation_0-rmse:0.06178\n[628]\tvalidation_0-rmse:0.06155\n[629]\tvalidation_0-rmse:0.06147\n[630]\tvalidation_0-rmse:0.06125\n[631]\tvalidation_0-rmse:0.06116\n[632]\tvalidation_0-rmse:0.06096\n[633]\tvalidation_0-rmse:0.06074\n[634]\tvalidation_0-rmse:0.06063\n[635]\tvalidation_0-rmse:0.06047\n[636]\tvalidation_0-rmse:0.06012\n[637]\tvalidation_0-rmse:0.05995\n[638]\tvalidation_0-rmse:0.05967\n[639]\tvalidation_0-rmse:0.05954\n[640]\tvalidation_0-rmse:0.05941\n[641]\tvalidation_0-rmse:0.05938\n[642]\tvalidation_0-rmse:0.05932\n[643]\tvalidation_0-rmse:0.05926\n[644]\tvalidation_0-rmse:0.05900\n[645]\tvalidation_0-rmse:0.05896\n[646]\tvalidation_0-rmse:0.05889\n[647]\tvalidation_0-rmse:0.05874\n[648]\tvalidation_0-rmse:0.05862\n[649]\tvalidation_0-rmse:0.05844\n[650]\tvalidation_0-rmse:0.05828\n[651]\tvalidation_0-rmse:0.05821\n[652]\tvalidation_0-rmse:0.05816\n[653]\tvalidation_0-rmse:0.05807\n[654]\tvalidation_0-rmse:0.05793\n[655]\tvalidation_0-rmse:0.05776\n[656]\tvalidation_0-rmse:0.05762\n[657]\tvalidation_0-rmse:0.05758\n[658]\tvalidation_0-rmse:0.05744\n[659]\tvalidation_0-rmse:0.05738\n[660]\tvalidation_0-rmse:0.05716\n[661]\tvalidation_0-rmse:0.05707\n[662]\tvalidation_0-rmse:0.05704\n[663]\tvalidation_0-rmse:0.05681\n[664]\tvalidation_0-rmse:0.05670\n[665]\tvalidation_0-rmse:0.05643\n[666]\tvalidation_0-rmse:0.05624\n[667]\tvalidation_0-rmse:0.05607\n[668]\tvalidation_0-rmse:0.05591\n[669]\tvalidation_0-rmse:0.05582\n[670]\tvalidation_0-rmse:0.05552\n[671]\tvalidation_0-rmse:0.05547\n[672]\tvalidation_0-rmse:0.05531\n[673]\tvalidation_0-rmse:0.05520\n[674]\tvalidation_0-rmse:0.05508\n[675]\tvalidation_0-rmse:0.05489\n[676]\tvalidation_0-rmse:0.05470\n[677]\tvalidation_0-rmse:0.05461\n[678]\tvalidation_0-rmse:0.05450\n[679]\tvalidation_0-rmse:0.05443\n[680]\tvalidation_0-rmse:0.05437\n[681]\tvalidation_0-rmse:0.05417\n[682]\tvalidation_0-rmse:0.05405\n[683]\tvalidation_0-rmse:0.05383\n[684]\tvalidation_0-rmse:0.05368\n[685]\tvalidation_0-rmse:0.05351\n[686]\tvalidation_0-rmse:0.05328\n[687]\tvalidation_0-rmse:0.05326\n[688]\tvalidation_0-rmse:0.05322\n[689]\tvalidation_0-rmse:0.05317\n[690]\tvalidation_0-rmse:0.05308\n[691]\tvalidation_0-rmse:0.05288\n[692]\tvalidation_0-rmse:0.05278\n[693]\tvalidation_0-rmse:0.05269\n[694]\tvalidation_0-rmse:0.05249\n[695]\tvalidation_0-rmse:0.05234\n[696]\tvalidation_0-rmse:0.05229\n[697]\tvalidation_0-rmse:0.05222\n[698]\tvalidation_0-rmse:0.05219\n[699]\tvalidation_0-rmse:0.05206\n[700]\tvalidation_0-rmse:0.05191\n[701]\tvalidation_0-rmse:0.05173\n[702]\tvalidation_0-rmse:0.05152\n[703]\tvalidation_0-rmse:0.05147\n[704]\tvalidation_0-rmse:0.05137\n[705]\tvalidation_0-rmse:0.05130\n[706]\tvalidation_0-rmse:0.05116\n[707]\tvalidation_0-rmse:0.05110\n[708]\tvalidation_0-rmse:0.05103\n[709]\tvalidation_0-rmse:0.05085\n[710]\tvalidation_0-rmse:0.05061\n[711]\tvalidation_0-rmse:0.05042\n[712]\tvalidation_0-rmse:0.05019\n[713]\tvalidation_0-rmse:0.05016\n[714]\tvalidation_0-rmse:0.05000\n[715]\tvalidation_0-rmse:0.04991\n[716]\tvalidation_0-rmse:0.04975\n[717]\tvalidation_0-rmse:0.04967\n[718]\tvalidation_0-rmse:0.04961\n[719]\tvalidation_0-rmse:0.04949\n[720]\tvalidation_0-rmse:0.04944\n[721]\tvalidation_0-rmse:0.04937\n[722]\tvalidation_0-rmse:0.04930\n[723]\tvalidation_0-rmse:0.04919\n[724]\tvalidation_0-rmse:0.04915\n[725]\tvalidation_0-rmse:0.04901\n[726]\tvalidation_0-rmse:0.04892\n[727]\tvalidation_0-rmse:0.04876\n[728]\tvalidation_0-rmse:0.04858\n[729]\tvalidation_0-rmse:0.04842\n[730]\tvalidation_0-rmse:0.04834\n[731]\tvalidation_0-rmse:0.04822\n[732]\tvalidation_0-rmse:0.04795\n[733]\tvalidation_0-rmse:0.04778\n[734]\tvalidation_0-rmse:0.04768\n[735]\tvalidation_0-rmse:0.04761\n[736]\tvalidation_0-rmse:0.04751\n[737]\tvalidation_0-rmse:0.04738\n[738]\tvalidation_0-rmse:0.04728\n[739]\tvalidation_0-rmse:0.04701\n[740]\tvalidation_0-rmse:0.04680\n[741]\tvalidation_0-rmse:0.04666\n[742]\tvalidation_0-rmse:0.04640\n[743]\tvalidation_0-rmse:0.04622\n[744]\tvalidation_0-rmse:0.04612\n[745]\tvalidation_0-rmse:0.04605\n[746]\tvalidation_0-rmse:0.04597\n[747]\tvalidation_0-rmse:0.04578\n[748]\tvalidation_0-rmse:0.04563\n[749]\tvalidation_0-rmse:0.04548\n[750]\tvalidation_0-rmse:0.04535\n[751]\tvalidation_0-rmse:0.04531\n[752]\tvalidation_0-rmse:0.04522\n[753]\tvalidation_0-rmse:0.04520\n[754]\tvalidation_0-rmse:0.04501\n[755]\tvalidation_0-rmse:0.04491\n[756]\tvalidation_0-rmse:0.04481\n[757]\tvalidation_0-rmse:0.04470\n[758]\tvalidation_0-rmse:0.04463\n[759]\tvalidation_0-rmse:0.04460\n[760]\tvalidation_0-rmse:0.04454\n[761]\tvalidation_0-rmse:0.04445\n[762]\tvalidation_0-rmse:0.04434\n[763]\tvalidation_0-rmse:0.04422\n[764]\tvalidation_0-rmse:0.04399\n[765]\tvalidation_0-rmse:0.04391\n[766]\tvalidation_0-rmse:0.04384\n[767]\tvalidation_0-rmse:0.04374\n[768]\tvalidation_0-rmse:0.04360\n[769]\tvalidation_0-rmse:0.04345\n[770]\tvalidation_0-rmse:0.04338\n[771]\tvalidation_0-rmse:0.04322\n[772]\tvalidation_0-rmse:0.04317\n[773]\tvalidation_0-rmse:0.04309\n[774]\tvalidation_0-rmse:0.04297\n[775]\tvalidation_0-rmse:0.04291\n[776]\tvalidation_0-rmse:0.04282\n[777]\tvalidation_0-rmse:0.04279\n[778]\tvalidation_0-rmse:0.04275\n[779]\tvalidation_0-rmse:0.04257\n[780]\tvalidation_0-rmse:0.04249\n[781]\tvalidation_0-rmse:0.04247\n[782]\tvalidation_0-rmse:0.04245\n[783]\tvalidation_0-rmse:0.04240\n[784]\tvalidation_0-rmse:0.04226\n[785]\tvalidation_0-rmse:0.04221\n[786]\tvalidation_0-rmse:0.04199\n[787]\tvalidation_0-rmse:0.04189\n[788]\tvalidation_0-rmse:0.04182\n[789]\tvalidation_0-rmse:0.04165\n[790]\tvalidation_0-rmse:0.04162\n[791]\tvalidation_0-rmse:0.04144\n[792]\tvalidation_0-rmse:0.04133\n[793]\tvalidation_0-rmse:0.04128\n[794]\tvalidation_0-rmse:0.04122\n[795]\tvalidation_0-rmse:0.04115\n[796]\tvalidation_0-rmse:0.04098\n[797]\tvalidation_0-rmse:0.04082\n[798]\tvalidation_0-rmse:0.04070\n[799]\tvalidation_0-rmse:0.04056\n[800]\tvalidation_0-rmse:0.04052\n[801]\tvalidation_0-rmse:0.04036\n[802]\tvalidation_0-rmse:0.04030\n[803]\tvalidation_0-rmse:0.04021\n[804]\tvalidation_0-rmse:0.04016\n[805]\tvalidation_0-rmse:0.04011\n[806]\tvalidation_0-rmse:0.03999\n[807]\tvalidation_0-rmse:0.03995\n[808]\tvalidation_0-rmse:0.03978\n[809]\tvalidation_0-rmse:0.03962\n[810]\tvalidation_0-rmse:0.03950\n[811]\tvalidation_0-rmse:0.03946\n[812]\tvalidation_0-rmse:0.03925\n[813]\tvalidation_0-rmse:0.03907\n[814]\tvalidation_0-rmse:0.03891\n[815]\tvalidation_0-rmse:0.03885\n[816]\tvalidation_0-rmse:0.03874\n[817]\tvalidation_0-rmse:0.03854\n[818]\tvalidation_0-rmse:0.03848\n[819]\tvalidation_0-rmse:0.03835\n[820]\tvalidation_0-rmse:0.03828\n[821]\tvalidation_0-rmse:0.03825\n[822]\tvalidation_0-rmse:0.03815\n[823]\tvalidation_0-rmse:0.03793\n[824]\tvalidation_0-rmse:0.03777\n[825]\tvalidation_0-rmse:0.03768\n[826]\tvalidation_0-rmse:0.03756\n[827]\tvalidation_0-rmse:0.03749\n[828]\tvalidation_0-rmse:0.03740\n[829]\tvalidation_0-rmse:0.03727\n[830]\tvalidation_0-rmse:0.03720\n[831]\tvalidation_0-rmse:0.03704\n[832]\tvalidation_0-rmse:0.03699\n[833]\tvalidation_0-rmse:0.03687\n[834]\tvalidation_0-rmse:0.03671\n[835]\tvalidation_0-rmse:0.03668\n[836]\tvalidation_0-rmse:0.03666\n[837]\tvalidation_0-rmse:0.03655\n[838]\tvalidation_0-rmse:0.03650\n[839]\tvalidation_0-rmse:0.03647\n[840]\tvalidation_0-rmse:0.03637\n[841]\tvalidation_0-rmse:0.03633\n[842]\tvalidation_0-rmse:0.03626\n[843]\tvalidation_0-rmse:0.03620\n[844]\tvalidation_0-rmse:0.03617\n[845]\tvalidation_0-rmse:0.03614\n[846]\tvalidation_0-rmse:0.03610\n[847]\tvalidation_0-rmse:0.03602\n[848]\tvalidation_0-rmse:0.03586\n[849]\tvalidation_0-rmse:0.03573\n[850]\tvalidation_0-rmse:0.03564\n[851]\tvalidation_0-rmse:0.03557\n[852]\tvalidation_0-rmse:0.03551\n[853]\tvalidation_0-rmse:0.03546\n[854]\tvalidation_0-rmse:0.03538\n[855]\tvalidation_0-rmse:0.03530\n[856]\tvalidation_0-rmse:0.03528\n[857]\tvalidation_0-rmse:0.03517\n[858]\tvalidation_0-rmse:0.03506\n[859]\tvalidation_0-rmse:0.03487\n[860]\tvalidation_0-rmse:0.03476\n[861]\tvalidation_0-rmse:0.03473\n[862]\tvalidation_0-rmse:0.03468\n[863]\tvalidation_0-rmse:0.03458\n[864]\tvalidation_0-rmse:0.03447\n[865]\tvalidation_0-rmse:0.03440\n[866]\tvalidation_0-rmse:0.03435\n[867]\tvalidation_0-rmse:0.03419\n[868]\tvalidation_0-rmse:0.03412\n[869]\tvalidation_0-rmse:0.03402\n[870]\tvalidation_0-rmse:0.03394\n[871]\tvalidation_0-rmse:0.03384\n[872]\tvalidation_0-rmse:0.03374\n[873]\tvalidation_0-rmse:0.03363\n[874]\tvalidation_0-rmse:0.03354\n[875]\tvalidation_0-rmse:0.03348\n[876]\tvalidation_0-rmse:0.03341\n[877]\tvalidation_0-rmse:0.03332\n[878]\tvalidation_0-rmse:0.03328\n[879]\tvalidation_0-rmse:0.03317\n[880]\tvalidation_0-rmse:0.03310\n[881]\tvalidation_0-rmse:0.03305\n[882]\tvalidation_0-rmse:0.03302\n[883]\tvalidation_0-rmse:0.03299\n[884]\tvalidation_0-rmse:0.03298\n[885]\tvalidation_0-rmse:0.03289\n[886]\tvalidation_0-rmse:0.03283\n[887]\tvalidation_0-rmse:0.03275\n[888]\tvalidation_0-rmse:0.03263\n[889]\tvalidation_0-rmse:0.03247\n[890]\tvalidation_0-rmse:0.03240\n[891]\tvalidation_0-rmse:0.03231\n[892]\tvalidation_0-rmse:0.03224\n[893]\tvalidation_0-rmse:0.03221\n[894]\tvalidation_0-rmse:0.03216\n[895]\tvalidation_0-rmse:0.03215\n[896]\tvalidation_0-rmse:0.03207\n[897]\tvalidation_0-rmse:0.03203\n[898]\tvalidation_0-rmse:0.03195\n[899]\tvalidation_0-rmse:0.03192\n[900]\tvalidation_0-rmse:0.03185\n[901]\tvalidation_0-rmse:0.03178\n[902]\tvalidation_0-rmse:0.03169\n[903]\tvalidation_0-rmse:0.03164\n[904]\tvalidation_0-rmse:0.03159\n[905]\tvalidation_0-rmse:0.03152\n[906]\tvalidation_0-rmse:0.03147\n[907]\tvalidation_0-rmse:0.03145\n[908]\tvalidation_0-rmse:0.03139\n[909]\tvalidation_0-rmse:0.03133\n[910]\tvalidation_0-rmse:0.03129\n[911]\tvalidation_0-rmse:0.03122\n[912]\tvalidation_0-rmse:0.03116\n[913]\tvalidation_0-rmse:0.03105\n[914]\tvalidation_0-rmse:0.03097\n[915]\tvalidation_0-rmse:0.03092\n[916]\tvalidation_0-rmse:0.03080\n[917]\tvalidation_0-rmse:0.03074\n[918]\tvalidation_0-rmse:0.03069\n[919]\tvalidation_0-rmse:0.03059\n[920]\tvalidation_0-rmse:0.03048\n[921]\tvalidation_0-rmse:0.03037\n[922]\tvalidation_0-rmse:0.03033\n[923]\tvalidation_0-rmse:0.03032\n[924]\tvalidation_0-rmse:0.03028\n[925]\tvalidation_0-rmse:0.03018\n[926]\tvalidation_0-rmse:0.03013\n[927]\tvalidation_0-rmse:0.03005\n[928]\tvalidation_0-rmse:0.02997\n[929]\tvalidation_0-rmse:0.02993\n[930]\tvalidation_0-rmse:0.02988\n[931]\tvalidation_0-rmse:0.02980\n[932]\tvalidation_0-rmse:0.02975\n[933]\tvalidation_0-rmse:0.02968\n[934]\tvalidation_0-rmse:0.02960\n[935]\tvalidation_0-rmse:0.02954\n[936]\tvalidation_0-rmse:0.02942\n[937]\tvalidation_0-rmse:0.02938\n[938]\tvalidation_0-rmse:0.02927\n[939]\tvalidation_0-rmse:0.02920\n[940]\tvalidation_0-rmse:0.02916\n[941]\tvalidation_0-rmse:0.02913\n[942]\tvalidation_0-rmse:0.02912\n[943]\tvalidation_0-rmse:0.02904\n[944]\tvalidation_0-rmse:0.02901\n[945]\tvalidation_0-rmse:0.02891\n[946]\tvalidation_0-rmse:0.02881\n[947]\tvalidation_0-rmse:0.02877\n[948]\tvalidation_0-rmse:0.02870\n[949]\tvalidation_0-rmse:0.02862\n[950]\tvalidation_0-rmse:0.02859\n[951]\tvalidation_0-rmse:0.02856\n[952]\tvalidation_0-rmse:0.02843\n[953]\tvalidation_0-rmse:0.02832\n[954]\tvalidation_0-rmse:0.02816\n[955]\tvalidation_0-rmse:0.02806\n[956]\tvalidation_0-rmse:0.02797\n[957]\tvalidation_0-rmse:0.02792\n[958]\tvalidation_0-rmse:0.02788\n[959]\tvalidation_0-rmse:0.02782\n[960]\tvalidation_0-rmse:0.02776\n[961]\tvalidation_0-rmse:0.02774\n[962]\tvalidation_0-rmse:0.02769\n[963]\tvalidation_0-rmse:0.02767\n[964]\tvalidation_0-rmse:0.02765\n[965]\tvalidation_0-rmse:0.02764\n[966]\tvalidation_0-rmse:0.02763\n[967]\tvalidation_0-rmse:0.02760\n[968]\tvalidation_0-rmse:0.02754\n[969]\tvalidation_0-rmse:0.02751\n[970]\tvalidation_0-rmse:0.02745\n[971]\tvalidation_0-rmse:0.02735\n[972]\tvalidation_0-rmse:0.02730\n[973]\tvalidation_0-rmse:0.02717\n[974]\tvalidation_0-rmse:0.02705\n[975]\tvalidation_0-rmse:0.02702\n[976]\tvalidation_0-rmse:0.02700\n[977]\tvalidation_0-rmse:0.02692\n[978]\tvalidation_0-rmse:0.02687\n[979]\tvalidation_0-rmse:0.02685\n[980]\tvalidation_0-rmse:0.02680\n[981]\tvalidation_0-rmse:0.02674\n[982]\tvalidation_0-rmse:0.02673\n[983]\tvalidation_0-rmse:0.02664\n[984]\tvalidation_0-rmse:0.02649\n[985]\tvalidation_0-rmse:0.02635\n[986]\tvalidation_0-rmse:0.02628\n[987]\tvalidation_0-rmse:0.02625\n[988]\tvalidation_0-rmse:0.02620\n[989]\tvalidation_0-rmse:0.02610\n[990]\tvalidation_0-rmse:0.02596\n[991]\tvalidation_0-rmse:0.02594\n[992]\tvalidation_0-rmse:0.02590\n[993]\tvalidation_0-rmse:0.02583\n[994]\tvalidation_0-rmse:0.02568\n[995]\tvalidation_0-rmse:0.02564\n[996]\tvalidation_0-rmse:0.02552\n[997]\tvalidation_0-rmse:0.02544\n[998]\tvalidation_0-rmse:0.02535\n[999]\tvalidation_0-rmse:0.02529\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<style>#sk-container-id-1 {\n  /* Definition of color scheme common for light and dark mode */\n  --sklearn-color-text: #000;\n  --sklearn-color-text-muted: #666;\n  --sklearn-color-line: gray;\n  /* Definition of color scheme for unfitted estimators */\n  --sklearn-color-unfitted-level-0: #fff5e6;\n  --sklearn-color-unfitted-level-1: #f6e4d2;\n  --sklearn-color-unfitted-level-2: #ffe0b3;\n  --sklearn-color-unfitted-level-3: chocolate;\n  /* Definition of color scheme for fitted estimators */\n  --sklearn-color-fitted-level-0: #f0f8ff;\n  --sklearn-color-fitted-level-1: #d4ebff;\n  --sklearn-color-fitted-level-2: #b3dbfd;\n  --sklearn-color-fitted-level-3: cornflowerblue;\n\n  /* Specific color for light theme */\n  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n  --sklearn-color-icon: #696969;\n\n  @media (prefers-color-scheme: dark) {\n    /* Redefinition of color scheme for dark theme */\n    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n    --sklearn-color-icon: #878787;\n  }\n}\n\n#sk-container-id-1 {\n  color: var(--sklearn-color-text);\n}\n\n#sk-container-id-1 pre {\n  padding: 0;\n}\n\n#sk-container-id-1 input.sk-hidden--visually {\n  border: 0;\n  clip: rect(1px 1px 1px 1px);\n  clip: rect(1px, 1px, 1px, 1px);\n  height: 1px;\n  margin: -1px;\n  overflow: hidden;\n  padding: 0;\n  position: absolute;\n  width: 1px;\n}\n\n#sk-container-id-1 div.sk-dashed-wrapped {\n  border: 1px dashed var(--sklearn-color-line);\n  margin: 0 0.4em 0.5em 0.4em;\n  box-sizing: border-box;\n  padding-bottom: 0.4em;\n  background-color: var(--sklearn-color-background);\n}\n\n#sk-container-id-1 div.sk-container {\n  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n     but bootstrap.min.css set `[hidden] { display: none !important; }`\n     so we also need the `!important` here to be able to override the\n     default hidden behavior on the sphinx rendered scikit-learn.org.\n     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n  display: inline-block !important;\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-text-repr-fallback {\n  display: none;\n}\n\ndiv.sk-parallel-item,\ndiv.sk-serial,\ndiv.sk-item {\n  /* draw centered vertical line to link estimators */\n  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n  background-size: 2px 100%;\n  background-repeat: no-repeat;\n  background-position: center center;\n}\n\n/* Parallel-specific style estimator block */\n\n#sk-container-id-1 div.sk-parallel-item::after {\n  content: \"\";\n  width: 100%;\n  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n  flex-grow: 1;\n}\n\n#sk-container-id-1 div.sk-parallel {\n  display: flex;\n  align-items: stretch;\n  justify-content: center;\n  background-color: var(--sklearn-color-background);\n  position: relative;\n}\n\n#sk-container-id-1 div.sk-parallel-item {\n  display: flex;\n  flex-direction: column;\n}\n\n#sk-container-id-1 div.sk-parallel-item:first-child::after {\n  align-self: flex-end;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:last-child::after {\n  align-self: flex-start;\n  width: 50%;\n}\n\n#sk-container-id-1 div.sk-parallel-item:only-child::after {\n  width: 0;\n}\n\n/* Serial-specific style estimator block */\n\n#sk-container-id-1 div.sk-serial {\n  display: flex;\n  flex-direction: column;\n  align-items: center;\n  background-color: var(--sklearn-color-background);\n  padding-right: 1em;\n  padding-left: 1em;\n}\n\n\n/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\nclickable and can be expanded/collapsed.\n- Pipeline and ColumnTransformer use this feature and define the default style\n- Estimators will overwrite some part of the style using the `sk-estimator` class\n*/\n\n/* Pipeline and ColumnTransformer style (default) */\n\n#sk-container-id-1 div.sk-toggleable {\n  /* Default theme specific background. It is overwritten whether we have a\n  specific estimator or a Pipeline/ColumnTransformer */\n  background-color: var(--sklearn-color-background);\n}\n\n/* Toggleable label */\n#sk-container-id-1 label.sk-toggleable__label {\n  cursor: pointer;\n  display: flex;\n  width: 100%;\n  margin-bottom: 0;\n  padding: 0.5em;\n  box-sizing: border-box;\n  text-align: center;\n  align-items: start;\n  justify-content: space-between;\n  gap: 0.5em;\n}\n\n#sk-container-id-1 label.sk-toggleable__label .caption {\n  font-size: 0.6rem;\n  font-weight: lighter;\n  color: var(--sklearn-color-text-muted);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n  /* Arrow on the left of the label */\n  content: \"▸\";\n  float: left;\n  margin-right: 0.25em;\n  color: var(--sklearn-color-icon);\n}\n\n#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n  color: var(--sklearn-color-text);\n}\n\n/* Toggleable content - dropdown */\n\n#sk-container-id-1 div.sk-toggleable__content {\n  max-height: 0;\n  max-width: 0;\n  overflow: hidden;\n  text-align: left;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content pre {\n  margin: 0.2em;\n  border-radius: 0.25em;\n  color: var(--sklearn-color-text);\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n  /* unfitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n  /* Expand drop-down */\n  max-height: 200px;\n  max-width: 100%;\n  overflow: auto;\n}\n\n#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n  content: \"▾\";\n}\n\n/* Pipeline/ColumnTransformer-specific style */\n\n#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator-specific style */\n\n/* Colorize estimator box */\n#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n#sk-container-id-1 div.sk-label label {\n  /* The background is the default theme color */\n  color: var(--sklearn-color-text-on-default-background);\n}\n\n/* On hover, darken the color of the background */\n#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n/* Label box, darken color on hover, fitted */\n#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n  color: var(--sklearn-color-text);\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Estimator label */\n\n#sk-container-id-1 div.sk-label label {\n  font-family: monospace;\n  font-weight: bold;\n  display: inline-block;\n  line-height: 1.2em;\n}\n\n#sk-container-id-1 div.sk-label-container {\n  text-align: center;\n}\n\n/* Estimator-specific */\n#sk-container-id-1 div.sk-estimator {\n  font-family: monospace;\n  border: 1px dotted var(--sklearn-color-border-box);\n  border-radius: 0.25em;\n  box-sizing: border-box;\n  margin-bottom: 0.5em;\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-0);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-0);\n}\n\n/* on hover */\n#sk-container-id-1 div.sk-estimator:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-2);\n}\n\n#sk-container-id-1 div.sk-estimator.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-2);\n}\n\n/* Specification for estimator info (e.g. \"i\" and \"?\") */\n\n/* Common style for \"i\" and \"?\" */\n\n.sk-estimator-doc-link,\na:link.sk-estimator-doc-link,\na:visited.sk-estimator-doc-link {\n  float: right;\n  font-size: smaller;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1em;\n  height: 1em;\n  width: 1em;\n  text-decoration: none !important;\n  margin-left: 0.5em;\n  text-align: center;\n  /* unfitted */\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n  color: var(--sklearn-color-unfitted-level-1);\n}\n\n.sk-estimator-doc-link.fitted,\na:link.sk-estimator-doc-link.fitted,\na:visited.sk-estimator-doc-link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\ndiv.sk-estimator:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link:hover,\n.sk-estimator-doc-link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\ndiv.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover,\ndiv.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n.sk-estimator-doc-link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n/* Span, style for the box shown on hovering the info icon */\n.sk-estimator-doc-link span {\n  display: none;\n  z-index: 9999;\n  position: relative;\n  font-weight: normal;\n  right: .2ex;\n  padding: .5ex;\n  margin: .5ex;\n  width: min-content;\n  min-width: 20ex;\n  max-width: 50ex;\n  color: var(--sklearn-color-text);\n  box-shadow: 2pt 2pt 4pt #999;\n  /* unfitted */\n  background: var(--sklearn-color-unfitted-level-0);\n  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n}\n\n.sk-estimator-doc-link.fitted span {\n  /* fitted */\n  background: var(--sklearn-color-fitted-level-0);\n  border: var(--sklearn-color-fitted-level-3);\n}\n\n.sk-estimator-doc-link:hover span {\n  display: block;\n}\n\n/* \"?\"-specific style due to the `<a>` HTML tag */\n\n#sk-container-id-1 a.estimator_doc_link {\n  float: right;\n  font-size: 1rem;\n  line-height: 1em;\n  font-family: monospace;\n  background-color: var(--sklearn-color-background);\n  border-radius: 1rem;\n  height: 1rem;\n  width: 1rem;\n  text-decoration: none;\n  /* unfitted */\n  color: var(--sklearn-color-unfitted-level-1);\n  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted {\n  /* fitted */\n  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n  color: var(--sklearn-color-fitted-level-1);\n}\n\n/* On hover */\n#sk-container-id-1 a.estimator_doc_link:hover {\n  /* unfitted */\n  background-color: var(--sklearn-color-unfitted-level-3);\n  color: var(--sklearn-color-background);\n  text-decoration: none;\n}\n\n#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n  /* fitted */\n  background-color: var(--sklearn-color-fitted-level-3);\n}\n</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=50,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=1000,\n             n_jobs=None, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>XGBRegressor</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.1.0/python/python_api.html#xgboost.XGBRegressor\">?<span>Documentation for XGBRegressor</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, device=None, early_stopping_rounds=50,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             feature_weights=None, gamma=None, grow_policy=None,\n             importance_type=None, interaction_constraints=None,\n             learning_rate=None, max_bin=None, max_cat_threshold=None,\n             max_cat_to_onehot=None, max_delta_step=None, max_depth=None,\n             max_leaves=None, min_child_weight=None, missing=nan,\n             monotone_constraints=None, multi_strategy=None, n_estimators=1000,\n             n_jobs=None, num_parallel_tree=None, ...)</pre></div> </div></div></div></div>\n```\n:::\n:::\n\n\n### Pitfall 2: High Learning Rate with Few Trees\n\n::: {#e18477e6 .cell execution_count=9}\n``` {.python .cell-code}\n# ❌ BAD: Fast learning, few trees → underfitting\nmodel = xgb.XGBRegressor(n_estimators=10, learning_rate=0.3)\n\n# ✅ GOOD: Slow learning, many trees → better generalization\nmodel = xgb.XGBRegressor(n_estimators=500, learning_rate=0.05)\n```\n:::\n\n\n### Pitfall 3: Ignoring Feature Scaling\n\n**Good news**: Gradient boosting doesn't require feature scaling!\n\n```python\n# No need to scale for tree-based methods\nfrom sklearn.preprocessing import StandardScaler\n\n# ❌ Unnecessary for XGBoost/LightGBM/CatBoost\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# ✅ Use raw features\nmodel.fit(X, y)  # Works great!\n```\n\n### Pitfall 4: Not Encoding Categorical Features (except CatBoost)\n\n::: {#36c26788 .cell execution_count=10}\n``` {.python .cell-code}\nimport pandas as pd\n\n# Data with categorical feature\ndf = pd.DataFrame({\n    'size': [1200, 1500, 1800],\n    'neighborhood': ['A', 'B', 'C'],\n    'price': [200, 250, 300]\n})\n\n# ❌ BAD: XGBoost/LightGBM can't handle strings directly\nmodel = xgb.XGBRegressor()\n# model.fit(df[['size', 'neighborhood']], df['price'])  # ERROR!\n\n# ✅ GOOD: Encode first (for XGBoost/LightGBM)\ndf['neighborhood_encoded'] = df['neighborhood'].astype('category').cat.codes\nmodel.fit(df[['size', 'neighborhood_encoded']], df['price'])\n\n# ✅ BEST: Use CatBoost\nmodel = CatBoostRegressor(cat_features=['neighborhood'])\nmodel.fit(df[['size', 'neighborhood']], df['price'])  # Handles strings!\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLearning rate set to 0.016352\n0:\tlearn: 40.6747393\ttotal: 839us\tremaining: 839ms\n1:\tlearn: 40.6747390\ttotal: 961us\tremaining: 480ms\n2:\tlearn: 40.5084607\ttotal: 1.13ms\tremaining: 377ms\n3:\tlearn: 40.5084603\ttotal: 1.17ms\tremaining: 292ms\n4:\tlearn: 40.3599814\ttotal: 1.26ms\tremaining: 250ms\n5:\tlearn: 40.2124950\ttotal: 1.35ms\tremaining: 224ms\n6:\tlearn: 40.0633224\ttotal: 1.4ms\tremaining: 198ms\n7:\tlearn: 39.9151440\ttotal: 1.44ms\tremaining: 179ms\n8:\tlearn: 39.7688441\ttotal: 2.27ms\tremaining: 251ms\n9:\tlearn: 39.7688428\ttotal: 2.32ms\tremaining: 230ms\n10:\tlearn: 39.7688416\ttotal: 2.35ms\tremaining: 211ms\n11:\tlearn: 39.6235172\ttotal: 2.39ms\tremaining: 197ms\n12:\tlearn: 39.4791652\ttotal: 2.45ms\tremaining: 186ms\n13:\tlearn: 39.3322744\ttotal: 3.15ms\tremaining: 222ms\n14:\tlearn: 39.3322717\ttotal: 3.28ms\tremaining: 215ms\n15:\tlearn: 39.1714813\ttotal: 3.38ms\tremaining: 208ms\n16:\tlearn: 39.1714787\ttotal: 3.4ms\tremaining: 196ms\n17:\tlearn: 39.0287736\ttotal: 3.44ms\tremaining: 187ms\n18:\tlearn: 38.8835586\ttotal: 3.48ms\tremaining: 180ms\n19:\tlearn: 38.7393105\ttotal: 3.52ms\tremaining: 173ms\n20:\tlearn: 38.7393095\ttotal: 3.54ms\tremaining: 165ms\n21:\tlearn: 38.5977527\ttotal: 3.57ms\tremaining: 159ms\n22:\tlearn: 38.5977502\ttotal: 3.58ms\tremaining: 152ms\n23:\tlearn: 38.5977478\ttotal: 4.34ms\tremaining: 176ms\n24:\tlearn: 38.5977454\ttotal: 4.37ms\tremaining: 170ms\n25:\tlearn: 38.4545614\ttotal: 4.49ms\tremaining: 168ms\n26:\tlearn: 38.3123315\ttotal: 4.57ms\tremaining: 165ms\n27:\tlearn: 38.3123313\ttotal: 4.6ms\tremaining: 160ms\n28:\tlearn: 38.1719078\ttotal: 4.65ms\tremaining: 156ms\n29:\tlearn: 38.0307220\ttotal: 4.68ms\tremaining: 151ms\n30:\tlearn: 38.0307218\ttotal: 4.71ms\tremaining: 147ms\n31:\tlearn: 37.8904779\ttotal: 5.5ms\tremaining: 166ms\n32:\tlearn: 37.7355816\ttotal: 5.69ms\tremaining: 167ms\n33:\tlearn: 37.7355816\ttotal: 5.71ms\tremaining: 162ms\n34:\tlearn: 37.7355816\ttotal: 5.74ms\tremaining: 158ms\n35:\tlearn: 37.5813185\ttotal: 5.82ms\tremaining: 156ms\n36:\tlearn: 37.4276861\ttotal: 5.91ms\tremaining: 154ms\n37:\tlearn: 37.2900791\ttotal: 5.97ms\tremaining: 151ms\n38:\tlearn: 37.1525784\ttotal: 6ms\tremaining: 148ms\n39:\tlearn: 37.0159831\ttotal: 6.06ms\tremaining: 145ms\n40:\tlearn: 36.8803000\ttotal: 6.11ms\tremaining: 143ms\n41:\tlearn: 36.7455237\ttotal: 6.15ms\tremaining: 140ms\n42:\tlearn: 36.6116489\ttotal: 6.2ms\tremaining: 138ms\n43:\tlearn: 36.6116433\ttotal: 6.22ms\tremaining: 135ms\n44:\tlearn: 36.4786620\ttotal: 6.25ms\tremaining: 133ms\n45:\tlearn: 36.3295373\ttotal: 6.37ms\tremaining: 132ms\n46:\tlearn: 36.1939725\ttotal: 6.41ms\tremaining: 130ms\n47:\tlearn: 36.0460115\ttotal: 6.54ms\tremaining: 130ms\n48:\tlearn: 36.0460062\ttotal: 6.57ms\tremaining: 128ms\n49:\tlearn: 36.0460011\ttotal: 6.6ms\tremaining: 125ms\n50:\tlearn: 35.9150736\ttotal: 6.65ms\tremaining: 124ms\n51:\tlearn: 35.7850232\ttotal: 6.68ms\tremaining: 122ms\n52:\tlearn: 35.6558448\ttotal: 6.71ms\tremaining: 120ms\n53:\tlearn: 35.5220062\ttotal: 6.73ms\tremaining: 118ms\n54:\tlearn: 35.3890570\ttotal: 6.77ms\tremaining: 116ms\n55:\tlearn: 35.2609227\ttotal: 6.82ms\tremaining: 115ms\n56:\tlearn: 35.1336476\ttotal: 6.85ms\tremaining: 113ms\n57:\tlearn: 35.0072266\ttotal: 6.88ms\tremaining: 112ms\n58:\tlearn: 34.8816546\ttotal: 6.9ms\tremaining: 110ms\n59:\tlearn: 34.7569266\ttotal: 6.93ms\tremaining: 109ms\n60:\tlearn: 34.6330376\ttotal: 6.98ms\tremaining: 107ms\n61:\tlearn: 34.5014976\ttotal: 7.02ms\tremaining: 106ms\n62:\tlearn: 34.3604554\ttotal: 7.07ms\tremaining: 105ms\n63:\tlearn: 34.2303207\ttotal: 7.11ms\tremaining: 104ms\n64:\tlearn: 34.1079410\ttotal: 7.14ms\tremaining: 103ms\n65:\tlearn: 33.9787542\ttotal: 7.18ms\tremaining: 102ms\n66:\tlearn: 33.8572831\ttotal: 7.21ms\tremaining: 100ms\n67:\tlearn: 33.7290372\ttotal: 7.25ms\tremaining: 99.3ms\n68:\tlearn: 33.6016412\ttotal: 7.28ms\tremaining: 98.2ms\n69:\tlearn: 33.4750899\ttotal: 7.31ms\tremaining: 97.2ms\n70:\tlearn: 33.3493783\ttotal: 8.09ms\tremaining: 106ms\n71:\tlearn: 33.2245016\ttotal: 8.21ms\tremaining: 106ms\n72:\tlearn: 33.1004547\ttotal: 8.23ms\tremaining: 105ms\n73:\tlearn: 32.9772327\ttotal: 8.26ms\tremaining: 103ms\n74:\tlearn: 32.8424218\ttotal: 8.33ms\tremaining: 103ms\n75:\tlearn: 32.7081620\ttotal: 8.39ms\tremaining: 102ms\n76:\tlearn: 32.5890722\ttotal: 8.44ms\tremaining: 101ms\n77:\tlearn: 32.4677504\ttotal: 8.49ms\tremaining: 100ms\n78:\tlearn: 32.3495396\ttotal: 8.52ms\tremaining: 99.3ms\n79:\tlearn: 32.2291060\ttotal: 8.54ms\tremaining: 98.3ms\n80:\tlearn: 32.1094739\ttotal: 8.57ms\tremaining: 97.2ms\n81:\tlearn: 31.9782103\ttotal: 8.62ms\tremaining: 96.5ms\n82:\tlearn: 31.8598607\ttotal: 8.65ms\tremaining: 95.6ms\n83:\tlearn: 31.7431615\ttotal: 8.7ms\tremaining: 94.8ms\n84:\tlearn: 31.6272431\ttotal: 8.74ms\tremaining: 94.1ms\n85:\tlearn: 31.5098404\ttotal: 8.77ms\tremaining: 93.2ms\n86:\tlearn: 31.3810282\ttotal: 8.82ms\tremaining: 92.6ms\n87:\tlearn: 31.2664350\ttotal: 8.87ms\tremaining: 92ms\n88:\tlearn: 31.1526090\ttotal: 8.91ms\tremaining: 91.2ms\n89:\tlearn: 31.0395458\ttotal: 8.93ms\tremaining: 90.3ms\n90:\tlearn: 30.9272409\ttotal: 8.97ms\tremaining: 89.6ms\n91:\tlearn: 30.8008103\ttotal: 9.03ms\tremaining: 89.2ms\n92:\tlearn: 30.6854504\ttotal: 9.07ms\tremaining: 88.5ms\n93:\tlearn: 30.5744319\ttotal: 9.11ms\tremaining: 87.8ms\n94:\tlearn: 30.4599149\ttotal: 9.14ms\tremaining: 87.1ms\n95:\tlearn: 30.3353947\ttotal: 9.19ms\tremaining: 86.5ms\n96:\tlearn: 30.2256480\ttotal: 9.25ms\tremaining: 86.1ms\n97:\tlearn: 30.1124323\ttotal: 9.28ms\tremaining: 85.4ms\n98:\tlearn: 30.0034975\ttotal: 9.32ms\tremaining: 84.8ms\n99:\tlearn: 29.8911089\ttotal: 9.36ms\tremaining: 84.2ms\n100:\tlearn: 29.7829801\ttotal: 9.39ms\tremaining: 83.6ms\n101:\tlearn: 29.6755771\ttotal: 9.43ms\tremaining: 83ms\n102:\tlearn: 29.5640837\ttotal: 9.45ms\tremaining: 82.3ms\n103:\tlearn: 29.4574763\ttotal: 9.49ms\tremaining: 81.8ms\n104:\tlearn: 29.3370541\ttotal: 9.73ms\tremaining: 82.9ms\n105:\tlearn: 29.2268267\ttotal: 9.87ms\tremaining: 83.2ms\n106:\tlearn: 29.1173308\ttotal: 9.9ms\tremaining: 82.6ms\n107:\tlearn: 29.0120185\ttotal: 9.94ms\tremaining: 82.1ms\n108:\tlearn: 28.8934174\ttotal: 10ms\tremaining: 81.9ms\n109:\tlearn: 28.7851656\ttotal: 10ms\tremaining: 81.3ms\n110:\tlearn: 28.6776326\ttotal: 10.1ms\tremaining: 80.7ms\n111:\tlearn: 28.5603984\ttotal: 10.2ms\tremaining: 80.6ms\n112:\tlearn: 28.4567898\ttotal: 10.2ms\tremaining: 80.2ms\n113:\tlearn: 28.3538763\ttotal: 10.3ms\tremaining: 79.9ms\n114:\tlearn: 28.2516540\ttotal: 10.3ms\tremaining: 79.3ms\n115:\tlearn: 28.1501185\ttotal: 10.3ms\tremaining: 78.8ms\n116:\tlearn: 28.0440199\ttotal: 10.4ms\tremaining: 78.3ms\n117:\tlearn: 27.9432376\ttotal: 10.4ms\tremaining: 77.9ms\n118:\tlearn: 27.8431332\ttotal: 10.5ms\tremaining: 77.4ms\n119:\tlearn: 27.7378777\ttotal: 10.5ms\tremaining: 76.9ms\n120:\tlearn: 27.6333193\ttotal: 10.5ms\tremaining: 76.5ms\n121:\tlearn: 27.5203543\ttotal: 10.6ms\tremaining: 76.2ms\n122:\tlearn: 27.4169136\ttotal: 10.6ms\tremaining: 75.9ms\n123:\tlearn: 27.3141591\ttotal: 10.7ms\tremaining: 75.7ms\n124:\tlearn: 27.2120866\ttotal: 10.7ms\tremaining: 75.2ms\n125:\tlearn: 27.1008436\ttotal: 10.8ms\tremaining: 75.1ms\n126:\tlearn: 26.9998637\ttotal: 10.9ms\tremaining: 74.7ms\n127:\tlearn: 26.9016535\ttotal: 10.9ms\tremaining: 74.4ms\n128:\tlearn: 26.8014121\ttotal: 11ms\tremaining: 74ms\n129:\tlearn: 26.6918479\ttotal: 11ms\tremaining: 73.9ms\n130:\tlearn: 26.5926799\ttotal: 11.1ms\tremaining: 73.5ms\n131:\tlearn: 26.4956603\ttotal: 11.1ms\tremaining: 73ms\n132:\tlearn: 26.3992911\ttotal: 11.1ms\tremaining: 72.6ms\n133:\tlearn: 26.3035682\ttotal: 11.2ms\tremaining: 72.1ms\n134:\tlearn: 26.2052577\ttotal: 11.2ms\tremaining: 71.7ms\n135:\tlearn: 26.1076002\ttotal: 11.2ms\tremaining: 71.2ms\n136:\tlearn: 26.0105918\ttotal: 11.3ms\tremaining: 70.9ms\n137:\tlearn: 25.9157072\ttotal: 11.3ms\tremaining: 70.6ms\n138:\tlearn: 25.8097638\ttotal: 11.4ms\tremaining: 70.4ms\n139:\tlearn: 25.7138590\ttotal: 11.4ms\tremaining: 70.1ms\n140:\tlearn: 25.6087408\ttotal: 11.5ms\tremaining: 69.8ms\n141:\tlearn: 25.5153252\ttotal: 11.5ms\tremaining: 69.6ms\n142:\tlearn: 25.4205115\ttotal: 11.5ms\tremaining: 69.2ms\n143:\tlearn: 25.3263284\ttotal: 11.6ms\tremaining: 68.9ms\n144:\tlearn: 25.2227944\ttotal: 11.6ms\tremaining: 68.7ms\n145:\tlearn: 25.1305106\ttotal: 11.8ms\tremaining: 69.3ms\n146:\tlearn: 25.0277771\ttotal: 12ms\tremaining: 69.6ms\n147:\tlearn: 24.9364862\ttotal: 12ms\tremaining: 69.2ms\n148:\tlearn: 24.8458074\ttotal: 12.1ms\tremaining: 68.9ms\n149:\tlearn: 24.7532013\ttotal: 12.1ms\tremaining: 68.5ms\n150:\tlearn: 24.6631927\ttotal: 12.1ms\tremaining: 68.1ms\n151:\tlearn: 24.5623696\ttotal: 12.2ms\tremaining: 67.9ms\n152:\tlearn: 24.4619586\ttotal: 12.2ms\tremaining: 67.8ms\n153:\tlearn: 24.3732835\ttotal: 12.3ms\tremaining: 67.5ms\n154:\tlearn: 24.2852038\ttotal: 12.3ms\tremaining: 67.2ms\n155:\tlearn: 24.1941464\ttotal: 12.4ms\tremaining: 66.8ms\n156:\tlearn: 24.1067191\ttotal: 12.4ms\tremaining: 66.5ms\n157:\tlearn: 24.0081708\ttotal: 12.4ms\tremaining: 66.3ms\n158:\tlearn: 23.9181474\ttotal: 12.5ms\tremaining: 66ms\n159:\tlearn: 23.8317225\ttotal: 12.5ms\tremaining: 65.7ms\n160:\tlearn: 23.7342984\ttotal: 12.6ms\tremaining: 65.4ms\n161:\tlearn: 23.6372726\ttotal: 12.6ms\tremaining: 65.2ms\n162:\tlearn: 23.5521289\ttotal: 12.6ms\tremaining: 64.9ms\n163:\tlearn: 23.4675580\ttotal: 12.7ms\tremaining: 64.6ms\n164:\tlearn: 23.3716226\ttotal: 12.7ms\tremaining: 64.4ms\n165:\tlearn: 23.2760794\ttotal: 12.8ms\tremaining: 64.2ms\n166:\tlearn: 23.1882819\ttotal: 12.8ms\tremaining: 63.9ms\n167:\tlearn: 23.0934882\ttotal: 12.9ms\tremaining: 63.7ms\n168:\tlearn: 23.0105704\ttotal: 12.9ms\tremaining: 63.4ms\n169:\tlearn: 22.9237686\ttotal: 12.9ms\tremaining: 63.1ms\n170:\tlearn: 22.8414664\ttotal: 13ms\tremaining: 62.9ms\n171:\tlearn: 22.7552967\ttotal: 13ms\tremaining: 62.6ms\n172:\tlearn: 22.6696976\ttotal: 13ms\tremaining: 62.4ms\n173:\tlearn: 22.5770239\ttotal: 13.1ms\tremaining: 62.2ms\n174:\tlearn: 22.4847290\ttotal: 13.2ms\tremaining: 62ms\n175:\tlearn: 22.3928114\ttotal: 13.2ms\tremaining: 61.8ms\n176:\tlearn: 22.3012696\ttotal: 13.2ms\tremaining: 61.6ms\n177:\tlearn: 22.2101020\ttotal: 13.3ms\tremaining: 61.3ms\n178:\tlearn: 22.1193071\ttotal: 13.3ms\tremaining: 61.1ms\n179:\tlearn: 22.0288834\ttotal: 13.4ms\tremaining: 60.9ms\n180:\tlearn: 21.9388293\ttotal: 13.4ms\tremaining: 60.8ms\n181:\tlearn: 21.8565389\ttotal: 13.5ms\tremaining: 60.5ms\n182:\tlearn: 21.7671894\ttotal: 13.5ms\tremaining: 60.2ms\n183:\tlearn: 21.6782051\ttotal: 13.5ms\tremaining: 60ms\n184:\tlearn: 21.5895846\ttotal: 13.6ms\tremaining: 60ms\n185:\tlearn: 21.5088384\ttotal: 13.6ms\tremaining: 59.7ms\n186:\tlearn: 21.4209103\ttotal: 13.7ms\tremaining: 59.5ms\n187:\tlearn: 21.3333416\ttotal: 13.7ms\tremaining: 59.3ms\n188:\tlearn: 21.2461309\ttotal: 14ms\tremaining: 59.9ms\n189:\tlearn: 21.1592768\ttotal: 14.1ms\tremaining: 60.3ms\n190:\tlearn: 21.0727776\ttotal: 14.2ms\tremaining: 60.1ms\n191:\tlearn: 20.9866321\ttotal: 14.2ms\tremaining: 59.9ms\n192:\tlearn: 20.9083692\ttotal: 14.3ms\tremaining: 59.7ms\n193:\tlearn: 20.8228958\ttotal: 14.3ms\tremaining: 59.5ms\n194:\tlearn: 20.7377718\ttotal: 14.4ms\tremaining: 59.3ms\n195:\tlearn: 20.6529958\ttotal: 14.4ms\tremaining: 59.2ms\n196:\tlearn: 20.5685664\ttotal: 14.5ms\tremaining: 59ms\n197:\tlearn: 20.4844821\ttotal: 14.5ms\tremaining: 58.8ms\n198:\tlearn: 20.4007415\ttotal: 14.6ms\tremaining: 58.6ms\n199:\tlearn: 20.3248859\ttotal: 14.6ms\tremaining: 58.4ms\n200:\tlearn: 20.2417978\ttotal: 14.6ms\tremaining: 58.2ms\n201:\tlearn: 20.1680108\ttotal: 14.7ms\tremaining: 58ms\n202:\tlearn: 20.0855639\ttotal: 14.7ms\tremaining: 57.8ms\n203:\tlearn: 20.0034542\ttotal: 14.8ms\tremaining: 57.6ms\n204:\tlearn: 19.9216800\ttotal: 14.8ms\tremaining: 57.4ms\n205:\tlearn: 19.8402402\ttotal: 14.8ms\tremaining: 57.2ms\n206:\tlearn: 19.7591333\ttotal: 14.9ms\tremaining: 57.1ms\n207:\tlearn: 19.6856607\ttotal: 14.9ms\tremaining: 56.8ms\n208:\tlearn: 19.6051857\ttotal: 15ms\tremaining: 56.7ms\n209:\tlearn: 19.5250397\ttotal: 15ms\tremaining: 56.5ms\n210:\tlearn: 19.4452214\ttotal: 15.1ms\tremaining: 56.3ms\n211:\tlearn: 19.3657293\ttotal: 15.1ms\tremaining: 56.2ms\n212:\tlearn: 19.2865622\ttotal: 15.2ms\tremaining: 56ms\n213:\tlearn: 19.2077187\ttotal: 15.2ms\tremaining: 55.8ms\n214:\tlearn: 19.1291976\ttotal: 15.2ms\tremaining: 55.6ms\n215:\tlearn: 19.0509974\ttotal: 15.3ms\tremaining: 55.5ms\n216:\tlearn: 18.9731170\ttotal: 15.3ms\tremaining: 55.3ms\n217:\tlearn: 18.8955549\ttotal: 15.4ms\tremaining: 55.2ms\n218:\tlearn: 18.8183098\ttotal: 15.5ms\tremaining: 55.1ms\n219:\tlearn: 18.7485412\ttotal: 15.5ms\tremaining: 54.9ms\n220:\tlearn: 18.6718972\ttotal: 15.5ms\tremaining: 54.8ms\n221:\tlearn: 18.5955665\ttotal: 15.6ms\tremaining: 54.6ms\n222:\tlearn: 18.5195478\ttotal: 15.6ms\tremaining: 54.5ms\n223:\tlearn: 18.4438399\ttotal: 15.7ms\tremaining: 54.4ms\n224:\tlearn: 18.3684415\ttotal: 15.7ms\tremaining: 54.2ms\n225:\tlearn: 18.2933513\ttotal: 15.8ms\tremaining: 54.1ms\n226:\tlearn: 18.2257294\ttotal: 15.9ms\tremaining: 54.3ms\n227:\tlearn: 18.1512226\ttotal: 16.1ms\tremaining: 54.5ms\n228:\tlearn: 18.0770204\ttotal: 16.2ms\tremaining: 54.4ms\n229:\tlearn: 18.0031216\ttotal: 16.2ms\tremaining: 54.3ms\n230:\tlearn: 17.9295248\ttotal: 16.3ms\tremaining: 54.2ms\n231:\tlearn: 17.8562289\ttotal: 16.3ms\tremaining: 54.1ms\n232:\tlearn: 17.7832327\ttotal: 16.4ms\tremaining: 53.9ms\n233:\tlearn: 17.7105348\ttotal: 16.4ms\tremaining: 53.8ms\n234:\tlearn: 17.6381342\ttotal: 16.5ms\tremaining: 53.7ms\n235:\tlearn: 17.5660295\ttotal: 16.5ms\tremaining: 53.5ms\n236:\tlearn: 17.4942195\ttotal: 16.6ms\tremaining: 53.5ms\n237:\tlearn: 17.4227032\ttotal: 16.6ms\tremaining: 53.3ms\n238:\tlearn: 17.3514792\ttotal: 16.7ms\tremaining: 53.2ms\n239:\tlearn: 17.2805463\ttotal: 16.8ms\tremaining: 53.1ms\n240:\tlearn: 17.2099034\ttotal: 16.8ms\tremaining: 53ms\n241:\tlearn: 17.1395494\ttotal: 16.9ms\tremaining: 52.9ms\n242:\tlearn: 17.0694829\ttotal: 17ms\tremaining: 52.8ms\n243:\tlearn: 16.9997028\ttotal: 17ms\tremaining: 52.7ms\n244:\tlearn: 16.9302080\ttotal: 17.1ms\tremaining: 52.6ms\n245:\tlearn: 16.8609974\ttotal: 17.1ms\tremaining: 52.4ms\n246:\tlearn: 16.7920696\ttotal: 17.2ms\tremaining: 52.3ms\n247:\tlearn: 16.7234236\ttotal: 17.8ms\tremaining: 54ms\n248:\tlearn: 16.6550583\ttotal: 17.9ms\tremaining: 53.9ms\n249:\tlearn: 16.5869724\ttotal: 17.9ms\tremaining: 53.8ms\n250:\tlearn: 16.5191649\ttotal: 18ms\tremaining: 53.7ms\n251:\tlearn: 16.4516345\ttotal: 18.2ms\tremaining: 53.9ms\n252:\tlearn: 16.3843802\ttotal: 18.3ms\tremaining: 54.1ms\n253:\tlearn: 16.3174009\ttotal: 18.4ms\tremaining: 54.1ms\n254:\tlearn: 16.2506953\ttotal: 18.5ms\tremaining: 53.9ms\n255:\tlearn: 16.1842625\ttotal: 18.5ms\tremaining: 53.8ms\n256:\tlearn: 16.1181012\ttotal: 18.6ms\tremaining: 53.6ms\n257:\tlearn: 16.0522104\ttotal: 18.6ms\tremaining: 53.5ms\n258:\tlearn: 15.9865890\ttotal: 18.6ms\tremaining: 53.4ms\n259:\tlearn: 15.9212358\ttotal: 18.7ms\tremaining: 53.3ms\n260:\tlearn: 15.8561498\ttotal: 18.8ms\tremaining: 53.2ms\n261:\tlearn: 15.7913299\ttotal: 18.8ms\tremaining: 53.1ms\n262:\tlearn: 15.7267749\ttotal: 18.9ms\tremaining: 52.9ms\n263:\tlearn: 15.6624839\ttotal: 18.9ms\tremaining: 52.8ms\n264:\tlearn: 15.5984556\ttotal: 19ms\tremaining: 52.8ms\n265:\tlearn: 15.5346892\ttotal: 19.1ms\tremaining: 52.7ms\n266:\tlearn: 15.4711833\ttotal: 19.1ms\tremaining: 52.5ms\n267:\tlearn: 15.4079372\ttotal: 19.2ms\tremaining: 52.4ms\n268:\tlearn: 15.3449495\ttotal: 19.2ms\tremaining: 52.3ms\n269:\tlearn: 15.2822194\ttotal: 19.3ms\tremaining: 52.2ms\n270:\tlearn: 15.2197456\ttotal: 19.3ms\tremaining: 52ms\n271:\tlearn: 15.1575273\ttotal: 19.4ms\tremaining: 51.9ms\n272:\tlearn: 15.0955634\ttotal: 19.4ms\tremaining: 51.7ms\n273:\tlearn: 15.0338527\ttotal: 19.5ms\tremaining: 51.6ms\n274:\tlearn: 14.9723943\ttotal: 19.5ms\tremaining: 51.5ms\n275:\tlearn: 14.9111872\ttotal: 19.6ms\tremaining: 51.4ms\n276:\tlearn: 14.8502302\ttotal: 19.6ms\tremaining: 51.2ms\n277:\tlearn: 14.7895225\ttotal: 19.7ms\tremaining: 51.1ms\n278:\tlearn: 14.7290629\ttotal: 19.7ms\tremaining: 51ms\n279:\tlearn: 14.6688505\ttotal: 19.8ms\tremaining: 51ms\n280:\tlearn: 14.6088842\ttotal: 19.9ms\tremaining: 50.8ms\n281:\tlearn: 14.5491631\ttotal: 19.9ms\tremaining: 50.7ms\n282:\tlearn: 14.4896862\ttotal: 19.9ms\tremaining: 50.5ms\n283:\tlearn: 14.4304523\ttotal: 20ms\tremaining: 50.4ms\n284:\tlearn: 14.3714606\ttotal: 20.1ms\tremaining: 50.3ms\n285:\tlearn: 14.3127101\ttotal: 20.1ms\tremaining: 50.2ms\n286:\tlearn: 14.2541997\ttotal: 20.1ms\tremaining: 50ms\n287:\tlearn: 14.1959286\ttotal: 20.2ms\tremaining: 49.9ms\n288:\tlearn: 14.1378956\ttotal: 20.5ms\tremaining: 50.3ms\n289:\tlearn: 14.0800999\ttotal: 20.6ms\tremaining: 50.5ms\n290:\tlearn: 14.0225405\ttotal: 20.7ms\tremaining: 50.5ms\n291:\tlearn: 13.9652163\ttotal: 20.8ms\tremaining: 50.4ms\n292:\tlearn: 13.9081265\ttotal: 20.8ms\tremaining: 50.3ms\n293:\tlearn: 13.8512701\ttotal: 20.9ms\tremaining: 50.1ms\n294:\tlearn: 13.7946461\ttotal: 20.9ms\tremaining: 50ms\n295:\tlearn: 13.7382536\ttotal: 21ms\tremaining: 49.9ms\n296:\tlearn: 13.6820916\ttotal: 21ms\tremaining: 49.8ms\n297:\tlearn: 13.6261592\ttotal: 21.1ms\tremaining: 49.7ms\n298:\tlearn: 13.5704555\ttotal: 21.1ms\tremaining: 49.6ms\n299:\tlearn: 13.5149795\ttotal: 21.2ms\tremaining: 49.5ms\n300:\tlearn: 13.4597302\ttotal: 21.3ms\tremaining: 49.4ms\n301:\tlearn: 13.4047068\ttotal: 21.3ms\tremaining: 49.3ms\n302:\tlearn: 13.3499084\ttotal: 21.4ms\tremaining: 49.2ms\n303:\tlearn: 13.2953340\ttotal: 22.2ms\tremaining: 50.8ms\n304:\tlearn: 13.2409827\ttotal: 22.2ms\tremaining: 50.7ms\n305:\tlearn: 13.1868535\ttotal: 22.3ms\tremaining: 50.6ms\n306:\tlearn: 13.1329457\ttotal: 22.4ms\tremaining: 50.5ms\n307:\tlearn: 13.0792582\ttotal: 22.4ms\tremaining: 50.4ms\n308:\tlearn: 13.0257902\ttotal: 22.5ms\tremaining: 50.3ms\n309:\tlearn: 12.9725407\ttotal: 22.6ms\tremaining: 50.3ms\n310:\tlearn: 12.9195090\ttotal: 22.8ms\tremaining: 50.5ms\n311:\tlearn: 12.8666940\ttotal: 22.9ms\tremaining: 50.4ms\n312:\tlearn: 12.8140950\ttotal: 22.9ms\tremaining: 50.3ms\n313:\tlearn: 12.7617110\ttotal: 23ms\tremaining: 50.2ms\n314:\tlearn: 12.7095411\ttotal: 23.1ms\tremaining: 50.1ms\n315:\tlearn: 12.6575845\ttotal: 23.1ms\tremaining: 50ms\n316:\tlearn: 12.6058403\ttotal: 23.2ms\tremaining: 49.9ms\n317:\tlearn: 12.5543076\ttotal: 23.2ms\tremaining: 49.8ms\n318:\tlearn: 12.5029856\ttotal: 23.3ms\tremaining: 49.7ms\n319:\tlearn: 12.4518734\ttotal: 23.3ms\tremaining: 49.6ms\n320:\tlearn: 12.4009702\ttotal: 23.4ms\tremaining: 49.4ms\n321:\tlearn: 12.3502750\ttotal: 23.4ms\tremaining: 49.3ms\n322:\tlearn: 12.2997871\ttotal: 23.5ms\tremaining: 49.2ms\n323:\tlearn: 12.2495055\ttotal: 23.5ms\tremaining: 49.1ms\n324:\tlearn: 12.1994296\ttotal: 23.6ms\tremaining: 49ms\n325:\tlearn: 12.1495583\ttotal: 23.7ms\tremaining: 48.9ms\n326:\tlearn: 12.0998909\ttotal: 23.7ms\tremaining: 48.8ms\n327:\tlearn: 12.0504265\ttotal: 23.7ms\tremaining: 48.6ms\n328:\tlearn: 12.0011644\ttotal: 23.8ms\tremaining: 48.6ms\n329:\tlearn: 11.9521036\ttotal: 23.9ms\tremaining: 48.5ms\n330:\tlearn: 11.9032434\ttotal: 23.9ms\tremaining: 48.3ms\n331:\tlearn: 11.8545830\ttotal: 24ms\tremaining: 48.2ms\n332:\tlearn: 11.8061215\ttotal: 24ms\tremaining: 48.2ms\n333:\tlearn: 11.7578580\ttotal: 24.1ms\tremaining: 48ms\n334:\tlearn: 11.7097919\ttotal: 24.1ms\tremaining: 47.9ms\n335:\tlearn: 11.6619223\ttotal: 24.2ms\tremaining: 47.8ms\n336:\tlearn: 11.6142483\ttotal: 24.2ms\tremaining: 47.7ms\n337:\tlearn: 11.5667693\ttotal: 24.3ms\tremaining: 47.6ms\n338:\tlearn: 11.5194843\ttotal: 24.3ms\tremaining: 47.5ms\n339:\tlearn: 11.4723927\ttotal: 24.4ms\tremaining: 47.4ms\n340:\tlearn: 11.4254936\ttotal: 24.5ms\tremaining: 47.3ms\n341:\tlearn: 11.3787861\ttotal: 24.5ms\tremaining: 47.1ms\n342:\tlearn: 11.3322697\ttotal: 24.7ms\tremaining: 47.3ms\n343:\tlearn: 11.2859433\ttotal: 24.9ms\tremaining: 47.6ms\n344:\tlearn: 11.2398064\ttotal: 25ms\tremaining: 47.5ms\n345:\tlearn: 11.1938581\ttotal: 25.1ms\tremaining: 47.4ms\n346:\tlearn: 11.1480976\ttotal: 25.1ms\tremaining: 47.3ms\n347:\tlearn: 11.1025242\ttotal: 25.2ms\tremaining: 47.2ms\n348:\tlearn: 11.0571370\ttotal: 25.2ms\tremaining: 47.1ms\n349:\tlearn: 11.0119355\ttotal: 25.3ms\tremaining: 47ms\n350:\tlearn: 10.9669187\ttotal: 25.4ms\tremaining: 46.9ms\n351:\tlearn: 10.9220859\ttotal: 25.4ms\tremaining: 46.8ms\n352:\tlearn: 10.8774364\ttotal: 25.5ms\tremaining: 46.7ms\n353:\tlearn: 10.8329695\ttotal: 25.5ms\tremaining: 46.6ms\n354:\tlearn: 10.7886843\ttotal: 25.6ms\tremaining: 46.5ms\n355:\tlearn: 10.7445802\ttotal: 25.7ms\tremaining: 46.4ms\n356:\tlearn: 10.7006563\ttotal: 25.7ms\tremaining: 46.3ms\n357:\tlearn: 10.6569120\ttotal: 25.8ms\tremaining: 46.2ms\n358:\tlearn: 10.6133466\ttotal: 25.8ms\tremaining: 46.1ms\n359:\tlearn: 10.5699592\ttotal: 25.8ms\tremaining: 46ms\n360:\tlearn: 10.5267492\ttotal: 25.9ms\tremaining: 45.8ms\n361:\tlearn: 10.4837159\ttotal: 26ms\tremaining: 45.8ms\n362:\tlearn: 10.4408584\ttotal: 26ms\tremaining: 45.7ms\n363:\tlearn: 10.3981762\ttotal: 26.1ms\tremaining: 45.6ms\n364:\tlearn: 10.3556685\ttotal: 26.1ms\tremaining: 45.5ms\n365:\tlearn: 10.3133345\ttotal: 26.2ms\tremaining: 45.4ms\n366:\tlearn: 10.2711736\ttotal: 26.2ms\tremaining: 45.3ms\n367:\tlearn: 10.2291850\ttotal: 26.3ms\tremaining: 45.2ms\n368:\tlearn: 10.1873681\ttotal: 26.3ms\tremaining: 45ms\n369:\tlearn: 10.1457222\ttotal: 26.4ms\tremaining: 44.9ms\n370:\tlearn: 10.1042464\ttotal: 26.4ms\tremaining: 44.8ms\n371:\tlearn: 10.0629403\ttotal: 26.5ms\tremaining: 44.7ms\n372:\tlearn: 10.0218030\ttotal: 26.5ms\tremaining: 44.6ms\n373:\tlearn: 9.9808339\ttotal: 26.6ms\tremaining: 44.5ms\n374:\tlearn: 9.9400322\ttotal: 26.7ms\tremaining: 44.5ms\n375:\tlearn: 9.8993974\ttotal: 27ms\tremaining: 44.8ms\n376:\tlearn: 9.8589286\ttotal: 27ms\tremaining: 44.7ms\n377:\tlearn: 9.8186253\ttotal: 27.1ms\tremaining: 44.6ms\n378:\tlearn: 9.7784868\ttotal: 27.2ms\tremaining: 44.5ms\n379:\tlearn: 9.7385123\ttotal: 28ms\tremaining: 45.6ms\n380:\tlearn: 9.6987013\ttotal: 28ms\tremaining: 45.5ms\n381:\tlearn: 9.6590530\ttotal: 28.1ms\tremaining: 45.5ms\n382:\tlearn: 9.6195668\ttotal: 28.1ms\tremaining: 45.3ms\n383:\tlearn: 9.5802420\ttotal: 28.2ms\tremaining: 45.2ms\n384:\tlearn: 9.5410780\ttotal: 28.2ms\tremaining: 45.1ms\n385:\tlearn: 9.5020741\ttotal: 28.3ms\tremaining: 45ms\n386:\tlearn: 9.4632296\ttotal: 28.3ms\tremaining: 44.8ms\n387:\tlearn: 9.4245439\ttotal: 28.3ms\tremaining: 44.7ms\n388:\tlearn: 9.3860164\ttotal: 28.4ms\tremaining: 44.6ms\n389:\tlearn: 9.3476463\ttotal: 28.5ms\tremaining: 44.5ms\n390:\tlearn: 9.3094331\ttotal: 28.5ms\tremaining: 44.4ms\n391:\tlearn: 9.2713762\ttotal: 28.6ms\tremaining: 44.3ms\n392:\tlearn: 9.2334748\ttotal: 28.6ms\tremaining: 44.2ms\n393:\tlearn: 9.1957284\ttotal: 28.7ms\tremaining: 44.1ms\n394:\tlearn: 9.1581362\ttotal: 28.7ms\tremaining: 44ms\n395:\tlearn: 9.1206978\ttotal: 28.8ms\tremaining: 43.9ms\n396:\tlearn: 9.0834123\ttotal: 28.8ms\tremaining: 43.8ms\n397:\tlearn: 9.0462794\ttotal: 28.9ms\tremaining: 43.7ms\n398:\tlearn: 9.0092982\ttotal: 29ms\tremaining: 43.7ms\n399:\tlearn: 8.9724682\ttotal: 29.1ms\tremaining: 43.6ms\n400:\tlearn: 8.9357887\ttotal: 29.1ms\tremaining: 43.5ms\n401:\tlearn: 8.8992592\ttotal: 29.2ms\tremaining: 43.4ms\n402:\tlearn: 8.8628790\ttotal: 29.3ms\tremaining: 43.3ms\n403:\tlearn: 8.8266476\ttotal: 29.3ms\tremaining: 43.2ms\n404:\tlearn: 8.7905643\ttotal: 29.3ms\tremaining: 43.1ms\n405:\tlearn: 8.7546284\ttotal: 29.4ms\tremaining: 43ms\n406:\tlearn: 8.7188395\ttotal: 29.4ms\tremaining: 42.9ms\n407:\tlearn: 8.6831969\ttotal: 29.5ms\tremaining: 42.8ms\n408:\tlearn: 8.6477000\ttotal: 29.6ms\tremaining: 42.7ms\n409:\tlearn: 8.6123482\ttotal: 29.6ms\tremaining: 42.6ms\n410:\tlearn: 8.5771409\ttotal: 29.7ms\tremaining: 42.5ms\n411:\tlearn: 8.5420776\ttotal: 29.7ms\tremaining: 42.4ms\n412:\tlearn: 8.5071575\ttotal: 29.8ms\tremaining: 42.3ms\n413:\tlearn: 8.4723803\ttotal: 29.8ms\tremaining: 42.2ms\n414:\tlearn: 8.4377452\ttotal: 29.9ms\tremaining: 42.1ms\n415:\tlearn: 8.4032517\ttotal: 29.9ms\tremaining: 42ms\n416:\tlearn: 8.3688992\ttotal: 30ms\tremaining: 42ms\n417:\tlearn: 8.3346871\ttotal: 30.1ms\tremaining: 41.9ms\n418:\tlearn: 8.3006149\ttotal: 30.1ms\tremaining: 41.8ms\n419:\tlearn: 8.2666820\ttotal: 30.2ms\tremaining: 41.7ms\n420:\tlearn: 8.2328878\ttotal: 30.2ms\tremaining: 41.6ms\n421:\tlearn: 8.1992318\ttotal: 30.3ms\tremaining: 41.5ms\n422:\tlearn: 8.1657133\ttotal: 30.3ms\tremaining: 41.4ms\n423:\tlearn: 8.1323319\ttotal: 30.4ms\tremaining: 41.3ms\n424:\tlearn: 8.0990869\ttotal: 30.5ms\tremaining: 41.2ms\n425:\tlearn: 8.0659779\ttotal: 30.5ms\tremaining: 41.1ms\n426:\tlearn: 8.0330041\ttotal: 30.6ms\tremaining: 41ms\n427:\tlearn: 8.0001652\ttotal: 30.6ms\tremaining: 40.9ms\n428:\tlearn: 7.9674605\ttotal: 30.7ms\tremaining: 40.8ms\n429:\tlearn: 7.9348896\ttotal: 30.7ms\tremaining: 40.7ms\n430:\tlearn: 7.9024517\ttotal: 30.8ms\tremaining: 40.7ms\n431:\tlearn: 7.8701465\ttotal: 31.1ms\tremaining: 40.8ms\n432:\tlearn: 7.8379734\ttotal: 31.2ms\tremaining: 40.9ms\n433:\tlearn: 7.8059317\ttotal: 31.3ms\tremaining: 40.8ms\n434:\tlearn: 7.7740211\ttotal: 31.3ms\tremaining: 40.7ms\n435:\tlearn: 7.7422409\ttotal: 31.4ms\tremaining: 40.6ms\n436:\tlearn: 7.7105906\ttotal: 31.5ms\tremaining: 40.5ms\n437:\tlearn: 7.6790697\ttotal: 31.5ms\tremaining: 40.4ms\n438:\tlearn: 7.6476777\ttotal: 31.5ms\tremaining: 40.3ms\n439:\tlearn: 7.6164140\ttotal: 31.6ms\tremaining: 40.2ms\n440:\tlearn: 7.5852781\ttotal: 31.6ms\tremaining: 40.1ms\n441:\tlearn: 7.5542694\ttotal: 31.7ms\tremaining: 40ms\n442:\tlearn: 7.5233876\ttotal: 31.8ms\tremaining: 40ms\n443:\tlearn: 7.4926320\ttotal: 31.8ms\tremaining: 39.9ms\n444:\tlearn: 7.4620021\ttotal: 31.9ms\tremaining: 39.8ms\n445:\tlearn: 7.4314974\ttotal: 31.9ms\tremaining: 39.7ms\n446:\tlearn: 7.4011175\ttotal: 32ms\tremaining: 39.6ms\n447:\tlearn: 7.3708617\ttotal: 32.1ms\tremaining: 39.5ms\n448:\tlearn: 7.3407296\ttotal: 32.1ms\tremaining: 39.4ms\n449:\tlearn: 7.3107207\ttotal: 32.1ms\tremaining: 39.3ms\n450:\tlearn: 7.2808345\ttotal: 32.2ms\tremaining: 39.2ms\n451:\tlearn: 7.2510704\ttotal: 32.2ms\tremaining: 39.1ms\n452:\tlearn: 7.2214281\ttotal: 32.3ms\tremaining: 39ms\n453:\tlearn: 7.1919069\ttotal: 32.4ms\tremaining: 38.9ms\n454:\tlearn: 7.1625064\ttotal: 32.4ms\tremaining: 38.8ms\n455:\tlearn: 7.1332260\ttotal: 32.5ms\tremaining: 38.7ms\n456:\tlearn: 7.1040654\ttotal: 32.5ms\tremaining: 38.7ms\n457:\tlearn: 7.0750240\ttotal: 32.6ms\tremaining: 38.6ms\n458:\tlearn: 7.0461013\ttotal: 32.6ms\tremaining: 38.5ms\n459:\tlearn: 7.0172968\ttotal: 32.7ms\tremaining: 38.4ms\n460:\tlearn: 6.9886101\ttotal: 32.7ms\tremaining: 38.3ms\n461:\tlearn: 6.9600407\ttotal: 32.8ms\tremaining: 38.2ms\n462:\tlearn: 6.9315880\ttotal: 32.8ms\tremaining: 38.1ms\n463:\tlearn: 6.9032517\ttotal: 32.9ms\tremaining: 38ms\n464:\tlearn: 6.8750312\ttotal: 33.1ms\tremaining: 38ms\n465:\tlearn: 6.8469261\ttotal: 33.2ms\tremaining: 38.1ms\n466:\tlearn: 6.8189359\ttotal: 33.3ms\tremaining: 38ms\n467:\tlearn: 6.7910600\ttotal: 33.3ms\tremaining: 37.9ms\n468:\tlearn: 6.7632982\ttotal: 33.4ms\tremaining: 37.8ms\n469:\tlearn: 6.7356498\ttotal: 33.4ms\tremaining: 37.7ms\n470:\tlearn: 6.7081145\ttotal: 33.5ms\tremaining: 37.6ms\n471:\tlearn: 6.6806917\ttotal: 33.5ms\tremaining: 37.5ms\n472:\tlearn: 6.6533811\ttotal: 33.6ms\tremaining: 37.4ms\n473:\tlearn: 6.6261820\ttotal: 33.6ms\tremaining: 37.3ms\n474:\tlearn: 6.5990942\ttotal: 33.7ms\tremaining: 37.2ms\n475:\tlearn: 6.5721171\ttotal: 33.7ms\tremaining: 37.1ms\n476:\tlearn: 6.5452503\ttotal: 33.8ms\tremaining: 37.1ms\n477:\tlearn: 6.5184933\ttotal: 33.9ms\tremaining: 37ms\n478:\tlearn: 6.4918457\ttotal: 33.9ms\tremaining: 36.9ms\n479:\tlearn: 6.4653070\ttotal: 34ms\tremaining: 36.8ms\n480:\tlearn: 6.4388769\ttotal: 34ms\tremaining: 36.7ms\n481:\tlearn: 6.4125547\ttotal: 34.1ms\tremaining: 36.6ms\n482:\tlearn: 6.3863402\ttotal: 34.1ms\tremaining: 36.5ms\n483:\tlearn: 6.3602329\ttotal: 34.2ms\tremaining: 36.4ms\n484:\tlearn: 6.3342322\ttotal: 34.2ms\tremaining: 36.3ms\n485:\tlearn: 6.3083379\ttotal: 34.3ms\tremaining: 36.3ms\n486:\tlearn: 6.2825494\ttotal: 34.3ms\tremaining: 36.2ms\n487:\tlearn: 6.2568663\ttotal: 34.4ms\tremaining: 36.1ms\n488:\tlearn: 6.2312883\ttotal: 34.4ms\tremaining: 36ms\n489:\tlearn: 6.2058148\ttotal: 34.5ms\tremaining: 35.9ms\n490:\tlearn: 6.1804454\ttotal: 34.6ms\tremaining: 35.8ms\n491:\tlearn: 6.1551797\ttotal: 34.6ms\tremaining: 35.8ms\n492:\tlearn: 6.1300174\ttotal: 34.7ms\tremaining: 35.7ms\n493:\tlearn: 6.1049578\ttotal: 34.7ms\tremaining: 35.6ms\n494:\tlearn: 6.0800008\ttotal: 34.8ms\tremaining: 35.5ms\n495:\tlearn: 6.0551457\ttotal: 34.8ms\tremaining: 35.4ms\n496:\tlearn: 6.0303923\ttotal: 34.9ms\tremaining: 35.3ms\n497:\tlearn: 6.0057401\ttotal: 34.9ms\tremaining: 35.2ms\n498:\tlearn: 5.9811886\ttotal: 35ms\tremaining: 35.2ms\n499:\tlearn: 5.9567375\ttotal: 35.2ms\tremaining: 35.2ms\n500:\tlearn: 5.9323864\ttotal: 35.3ms\tremaining: 35.1ms\n501:\tlearn: 5.9081348\ttotal: 35.3ms\tremaining: 35.1ms\n502:\tlearn: 5.8839823\ttotal: 35.4ms\tremaining: 35ms\n503:\tlearn: 5.8599286\ttotal: 35.4ms\tremaining: 34.9ms\n504:\tlearn: 5.8359732\ttotal: 35.5ms\tremaining: 34.8ms\n505:\tlearn: 5.8121157\ttotal: 35.6ms\tremaining: 34.7ms\n506:\tlearn: 5.7883558\ttotal: 35.6ms\tremaining: 34.6ms\n507:\tlearn: 5.7646930\ttotal: 35.7ms\tremaining: 34.5ms\n508:\tlearn: 5.7411269\ttotal: 35.7ms\tremaining: 34.5ms\n509:\tlearn: 5.7176572\ttotal: 35.8ms\tremaining: 34.4ms\n510:\tlearn: 5.6942834\ttotal: 35.8ms\tremaining: 34.3ms\n511:\tlearn: 5.6710052\ttotal: 35.9ms\tremaining: 34.2ms\n512:\tlearn: 5.6478221\ttotal: 35.9ms\tremaining: 34.1ms\n513:\tlearn: 5.6247338\ttotal: 36ms\tremaining: 34ms\n514:\tlearn: 5.6017399\ttotal: 36ms\tremaining: 33.9ms\n515:\tlearn: 5.5788400\ttotal: 36.1ms\tremaining: 33.9ms\n516:\tlearn: 5.5560337\ttotal: 36.2ms\tremaining: 33.8ms\n517:\tlearn: 5.5333207\ttotal: 36.2ms\tremaining: 33.7ms\n518:\tlearn: 5.5107004\ttotal: 36.2ms\tremaining: 33.6ms\n519:\tlearn: 5.4881727\ttotal: 36.3ms\tremaining: 33.5ms\n520:\tlearn: 5.4657370\ttotal: 36.3ms\tremaining: 33.4ms\n521:\tlearn: 5.4433931\ttotal: 36.4ms\tremaining: 33.3ms\n522:\tlearn: 5.4211405\ttotal: 36.5ms\tremaining: 33.3ms\n523:\tlearn: 5.3989789\ttotal: 36.5ms\tremaining: 33.2ms\n524:\tlearn: 5.3769079\ttotal: 36.6ms\tremaining: 33.1ms\n525:\tlearn: 5.3549271\ttotal: 36.6ms\tremaining: 33ms\n526:\tlearn: 5.3330361\ttotal: 36.7ms\tremaining: 32.9ms\n527:\tlearn: 5.3112347\ttotal: 36.8ms\tremaining: 32.9ms\n528:\tlearn: 5.2895224\ttotal: 36.8ms\tremaining: 32.8ms\n529:\tlearn: 5.2678988\ttotal: 36.9ms\tremaining: 32.7ms\n530:\tlearn: 5.2463636\ttotal: 36.9ms\tremaining: 32.6ms\n531:\tlearn: 5.2249165\ttotal: 37ms\tremaining: 32.5ms\n532:\tlearn: 5.2035570\ttotal: 37.2ms\tremaining: 32.6ms\n533:\tlearn: 5.1822849\ttotal: 37.3ms\tremaining: 32.5ms\n534:\tlearn: 5.1610997\ttotal: 37.3ms\tremaining: 32.5ms\n535:\tlearn: 5.1400011\ttotal: 37.4ms\tremaining: 32.4ms\n536:\tlearn: 5.1189888\ttotal: 37.4ms\tremaining: 32.3ms\n537:\tlearn: 5.0980624\ttotal: 37.5ms\tremaining: 32.2ms\n538:\tlearn: 5.0772215\ttotal: 37.6ms\tremaining: 32.1ms\n539:\tlearn: 5.0564658\ttotal: 37.6ms\tremaining: 32ms\n540:\tlearn: 5.0357950\ttotal: 37.7ms\tremaining: 32ms\n541:\tlearn: 5.0152087\ttotal: 37.7ms\tremaining: 31.9ms\n542:\tlearn: 4.9947065\ttotal: 37.8ms\tremaining: 31.8ms\n543:\tlearn: 4.9742881\ttotal: 37.8ms\tremaining: 31.7ms\n544:\tlearn: 4.9539532\ttotal: 37.9ms\tremaining: 31.6ms\n545:\tlearn: 4.9337015\ttotal: 37.9ms\tremaining: 31.5ms\n546:\tlearn: 4.9135325\ttotal: 38ms\tremaining: 31.5ms\n547:\tlearn: 4.8934460\ttotal: 38ms\tremaining: 31.4ms\n548:\tlearn: 4.8734416\ttotal: 38.1ms\tremaining: 31.3ms\n549:\tlearn: 4.8535190\ttotal: 38.2ms\tremaining: 31.2ms\n550:\tlearn: 4.8336778\ttotal: 38.2ms\tremaining: 31.2ms\n551:\tlearn: 4.8139177\ttotal: 38.3ms\tremaining: 31.1ms\n552:\tlearn: 4.7942384\ttotal: 38.3ms\tremaining: 31ms\n553:\tlearn: 4.7746395\ttotal: 38.4ms\tremaining: 30.9ms\n554:\tlearn: 4.7551208\ttotal: 38.5ms\tremaining: 30.8ms\n555:\tlearn: 4.7356819\ttotal: 38.5ms\tremaining: 30.8ms\n556:\tlearn: 4.7163224\ttotal: 38.6ms\tremaining: 30.7ms\n557:\tlearn: 4.6970421\ttotal: 38.6ms\tremaining: 30.6ms\n558:\tlearn: 4.6778406\ttotal: 38.7ms\tremaining: 30.5ms\n559:\tlearn: 4.6587176\ttotal: 38.7ms\tremaining: 30.4ms\n560:\tlearn: 4.6396727\ttotal: 38.8ms\tremaining: 30.3ms\n561:\tlearn: 4.6207058\ttotal: 38.8ms\tremaining: 30.3ms\n562:\tlearn: 4.6018163\ttotal: 38.9ms\tremaining: 30.2ms\n563:\tlearn: 4.5830041\ttotal: 38.9ms\tremaining: 30.1ms\n564:\tlearn: 4.5642688\ttotal: 39ms\tremaining: 30ms\n565:\tlearn: 4.5456100\ttotal: 39ms\tremaining: 29.9ms\n566:\tlearn: 4.5270276\ttotal: 39.1ms\tremaining: 29.9ms\n567:\tlearn: 4.5085211\ttotal: 39.1ms\tremaining: 29.8ms\n568:\tlearn: 4.4900903\ttotal: 39.4ms\tremaining: 29.8ms\n569:\tlearn: 4.4717348\ttotal: 39.5ms\tremaining: 29.8ms\n570:\tlearn: 4.4534543\ttotal: 39.5ms\tremaining: 29.7ms\n571:\tlearn: 4.4352486\ttotal: 39.6ms\tremaining: 29.6ms\n572:\tlearn: 4.4171173\ttotal: 39.7ms\tremaining: 29.6ms\n573:\tlearn: 4.3990601\ttotal: 39.7ms\tremaining: 29.5ms\n574:\tlearn: 4.3810768\ttotal: 39.8ms\tremaining: 29.4ms\n575:\tlearn: 4.3631669\ttotal: 39.8ms\tremaining: 29.3ms\n576:\tlearn: 4.3453303\ttotal: 39.9ms\tremaining: 29.2ms\n577:\tlearn: 4.3275666\ttotal: 39.9ms\tremaining: 29.1ms\n578:\tlearn: 4.3098755\ttotal: 40ms\tremaining: 29.1ms\n579:\tlearn: 4.2922567\ttotal: 40ms\tremaining: 29ms\n580:\tlearn: 4.2747100\ttotal: 40.1ms\tremaining: 28.9ms\n581:\tlearn: 4.2572350\ttotal: 40.1ms\tremaining: 28.8ms\n582:\tlearn: 4.2398314\ttotal: 40.1ms\tremaining: 28.7ms\n583:\tlearn: 4.2224990\ttotal: 40.2ms\tremaining: 28.6ms\n584:\tlearn: 4.2052374\ttotal: 40.2ms\tremaining: 28.6ms\n585:\tlearn: 4.1880464\ttotal: 40.3ms\tremaining: 28.5ms\n586:\tlearn: 4.1709256\ttotal: 40.4ms\tremaining: 28.4ms\n587:\tlearn: 4.1538749\ttotal: 40.4ms\tremaining: 28.3ms\n588:\tlearn: 4.1368939\ttotal: 40.5ms\tremaining: 28.2ms\n589:\tlearn: 4.1199822\ttotal: 40.5ms\tremaining: 28.1ms\n590:\tlearn: 4.1031398\ttotal: 40.6ms\tremaining: 28.1ms\n591:\tlearn: 4.0863661\ttotal: 40.7ms\tremaining: 28ms\n592:\tlearn: 4.0696611\ttotal: 40.7ms\tremaining: 27.9ms\n593:\tlearn: 4.0530243\ttotal: 40.7ms\tremaining: 27.8ms\n594:\tlearn: 4.0364555\ttotal: 40.8ms\tremaining: 27.8ms\n595:\tlearn: 4.0199545\ttotal: 40.8ms\tremaining: 27.7ms\n596:\tlearn: 4.0035209\ttotal: 40.9ms\tremaining: 27.6ms\n597:\tlearn: 3.9871545\ttotal: 40.9ms\tremaining: 27.5ms\n598:\tlearn: 3.9708550\ttotal: 41ms\tremaining: 27.4ms\n599:\tlearn: 3.9546222\ttotal: 41ms\tremaining: 27.4ms\n600:\tlearn: 3.9384557\ttotal: 41.1ms\tremaining: 27.3ms\n601:\tlearn: 3.9223553\ttotal: 41.2ms\tremaining: 27.2ms\n602:\tlearn: 3.9063207\ttotal: 41.2ms\tremaining: 27.1ms\n603:\tlearn: 3.8903516\ttotal: 41.3ms\tremaining: 27ms\n604:\tlearn: 3.8744479\ttotal: 41.5ms\tremaining: 27.1ms\n605:\tlearn: 3.8586091\ttotal: 41.7ms\tremaining: 27.1ms\n606:\tlearn: 3.8428352\ttotal: 41.7ms\tremaining: 27ms\n607:\tlearn: 3.8271256\ttotal: 41.8ms\tremaining: 26.9ms\n608:\tlearn: 3.8114804\ttotal: 42.4ms\tremaining: 27.2ms\n609:\tlearn: 3.7958990\ttotal: 42.5ms\tremaining: 27.2ms\n610:\tlearn: 3.7803814\ttotal: 42.5ms\tremaining: 27.1ms\n611:\tlearn: 3.7649272\ttotal: 42.6ms\tremaining: 27ms\n612:\tlearn: 3.7495362\ttotal: 42.7ms\tremaining: 26.9ms\n613:\tlearn: 3.7342081\ttotal: 42.7ms\tremaining: 26.9ms\n614:\tlearn: 3.7189426\ttotal: 42.8ms\tremaining: 26.8ms\n615:\tlearn: 3.7037396\ttotal: 42.8ms\tremaining: 26.7ms\n616:\tlearn: 3.6885987\ttotal: 42.9ms\tremaining: 26.6ms\n617:\tlearn: 3.6735197\ttotal: 43ms\tremaining: 26.6ms\n618:\tlearn: 3.6585024\ttotal: 43.1ms\tremaining: 26.5ms\n619:\tlearn: 3.6435464\ttotal: 43.1ms\tremaining: 26.4ms\n620:\tlearn: 3.6286516\ttotal: 43.2ms\tremaining: 26.3ms\n621:\tlearn: 3.6138177\ttotal: 43.2ms\tremaining: 26.3ms\n622:\tlearn: 3.5990444\ttotal: 43.2ms\tremaining: 26.2ms\n623:\tlearn: 3.5843315\ttotal: 43.3ms\tremaining: 26.1ms\n624:\tlearn: 3.5696787\ttotal: 43.3ms\tremaining: 26ms\n625:\tlearn: 3.5550859\ttotal: 43.4ms\tremaining: 25.9ms\n626:\tlearn: 3.5405527\ttotal: 43.5ms\tremaining: 25.9ms\n627:\tlearn: 3.5260789\ttotal: 43.7ms\tremaining: 25.9ms\n628:\tlearn: 3.5116643\ttotal: 43.9ms\tremaining: 25.9ms\n629:\tlearn: 3.4973086\ttotal: 44ms\tremaining: 25.8ms\n630:\tlearn: 3.4830116\ttotal: 44ms\tremaining: 25.7ms\n631:\tlearn: 3.4687731\ttotal: 44.1ms\tremaining: 25.7ms\n632:\tlearn: 3.4545927\ttotal: 44.1ms\tremaining: 25.6ms\n633:\tlearn: 3.4404704\ttotal: 44.2ms\tremaining: 25.5ms\n634:\tlearn: 3.4264057\ttotal: 44.3ms\tremaining: 25.4ms\n635:\tlearn: 3.4123986\ttotal: 44.3ms\tremaining: 25.4ms\n636:\tlearn: 3.3984487\ttotal: 44.4ms\tremaining: 25.3ms\n637:\tlearn: 3.3845558\ttotal: 44.5ms\tremaining: 25.2ms\n638:\tlearn: 3.3707198\ttotal: 44.5ms\tremaining: 25.2ms\n639:\tlearn: 3.3569403\ttotal: 44.6ms\tremaining: 25.1ms\n640:\tlearn: 3.3432171\ttotal: 44.6ms\tremaining: 25ms\n641:\tlearn: 3.3295500\ttotal: 44.7ms\tremaining: 24.9ms\n642:\tlearn: 3.3159388\ttotal: 44.8ms\tremaining: 24.9ms\n643:\tlearn: 3.3023833\ttotal: 44.8ms\tremaining: 24.8ms\n644:\tlearn: 3.2888831\ttotal: 44.9ms\tremaining: 24.7ms\n645:\tlearn: 3.2754382\ttotal: 44.9ms\tremaining: 24.6ms\n646:\tlearn: 3.2620482\ttotal: 45ms\tremaining: 24.5ms\n647:\tlearn: 3.2487129\ttotal: 45ms\tremaining: 24.4ms\n648:\tlearn: 3.2354322\ttotal: 45ms\tremaining: 24.4ms\n649:\tlearn: 3.2222057\ttotal: 45.1ms\tremaining: 24.3ms\n650:\tlearn: 3.2090334\ttotal: 45.2ms\tremaining: 24.2ms\n651:\tlearn: 3.1959148\ttotal: 45.2ms\tremaining: 24.1ms\n652:\tlearn: 3.1828499\ttotal: 45.3ms\tremaining: 24.1ms\n653:\tlearn: 3.1698384\ttotal: 45.3ms\tremaining: 24ms\n654:\tlearn: 3.1568801\ttotal: 45.4ms\tremaining: 23.9ms\n655:\tlearn: 3.1439748\ttotal: 45.4ms\tremaining: 23.8ms\n656:\tlearn: 3.1311222\ttotal: 45.5ms\tremaining: 23.7ms\n657:\tlearn: 3.1183222\ttotal: 45.5ms\tremaining: 23.7ms\n658:\tlearn: 3.1055745\ttotal: 45.7ms\tremaining: 23.7ms\n659:\tlearn: 3.0928789\ttotal: 45.9ms\tremaining: 23.6ms\n660:\tlearn: 3.0802352\ttotal: 45.9ms\tremaining: 23.6ms\n661:\tlearn: 3.0676432\ttotal: 46ms\tremaining: 23.5ms\n662:\tlearn: 3.0551027\ttotal: 46ms\tremaining: 23.4ms\n663:\tlearn: 3.0426134\ttotal: 46.1ms\tremaining: 23.3ms\n664:\tlearn: 3.0301752\ttotal: 46.2ms\tremaining: 23.3ms\n665:\tlearn: 3.0177879\ttotal: 46.2ms\tremaining: 23.2ms\n666:\tlearn: 3.0054512\ttotal: 46.3ms\tremaining: 23.1ms\n667:\tlearn: 2.9931649\ttotal: 46.3ms\tremaining: 23ms\n668:\tlearn: 2.9809288\ttotal: 46.4ms\tremaining: 22.9ms\n669:\tlearn: 2.9687428\ttotal: 46.4ms\tremaining: 22.9ms\n670:\tlearn: 2.9566066\ttotal: 46.5ms\tremaining: 22.8ms\n671:\tlearn: 2.9445200\ttotal: 46.5ms\tremaining: 22.7ms\n672:\tlearn: 2.9324828\ttotal: 46.6ms\tremaining: 22.6ms\n673:\tlearn: 2.9204948\ttotal: 46.6ms\tremaining: 22.6ms\n674:\tlearn: 2.9085558\ttotal: 46.7ms\tremaining: 22.5ms\n675:\tlearn: 2.8966656\ttotal: 46.8ms\tremaining: 22.4ms\n676:\tlearn: 2.8848241\ttotal: 46.8ms\tremaining: 22.3ms\n677:\tlearn: 2.8730309\ttotal: 46.9ms\tremaining: 22.3ms\n678:\tlearn: 2.8612859\ttotal: 47ms\tremaining: 22.2ms\n679:\tlearn: 2.8495890\ttotal: 47ms\tremaining: 22.1ms\n680:\tlearn: 2.8379399\ttotal: 47.1ms\tremaining: 22ms\n681:\tlearn: 2.8263384\ttotal: 47.1ms\tremaining: 22ms\n682:\tlearn: 2.8147843\ttotal: 47.2ms\tremaining: 21.9ms\n683:\tlearn: 2.8032775\ttotal: 47.2ms\tremaining: 21.8ms\n684:\tlearn: 2.7918177\ttotal: 47.3ms\tremaining: 21.8ms\n685:\tlearn: 2.7804047\ttotal: 47.4ms\tremaining: 21.7ms\n686:\tlearn: 2.7690384\ttotal: 47.4ms\tremaining: 21.6ms\n687:\tlearn: 2.7577186\ttotal: 47.5ms\tremaining: 21.5ms\n688:\tlearn: 2.7464451\ttotal: 47.6ms\tremaining: 21.5ms\n689:\tlearn: 2.7352176\ttotal: 47.6ms\tremaining: 21.4ms\n690:\tlearn: 2.7240360\ttotal: 47.8ms\tremaining: 21.4ms\n691:\tlearn: 2.7129002\ttotal: 47.9ms\tremaining: 21.3ms\n692:\tlearn: 2.7018098\ttotal: 47.9ms\tremaining: 21.2ms\n693:\tlearn: 2.6907648\ttotal: 48ms\tremaining: 21.2ms\n694:\tlearn: 2.6797650\ttotal: 48ms\tremaining: 21.1ms\n695:\tlearn: 2.6688101\ttotal: 48.1ms\tremaining: 21ms\n696:\tlearn: 2.6579000\ttotal: 48.1ms\tremaining: 20.9ms\n697:\tlearn: 2.6470345\ttotal: 48.2ms\tremaining: 20.9ms\n698:\tlearn: 2.6362134\ttotal: 48.2ms\tremaining: 20.8ms\n699:\tlearn: 2.6254366\ttotal: 48.3ms\tremaining: 20.7ms\n700:\tlearn: 2.6147038\ttotal: 48.4ms\tremaining: 20.6ms\n701:\tlearn: 2.6040149\ttotal: 48.4ms\tremaining: 20.5ms\n702:\tlearn: 2.5933697\ttotal: 48.4ms\tremaining: 20.5ms\n703:\tlearn: 2.5827680\ttotal: 48.5ms\tremaining: 20.4ms\n704:\tlearn: 2.5722096\ttotal: 48.5ms\tremaining: 20.3ms\n705:\tlearn: 2.5616944\ttotal: 48.6ms\tremaining: 20.2ms\n706:\tlearn: 2.5512222\ttotal: 48.6ms\tremaining: 20.1ms\n707:\tlearn: 2.5407928\ttotal: 48.7ms\tremaining: 20.1ms\n708:\tlearn: 2.5304061\ttotal: 48.7ms\tremaining: 20ms\n709:\tlearn: 2.5200618\ttotal: 48.8ms\tremaining: 19.9ms\n710:\tlearn: 2.5097598\ttotal: 48.8ms\tremaining: 19.8ms\n711:\tlearn: 2.4994999\ttotal: 48.9ms\tremaining: 19.8ms\n712:\tlearn: 2.4892819\ttotal: 49ms\tremaining: 19.7ms\n713:\tlearn: 2.4791057\ttotal: 49ms\tremaining: 19.6ms\n714:\tlearn: 2.4689711\ttotal: 49.1ms\tremaining: 19.6ms\n715:\tlearn: 2.4588780\ttotal: 49.1ms\tremaining: 19.5ms\n716:\tlearn: 2.4488261\ttotal: 49.2ms\tremaining: 19.4ms\n717:\tlearn: 2.4388153\ttotal: 49.2ms\tremaining: 19.3ms\n718:\tlearn: 2.4288454\ttotal: 49.3ms\tremaining: 19.3ms\n719:\tlearn: 2.4189163\ttotal: 49.3ms\tremaining: 19.2ms\n720:\tlearn: 2.4090278\ttotal: 49.4ms\tremaining: 19.1ms\n721:\tlearn: 2.3991797\ttotal: 49.4ms\tremaining: 19ms\n722:\tlearn: 2.3893718\ttotal: 49.5ms\tremaining: 19ms\n723:\tlearn: 2.3796041\ttotal: 49.5ms\tremaining: 18.9ms\n724:\tlearn: 2.3698762\ttotal: 49.6ms\tremaining: 18.8ms\n725:\tlearn: 2.3601882\ttotal: 49.6ms\tremaining: 18.7ms\n726:\tlearn: 2.3505397\ttotal: 50ms\tremaining: 18.8ms\n727:\tlearn: 2.3409307\ttotal: 50.1ms\tremaining: 18.7ms\n728:\tlearn: 2.3313610\ttotal: 50.1ms\tremaining: 18.6ms\n729:\tlearn: 2.3218304\ttotal: 50.2ms\tremaining: 18.6ms\n730:\tlearn: 2.3123388\ttotal: 50.3ms\tremaining: 18.5ms\n731:\tlearn: 2.3028859\ttotal: 50.3ms\tremaining: 18.4ms\n732:\tlearn: 2.2934717\ttotal: 50.4ms\tremaining: 18.3ms\n733:\tlearn: 2.2840960\ttotal: 50.4ms\tremaining: 18.3ms\n734:\tlearn: 2.2747586\ttotal: 50.5ms\tremaining: 18.2ms\n735:\tlearn: 2.2654594\ttotal: 50.5ms\tremaining: 18.1ms\n736:\tlearn: 2.2561982\ttotal: 50.6ms\tremaining: 18ms\n737:\tlearn: 2.2469749\ttotal: 50.6ms\tremaining: 18ms\n738:\tlearn: 2.2377892\ttotal: 50.7ms\tremaining: 17.9ms\n739:\tlearn: 2.2286412\ttotal: 50.7ms\tremaining: 17.8ms\n740:\tlearn: 2.2195305\ttotal: 50.8ms\tremaining: 17.8ms\n741:\tlearn: 2.2104570\ttotal: 50.8ms\tremaining: 17.7ms\n742:\tlearn: 2.2014207\ttotal: 50.9ms\tremaining: 17.6ms\n743:\tlearn: 2.1924213\ttotal: 51ms\tremaining: 17.5ms\n744:\tlearn: 2.1834587\ttotal: 51ms\tremaining: 17.5ms\n745:\tlearn: 2.1745327\ttotal: 51.1ms\tremaining: 17.4ms\n746:\tlearn: 2.1656432\ttotal: 51.1ms\tremaining: 17.3ms\n747:\tlearn: 2.1567900\ttotal: 51.2ms\tremaining: 17.2ms\n748:\tlearn: 2.1479731\ttotal: 51.2ms\tremaining: 17.2ms\n749:\tlearn: 2.1391922\ttotal: 51.3ms\tremaining: 17.1ms\n750:\tlearn: 2.1304472\ttotal: 51.3ms\tremaining: 17ms\n751:\tlearn: 2.1217379\ttotal: 51.4ms\tremaining: 16.9ms\n752:\tlearn: 2.1130642\ttotal: 51.4ms\tremaining: 16.9ms\n753:\tlearn: 2.1044260\ttotal: 51.5ms\tremaining: 16.8ms\n754:\tlearn: 2.0958231\ttotal: 51.5ms\tremaining: 16.7ms\n755:\tlearn: 2.0872554\ttotal: 51.6ms\tremaining: 16.6ms\n756:\tlearn: 2.0787227\ttotal: 51.6ms\tremaining: 16.6ms\n757:\tlearn: 2.0702249\ttotal: 51.8ms\tremaining: 16.5ms\n758:\tlearn: 2.0617618\ttotal: 51.9ms\tremaining: 16.5ms\n759:\tlearn: 2.0533333\ttotal: 52ms\tremaining: 16.4ms\n760:\tlearn: 2.0449393\ttotal: 52ms\tremaining: 16.3ms\n761:\tlearn: 2.0365796\ttotal: 52.1ms\tremaining: 16.3ms\n762:\tlearn: 2.0282540\ttotal: 52.1ms\tremaining: 16.2ms\n763:\tlearn: 2.0199625\ttotal: 52.2ms\tremaining: 16.1ms\n764:\tlearn: 2.0117049\ttotal: 52.2ms\tremaining: 16ms\n765:\tlearn: 2.0034811\ttotal: 52.2ms\tremaining: 16ms\n766:\tlearn: 1.9952909\ttotal: 52.3ms\tremaining: 15.9ms\n767:\tlearn: 1.9871341\ttotal: 52.4ms\tremaining: 15.8ms\n768:\tlearn: 1.9790107\ttotal: 52.4ms\tremaining: 15.7ms\n769:\tlearn: 1.9709205\ttotal: 52.4ms\tremaining: 15.7ms\n770:\tlearn: 1.9628634\ttotal: 52.5ms\tremaining: 15.6ms\n771:\tlearn: 1.9548392\ttotal: 52.5ms\tremaining: 15.5ms\n772:\tlearn: 1.9468478\ttotal: 52.6ms\tremaining: 15.4ms\n773:\tlearn: 1.9388891\ttotal: 52.6ms\tremaining: 15.4ms\n774:\tlearn: 1.9309629\ttotal: 52.7ms\tremaining: 15.3ms\n775:\tlearn: 1.9230691\ttotal: 52.8ms\tremaining: 15.2ms\n776:\tlearn: 1.9152076\ttotal: 52.8ms\tremaining: 15.2ms\n777:\tlearn: 1.9073783\ttotal: 52.8ms\tremaining: 15.1ms\n778:\tlearn: 1.8995809\ttotal: 52.9ms\tremaining: 15ms\n779:\tlearn: 1.8918154\ttotal: 53ms\tremaining: 14.9ms\n780:\tlearn: 1.8840817\ttotal: 53ms\tremaining: 14.9ms\n781:\tlearn: 1.8763796\ttotal: 53ms\tremaining: 14.8ms\n782:\tlearn: 1.8687089\ttotal: 53.1ms\tremaining: 14.7ms\n783:\tlearn: 1.8610696\ttotal: 53.2ms\tremaining: 14.6ms\n784:\tlearn: 1.8534616\ttotal: 53.2ms\tremaining: 14.6ms\n785:\tlearn: 1.8458846\ttotal: 53.3ms\tremaining: 14.5ms\n786:\tlearn: 1.8383387\ttotal: 53.4ms\tremaining: 14.4ms\n787:\tlearn: 1.8308235\ttotal: 53.4ms\tremaining: 14.4ms\n788:\tlearn: 1.8233391\ttotal: 53.5ms\tremaining: 14.3ms\n789:\tlearn: 1.8158853\ttotal: 53.5ms\tremaining: 14.2ms\n790:\tlearn: 1.8084620\ttotal: 53.6ms\tremaining: 14.2ms\n791:\tlearn: 1.8010690\ttotal: 53.6ms\tremaining: 14.1ms\n792:\tlearn: 1.7937062\ttotal: 53.8ms\tremaining: 14ms\n793:\tlearn: 1.7863735\ttotal: 53.8ms\tremaining: 14ms\n794:\tlearn: 1.7790708\ttotal: 53.9ms\tremaining: 13.9ms\n795:\tlearn: 1.7717980\ttotal: 53.9ms\tremaining: 13.8ms\n796:\tlearn: 1.7645549\ttotal: 54ms\tremaining: 13.8ms\n797:\tlearn: 1.7573414\ttotal: 54ms\tremaining: 13.7ms\n798:\tlearn: 1.7501574\ttotal: 54.1ms\tremaining: 13.6ms\n799:\tlearn: 1.7430027\ttotal: 54.1ms\tremaining: 13.5ms\n800:\tlearn: 1.7358773\ttotal: 54.2ms\tremaining: 13.5ms\n801:\tlearn: 1.7287811\ttotal: 54.2ms\tremaining: 13.4ms\n802:\tlearn: 1.7217138\ttotal: 54.3ms\tremaining: 13.3ms\n803:\tlearn: 1.7146755\ttotal: 54.4ms\tremaining: 13.3ms\n804:\tlearn: 1.7076659\ttotal: 54.4ms\tremaining: 13.2ms\n805:\tlearn: 1.7006849\ttotal: 54.5ms\tremaining: 13.1ms\n806:\tlearn: 1.6937325\ttotal: 54.6ms\tremaining: 13ms\n807:\tlearn: 1.6868085\ttotal: 54.6ms\tremaining: 13ms\n808:\tlearn: 1.6799129\ttotal: 54.7ms\tremaining: 12.9ms\n809:\tlearn: 1.6730454\ttotal: 54.7ms\tremaining: 12.8ms\n810:\tlearn: 1.6662060\ttotal: 54.8ms\tremaining: 12.8ms\n811:\tlearn: 1.6593945\ttotal: 54.9ms\tremaining: 12.7ms\n812:\tlearn: 1.6526109\ttotal: 54.9ms\tremaining: 12.6ms\n813:\tlearn: 1.6458551\ttotal: 55ms\tremaining: 12.6ms\n814:\tlearn: 1.6391268\ttotal: 55ms\tremaining: 12.5ms\n815:\tlearn: 1.6324260\ttotal: 55ms\tremaining: 12.4ms\n816:\tlearn: 1.6257527\ttotal: 55.1ms\tremaining: 12.4ms\n817:\tlearn: 1.6191066\ttotal: 55.2ms\tremaining: 12.3ms\n818:\tlearn: 1.6124877\ttotal: 55.2ms\tremaining: 12.2ms\n819:\tlearn: 1.6058959\ttotal: 55.3ms\tremaining: 12.1ms\n820:\tlearn: 1.5993310\ttotal: 55.3ms\tremaining: 12.1ms\n821:\tlearn: 1.5927929\ttotal: 55.4ms\tremaining: 12ms\n822:\tlearn: 1.5862815\ttotal: 55.4ms\tremaining: 11.9ms\n823:\tlearn: 1.5797968\ttotal: 55.5ms\tremaining: 11.8ms\n824:\tlearn: 1.5733386\ttotal: 55.5ms\tremaining: 11.8ms\n825:\tlearn: 1.5669068\ttotal: 55.6ms\tremaining: 11.7ms\n826:\tlearn: 1.5605013\ttotal: 55.6ms\tremaining: 11.6ms\n827:\tlearn: 1.5541220\ttotal: 55.7ms\tremaining: 11.6ms\n828:\tlearn: 1.5477687\ttotal: 55.9ms\tremaining: 11.5ms\n829:\tlearn: 1.5414414\ttotal: 56ms\tremaining: 11.5ms\n830:\tlearn: 1.5351400\ttotal: 56ms\tremaining: 11.4ms\n831:\tlearn: 1.5288644\ttotal: 56.1ms\tremaining: 11.3ms\n832:\tlearn: 1.5226144\ttotal: 56.1ms\tremaining: 11.2ms\n833:\tlearn: 1.5163899\ttotal: 56.2ms\tremaining: 11.2ms\n834:\tlearn: 1.5101909\ttotal: 56.2ms\tremaining: 11.1ms\n835:\tlearn: 1.5040173\ttotal: 56.3ms\tremaining: 11ms\n836:\tlearn: 1.4978688\ttotal: 56.3ms\tremaining: 11ms\n837:\tlearn: 1.4917456\ttotal: 56.4ms\tremaining: 10.9ms\n838:\tlearn: 1.4856473\ttotal: 56.4ms\tremaining: 10.8ms\n839:\tlearn: 1.4795740\ttotal: 56.5ms\tremaining: 10.8ms\n840:\tlearn: 1.4735255\ttotal: 56.5ms\tremaining: 10.7ms\n841:\tlearn: 1.4675017\ttotal: 56.6ms\tremaining: 10.6ms\n842:\tlearn: 1.4615026\ttotal: 56.7ms\tremaining: 10.6ms\n843:\tlearn: 1.4555279\ttotal: 56.7ms\tremaining: 10.5ms\n844:\tlearn: 1.4495777\ttotal: 56.7ms\tremaining: 10.4ms\n845:\tlearn: 1.4436519\ttotal: 56.8ms\tremaining: 10.3ms\n846:\tlearn: 1.4377502\ttotal: 56.8ms\tremaining: 10.3ms\n847:\tlearn: 1.4318727\ttotal: 56.9ms\tremaining: 10.2ms\n848:\tlearn: 1.4260192\ttotal: 57ms\tremaining: 10.1ms\n849:\tlearn: 1.4201896\ttotal: 57ms\tremaining: 10.1ms\n850:\tlearn: 1.4143839\ttotal: 57.1ms\tremaining: 9.99ms\n851:\tlearn: 1.4086019\ttotal: 57.1ms\tremaining: 9.92ms\n852:\tlearn: 1.4028435\ttotal: 57.2ms\tremaining: 9.85ms\n853:\tlearn: 1.3971087\ttotal: 57.2ms\tremaining: 9.78ms\n854:\tlearn: 1.3913973\ttotal: 57.3ms\tremaining: 9.71ms\n855:\tlearn: 1.3857093\ttotal: 57.3ms\tremaining: 9.64ms\n856:\tlearn: 1.3800445\ttotal: 57.4ms\tremaining: 9.57ms\n857:\tlearn: 1.3744029\ttotal: 57.4ms\tremaining: 9.5ms\n858:\tlearn: 1.3687843\ttotal: 57.5ms\tremaining: 9.43ms\n859:\tlearn: 1.3631887\ttotal: 57.5ms\tremaining: 9.37ms\n860:\tlearn: 1.3576160\ttotal: 57.6ms\tremaining: 9.3ms\n861:\tlearn: 1.3520661\ttotal: 57.8ms\tremaining: 9.25ms\n862:\tlearn: 1.3465388\ttotal: 57.9ms\tremaining: 9.19ms\n863:\tlearn: 1.3410342\ttotal: 58ms\tremaining: 9.12ms\n864:\tlearn: 1.3355520\ttotal: 58ms\tremaining: 9.05ms\n865:\tlearn: 1.3300923\ttotal: 58ms\tremaining: 8.98ms\n866:\tlearn: 1.3246549\ttotal: 58.1ms\tremaining: 8.91ms\n867:\tlearn: 1.3192397\ttotal: 58.2ms\tremaining: 8.84ms\n868:\tlearn: 1.3138467\ttotal: 58.2ms\tremaining: 8.78ms\n869:\tlearn: 1.3084756\ttotal: 58.3ms\tremaining: 8.7ms\n870:\tlearn: 1.3031266\ttotal: 58.3ms\tremaining: 8.63ms\n871:\tlearn: 1.2977994\ttotal: 58.4ms\tremaining: 8.56ms\n872:\tlearn: 1.2924940\ttotal: 58.4ms\tremaining: 8.5ms\n873:\tlearn: 1.2872103\ttotal: 58.5ms\tremaining: 8.43ms\n874:\tlearn: 1.2819482\ttotal: 58.5ms\tremaining: 8.36ms\n875:\tlearn: 1.2767076\ttotal: 58.6ms\tremaining: 8.29ms\n876:\tlearn: 1.2714884\ttotal: 58.6ms\tremaining: 8.22ms\n877:\tlearn: 1.2662906\ttotal: 58.7ms\tremaining: 8.15ms\n878:\tlearn: 1.2611140\ttotal: 58.7ms\tremaining: 8.08ms\n879:\tlearn: 1.2559585\ttotal: 58.8ms\tremaining: 8.01ms\n880:\tlearn: 1.2508242\ttotal: 58.8ms\tremaining: 7.94ms\n881:\tlearn: 1.2457108\ttotal: 58.9ms\tremaining: 7.88ms\n882:\tlearn: 1.2406183\ttotal: 58.9ms\tremaining: 7.81ms\n883:\tlearn: 1.2355467\ttotal: 59ms\tremaining: 7.74ms\n884:\tlearn: 1.2304958\ttotal: 59.1ms\tremaining: 7.67ms\n885:\tlearn: 1.2254655\ttotal: 59.1ms\tremaining: 7.61ms\n886:\tlearn: 1.2204558\ttotal: 59.2ms\tremaining: 7.54ms\n887:\tlearn: 1.2154666\ttotal: 59.2ms\tremaining: 7.47ms\n888:\tlearn: 1.2104978\ttotal: 59.3ms\tremaining: 7.4ms\n889:\tlearn: 1.2055492\ttotal: 59.3ms\tremaining: 7.33ms\n890:\tlearn: 1.2006210\ttotal: 59.4ms\tremaining: 7.26ms\n891:\tlearn: 1.1957128\ttotal: 59.4ms\tremaining: 7.2ms\n892:\tlearn: 1.1908247\ttotal: 59.5ms\tremaining: 7.13ms\n893:\tlearn: 1.1859566\ttotal: 59.5ms\tremaining: 7.06ms\n894:\tlearn: 1.1811085\ttotal: 59.6ms\tremaining: 6.99ms\n895:\tlearn: 1.1762801\ttotal: 59.6ms\tremaining: 6.92ms\n896:\tlearn: 1.1714715\ttotal: 59.7ms\tremaining: 6.85ms\n897:\tlearn: 1.1666825\ttotal: 59.7ms\tremaining: 6.78ms\n898:\tlearn: 1.1619131\ttotal: 59.8ms\tremaining: 6.71ms\n899:\tlearn: 1.1571632\ttotal: 60ms\tremaining: 6.67ms\n900:\tlearn: 1.1524327\ttotal: 60.8ms\tremaining: 6.68ms\n901:\tlearn: 1.1477216\ttotal: 60.9ms\tremaining: 6.61ms\n902:\tlearn: 1.1430297\ttotal: 60.9ms\tremaining: 6.54ms\n903:\tlearn: 1.1383570\ttotal: 61ms\tremaining: 6.47ms\n904:\tlearn: 1.1337034\ttotal: 61ms\tremaining: 6.41ms\n905:\tlearn: 1.1290688\ttotal: 61.1ms\tremaining: 6.34ms\n906:\tlearn: 1.1244531\ttotal: 61.2ms\tremaining: 6.27ms\n907:\tlearn: 1.1198564\ttotal: 61.2ms\tremaining: 6.2ms\n908:\tlearn: 1.1152784\ttotal: 61.3ms\tremaining: 6.13ms\n909:\tlearn: 1.1107191\ttotal: 61.3ms\tremaining: 6.06ms\n910:\tlearn: 1.1061785\ttotal: 61.4ms\tremaining: 6ms\n911:\tlearn: 1.1016565\ttotal: 61.4ms\tremaining: 5.93ms\n912:\tlearn: 1.0971529\ttotal: 61.5ms\tremaining: 5.86ms\n913:\tlearn: 1.0926677\ttotal: 61.5ms\tremaining: 5.79ms\n914:\tlearn: 1.0882009\ttotal: 61.6ms\tremaining: 5.72ms\n915:\tlearn: 1.0837523\ttotal: 61.6ms\tremaining: 5.65ms\n916:\tlearn: 1.0793220\ttotal: 61.7ms\tremaining: 5.58ms\n917:\tlearn: 1.0749097\ttotal: 61.7ms\tremaining: 5.51ms\n918:\tlearn: 1.0705155\ttotal: 61.8ms\tremaining: 5.45ms\n919:\tlearn: 1.0661392\ttotal: 61.9ms\tremaining: 5.38ms\n920:\tlearn: 1.0617808\ttotal: 61.9ms\tremaining: 5.31ms\n921:\tlearn: 1.0574403\ttotal: 62ms\tremaining: 5.24ms\n922:\tlearn: 1.0531174\ttotal: 62.1ms\tremaining: 5.18ms\n923:\tlearn: 1.0488123\ttotal: 62.1ms\tremaining: 5.11ms\n924:\tlearn: 1.0445248\ttotal: 62.2ms\tremaining: 5.04ms\n925:\tlearn: 1.0402547\ttotal: 62.3ms\tremaining: 4.97ms\n926:\tlearn: 1.0360022\ttotal: 62.3ms\tremaining: 4.91ms\n927:\tlearn: 1.0317670\ttotal: 62.4ms\tremaining: 4.84ms\n928:\tlearn: 1.0275491\ttotal: 62.4ms\tremaining: 4.77ms\n929:\tlearn: 1.0233485\ttotal: 62.5ms\tremaining: 4.7ms\n930:\tlearn: 1.0191651\ttotal: 62.6ms\tremaining: 4.64ms\n931:\tlearn: 1.0149987\ttotal: 62.6ms\tremaining: 4.57ms\n932:\tlearn: 1.0108494\ttotal: 62.7ms\tremaining: 4.5ms\n933:\tlearn: 1.0067171\ttotal: 62.7ms\tremaining: 4.43ms\n934:\tlearn: 1.0026016\ttotal: 62.8ms\tremaining: 4.36ms\n935:\tlearn: 0.9985030\ttotal: 62.8ms\tremaining: 4.29ms\n936:\tlearn: 0.9944211\ttotal: 62.9ms\tremaining: 4.23ms\n937:\tlearn: 0.9903559\ttotal: 62.9ms\tremaining: 4.16ms\n938:\tlearn: 0.9863073\ttotal: 63ms\tremaining: 4.09ms\n939:\tlearn: 0.9822753\ttotal: 63ms\tremaining: 4.02ms\n940:\tlearn: 0.9782598\ttotal: 63.1ms\tremaining: 3.96ms\n941:\tlearn: 0.9742606\ttotal: 63.1ms\tremaining: 3.89ms\n942:\tlearn: 0.9702778\ttotal: 63.2ms\tremaining: 3.82ms\n943:\tlearn: 0.9663114\ttotal: 63.3ms\tremaining: 3.75ms\n944:\tlearn: 0.9623611\ttotal: 63.3ms\tremaining: 3.68ms\n945:\tlearn: 0.9584269\ttotal: 63.3ms\tremaining: 3.62ms\n946:\tlearn: 0.9545089\ttotal: 63.4ms\tremaining: 3.55ms\n947:\tlearn: 0.9506069\ttotal: 63.5ms\tremaining: 3.48ms\n948:\tlearn: 0.9467208\ttotal: 63.5ms\tremaining: 3.41ms\n949:\tlearn: 0.9428506\ttotal: 63.6ms\tremaining: 3.35ms\n950:\tlearn: 0.9389962\ttotal: 63.6ms\tremaining: 3.28ms\n951:\tlearn: 0.9351576\ttotal: 64.4ms\tremaining: 3.25ms\n952:\tlearn: 0.9313347\ttotal: 64.5ms\tremaining: 3.18ms\n953:\tlearn: 0.9275274\ttotal: 64.6ms\tremaining: 3.11ms\n954:\tlearn: 0.9237356\ttotal: 64.7ms\tremaining: 3.05ms\n955:\tlearn: 0.9199594\ttotal: 64.7ms\tremaining: 2.98ms\n956:\tlearn: 0.9161986\ttotal: 64.8ms\tremaining: 2.91ms\n957:\tlearn: 0.9124532\ttotal: 64.8ms\tremaining: 2.84ms\n958:\tlearn: 0.9087231\ttotal: 64.9ms\tremaining: 2.77ms\n959:\tlearn: 0.9050082\ttotal: 64.9ms\tremaining: 2.71ms\n960:\tlearn: 0.9013086\ttotal: 65ms\tremaining: 2.64ms\n961:\tlearn: 0.8976240\ttotal: 65.1ms\tremaining: 2.57ms\n962:\tlearn: 0.8939545\ttotal: 65.1ms\tremaining: 2.5ms\n963:\tlearn: 0.8903000\ttotal: 65.2ms\tremaining: 2.43ms\n964:\tlearn: 0.8866605\ttotal: 65.3ms\tremaining: 2.37ms\n965:\tlearn: 0.8830358\ttotal: 65.3ms\tremaining: 2.3ms\n966:\tlearn: 0.8794260\ttotal: 65.4ms\tremaining: 2.23ms\n967:\tlearn: 0.8758309\ttotal: 65.4ms\tremaining: 2.16ms\n968:\tlearn: 0.8722505\ttotal: 65.5ms\tremaining: 2.1ms\n969:\tlearn: 0.8686847\ttotal: 65.6ms\tremaining: 2.03ms\n970:\tlearn: 0.8651335\ttotal: 65.6ms\tremaining: 1.96ms\n971:\tlearn: 0.8615969\ttotal: 65.7ms\tremaining: 1.89ms\n972:\tlearn: 0.8580747\ttotal: 65.7ms\tremaining: 1.82ms\n973:\tlearn: 0.8545669\ttotal: 65.8ms\tremaining: 1.75ms\n974:\tlearn: 0.8510734\ttotal: 65.8ms\tremaining: 1.69ms\n975:\tlearn: 0.8475942\ttotal: 65.9ms\tremaining: 1.62ms\n976:\tlearn: 0.8441292\ttotal: 65.9ms\tremaining: 1.55ms\n977:\tlearn: 0.8406784\ttotal: 66ms\tremaining: 1.48ms\n978:\tlearn: 0.8372417\ttotal: 66ms\tremaining: 1.42ms\n979:\tlearn: 0.8338191\ttotal: 66.1ms\tremaining: 1.35ms\n980:\tlearn: 0.8304104\ttotal: 66.1ms\tremaining: 1.28ms\n981:\tlearn: 0.8270157\ttotal: 66.2ms\tremaining: 1.21ms\n982:\tlearn: 0.8236349\ttotal: 66.9ms\tremaining: 1.16ms\n983:\tlearn: 0.8202679\ttotal: 67ms\tremaining: 1.09ms\n984:\tlearn: 0.8169146\ttotal: 67ms\tremaining: 1.02ms\n985:\tlearn: 0.8135751\ttotal: 67.1ms\tremaining: 952us\n986:\tlearn: 0.8102492\ttotal: 67.2ms\tremaining: 884us\n987:\tlearn: 0.8069369\ttotal: 67.2ms\tremaining: 816us\n988:\tlearn: 0.8036381\ttotal: 67.3ms\tremaining: 748us\n989:\tlearn: 0.8003528\ttotal: 67.3ms\tremaining: 679us\n990:\tlearn: 0.7970810\ttotal: 67.3ms\tremaining: 611us\n991:\tlearn: 0.7938225\ttotal: 67.4ms\tremaining: 543us\n992:\tlearn: 0.7905774\ttotal: 67.4ms\tremaining: 475us\n993:\tlearn: 0.7873455\ttotal: 67.5ms\tremaining: 407us\n994:\tlearn: 0.7841268\ttotal: 67.6ms\tremaining: 339us\n995:\tlearn: 0.7809213\ttotal: 67.6ms\tremaining: 271us\n996:\tlearn: 0.7777289\ttotal: 67.7ms\tremaining: 203us\n997:\tlearn: 0.7745496\ttotal: 67.7ms\tremaining: 135us\n998:\tlearn: 0.7713832\ttotal: 67.8ms\tremaining: 67us\n999:\tlearn: 0.7682298\ttotal: 67.8ms\tremaining: 0us\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n<catboost.core.CatBoostRegressor at 0x127c2e710>\n```\n:::\n:::\n\n\n## Installation Guide\n\n```bash\n# XGBoost\npip install xgboost\n\n# LightGBM\npip install lightgbm\n\n# CatBoost\npip install catboost\n\n# All at once\npip install xgboost lightgbm catboost\n```\n\n## Key Takeaways\n\n1. **Gradient boosting builds trees sequentially**, each correcting errors of previous trees\n2. **Learning rate controls** how much each tree contributes (lower = better generalization)\n3. **XGBoost, LightGBM, and CatBoost** are the three leading implementations\n4. **LightGBM** is fastest on large datasets\n5. **CatBoost** excels with categorical features\n6. **XGBoost** is the most popular and well-documented\n7. **Always use early stopping** with a validation set\n8. **Gradient boosting typically outperforms random forests** on structured data\n9. **No feature scaling needed** for tree-based methods\n10. **Trade-off**: More complex to tune than random forests, but worth it for accuracy\n\n## Practice Problems\n\n1. **Conceptual**: Explain why gradient boosting can overfit more easily than random forests, even though both use decision trees.\n\n2. **Applied**: Load the Boston housing dataset and compare:\n   - Random Forest\n   - XGBoost\n   - LightGBM\n   - CatBoost\n   \n   Use cross-validation and report mean RMSE and training time for each.\n\n3. **Hyperparameter Tuning**: \n   - Start with `n_estimators=1000, learning_rate=0.01`\n   - Then try `n_estimators=100, learning_rate=0.1`\n   - Compare results. Which works better? Why?\n\n4. **Challenge**: Create a dataset with a categorical feature that has 100 unique values. Compare:\n   - XGBoost with one-hot encoding\n   - XGBoost with label encoding\n   - CatBoost with native categorical handling\n   \n   Which performs best? Why?\n\n## Summary\n\nGradient boosting methods represent the state-of-the-art for structured/tabular data. While they require more careful tuning than random forests, the performance gains are substantial. In practice:\n\n- **Start with XGBoost** for most problems (great documentation, wide adoption)\n- **Switch to LightGBM** if you have large datasets or need speed\n- **Use CatBoost** if you have many categorical features or want minimal tuning\n\nUnderstanding these methods is essential for any data scientist working with structured data, and they're often the first choice for Kaggle competitions and production ML systems.\n\n---\n\n*Next steps*: Explore ensemble methods that combine different model types, and dive deeper into hyperparameter optimization techniques like Bayesian optimization.\n\n",
    "supporting": [
      "18B-gradient-boosting-methods_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}