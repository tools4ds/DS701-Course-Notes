{
  "hash": "00b9cdbd2c1ed0bc882efae42df7251f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Dimensionality Reduction - PCA + t-SNE\njupyter: python3\nbibliography: references.bib\nnocite: |\n  @novembre2008genes, @strang2022introduction\n---\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/11-Dimensionality-Reduction-SVD-II.ipynb)\n\nWe previously learned how to use the SVD as a tool for constructing low-rank matrices.\n\nWe now consider it as a tool for transforming (i.e., reducing the dimension of) our data. \n\n## Overview\n\nCollected data is often high-dimensional. The high-dimensionality of, or the large number of features in a dataset is challenging to work with. \n\n![](figs/elephant_perspective.png){fig-align=\"center\" width=50%}\n\n## High-Dimensional Challenges\n\nWe have seen some of these challenges already, in particular:\n\n:::: {.incremental}\n- the curse of dimensionality, where data points become sparse in higher dimensions and distance metrics have less meaning,\n- overfitting, where high-dimensional data can lead to overly complex models that fit to noise in the data as opposed to the actual signal,\n- computational complexity, high-dimensional data requires more computing power and memory,\n- visualization, where high-dimensional data makes understanding and interpreting the data difficult.\n::::\n--- \n\nHow can we reduce the dimension of our data but still preserve the most important information in our dataset?\n\n:::: {.fragment}\nWe consider two techniques:\n\n:::: {.incremental}\n- Principle Component Analysis (PCA)\n- t-distributed stochastic neighbor embedding (t-SNE)\n::::\n::::\n\n:::: {.fragment}\nWe will demonstrate the relationship between PCA and the SVD.\n\nt-SNE is an alternative nonlinear method for dimensionality reduction.\n::::\n\n# PCA \n\n## Dimensionality Reduction \n\nInput: $\\mathbf{x}_1,\\ldots, \\mathbf{x}_m$ with  $\\mathbf{x}_i \\in \\mathbb{R}^n \\: \\: \\forall \\: i \\in \\{1, \\ldots, n\\}.$ \n\nOutput: $\\mathbf{y}_1,\\dots, \\mathbf{y}_m$  with  $\\mathbf{y}_i \\in \\mathbb{R}^d \\: \\: \\forall \\: i \\in \\{1, \\dots, n\\}$. \n\nThe goal is to compute the new data points $\\mathbf{y}_i$ such that <font color=\"red\"> $d << n$ </font> while still preserving the most information contained in the data points $\\mathbf{x}_i$.\n\nBe aware that row i of the data matrix $X_0$ is the $n$ dimensional vector $\\mathbf{x}_i$. This keeps our matrix in the structure $m$ data rows and $n$ features (columns). The same is true for $Y$ (i.e., there are $m$ rows with $d$ features).\n\n$$\nX_0 = \n\\begin{bmatrix} \nx_{11} & x_{12} & \\dots & x_{1n} \\\\\nx_{21} & x_{22} & \\dots & x_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nx_{m1} & x_{m2} & \\dots & x_{mn} \n\\end{bmatrix} \\:\n\\xrightarrow[\\text{PCA}]{} \\:\nY = \\begin{bmatrix} \ny_{11} & y_{12} & \\dots & y_{1d} \\\\\ny_{21} & y_{22} & \\dots & y_{2d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\ny_{m1} & y_{m2} & \\dots & y_{md} \n\\end{bmatrix}\n$$\n\n## PCA Application: Genes Mirror Geography\n\nWe consider a dataset from @novembre2008genes. The authors collected DNA data called SNPs (single nucleotide polymorphism) from 3000 individuals in Europe. \n\nSNPs describe the changes in DNA from a common base pair (A,T,C,G). A value of 0 means no changes to the base pair, a value of 1 means 1 change to the base pair, and a value of 2 means both base pairs change.\n\nThe data for each individual consisted of approximately 500k SNPs. This means the data matrix we are working with is 3000 x 500k.\n\n---\n\nThe authors performed PCA and plotted 1387 of the individuals in the reduced dimensions. \n\n![Image Credit @novembre2008genes](figs/PCA_genes_Europe.png){width=50% fig-align=\"center\"}\n\nFor comparison, a color coded map of western Europe is added and the same color coding was applied to the data samples by country of origin.\n\n---\n\nKey observations:\n\n:::: {.incremental}\n- the first principal components of the data almost reproduce the map of Europe, i.e., they appear to correspond to latitude and longitude.\n- SNPs are similar geographically\n- DNA of an individual reveals their birthplace within a few hundred kilometers\n::::\n\n:::: {.fragment}\nWould a similar study in the USA be effective?\n::::\n\n## PCA Overview\n\nThe following describes the process to perform PCA and obtain your reduced data set. \n\nThe goal is given:\n\nInput: $X_0\\in\\mathbb{R}^{m\\times n}$, produce\n\nOutput: $Y\\in\\mathbb{R}^{m\\times d}$ with $d << n$.\n\n## PCA Step 1: Center the Data\n\nThe first step is to center the data (subtract the mean of each column): $X_0 \\rightarrow X$.\n\nExample:\n$$\nX_0 = \n\\begin{bmatrix}\n90 & 60 & 60\\\\\n80 & 60 & 70 \\end{bmatrix}\n$$\n\nThe mean, per column, across rows is:\n$$\n\\boldsymbol{\\mu} = \\begin{bmatrix} 85 & 60 & 65 \\end{bmatrix}\n$$\n\nThe mean-centered dataset is\n\n$$\nX = \\begin{bmatrix}\n5 & 0 & -5\\\\\n-5 & 0 & 5 \\end{bmatrix} \n$$ \n\n\nThe next step is to determine the directions of the data that correspond to the largest variances. These are the principal components.\n\n## Least Squares Interpretation\n\nCentered data often clusters along a line (or other low-dimensional subspace of $\\mathbb{R}^{n}$).\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n![](figs/PCA_variance.png){width=65% fig-align=\"center\"}\n\nThe sum of **variances** (squared distances to the mean) of the projected points is a **maximum**.\n:::\n::: {.column width=\"50%\"}\n![](figs/PCA_residual.png){width=65% fig-align=\"center\"}\n \nThe sum of **residuals** (squared distances from the points to the line) is a **minimum**.\n:::\n::::\n\n:::: {.fragment}\nWhat is the statistical entity that measures the variability in the data?\n::::\n\n:::: {.fragment}\nAnswer: the covariance matrix.\n::::\n\n## Covariance Matrix\n\nLet $X\\in\\mathbb{R}^{m\\times n}$ contain the centered data. The sample covariance matrix for zero-mean data is:\n\n$$\nS = \\frac{1}{m-1}X^{T}X\n$$\n\n::: {.content-visible when-profile=\"slides\"}\n_(See [notes](./11-Dimensionality-Reduction-SVD-II.qmd#understanding-the-formula) for derivation.)_\n:::\n\nExample:   \n$$\nX = \\begin{bmatrix}\n5 & 0 & -5\\\\\n-5 & 0 & 5 \\end{bmatrix} \n$$\n\n$$ \nS =  \n\\begin{bmatrix}\n5 & -5\\\\\n0 & 0\\\\\n-5 & 5 \\end{bmatrix} \n\\begin{bmatrix}\n5 & 0 & -5\\\\\n-5 & 0 & 5 \\end{bmatrix}\n=\n\\begin{bmatrix}\n50 & 0 & -50\\\\\n0 & 0 & 0\\\\\n-50 & 0 & 50 \\end{bmatrix}\n$$\n\nThe matrix $S$ is symmetric, i.e., $S = S^{T}.$ \n\n\n::: {.content-visible when-profile=\"web\"}\n## Understanding the Formula\n\nThe formula is:\n$$S = \\frac{1}{m-1}X^{T}X$$\n\nwhere $X\\in\\mathbb{R}^{m\\times n}$ contains **centered data**.\n\n#### Key Setup\n\nFirst, let's understand what $X$ represents:\n\n- **$m$ rows** = number of observations/samples\n- **$n$ columns** = number of features/variables\n- **Centered data** means each column has mean 0 (we've subtracted the column mean from each entry)\n\nSo if we denote the columns of $X$ as vectors, we have:\n$$X = \\begin{bmatrix} | & | & & | \\\\ \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_n \\\\ | & | & & | \\end{bmatrix}$$\n\nwhere each $\\mathbf{x}_j \\in \\mathbb{R}^m$ is a centered column vector (mean = 0).\n\n#### What Does $X^T X$ Compute?\n\nWhen we compute $X^T X$:\n\n- Dimensions: $(n \\times m) \\cdot (m \\times n) = n \\times n$\n- The $(i,j)$ entry of $X^T X$ is: $(\\mathbf{x}_i)^T \\mathbf{x}_j = \\sum_{k=1}^{m} x_{ki} \\cdot x_{kj}$\n\nThis is the **dot product** between column $i$ and column $j$ of $X$.\n\n#### Why This Gives Covariance\n\nThe sample covariance between features $i$ and $j$ is defined as:\n$$\\text{Cov}(i,j) = \\frac{1}{m-1}\\sum_{k=1}^{m}(x_{ki} - \\bar{x}_i)(x_{kj} - \\bar{x}_j)$$\n\nBut since the data is **centered**, we have $\\bar{x}_i = 0$ and $\\bar{x}_j = 0$, so:\n$$\\text{Cov}(i,j) = \\frac{1}{m-1}\\sum_{k=1}^{m}x_{ki} \\cdot x_{kj} = \\frac{1}{m-1}(\\mathbf{x}_i)^T \\mathbf{x}_j$$\n\nThis is exactly the $(i,j)$ entry of $\\frac{1}{m-1}X^T X$!\n\n#### The Result\n\nTherefore, $S = \\frac{1}{m-1}X^{T}X$ is an $n \\times n$ matrix where:\n\n- **Diagonal entries** $S_{ii}$ = variance of feature $i$\n- **Off-diagonal entries** $S_{ij}$ = covariance between features $i$ and $j$\n\nThis is precisely the **sample covariance matrix**.\n\n**Note:** The factor of $(m-1)$ instead of $m$ gives us the unbiased sample estimator (Bessel's correction).\n\n:::\n\n\n## Spectral Decomposition\n\nWe use the fact that the covariance matrix $S$ is symmetric to apply the Spectral Decomposition, which states:\n\n> Every real symmetric matrix $S$ has the factorization $V\\Lambda V^{T}$, where $\\Lambda$ is a diagonal matrix that contains the eigenvalues of S and the columns of $V$ are orthogonal eigenvectors of $S$.\n\nThis is a special case of eigendecomposition for symmetric matrices.\n\nYou can refresh your memory about this in the [Linear Algebra Refresher](04-Linear-Algebra-Refresher.qmd#eigendecomposition).\n\n---\n\nThe covariance matrix $S \\in \\mathbb{R}^{n\\times n}$ has the spectral decomposition $S = V\\Lambda V^T$ where\n\n$$\n\\Lambda = \n\\begin{bmatrix}\n\\lambda_1                                   \\\\\n& \\lambda_2             &   & \\text{\\Large0}\\\\\n&               & \\ddots                \\\\\n& \\text{\\Large0} &   & \\lambda_{n-1}            \\\\\n&               &   &   & \\lambda_n\n\\end{bmatrix}\n$$\nwith $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_{n-1} \\geq \\lambda_n$, and\n$$\nV = \\begin{bmatrix} \n\\bigg| & \\bigg| &  & \\bigg| & \\bigg| \\\\\n\\mathbf{v}_1   & \\mathbf{v}_2  & \\dots & \\mathbf{v}_{n-1}   & \\mathbf{v}_n  \\\\\n\\bigg| & \\bigg| & & \\bigg| & \\bigg|\n\\end{bmatrix}\n$$ \n \nwith $S\\mathbf{v}_i = \\lambda_i \\mathbf{v}_i$ and $\\mathbf{v}_i \\perp \\mathbf{v}_j$ for $i\\neq j$.\n\n---\n\nThe previous decomposition was for all $n$ dimensions. To obtain our reduced data, we take the first $d$ columns of $V$, i.e.,\n\n$$\nV' = \\begin{bmatrix} \n\\bigg| & \\bigg| &  & \\bigg|   \\\\\n\\mathbf{v}_1   & \\mathbf{v}_2  & \\dots & \\mathbf{v}_{d}  \\\\\n\\bigg| & \\bigg| & & \\bigg| \n\\end{bmatrix},\n$$ \nand the $d\\times d$ upper block of matrix $\\Lambda$\n$$\n\\Lambda' = \n\\begin{bmatrix}\n\\lambda_1  &  & \\\\\n  & \\lambda_2  &   \\\\\n&               & \\ddots                \\\\\n&               &   &   & \\lambda_d\n\\end{bmatrix}.\n$$\n\n## PCA Interpretation\n\n* The direction $\\mathbf{v}_i$ is the $i$-th principal component and the corresponding $\\lambda_i$ accounts for the $i$-th largest variance in the dataset.\n\n  * In other words, $\\mathbf{v}_1$ is the first principal component and is the direction that accounts for the most variance in the dataset. \n\n  * The vector $\\mathbf{v}_d$ is the $d$-th principal component and is the direction that accounts for the $d$-th most variance in the dataset. \n\nThe reduced data matrix is obtained by computing\n\n$$\nY = XV'.\n$$\n\n\n## Spatial Interpretation -- Dimensions\n\nThe operation $Y = XV'$ performs a **change of basis** and **projection** of your data:\n\n- $X \\in \\mathbb{R}^{m \\times n}$: each of the $m$ rows is a data point in the original $n$-dimensional feature space\n- $V' \\in \\mathbb{R}^{n \\times d}$: contains the first $d$ principal components as columns (these are orthogonal vectors)\n- $Y \\in \\mathbb{R}^{m \\times d}$: each row is now a data point in the new $d$-dimensional space\n\n## Geometric Interpretation\n\n1. **Rotation**: The multiplication by $V'$ rotates your coordinate system. The columns of $V'$ define a new set of $d$ orthogonal axes that point in the directions of maximum variance.\n\n2. **Projection**: Each data point in $X$ is projected onto this new coordinate system. For each row vector $\\mathbf{x}_i$ (a point in $\\mathbb{R}^n$):\n   $$\\mathbf{y}_i = \\mathbf{x}_i V' = \\begin{bmatrix} \\mathbf{x}_i \\cdot \\mathbf{v}_1 & \\mathbf{x}_i \\cdot \\mathbf{v}_2 & \\cdots & \\mathbf{x}_i \\cdot \\mathbf{v}_d \\end{bmatrix}$$\n\n3. **Dimensionality Reduction**: You're effectively taking each point in $n$-dimensional space and expressing it using only $d$ coordinates - the coordinates along the principal component directions. You're discarding the components along directions with low variance.\n\n#### Intuitive View\n\nThink of it as finding the \"best viewing angle\" for your data. If you have data in 3D that's mostly flat (like a pancake), PCA finds that the data lies approximately in a 2D plane. $V'$ defines that plane's orientation, and $Y$ gives you the coordinates of each point within that plane.\n\n\n----\n\nReturning to our example\n$$X \n= \\begin{bmatrix}\n  5 & 0 & -5\\\\\n-5 & 0 & 5 \\end{bmatrix}, \n\\quad\nS = \n\\begin{bmatrix}\n50 & 0 & -50\\\\\n0 & 0 & 0\\\\\n-50 & 0 & 50 \n\\end{bmatrix}\n$$\n\nEigenvalues of $S$: $\\lambda_1 = 100, \\: \\lambda_2 = \\lambda_3 =0$\n\nEigenvectors of $S$:\n$$\n\\mathbf{v}_1 = \n\\begin{bmatrix}\n0.7071 \\\\ 0 \\\\  -0.7071\n\\end{bmatrix}, \\:\n\\mathbf{v}_2 = \\begin{bmatrix}\n0.7071 \\\\ 0 \\\\  0.7071\n\\end{bmatrix}, \\:\n\\mathbf{v}_3 = \\begin{bmatrix}\n0 \\\\ 1 \\\\ 0\n\\end{bmatrix}\n$$\n\nWhat is the value of the total variance in the data?\n\n:::: {.fragment}\nTotal variance: $T = 100$.\n\nThe first principal component accounts for all the variance in this example dataset.\n::::\n\n## PCA Summary\n\n- The columns of $V$ are the principal directions (or components). \n- The reduced dimension data is the projection of the data in the direction of the principal directions, i.e.,  $Y=XV'$.\n- The total variance $T$ in the data is the sum of all eigenvalues: $T = \\lambda_1 + \\lambda_2 + \\dots + \\lambda_n.$\n- The first eigenvector $\\mathbf{v}_1$ points in the most significant direction of the data. This direction explains the largest fraction $\\lambda_1/T$ of the total variance.\n- The second eigenvector $\\mathbf{v}_2$ accounts for a smaller fraction $\\lambda_2/T$.\n- The **explained variance** of component $i$ is the value $\\lambda_i/T$. As $i\\rightarrow n$ the explained variance gets smaller and approaches $\\lambda_n/T$.\n- The **cumulative (total) explained variance** is the sum of the explained variances up to component $k$, e.g. $\\sum_{i=1}^{k} \\frac{\\lambda_i}{T}$. The total explained variance for all eigenvalues is 1.\n\n\n## Case Study: Random Data\n\nLet's consider some randomly generated data consisting of 2 features.\n\n::: {#ae8f15e2 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(seed=42)\n\nimport numpy as np\n\n# Set the seed for reproducibility\nseed = 42\nrng = np.random.default_rng(seed)\n\nn_samples = 500\nC = np.array([[0.1, 0.6], [2., .6]])\nX0 = rng.standard_normal((n_samples, 2)) @ C + np.array([-6, 3])\nX = X0 - X0.mean(axis=0)\n\nplt.scatter(X[:, 0], X[:, 1])\nplt.axis('equal')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-revealjs/cell-2-output-1.png){width=792 height=411 fig-align='center'}\n:::\n:::\n\n\n---\n\nLet's do PCA and plot the principle components over our dataset. As expected, the principal components are orthogonal and point in the directions of the maximum variance.\n\n::: {#9ec40183 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\norigin = [0, 0]\n\n# Compute the covariance matrix\ncov_matrix = np.cov(X, rowvar=False)\n\n# Calculate the eigenvalues and eigenvectors\neigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n\n# Sort the eigenvalues and eigenvectors in descending order\nsorted_indices = np.argsort(eigenvalues)[::-1]\neigenvalues = eigenvalues[sorted_indices]\neigenvectors = eigenvectors[:, sorted_indices]\n\n# Plot the original dataset\nplt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n\n# Plot the principal components\nfor i in range(len(eigenvalues)):\n    plt.quiver(origin[0], origin[1], -eigenvectors[0, i], -eigenvectors[1, i],\n               angles='xy', scale_units='xy', scale=1, color=['r', 'g'][i])\n\nplt.title('Principal Components on Original Dataset')\nplt.axis('equal')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-revealjs/cell-3-output-1.png){width=792 height=431 fig-align='center'}\n:::\n:::\n\n\n## Case Study: Digits\n\nLet's consider another example using the MNIST dataset.\n\n::: {#bf4e2394 .cell execution_count=4}\n``` {.python .cell-code}\nfrom sklearn import datasets\ndigits = datasets.load_digits()\n\nplt.figure(figsize=(6, 6),)\nfor i in range(8):\n    plt.subplot(2, 4, i + 1)\n    plt.imshow(digits.images[i], cmap='gray_r')\n    plt.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-revealjs/cell-4-output-1.png){width=566 height=374 fig-align='center'}\n:::\n:::\n\n\n---\n\nWe first stack the columns of each digit on top of each other (this operation is called vectorizing) and perform PCA on the 64-D representation of the digits.\n\nWe can plot the explained variance ratio and the total (cumulative) explained variance ratio.\n\n::: {#7671146e .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\n\nX = digits.data\ny = digits.target\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npca = PCA()\nX_pca = pca.fit_transform(X)\n\n# Create subplots\nfig, axs = plt.subplots(2, 1, figsize=(6, 4))\n\n# Scree plot (graph of eigenvalues corresponding to PC number)\n# This shows the explained variance ratio\naxs[0].plot(np.arange(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o', linestyle='--')\naxs[0].set_title('Scree Plot')\naxs[0].set_xlabel('Principal Component')\naxs[0].set_ylabel('Explained Variance\\n Ratio')\naxs[0].grid(True)\n\n# Cumulative explained variance plot\naxs[1].plot(np.arange(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\naxs[1].set_title('Cumulative Explained Variance Plot')\naxs[1].set_xlabel('Principal Component')\naxs[1].set_ylabel('Cumulative Explained\\n Variance')\naxs[1].grid(True)\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-revealjs/cell-5-output-1.png){width=566 height=374 fig-align='center'}\n:::\n:::\n\n\n---\n\nLet's plot the data in the first 2 principal component directions. We'll use the digit labels to color each digit in the reduced space.\n\n::: {#2520f438 .cell execution_count=6}\n``` {.python .cell-code}\nplt.figure(figsize=(5, 5))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='tab20', edgecolor='k', s=50)\nplt.title('Digits in PC1 and PC2 Space')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\n\n# Create a legend with discrete labels\nlegend_labels = np.unique(y)\nhandles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.tab20(i / 9), markersize=10) for i in legend_labels]\nplt.legend(handles, legend_labels, title=\"Digit Label\", bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-revealjs/cell-6-output-1.png){width=548 height=449 fig-align='center'}\n:::\n:::\n\n\n---\n\nWe observe the following in our plot of the digits in the first two principal components:\n\n- There is a decent clustering of some of our digits, in particular 0, 2, 3, 4, and 6.\n- The numbers 0 and 6 seem to be relatively close to each other in this space.\n- There is not a very clear separation of the number 5 from some of the other points.\n\n\n## To scale or not to scale\n\nConsider a situation where we have age (years) and height (feet) data for 4 people.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n| Person | Age [years] | Height [feet] |\n|--------|-------------|---------------|\n| A      | 25          | 6.232         |\n| B      | 30          | 6.232         |\n| C      | 25          | 5.248         |\n| D      | 30          | 5.248         |\n\n:::\n::: {.column width=\"50%\"}\n![](figs/Scaling_feet.png){width=75% fig-align=\"center\"}\n:::\n::::\n\nNotice that the dominant direction of the variance is aligned horizontally.\n\n---\n\nWhat if the height is in cm?\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n| Person | Age [years] | Height [cm] |\n|--------|-------------|-------------|\n| A      | 25          | 189.95      |\n| B      | 30          | 189.95      |\n| C      | 25          | 159.96      |\n| D      | 30          | 159.96      |\n\n:::\n::: {.column width=\"50%\"}\n![](figs/Scaling_cm.png){width=75% fig-align=\"center\"}\n:::\n::::\n\nNotice that the dominant direction of the variance is aligned vertically.\n\n---\n\nLet's standardize our data.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n| Person | Age [years] | Height [cm] |\n|--------|-------------|-------------|\n| A      | -1          | 1           |\n| B      | 1           | 1           |\n| C      | -1          | -1          |\n| D      | 1           | -1          |\n\n:::\n::: {.column width=\"50%\"}\n![](figs/Scaling.png){width=75% fig-align=\"center\"}\n:::\n::::\n\nWhen we normalize our data, we observe an equal distribution of the variance.\n\n--- \n\nWhat quantity is represented by $\\frac{Cov(X, Y)}{\\sigma_X\\sigma_Y}$, where $X$ and $Y$ represent the random variables of\na person's age and height, respectively?\n\n:::: {.fragment}\nThis is the correlation matrix. When you _standardize_ your data you are no longer working with the covariance matrix but the correlation matrix.\n::::\n\n:::: {.fragment}\nPCA on a standardized dataset and working with the correlation matrix is the best practice when your data has very different scales. \n\nPCA identifies the directions with the maximum variance and large scale data can disproportionately influence the results of your PCA.\n::::\n\n\n## Relationship to the SVD\n\nRecall that the SVD of a (mean-centered) data matrix $X\\in\\mathbb{R}^{m\\times n}$ ($m>n$) is\n\n$$\nX = U\\Sigma V^T.\n$$\n\n::: {#5bc5e7a6 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-revealjs/cell-7-output-1.png){width=763 height=389 fig-align='center'}\n:::\n:::\n\n\n---\n\nHow can we relate this to what we learned in PCA?\n\nIn PCA, we computed an eigen-decomposition of the covariance matrix, i.e., $S=V\\Lambda V^{T}$.\n\nConsider the following computation using the SVD of $X$\n\n$$\n\\begin{align*}\nX^{T}X\n= \n(U\\Sigma V^T)^{T}(U\\Sigma V^T)\n=\nV\\Sigma U^TU \\Sigma V^{T}\n= \nV\\Sigma^{2}V^T\n\\end{align*}\n$$\n\nWe see that eigenvalues $\\lambda_i$ of the matrix $S$ are the squares $\\sigma_{i}^{2}$ of the singular values $\\Sigma$ (scaled by $\\frac{1}{m-1}$).\n\n---\n\nIn practice, PCA is done by computing the SVD of your data matrix as opposed to forming the covariance matrix and computing the eigenvalues. \n\nThe principal components are the right singular vectors $V$. The projected data is computed as $XV$. The eigenvalues of $S$ are obtained from squaring the entries of $\\Sigma$ and scaling them by $\\frac{1}{m-1}$.\n\nReasons to compute PCA this way are\n\n:::: {.incremental}\n- **Numerical Stability**: SVD is a numerically stable method, which means it can handle datasets with high precision and avoid issues related to floating-point arithmetic errors.\n\n- **Efficiency**: SVD is computationally efficient, especially for large datasets. Many optimized algorithms and libraries (like those in NumPy and scikit-learn) leverage SVD for fast computations.\n\n- **Direct Computation of Principal Components**: SVD directly provides the principal components (right singular vectors) and the singular values, which are related to the explained variance. This makes the process straightforward and avoids the need to compute the covariance matrix explicitly.\n\n::: {.content-visible when-profile=\"web\"}\n\n- **Memory Efficiency**: For large datasets, SVD can be more memory-efficient. Techniques like truncated SVD can be used to compute only the top principal components, reducing memory usage.\n\n- **Versatility**: SVD is a fundamental linear algebra technique used in various applications beyond PCA, such as signal processing, image compression, and solving linear systems. This versatility makes it a well-studied and widely implemented method.\n:::\n::::\n\n## Pros and Cons\n\nHere are some advantages and disadvantages of using PCA.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n**Advantages**\n\n+ Allows for visualizations\n+ Removes redundant variables\n+ Prevents overfitting\n+ Speeds up other ML algorithms\n:::\n::: {.column width=\"50%\"}\n**Disadvantages**\n\n- reduces interpretability\n- can result in information loss\n- can be less effective than non-linear methods\n:::\n::::\n\n# t-SNE\n\n## What is t-SNE?\n\n**t-SNE** stands for **t-distributed Stochastic Neighbor Embedding**\n\n- A dimensionality reduction technique specifically designed for visualization\n- Transforms high-dimensional data (hundreds or thousands of features) into 2D or 3D\n- Goal: Preserve the local structure of your data while making it viewable\n- Developed by Laurens van der Maaten and Geoffrey Hinton in 2008, see @van2008visualizing.\n\nThink of it as creating a \"map\" of your data where similar points stay close together.\n\n## Why Do We Need t-SNE?\n\n**The Curse of Dimensionality**\n\n- Real datasets often have many features (dimensions)\n- Impossible to visualize data with 100+ dimensions\n- Traditional methods (like PCA) can lose important patterns\n\n**What t-SNE offers:**\n\n- Reveals clusters and patterns in complex data\n- Preserves neighborhood relationships between points\n- Creates intuitive, interpretable visualizations\n\n## How Does t-SNE Work?\n\n### The Big Picture\n\n**Step 1: Measure Similarity in High Dimensions**\n\n- Calculate how \"similar\" each pair of points is in the original space\n- Similar points get high probability, distant points get low probability\n\n**Step 2: Create a Random Low-Dimensional Map**\n\n- Start with random positions for all points in 2D/3D\n\n**Step 3: Optimize the Map**\n\n- Iteratively move points around to match the high-dimensional similarities\n- Points that were neighbors in high-D should be neighbors in low-D\n\n## Step 1: Compute Pairwise Similarities\n\n- Calculate pairwise similarities between points in the high-dimensional space.\n- Use a Gaussian distribution to convert distances into probabilities.\n- The probability $p_{j\\vert i}$ between points $i$ and $j$ is given by:\n  $$\n  p_{j\\vert i} = \\frac{\\exp(-\\|x_i - x_j\\|^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(-\\|x_i - x_k\\|^2 / 2\\sigma_i^2)}.\n  $$\n\n- The $p_{j\\vert i}$ value represents the conditional probability that point $x_i$ would choose $x_j$ as its neighbor. \n- Note that we set $p_{i\\vert i} = 0$ -- we don't consider a point to be its own neighbor.\n- To create symmetric joint probabilities, t-SNE combines these two conditional probabilities into a single joint probability $p_{ij} = \\frac{p_{j\\vert i} + p_{i\\vert j}}{2d}$ where $d$ is the dimension of the point. This is because the probabilities $p_{j\\vert i}$ and $p_{i\\vert j}$ are different. Using this value of $p_{ij}$ is called symmetric t-SNE.\n\n## Step 1: Choosing $\\sigma$\n\n- The variance is calculated within a neighborhood of a point $x_i$ determined by a hyperparameter called the **perplexity**.\n- As a result the perplexity controls the effective number of neighbors. \n- A high perplexity means more neighbors for each point. A low perplexity means less neighbors are considered for each point.\n\n::: {.content-visible when-profile=\"slides\"}\n> See notes for definition of perplexity.\n:::\n\n::: {.content-visible when-profile=\"web\"}\n- The perplexity is defined as $\\operatorname{Perp}(P_i) = 2^{H(P_i)}$, where $H(P_i)$ is the entropy of $P_i$. $P_i$ is the probability distribution induced by $\\sigma_i$.\n- The entropy $H(P_i) = -\\sum_j p_{j\\vert i} \\log_2{(p_{j\\vert i})}$.\n- The value for $\\sigma_i$ is computed to produce a distribution $P_i$ which equals the chosen value of the perplexity. \n:::\n\nThe perplexity commonly ranges  between 5 and 50.\n\n## Visualizing the Pairwise Similarities\n\nThe Gaussian distribution is centered at point $x_i$ and the points $x_j$ that are further away have less probability of being chosen as neighbor.\n\n::: {#2dac2dd3 .cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import t\n\nmean = 0\nstd_dev = 1\n\nx = np.linspace(-4, 4, 100)\n\ngaussian_y = (1/(std_dev * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mean) / std_dev)**2)\n\n# Generate data for Student's t-distribution with 10 degrees of freedom\ndf = 1\nt_y = t.pdf(x, df)\n\n# Create the plot\nplt.plot(x, gaussian_y, label='Gaussian Distribution')\nplt.plot(x, t_y, label=\"Student's t-Distribution\", linestyle='dashed')\n\n# Add labels and title\nplt.xlabel('X-axis')\nplt.ylabel('Probability Density')\nplt.title('Gaussian Distribution and Student\\'s t-Distribution')\nplt.legend()\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-revealjs/cell-8-output-1.png){width=821 height=449 fig-align='center'}\n:::\n:::\n\n\n## Step 2: Define Low-Dimensional Map\n\n- Initialize points randomly in the low-dimensional space.\n- Define a similar probability distribution using a simplified Student-t distribution:\n  $$\n  q_{ij} = \\frac{(1 + \\|y_i - y_j\\|^2)^{-1}}{\\sum_{k \\neq l} (1 + \\|y_k - y_l\\|^2)^{-1}}.\n  $$\n\n## Step 3: Minimize Kullback-Leibler Divergence\n\n- Minimize the Kullback-Leibler (KL) divergence between the high-dimensional and low-dimensional distributions:\n  $$\n  KL(P \\| Q) = \\sum_{i \\neq j} p_{ij} \\log \\frac{p_{ij}}{q_{ij}}.\n  $$\n- The KL divergence measures how different distribution $P$ is from distribution $Q$. In other words, how different are each of the higher dimensional $P_i$ point distributions from the lower dimensional $Q_i$ distributions.\n- By minimizing this function, we ensure that the lower dimensional point clusters are similar to the higher dimensional clusters.\n- To minimize the KL divergence we use gradient descent on $\\partial KL / \\partial y_i$ to iteratively adjust the positions of points, $y_i$, in the low-dimensional space.\n\n\n\n## Case Study: Digits\n\nLet's consider how t-SNE performs clustering the MNIST digits datset.\n\n::: {#98faf62e .cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.manifold import TSNE\nfrom sklearn import datasets\n\ndigits = datasets.load_digits()\nX = digits.data\ny = digits.target\n\ntsne = TSNE(n_components=2, random_state=42)\nX_tsne = tsne.fit_transform(X)\n\nplt.figure(figsize=(5, 5))\nscatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='tab20', edgecolor='k', s=50)\nplt.title('Digits in t-SNE Space')\nplt.xlabel('tSNE Component 1')\nplt.ylabel('tSNE Component 2')\n\n# Create a legend with discrete labels\nlegend_labels = np.unique(y)\nhandles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.tab20(i / 9), markersize=10) for i in legend_labels]\nplt.legend(handles, legend_labels, title=\"Digit Label\", bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](11-Dimensionality-Reduction-SVD-II_files/figure-revealjs/cell-9-output-1.png){width=548 height=449 fig-align='center'}\n:::\n:::\n\n\n## The Key Insight\n\n**t-SNE uses different probability distributions for each step:**\n\n- **High dimensions**: Gaussian (normal) distribution\n  - Measures similarity based on Euclidean distance\n  \n- **Low dimensions**: Student's t-distribution\n  - Has \"heavier tails\" than Gaussian\n  - Allows moderate distances in low-D to represent larger distances in high-D\n  - Helps prevent crowding and creates better separation\n\nThis asymmetry is what makes t-SNE effective!\n\n## When to Use t-SNE\n\n**Great for:**\n\n- Exploratory data analysis\n- Visualizing clusters in labeled data\n- Understanding complex datasets (images, text, genomics)\n- Finding patterns you didn't know existed\n\n**Common applications:**\n\n- Image datasets (like MNIST digits)\n- Single-cell RNA sequencing data\n- Word embeddings\n- Customer segmentation\n\n## Important Limitations\n\n::: {.columns}\n::: {.column width=\"50%\"}\n\n**1. It's for visualization only**\n\n- Cannot use for dimensionality reduction before modeling\n- The low-dimensional representation changes each run\n\n**2. Computationally expensive**\n\n- Slow on large datasets (>10,000 points)\n- Consider using approximations or sampling\n:::\n::: {.column width=\"50%\"}\n\n**3. Hyperparameters matter**\n\n- Perplexity (neighborhood size) significantly affects results\n- Typical range: 5-50, default: 30\n\n**4. Distances are not meaningful**\n\n- Size of clusters doesn't mean anything\n- Distance between clusters is not interpretable\n- Only focus on local neighborhoods\n\n:::\n:::\n\n## Key Hyperparameter: Perplexity\n\n**Perplexity** controls how t-SNE balances local vs. global structure\n\n- Think of it as \"how many neighbors to consider\"\n- Low perplexity (5-10): Focuses on very local structure\n- High perplexity (30-50): Considers broader neighborhood\n\n**Rule of thumb:**\n- Should be less than the number of points\n- Try multiple values to see which reveals the best structure\n- Default of 30 often works well\n\n\n\n\n## Practical Tips\n\n::: {.columns}\n::: {.column}\n\n**1. Preprocessing matters**\n\n- Scale/normalize your features first\n- Consider PCA preprocessing for high dimensions\n\n**2. Try different perplexities**\n\n- Run 3-5 different values\n- Look for consistent patterns across runs\n\n:::\n::: {.column}\n\n**3. Run multiple times**\n\n- Results vary due to random initialization\n- Look for stable structures\n\n**4. Don't over-interpret**\n\n- Focus on local clusters, not global geometry\n- Use alongside other analysis methods\n\n:::\n:::\n\n## Summary\n\n**t-SNE is a powerful visualization tool that:**\n\n- Makes high-dimensional data visible\n- Preserves local neighborhood structure\n- Reveals clusters and patterns effectively\n\n**Remember:**\n\n- Only for visualization, not for modeling\n- Computationally intensive\n- Hyperparameters (especially perplexity) matter\n- Always complement with other analysis methods\n\n## Recap: t-SNE vs PCA\n\n| Feature | PCA | t-SNE |\n|---------|-----|-------|\n| **Speed** | Fast | Slow |\n| **Purpose** | Dimensionality reduction | Visualization |\n| **Preserves** | Global variance | Local neighborhoods |\n| **Deterministic?** | Yes | No (random initialization) |\n| **Distances** | Meaningful | Not meaningful |\n| **Further analysis?** | Yes | No |\n\n**Best practice**: Use PCA first to reduce to ~50 dimensions, then apply t-SNE\n\n## In-Class Exercise: Dimensionality Reduction with PCA and t-SNE\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/class_activity_notebooks/11-InClass-Exercise-Dimensionality-Reduction.ipynb)\n\n---\n\n",
    "supporting": [
      "11-Dimensionality-Reduction-SVD-II_files"
    ],
    "filters": [],
    "includes": {}
  }
}