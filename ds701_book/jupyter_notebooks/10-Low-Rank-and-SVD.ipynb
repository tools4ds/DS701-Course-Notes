{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: SVD - Low Rank Approximations\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "# Low Rank Approximations\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/10-Low-Rank-and-SVD.ipynb)\n",
        "\n",
        "We now consider applications of the Singular Value Decomposition (SVD).\n",
        "\n",
        "> SVD is \"the Swiss Army Knife of Numerical Linear Algebra.”\n",
        "\n",
        "Dianne O’Leary, MMDS ’06 (Workshop on Algorithms for Modern Massive Data Sets)\n",
        "\n",
        "## Applications\n",
        "\n",
        "We will see how the SVD is used for\n",
        "\n",
        ":::: {.incremental}\n",
        "- low rank approximations\n",
        "- dimensionality reduction\n",
        "::::\n",
        "\n",
        "\n",
        "## Singular Vectors and Values\n",
        "\n",
        "For \n",
        "\n",
        "$$\n",
        "A\\in\\mathbb{R}^{m\\times n} \\text{ with } m>n \\text{ and rank } k,\n",
        "$$\n",
        "\n",
        "there exists a set of orthogonal vectors $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n\\}$\n",
        "\n",
        "and a set of orthogonal vectors $\\{\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_m\\}$\n",
        "\n",
        "such that\n",
        "\n",
        "$$\n",
        "A\\mathbf{v}_1 = \\sigma_1 \\mathbf{u}_1 \\quad\\cdots\\quad A\\mathbf{v}_k = \\sigma_k \\mathbf{u}_k, \\quad A\\mathbf{v}_{k+1} = 0 \\quad\\cdots\\quad A\\mathbf{v}_n = 0.\n",
        "$$\n",
        "\n",
        "where $\\sigma_1, \\sigma_2, \\dots, \\sigma_k$ are the _singular values_ of $A$.\n",
        "\n",
        "## Singular Value Decomposition\n",
        "\n",
        "We can collect the vectors $\\mathbf{v}_i$ into a matrix $V$ and the vectors $\\mathbf{u}_i$ into a matrix $U$.\n",
        "$$\n",
        "A\n",
        "\\begin{bmatrix}\n",
        "\\vert & \\vert &   & \\vert \\\\\n",
        "\\mathbf{v}_1   & \\mathbf{v}_2  & \\dots  & \\mathbf{v}_n  \\\\\n",
        "\\vert & \\vert &  & \\vert\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "\\vert & \\vert &   & \\vert \\\\\n",
        "\\mathbf{u}_1   & \\mathbf{u}_2  & \\dots  & \\mathbf{u}_m  \\\\\n",
        "\\vert & \\vert &  & \\vert\n",
        "\\end{bmatrix}\n",
        "\\left[\n",
        "\\begin{array}{c|c}\n",
        "\\begin{matrix}\n",
        "\\sigma_1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "0 & \\cdots & \\sigma_k\n",
        "\\end{matrix}\n",
        "&\n",
        "\\mathbf{0}\n",
        "\\\\\n",
        "\\hline\n",
        "\\mathbf{0} & \\mathbf{0}\n",
        "\\end{array}\n",
        "\\right]\n",
        ".\n",
        "$$\n",
        "\n",
        "We call the $\\mathbf{v}_i$ the __right singular vectors__ and the $\\mathbf{u}_i$ the __left singular vectors__.\n",
        "\n",
        "\n",
        "## Singular Value Decomposition\n",
        "\n",
        "\n",
        "And because $V$ is an orthogonal matrix, we have $V V^T = I$, so we can right multiply both sides by $V^T$ to get\n",
        "\n",
        "$$\n",
        "A\n",
        " =\n",
        "\\begin{bmatrix}\n",
        "\\vert & \\vert &   & \\vert \\\\\n",
        "\\mathbf{u}_1   & \\mathbf{u}_2  & \\dots  & \\mathbf{u}_m  \\\\\n",
        "\\vert & \\vert &  & \\vert\n",
        "\\end{bmatrix}\n",
        "\\left[\n",
        "\\begin{array}{c|c}\n",
        "\\begin{matrix}\n",
        "\\sigma_1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "0 & \\cdots & \\sigma_k\n",
        "\\end{matrix}\n",
        "&\n",
        "\\mathbf{0}\n",
        "\\\\\n",
        "\\hline\n",
        "\\mathbf{0} & \\mathbf{0}\n",
        "\\end{array}\n",
        "\\right]\n",
        "\\begin{bmatrix}\n",
        "\\vert & \\vert &   & \\vert \\\\\n",
        "\\mathbf{v}_1   & \\mathbf{v}_2  & \\dots  & \\mathbf{v}_n  \\\\\n",
        "\\vert & \\vert &  & \\vert\n",
        "\\end{bmatrix}^T.\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "We can write this as\n",
        "\n",
        "$$\n",
        "A = U\\Sigma V^{T}.\n",
        "$$\n",
        "\n",
        "\n",
        "## Singular Value Decomposition\n",
        "\n",
        "The SVD of a matrix $A\\in\\mathbb{R}^{m\\times n}$ (where $m>n$) is\n",
        "\n",
        "$$\n",
        "A = U\\Sigma V^{T},\n",
        "$$\n",
        "\n",
        "where\n",
        "\n",
        ":::: {.incremental}\n",
        "- $U$ has dimension $m\\times n$. The columns of $U$ are orthogonal. The columns of $U$ are the __left singular vectors__.\n",
        "- $\\Sigma$ has dimension $n\\times n$. The only non-zero values are on the main diagonal and they are nonnegative real numbers  $\\sigma_1\\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_k$ and $\\sigma_{k+1} = \\ldots = \\sigma_n = 0$. These are called the __singular values__ of $A$.\n",
        "- $V$ has dimension $n \\times n$. The columns of $V$ are orthogonal. The columns of $V$ are the __right singular vectors__.\n",
        "::::\n",
        "\n",
        "## SVD Matrix Shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Create a figure and axis\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Draw matrix A\n",
        "rect_A = patches.Rectangle((0, 0), 2, 3, linewidth=1, edgecolor='black', facecolor='none')\n",
        "ax.add_patch(rect_A)\n",
        "ax.text(1, 1.5, r'$A$', fontsize=20, ha='center', va='center')\n",
        "ax.text(1, -0.5, r'$(m \\times n)$', fontsize=12, ha='center', va='center')\n",
        "\n",
        "# Draw equal sign\n",
        "ax.text(2.5, 1.5, r'$=$', fontsize=20, ha='center', va='center')\n",
        "\n",
        "# Draw matrix U\n",
        "rect_U = patches.Rectangle((3, 0), 2, 3, linewidth=1, edgecolor='black', facecolor='none')\n",
        "ax.add_patch(rect_U)\n",
        "ax.text(4, 1.5, r'$U$', fontsize=20, ha='center', va='center')\n",
        "ax.text(4, -0.5, r'$(m \\times n)$', fontsize=12, ha='center', va='center')\n",
        "\n",
        "# Draw Sigma\n",
        "rect_Sigma = patches.Rectangle((5.5, 1), 2, 2, linewidth=1, edgecolor='black', facecolor='none')\n",
        "ax.add_patch(rect_Sigma)\n",
        "ax.text(6.5, 2, r'$\\Sigma$', fontsize=20, ha='center', va='center')\n",
        "ax.text(6.5, 0.5, r'$(n \\times n)$', fontsize=12, ha='center', va='center')\n",
        "\n",
        "# Draw matrix V^T with the same dimensions as Sigma\n",
        "rect_VT = patches.Rectangle((8, 1), 2, 2, linewidth=1, edgecolor='black', facecolor='none')\n",
        "ax.add_patch(rect_VT)\n",
        "ax.text(9, 2, r'$V^T$', fontsize=20, ha='center', va='center')\n",
        "ax.text(9, 0.5, r'$(n \\times n)$', fontsize=12, ha='center', va='center')\n",
        "\n",
        "# Set limits and remove axes\n",
        "ax.set_xlim(-1, 11)\n",
        "ax.set_ylim(-2, 4)\n",
        "ax.axis('off')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SVD Properties\n",
        "\n",
        "<br><br>\n",
        "\n",
        "* The SVD of a matrix always exists.\n",
        "\n",
        "    * The existence of the SVD was proven in 1936 by [Carl Eckart and Gale Young](https://www.cambridge.org/core/journals/psychometrika/article/abs/approximation-of-one-matrix-by-another-of-lower-rank/B29672E1EDD0FA1B7611D4DFAFC321B3?utm_campaign=shareaholic&utm_medium=copy_link&utm_source=bookmark).\n",
        "\n",
        "* The singular values are uniquely determined.\n",
        "\n",
        "* The left and right singular vectors are uniquely determined up to $\\pm 1$.\n",
        "\n",
        "## Outer Products\n",
        "\n",
        "The SVD can also be represented as a sum of outer products\n",
        "\n",
        "$$ \n",
        "A = \\sum_{i=1}^{n} \\sigma_{i}\\mathbf{u}_i\\mathbf{v}_{i}^{T},\n",
        "$$\n",
        "\n",
        "where $\\mathbf{u}_i, \\mathbf{v}_{i}$ are the $i$-th columns of $U$ and $V$, respectively.\n",
        "\n",
        "## Outer Products, continued\n",
        "\n",
        "An outer product of a $m\\times 1$ vector and a $1\\times n$ vector is a $m\\times n$ matrix.\n",
        "\n",
        "$$\\mathbf{u}_i\\mathbf{v}_{i}^{T}=\n",
        "\\begin{bmatrix}\n",
        "u_{i1} \\\\\n",
        "u_{i2} \\\\\n",
        "\\vdots \\\\\n",
        "u_{im} \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "v_{i1} & v_{i2} & \\cdots & v_{in} \\\\\n",
        "\\end{bmatrix} = \n",
        "\\begin{bmatrix}\n",
        "u_{i1}v_{i1} & u_{i1}v_{i2} & \\cdots & u_{i1}v_{in} \\\\\n",
        "u_{i2}v_{i1} & u_{i2}v_{i2}  & \\cdots & u_{i2}v_{in} \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "u_{im}v_{i1} &  u_{im}v_{i2}  & \\cdots & u_{im}v_{in} \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "It is a rank-1 matrix.  How can you tell?\n",
        "\n",
        ":::: {.fragment}\n",
        "**Alternate Interpretation:**\n",
        "\n",
        "The SVD decomposes $A$ into a linear combination of rank-1 matrices. \n",
        "\n",
        "The singular value tells us the weight (contribution) of each rank-1 matrix to the matrix $A$.\n",
        "::::\n",
        "\n",
        "# Lecture Organization\n",
        "\n",
        "In this lecture we first discuss:\n",
        "\n",
        "* Theoretical properties of the SVD related to\n",
        "    * matrix rank\n",
        "    * determining the best low rank approximations to a matrix\n",
        "\n",
        ":::: {.fragment}\n",
        "* We will then apply these results when we consider data matrices from the following applications \n",
        "    * internet traffic data\n",
        "    * social media data\n",
        "    * image data\n",
        "    * movie data\n",
        "\n",
        ":::\n",
        "\n",
        "# SVD Properties\n",
        "\n",
        "## Matrix Rank\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "Let's review some definitions.\n",
        ":::\n",
        "\n",
        "Let $A\\in\\mathbb{R}^{m\\times n}$ be a real matrix such that with $m>n$.\n",
        "\n",
        ":::: {.fragment}\n",
        "The __rank__ of $A$ is the number of linearly independent rows or columns of the matrix. \n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "The largest value that a matrix rank can take is $\\min(m,n)$. Since we assumed $m>n$, the largest value of the rank is $n$.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "If the matrix $A$ has rank equal to $n$, then we say it is **full rank**.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "However, it can happen that the rank of a matrix is __less__ than $\\min(m,n)$. In this case we say that $A$ is **rank-deficient**.\n",
        "::::\n",
        "\n",
        "## Matrix Rank and Column Space\n",
        "\n",
        "The dimension of the column space of $A$ is the **smallest number of vectors that suffice to construct the columns of $A$.**\n",
        "\n",
        "And it is equal to the rank of the matrix.\n",
        "\n",
        ":::: {.fragment}\n",
        "If the dimension of the column spaces is $k$, then there exists a set of vectors $\\{\\mathbf{c}_1, \\mathbf{c}_2, \\dots, \\mathbf{c}_k\\}$ such that every column $\\mathbf{a}_i$ of $A$ can be expressed as:\n",
        "\n",
        "$$\\mathbf{a}_i = r_{1i}\\mathbf{c}_1 + r_{2i}\\mathbf{c}_2 + \\dots + r_{ki}\\mathbf{c}_k\\quad i=1,\\ldots,n.$$\n",
        "::::\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "To store a matrix $A \\in \\mathbb{R}^{m\\times n}$ we need to store $mn$ values.\n",
        "\n",
        "However, if $A$ has rank $k$, it can be factored as $A = CR$,\n",
        "$$\n",
        "A =\n",
        "\\begin{bmatrix} \n",
        "\\bigg| & \\bigg| &   & \\bigg| \\\\\n",
        "\\mathbf{c}_1   & \\mathbf{c}_2  & \\dots  & \\mathbf{c}_k  \\\\\n",
        "\\bigg| & \\bigg| &  & \\bigg|\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "\\vert & \\vert &   & \\vert \\\\\n",
        "\\mathbf{r}_1   & \\mathbf{r}_2  & \\dots  & \\mathbf{r}_n  \\\\\n",
        "\\vert & \\vert &  & \\vert\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "where $C \\in \\mathbb{R}^{m\\times k}$ and $R \\in \\mathbb{R}^{k \\times n}$.\n",
        "\n",
        "This requires $k(m+n)$ values, which could be much smaller than $mn$ when $k$ is small, e.g. $k < \\frac{mn}{m+n}$.\n",
        "\n",
        "\n",
        "\n",
        "## Low Effective Rank\n",
        "\n",
        "In many situations we want to __approximate__ a matrix $A$ with a low-rank matrix $A^{(k)}.$\n",
        "\n",
        ":::: {.fragment}\n",
        "To talk about when one matrix *approximates* another, we need a norm for matrices.  \n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "We will use the __Frobenius norm__, which is defined as\n",
        "\n",
        "$$\\Vert A\\Vert_F = \\sqrt{\\sum a_{ij}^2}.$$\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "Observe that this is the $\\ell_2$ norm for a vectorized matrix, i.e., by  stacking the columns of the matrix $A$ to form a vector of length $mn$. \n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "To quantify when one matrix is *close* to another, we define the distance function:\n",
        "\n",
        "$$ \n",
        "\\operatorname{dist}(A,B) = \\Vert A-B\\Vert_F. \n",
        "$$\n",
        "\n",
        "This can be viewed as Euclidean distance between $mn$-dimensional vectors.\n",
        "\n",
        "We define the optimal __rank-$k$ approximation__ to $A$ as \n",
        "\n",
        "$$\n",
        "A^{(k)} =\\mathop{\\arg\\min}\\limits_{\\{B~|~\\operatorname{Rank} B = k\\}} \\Vert A-B\\Vert_F.\n",
        "$$\n",
        "\n",
        "In other words, $A^{(k)}$ is the closest rank-$k$ matrix to $A$.\n",
        "\n",
        "## Finding Rank-$k$ Approximations\n",
        "\n",
        "How can we find the optimal rank-$k$ approximation to a matrix $A$?\n",
        "\n",
        ":::: {.fragment}\n",
        "The __Singular Value Decomposition (SVD).__\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "Why?\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "The SVD  gives the best rank-$k$ approximation to $A$ for __every__ $k$ up to the rank of $A$.\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "To form the best rank-$k$ approximation to using the SVD you calculate\n",
        "\n",
        "$$ A^{(k)} = U'\\Sigma'(V')^T,$$\n",
        "\n",
        "where\n",
        "\n",
        ":::: {.incremental}\n",
        "* $U'$ are the $k$ leftmost columns of $U$, \n",
        "* $\\Sigma'$ is the $k\\times k$ upper left sub-matrix of $\\Sigma$, and \n",
        "* $V'$ is the $k$ leftmost columns of $V$ (or the $k$ topmost rows of $V^T$)\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "Full SVD:\n",
        "\n",
        "$$\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "\\vert & \\vert &   & \\vert \\\\\n",
        "\\mathbf{u}_1   & \\mathbf{u}_2  & \\dots  & \\mathbf{u}_m  \\\\\n",
        "\\vert & \\vert &  & \\vert\n",
        "\\end{bmatrix}\n",
        "\\left[\n",
        "\\begin{array}{c|c}\n",
        "\\begin{matrix}\n",
        "\\sigma_1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "0 & \\cdots & \\sigma_k\n",
        "\\end{matrix}\n",
        "&\n",
        "\\mathbf{0}\n",
        "\\\\\n",
        "\\hline\n",
        "\\mathbf{0} & \\mathbf{0}\n",
        "\\end{array}\n",
        "\\right]\n",
        "\\begin{bmatrix}\n",
        "- & \\mathbf{v}_1^T & - \\\\\n",
        "- & \\mathbf{v}_2^T & - \\\\\n",
        "  & \\vdots &   \\\\\n",
        "- & \\mathbf{v}_n^T & -\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Rank-$k$ SVD:\n",
        "\n",
        "$$\n",
        "A =\n",
        "\\begin{bmatrix}\n",
        "\\vert & \\vert &   & \\vert \\\\\n",
        "\\mathbf{u}_1   & \\mathbf{u}_2  & \\dots  & \\mathbf{u}_k  \\\\\n",
        "\\vert & \\vert &  & \\vert\n",
        "\\end{bmatrix}\n",
        "\\left[\n",
        "\\begin{array}{c|c}\n",
        "\\begin{matrix}\n",
        "\\sigma_1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "0 & \\cdots & \\sigma_k\n",
        "\\end{matrix}\n",
        "&\n",
        "\\mathbf{0}\n",
        "\\\\\n",
        "\\hline\n",
        "\\mathbf{0} & \\mathbf{0}\n",
        "\\end{array}\n",
        "\\right]\n",
        "\\begin{bmatrix}\n",
        "- & \\mathbf{v}_1^T & - \\\\\n",
        "- & \\mathbf{v}_2^T & - \\\\\n",
        "  & \\vdots &   \\\\\n",
        "- & \\mathbf{v}_k^T & -\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "For a matrix $A$ of rank $n$, we can prove that\n",
        "\n",
        "$$\\Vert A-A^{(k)}\\Vert_F^2 = \\sum_{i=k+1}^n\\sigma^2_i.$$\n",
        "\n",
        "This means that the distance (in Frobenius norm) of the best rank-$k$ approximation $A^{(k)}$ from $A$ is equal to $\\sqrt{\\sum_{i=k+1}^n\\sigma^2_i}$.\n",
        "\n",
        "# Low Rank Approximations in Practice \n",
        "\n",
        "## Models are simplifications\n",
        "\n",
        "::: {.fragment}\n",
        "One way of thinking about modeling or clustering is that we are building a \n",
        "__simplification__ of the data. \n",
        ":::\n",
        "\n",
        ":::: {.fragment}\n",
        "That is, a model is a description of the data, that is simpler than the data.\n",
        ":::\n",
        "\n",
        ":::: {.fragment}\n",
        "In particular, instead of thinking of the data as thousands or millions of \n",
        "individual data points, we might think of it in terms of a small number of \n",
        "clusters, or a parametric distribution, etc.\n",
        ":::\n",
        "\n",
        ":::: {.fragment}\n",
        "From this simpler description, we hope to gain __insight.__\n",
        ":::\n",
        "\n",
        ":::: {.fragment}\n",
        "There is an interesting question here:  __why__ does this process often lead to insight?   \n",
        ":::\n",
        "\n",
        ":::: {.fragment}\n",
        "That is, why does it happen so often that a large dataset can be described in\n",
        "terms of a much simpler model?\n",
        ":::\n",
        "\n",
        "## William of Ockham\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"40%\"}\n",
        "![](figs/L10-William-of-Ockham.png){width=80%}\n",
        "\n",
        "[Source](https://commons.wikimedia.org/w/index.php?curid=5523066)\n",
        ":::\n",
        "::: {.column width=\"60%\"}\n",
        "William of Ockham (c. 1300 AD) said:\n",
        "\n",
        ":::: {.fragment}\n",
        "\n",
        "> Non sunt multiplicanda entia sine necessitate\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "or, in other words:\n",
        "\n",
        "> Entities must not be multiplied beyond necessity.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "by which he meant:\n",
        "\n",
        "> Among competing hypotheses, the one with the fewest assumptions should be selected.\n",
        "::::\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: aside\n",
        "This has come to be known as \"Occam's razor.\"\n",
        ":::\n",
        "\n",
        "\n",
        "## Occam's Razor\n",
        "\n",
        "William was saying that it is more common for a set of observations to be determined by a simple process than a complex process.\n",
        "\n",
        ":::: {.fragment}\n",
        "In other words, the world is full of simple (but often hidden) patterns.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "From which one can justify the observation that *modeling works surprisingly often*.\n",
        "::::\n",
        "\n",
        "## Low Effective Rank of Data Matrices\n",
        "\n",
        "In general, a data matrix $A\\in\\mathbb{R}^{m\\times n}$  is usually __full rank__, meaning that $\\operatorname{Rank}(A)\\equiv p = \\min(m, n)$.\n",
        "\n",
        ":::: {.fragment}\n",
        "However, it is possible to encounter data matrices that have __low effective rank__.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "This means that we can approximate $A$ by some $A^{(k)}$ for which $k \\ll p$.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "For any data matrix, we can judge when this is the case by looking at its singular values, because the singular values tell us the distance to the nearest rank-$k$ matrix.\n",
        "::::\n",
        "\n",
        "## Traffic Data\n",
        "\n",
        "Let's see how this theory can be used in practice  and investigate some real data.\n",
        "\n",
        "We'll look at data traffic on the Abilene network:\n",
        "\n",
        "![](figs/L10-Abilene-map.png){fig-align=\"center\" width=\"200px}\n",
        "\n",
        "Source: Internet2, circa 2005\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with open('data/net-traffic/AbileneFlows/odnames','r') as f:\n",
        "    odnames = [line.strip() for line in f]\n",
        "dates = pd.date_range('9/1/2003', freq = '10min', periods = 1008)\n",
        "Atraf = pd.read_table('data/net-traffic/AbileneFlows/X', sep='  ', header=None, names=odnames, engine='python')\n",
        "Atraf.index = dates\n",
        "Atraf.head(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "Atraf.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we would expect, our traffic matrix has rank 121:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "np.linalg.matrix_rank(Atraf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However -- perhaps it has low __effective__ rank.\n",
        "\n",
        "The `numpy` routine for computing the SVD is `np.linalg.svd`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "u, s, vt = np.linalg.svd(Atraf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Now let's look at the singular values of `Atraf` to see if it can be usefully approximated as a low-rank matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "fig = plt.figure(figsize=(5, 3))\n",
        "plt.plot(range(1, 1+len(s)), s)\n",
        "plt.xlabel(r'$k$', size=20)\n",
        "plt.ylabel(r'$\\sigma_k$', size=20)\n",
        "plt.ylim(ymin=0)\n",
        "plt.xlim(xmin=-1)\n",
        "plt.title(r'Singular Values of $A$', size=20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This classic, sharp-elbow tells us that a few singular values are very large, and most singular values are quite small.\n",
        "\n",
        "---\n",
        "\n",
        "Zooming in for just small $k$ values, we can see that the elbow is around 4 - 6 singular values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "fig = plt.figure(figsize=(5, 3))\n",
        "Anorm = np.linalg.norm(Atraf)\n",
        "plt.plot(range(1, 21), s[0:20]/Anorm, '.-')\n",
        "plt.xlim([0.5, 20])\n",
        "plt.ylim([0, 1])\n",
        "plt.xlabel(r'$k$', size=20)\n",
        "plt.xticks(range(1, 21))\n",
        "plt.ylabel(r'$\\sigma_k$', size=20);\n",
        "plt.title(r'Singular Values of $A$',size=20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This pattern of singular values suggests __low effective rank.__\n",
        "\n",
        "---\n",
        "\n",
        "Let's use the formula above to compute the relative error of a rank-$k$ approximation to $A$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "fig = plt.figure(figsize=(5, 3))\n",
        "Anorm = np.linalg.norm(Atraf)\n",
        "err = np.cumsum(s[::-1]**2)\n",
        "err = np.sqrt(err[::-1])\n",
        "plt.plot(range(0, 20), err[:20]/Anorm, '.-')\n",
        "plt.xlim([0, 20])\n",
        "plt.ylim([0, 1])\n",
        "plt.xticks(range(1, 21))\n",
        "plt.xlabel(r'$k$', size = 16)\n",
        "plt.ylabel(r'relative F-norm error', size=16)\n",
        "plt.title(r'Relative Error of rank-$k$ approximation to $A$', size=16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Remarkably, we are down to 9% relative error using only a rank 20 approximation to $A$.\n",
        "\n",
        "---\n",
        "\n",
        "So instead of storing \n",
        "\n",
        "* $mn =$ (1008 $\\cdot$ 121) = 121,968 values, \n",
        "\n",
        "we only need to store \n",
        "\n",
        "* $k(m+n)$ = 20 $\\cdot$ (1008 + 121) = 22,580 values, \n",
        "\n",
        "which is an 81% reduction in size.\n",
        "\n",
        "## Low Effective Rank is Common\n",
        "\n",
        "In practice __many__ datasets have low effective rank.   \n",
        "\n",
        "We consider the following examples:\n",
        "\n",
        ":::: {.incremental}\n",
        "* Likes on Facebook,\n",
        "* Yelp reviews and Tweets (the site formerly known as Twitter),\n",
        "* User preferences over time,\n",
        "* Images.\n",
        "::::\n",
        "\n",
        "## Likes on Facebook\n",
        "\n",
        "Here, the matrices are \n",
        "\n",
        ":::: {.incremental}\n",
        "1. Number of likes:  Timebins $\\times$ Users\n",
        "2. Number of likes:  Users $\\times$ Page Categories\n",
        "3. Entropy of likes across categories:  Timebins $\\times$ Users\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "![](figs/L10-facebook.png){fig-align=\"center\" width=\"50%\"}\n",
        "\n",
        "Source: [Viswanath et al., Usenix Security, 2014]\n",
        "::::\n",
        "\n",
        "## Social Media Activity\n",
        "\n",
        "Here, the matrices are \n",
        "\n",
        ":::: {.incremental}\n",
        "1. Number of Yelp reviews:  Timebins $\\times$ Users\n",
        "2. Number of Yelp reviews:  Users $\\times$ Yelp Categories\n",
        "3. Number of Tweets:  Users $\\times$ Topic Categories\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "![](figs/L10-yelp-twitter.png){fig-align=\"center\" width=\"50%\"}\n",
        "\n",
        "Source: [Viswanath et al., Usenix Security, 2014]\n",
        "::::\n",
        "\n",
        "\n",
        "## Netflix\n",
        "\n",
        "Example: the Netflix prize worked with partially-observed (sparse) rating matrices like this:\n",
        "\n",
        "::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        " & & & \\vdots & & & \\\\\n",
        " & & 3 & 2 & & 1 &\\\\\n",
        " & 1 & & 1 & & & \\\\\n",
        "\\dots & & 2 & & 4 & & \\dots\\\\\n",
        " & 5 & 5 & & 4 & & \\\\\n",
        " & 1 & & & 1 & 5 & \\\\\n",
        " & & & \\vdots & & & \\\\\n",
        "\\end{bmatrix},\n",
        "$$\n",
        "\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "\n",
        "\n",
        ":::: {.fragment}\n",
        "where the rows correspond to users, the columns to movies, and the entries are ratings.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "Although the problem matrix was of size 500,000 $\\times$ 18,000, the winning approach modeled the matrix as having __rank 20 to 40.__\n",
        "\n",
        "Source: [Koren et al, IEEE Computer, 2009]\n",
        "::::\n",
        "\n",
        ":::\n",
        "::: \n",
        "\n",
        "## Images\n",
        "\n",
        "Image data often shows low effective rank.\n",
        "\n",
        "For example, here is an original $512 \\times 512$ photo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "boat = np.loadtxt('data/images/boat/boat.dat')\n",
        "import matplotlib.cm as cm\n",
        "plt.figure()\n",
        "plt.imshow(boat, cmap=cm.Greys_r)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Treat the image as a matrix and then compute the singular values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "u, s, vt = np.linalg.svd(boat, full_matrices=False)\n",
        "plt.plot(s)\n",
        "plt.xlabel('$k$', size=16)\n",
        "plt.ylabel(r'$\\sigma_k$', size=16)\n",
        "plt.title('Singular Values of Boat Image', size=16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "This image is 512 $\\times$ 512. As a matrix, it has rank of 512.   \n",
        "\n",
        "But its _effective_ rank is low.\n",
        "\n",
        "Based on the previous plot, its effective rank is perhaps 40.\n",
        "\n",
        "Let's find the closest rank-40 matrix and view it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "u, s, vt = np.linalg.svd(boat, full_matrices=False)\n",
        "s[40:] = 0\n",
        "boatApprox = u @ np.diag(s) @ vt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "plt.figure(figsize=(9, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(boatApprox, cmap=cm.Greys_r)\n",
        "plt.axis('off')\n",
        "plt.title('Rank 40 Boat')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(boat, cmap=cm.Greys_r)\n",
        "plt.axis('off')\n",
        "plt.title('Full Rank 512 Boat')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretations of Low Effective Rank\n",
        "\n",
        "How can we understand the low-effective-rank phenomenon in general?\n",
        "\n",
        ":::: {.fragment}\n",
        "There are two helpful interpretations:\n",
        "\n",
        ":::: {.incremental}\n",
        "1. Common Patterns\n",
        "2. Latent Factors\n",
        "::::\n",
        "::::\n",
        "\n",
        "## Low Rank Implies Common Patterns\n",
        "\n",
        "The first interpretation of low-rank behavior is in answering the question:\n",
        "\n",
        ":::: {.fragment}\n",
        "\"What is the strongest pattern in the data?\"\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "Remember that using the SVD we form the low-rank approximation as\n",
        "\n",
        "$$ A^{(k)} =  U'\\Sigma'(V')^T$$\n",
        "\n",
        "and\n",
        "\n",
        ":::: {.incremental}\n",
        "\n",
        "- $U'$ are the $k$ leftmost columns of $U$, \n",
        "- $\\Sigma'$ is the $k\\times k$ upper left submatrix of $\\Sigma$, and \n",
        "- $V'$ are the $k$ leftmost columns of $V$.\n",
        "::::\n",
        "::::\n",
        " \n",
        ":::: {.fragment}\n",
        "In this interpretation, we think of each column of $A^{(k)}$ as a combination of the columns of $U'$.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "How can this be helpful? \n",
        "::::\n",
        "\n",
        "\n",
        "## Common Patterns: Traffic Example\n",
        "\n",
        "Consider the set of traffic traces. There are clearly some common patterns. How can we find them?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "with open('data/net-traffic/AbileneFlows/odnames','r') as f:\n",
        "    odnames = [line.strip() for line in f]\n",
        "dates = pd.date_range('9/1/2003', freq='10min', periods=1008)\n",
        "Atraf = pd.read_table('data/net-traffic/AbileneFlows/X', sep='  ', header=None, names=odnames, engine='python')\n",
        "Atraf.index = dates\n",
        "plt.figure(figsize=(10, 8))\n",
        "for i in range(1, 13):\n",
        "    ax = plt.subplot(4, 3, i)\n",
        "    Atraf.iloc[:, i-1].plot()\n",
        "    plt.title(odnames[i])\n",
        "plt.subplots_adjust(hspace=1)\n",
        "plt.suptitle('Twelve Example Traffic Traces', size=20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Let's use as our example $\\mathbf{a}_1,$ the first column of $A$.\n",
        "\n",
        "This happens to be the ATLA-CHIN flow.\n",
        "\n",
        "The earlier SVD equation tells us that\n",
        "\n",
        "$$\\mathbf{a}_1 \\approx v_{11}\\sigma_1\\mathbf{u}_1 + v_{12}\\sigma_2\\mathbf{u}_2 + \\dots + v_{1k}\\sigma_k\\mathbf{u}_k.$$\n",
        "\n",
        "In other words, $\\mathbf{u}_1$ (the first column of $U$) is the \"strongest\" pattern occurring in $A$, and its strength is measured by $\\sigma_1$.\n",
        "\n",
        "---\n",
        "\n",
        "Here is a view of the first 2 columns of $U\\Sigma$ for the traffic matrix data.\n",
        "\n",
        "These are the strongest patterns occurring across all of the 121 traces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "u, s, vt = np.linalg.svd(Atraf, full_matrices=False)\n",
        "uframe = pd.DataFrame(u @ np.diag(s), index=pd.date_range('9/1/2003', freq = '10min', periods = 1008))\n",
        "uframe[0].plot(color='r', label='Column 1')\n",
        "uframe[1].plot(label='Column 2')\n",
        "plt.legend(loc='best')\n",
        "plt.title('First Two Columns of $U$')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Low Rank Defines Latent Factors\n",
        "\n",
        "The next interpretation of low-rank behavior is that it exposes \"latent factors\" that describe the data.\n",
        "\n",
        ":::: {.fragment}\n",
        "In this interpretation, we think of each element of $A^{(k)}=U'\\Sigma'(V')^T$ as the inner product of a row of $U'\\Sigma'$ and a column of $(V')^{T}$ (equivalently a row of $V'$).\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "Let's say we are working with a matrix of users and items.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "In particular, let the items be movies and matrix entries be ratings, as in the Netflix prize.\n",
        "::::\n",
        "\n",
        "## Latent Factors: Netflix example\n",
        "\n",
        "Recall the structure from earlier:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\vdots & \\vdots &  & \\vdots \\\\\n",
        "\\mathbf{a}_{1} & \\mathbf{a}_{2} & \\cdots & \\mathbf{a}_{n} \\\\\n",
        "\\vdots & \\vdots &  & \\vdots \\\\\n",
        "\\end{bmatrix} \n",
        "\\approx\n",
        "\\underbrace{\n",
        "\\begin{bmatrix}\n",
        "\\vdots &  \\vdots  \\\\\n",
        "\\sigma_1 \\mathbf{u}_1 & \\sigma_k \\mathbf{u}_{k} \\\\\n",
        "\\vdots& \\vdots  \\\\\n",
        "\\end{bmatrix}\n",
        "}_{\\tilde{U}\\in\\mathbb{R}^{m\\times k}}\n",
        "\\underbrace{\n",
        "\\begin{bmatrix}\n",
        "\\cdots & \\mathbf{v}_{1}^{T} & \\cdots   \\\\\n",
        "\\cdots & \\mathbf{v}_{k}^{T} & \\cdots   \\\\\n",
        "\\end{bmatrix}\n",
        "}_{\\tilde{V}\\in\\mathbb{R}^{k\\times n}},\n",
        "$$\n",
        "\n",
        "where the rows of $A$ are the users and the columns are movie ratings.\n",
        "\n",
        "Then the rating that a user gives a movie is the inner product of a $k$-element vector that corresponds to the user, and a $k$-element vector that corresponds to the movie.\n",
        "\n",
        "In other words we have:\n",
        "    \n",
        "$$ \n",
        "a_{ij} = \\sum_{p=1}^{k} \\tilde{U}_{ip} \\tilde{V}_{pj}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "We can think of user $i$'s preferences as being captured by row $i$ of $\\tilde{U}$, which is a point in $\\mathbb{R}^k$.  \n",
        "\n",
        ":::: {.fragment}\n",
        "We have described everything we need to know to predict user $i$'s ratings via a $k$-element vector.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "The $k$-element vector is called a __latent factor.__\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "Likewise, we can think of column $j$ of $\\tilde{V}$ as a \"description\" of movie $j$ (another latent factor).\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "The value in using latent factors comes from the summarization of user preferences, and the predictive power one obtains.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "For example, the winning entry in the Netflix prize competition modeled user preferences with 20 latent factors.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "The remarkable thing is that a person's preferences for all 18,000 movies can be reasonably well captured in a vector of dimension 20!\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "Here is a figure from the paper that described the winning strategy in the Netflix prize.\n",
        "\n",
        "It shows a hypothetical __latent space__ in which each user, and each movie, is represented by a latent vector.\n",
        "\n",
        "![](figs/L10-Movie-Latent-Space.png){fig-align=\"center\" width=\"45%\"}\n",
        "\n",
        "Source: Koren et al, IEEE Computer, 2009 \n",
        "\n",
        "In practice, this is perhaps a 20- or 40-dimensional space.\n",
        "\n",
        "---\n",
        "\n",
        "Here are some representations of movies in that space (reduced to 2-D).\n",
        "\n",
        "Notice how the space seems to capture similarity among movies!\n",
        "\n",
        "![](figs/L10-Netflix-Latent-Factors.png){fig-align=\"center\" width=\"55%\"}\n",
        "\n",
        "Source: Koren et al, IEEE Computer, 2009 \n",
        "\n",
        "## Summary\n",
        "\n",
        ":::: {.incremental}\n",
        "* When we are working with data matrices, it is valuable to consider the __effective rank__.\n",
        "* Many (many) datasets in real life show __low effective rank__.\n",
        "* This property can be explored precisely using the Singular Value Decomposition of the matrix.\n",
        "* When low effective rank is present\n",
        "    * the matrix can be compressed with only small loss of accuracy,\n",
        "    * we can extract the *strongest* patterns in the data,\n",
        "    * we can describe each data item in terms of the inner product of __latent factors__.\n",
        "::::"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}