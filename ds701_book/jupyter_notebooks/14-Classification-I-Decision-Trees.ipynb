{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Decision Trees and Random Forests\"\n",
        "jupyter: python3\n",
        "bibliography: references.bib\n",
        "nocite: |\n",
        "  @Hastie2009\n",
        "---\n",
        "\n",
        "## Outline\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/14-Classification-I-Decision-Trees.ipynb)\n",
        "\n",
        "- Build a decision tree manually\n",
        "- Look at single and collective impurity measures\n",
        "- Selecting splitting attributes and test conditions\n",
        "- Scikit-learn implementation\n",
        "- Model training and evaluation\n",
        "- Bias and Variance\n",
        "- Random forests\n",
        "\n",
        "## Introduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "from sklearn import tree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll now start looking into how to build models to predict an outcome variable from labeled data.\n",
        "\n",
        "**Classification** problems:\n",
        "\n",
        "- predict a category\n",
        "- e.g., spam/not spam, fraud/not fraud, default/not default, malignant/benign, etc.\n",
        "\n",
        "**Regression** problems:\n",
        "\n",
        "- predict a numeric value\n",
        "- e.g., price of a house, salary of a person, etc.\n",
        "\n",
        "\n",
        "## Loan Default Example\n",
        "\n",
        "We'll use an example from [@Tan2018].\n",
        "\n",
        "![](figs/L14-terrier-savings-logo.png){height=\"200px\"}\n",
        "\n",
        "You are a loan officer at **Terrier Savings and Loan**. \n",
        "\n",
        "You have a dataset on loans that you have made in the past.\n",
        "\n",
        "You want to build a model to predict whether a loan will default.\n",
        "\n",
        "## Loans Data Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Check if the directory exists, if not, create it\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "if not os.path.exists('data/loans.csv'):\n",
        "    url = 'https://raw.githubusercontent.com/tools4ds/DS701-Course-Notes/refs/heads/main/ds701_book/data/loans.csv'\n",
        "    urllib.request.urlretrieve(url, 'data/loans.csv')\n",
        "\n",
        "loans = pd.read_csv('data/loans.csv', index_col=0)\n",
        "loans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loans Data Set Summary\n",
        "\n",
        "Here's the summary info of the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loans.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert to Categorical Data Types\n",
        "\n",
        "Since some of the fields are categorical, let's convert them to categorical data types."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "loans['Home Owner'] = loans['Home Owner'].astype('category')\n",
        "loans['Marital Status'] = loans['Marital Status'].astype('category')\n",
        "loans['Defaulted Borrower'] = loans['Defaulted Borrower'].astype('category')\n",
        "loans.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Model\n",
        "\n",
        "Looking at the table, let's just start with the simplest model possible and just\n",
        "predict that no one will default.\n",
        "\n",
        "So the output of our model is just to always predict \"No\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import graphviz\n",
        "\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    node [shape=box, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"values = [# No, # Yes] =[7,3]\\\\ndefaulted = No\\\\nerror = 30%\", fillcolor=\"#ffffff\"] ;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- TODO: Look at this reference to consider pydotplus for size control -->\n",
        "\n",
        "\n",
        "We see a 30% error rate since 3 out of 10 loans defaulted.\n",
        "\n",
        "---\n",
        "\n",
        "Let's split the data based on the \"Home Owner\" field. (`values = [# No, # Yes]`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import graphviz\n",
        "\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    node [shape=box, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"Home Owner?\\\\n---\\\\nsamples = 10\\\\nvalues = [7, 3]\\\\ndefaulted = No\\\\nerror = 30%\", fillcolor=\"#ffffff\"] ;\n",
        "    1 [label=\"samples = 3\\\\nvalue = [3, 0]\\\\ndefaulted = No\\\\nerror = 0%\", fillcolor=\"#e58139\"] ;\n",
        "    2 [label=\"samples = 7\\\\nvalue = [4, 3]\\\\ndefaulted = No\\\\nerror = 43%\", fillcolor=\"#ffffff\"] ;\n",
        "    0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"Yes\"] ;\n",
        "    0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"No\"] ;}\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.fragment}\n",
        "We see that the left node (`Home Owner == Yes`) has a 0% error rate since all the samples are `Defaulted == No`. We don't split this node since all the samples are of the same class. We call this node a **leaf node** and we'll color it orange.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "The right node (`Home Owner == No`) has a 43% error rate since 3 out of 7 loans defaulted. \n",
        "\n",
        "Let's split this node into two nodes based on the **Marital Status** field.\n",
        ":::\n",
        "---\n",
        "\n",
        "Let's split on the \"Marital Status\" field.\n",
        "\n",
        "We see that the 3 defaulted loans are all for single or divorced people. Since the node is\n",
        "all one class, we don't split this node and we call it a **leaf node**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import graphviz\n",
        "\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    node [shape=box, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"Home Owner?\\\\n---\\\\nsamples = 10\\\\nvalues = [7, 3]\\\\ndefaulted = No\\\\nerror = 30%\", fillcolor=\"#ffffff\"] ;\n",
        "    1 [label=\"samples = 3\\\\nvalue = [3, 0]\\\\ndefaulted = No\\\\nerror = 0%\", fillcolor=\"#e58139\"] ;\n",
        "    2 [label=\"Marital Status?\\\\n---\\\\nsamples = 7\\\\nvalue = [4, 3]\\\\ndefaulted = Yes\", fillcolor=\"#ffffff\"] ;\n",
        "    3 [label=\"samples = 4\\\\nvalue = [1, 3]\\\\ndefaulted = Yes\\\\nerror = 25%\", fillcolor=\"#ffffff\"] ;\n",
        "    4 [label=\"samples = 3\\\\nvalue = [3, 0]\\\\ndefaulted = No\\\\nerror = 0%\", fillcolor=\"#e58139\"] ;\n",
        "    0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"Yes\"] ;\n",
        "    0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"No\"] ;\n",
        "    2 -> 3 [labeldistance=2.5, labelangle=45, headlabel=\"Single,\\\\nDivorced\"] ;\n",
        "    2 -> 4 [labeldistance=2.5, labelangle=-45, headlabel=\"Married\"] ;}\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- \n",
        "\n",
        "We can list the subsets for the two criteria to calculate the error rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'Table: Home Owner == No and Marital Status == Single or Divorced --> Defaulted == Yes'\n",
        "#| fig-cap-location: top\n",
        "loans[(loans['Home Owner'] == \"No\") & (loans['Marital Status'].isin(['Single', 'Divorced']))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.fragment}\n",
        "Error rate for predicting `Defaulted == Yes` is 25%.\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "and..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: 'Table: Home Owner == No and Marital Status == Married --> Defaulted == No'\n",
        "#| fig-cap-location: top\n",
        "loans[(loans['Home Owner'] == \"No\") & (loans['Marital Status'] == \"Married\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.fragment}\n",
        "Error rate for predicting `Defaulted == No` is 0%.\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "* Let's try to split on the \"Annual Income\" field. \n",
        "\n",
        "* We see that the person with income of 70K doesn't default, so we split the node into two nodes based on the \"Income\" field. \n",
        "\n",
        "* We arbitrarily pick a threshold of $75K.\n",
        ":::\n",
        "::: {.column width=\"50%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import graphviz\n",
        "\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    graph [size=\"8,8\"];\n",
        "    node [shape=box, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"Home Owner\\\\n---\\\\nsamples = 10\\\\nvalues = [7, 3]\\\\ndefaulted = No\\\\nerror = 30%\", fillcolor=\"#ffffff\"] ;\n",
        "    1 [label=\"samples = 3\\\\nvalue = [3, 0]\\\\ndefaulted = No\\\\nerror = 0%\", fillcolor=\"#e58139\"] ;\n",
        "    2 [label=\"Marital Status\\\\n---\\\\nsamples = 7\\\\nvalue = [4, 3]\\\\ndefaulted = Yes\", fillcolor=\"#ffffff\"] ;\n",
        "    3 [label=\"Income <= 75K\\\\nsamples = 4\\\\nvalue = [1, 3]\\\\ndefaulted = Yes\\\\nerror = 25%\", fillcolor=\"#ffffff\"] ;\n",
        "    4 [label=\"samples = 3\\\\nvalue = [3, 0]\\\\ndefaulted = No\\\\nerror = 0%\", fillcolor=\"#e58139\"] ;\n",
        "    5 [label=\"samples = 1\\\\nvalue = [1, 0]\\\\ndefaulted = No\\\\nerror = 0%\", fillcolor=\"#e58139\"] ;\n",
        "    6 [label=\"samples = 3\\\\nvalue = [0, 3]\\\\ndefaulted = Yes\\\\nerror = 0%\", fillcolor=\"#e58139\"] ;\n",
        "    0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"Yes\"] ;\n",
        "    0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"No\"] ;\n",
        "    2 -> 3 [labeldistance=2.5, labelangle=45, headlabel=\"Single,\\\\nDivorced\"] ;\n",
        "    2 -> 4 [labeldistance=2.5, labelangle=-45, headlabel=\"Married\"] ;\n",
        "    3 -> 5 [labeldistance=2.5, labelangle=45, headlabel=\"Yes\"] ;\n",
        "    3 -> 6 [labeldistance=2.5, labelangle=-45, headlabel=\"No\"] ;}\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::::\n",
        "\n",
        "## Evaluating the Model\n",
        "\n",
        "::: {.incremental}\n",
        "* We've dispositioned every data point by walking down the tree to a leaf node.\n",
        "\n",
        "* How do we know if this tree is good? \n",
        "\n",
        "    * We arbitrarily picked the order of the fields to split on.\n",
        "\n",
        "* Is there a way to systematically pick the order of the fields to split on? \n",
        "\n",
        "    * This is called the **splitting criterion**.\n",
        "\n",
        "* There's also the question of when to stop splitting, or the **stopping criterion**. \n",
        "\n",
        "* So far, we stopped splitting when we reached a node of pure class but there are \n",
        "reasons to stop splitting even without pure classes, which we'll see later.\n",
        ":::\n",
        "\n",
        "## Specifying the Test Condition\n",
        "\n",
        "Before we continue, we should take a moment to consider how we specify a test condition of a node.\n",
        "\n",
        "How we specify a test condition depends on the attribute type which can be:\n",
        "\n",
        "* Binary (Boolean)\n",
        "* Nominal (Categorical, e.g., cat, dog, bird)\n",
        "* Ordinal (e.g., Small, Medium, Large)\n",
        "* Continuous (e.g., 1.5, 2.1, 3.7)\n",
        "\n",
        "And depends on the number of ways to split:\n",
        "\n",
        "* __multi-way__\n",
        "* __binary__\n",
        "\n",
        "---\n",
        "\n",
        "For a __Nominal (Categorical)__ attribute:\n",
        "\n",
        "* In a __Multi-way split__ we can use as many partitions as there are distinct values of the attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import graphviz\n",
        "\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    node [shape=oval, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"Marital Status\", fillcolor=\"#ffffff\"] ;\n",
        "    1 [label=\"Single\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    2 [label=\"Divorced\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    3 [label=\"Married\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    0 -> 1 ;\n",
        "    0 -> 2 ;\n",
        "    0 -> 3 ; }\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "For a __Nominal (Categorical)__ attribute:\n",
        "\n",
        "* In a __Binary split__ we divide the values into two groups.  \n",
        "\n",
        "* In this case, we need to find an optimal partitioning of values into groups, which we discuss shortly.\n",
        "\n",
        "::: {layout-ncol=3}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import graphviz\n",
        "\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    node [shape=oval, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"Marital Status\", fillcolor=\"#ffffff\"] ;\n",
        "    1 [label=\"Single,\\\\nDivorced\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    2 [label=\"Married\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    0 -> 1 ;\n",
        "    0 -> 2 ; }\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    node [shape=oval, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"Marital Status\", fillcolor=\"#ffffff\"] ;\n",
        "    1 [label=\"Single\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    2 [label=\"Married,\\\\nDivorced\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    0 -> 1 ;\n",
        "    0 -> 2 ; }\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    node [shape=oval, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"Marital Status\", fillcolor=\"#ffffff\"] ;\n",
        "    1 [label=\"Single,\\\\nMarried\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    2 [label=\"Divorced\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    0 -> 1 ;\n",
        "    0 -> 2 ; }\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "For an __Ordinal__ attribute, we can use a multi-way split with as many partitions\n",
        "as there are distinct values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    node [shape=oval, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"Shirt\\\\nSize\", fillcolor=\"#ffffff\"] ;\n",
        "    1 [label=\"Small\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    2 [label=\"Medium\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    3 [label=\"Large\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    4 [label=\"X-Large\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    0 -> 1 ;\n",
        "    0 -> 2 ;\n",
        "    0 -> 3 ;\n",
        "    0 -> 4 ; }\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Or we can use a binary split as long we preserve the ordering of the values.\n",
        "\n",
        "::: {layout-ncol=2}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    node [shape=oval, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"Shirt\\\\nSize\", fillcolor=\"#ffffff\"] ;\n",
        "    1 [label=\"Small,\\\\nMedium\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    2 [label=\"Large,\\\\nX-Large\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    0 -> 1 ;\n",
        "    0 -> 2 ; }\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    node [shape=oval, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"Shirt\\\\nSize\", fillcolor=\"#ffffff\"] ;\n",
        "    1 [label=\"Small\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    2 [label=\"Medium, Large,\\\\nX-Large\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    0 -> 1 ;\n",
        "    0 -> 2 ; }\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.callout-warning}\n",
        "Be careful not to violate the ordering of values such as {Small, Large} and {Medium, X-Large}.\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "A __Continuous__ attribute can be handled two ways:\n",
        "\n",
        "::: {layout-ncol=2}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-cap: 'It can be thresholded to form a binary split.'\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    node [shape=oval, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"Income\\\\n<= 75K\", fillcolor=\"#ffffff\"] ;\n",
        "    1 [label=\"Yes\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    2 [label=\"No\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    0 -> 1 ;\n",
        "    0 -> 2 ; }\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-cap: 'Or it can be split into contiguous ranges to form an ordinal categorical attribute.'\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    node [shape=oval, style=\"filled, rounded\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"Income\", fillcolor=\"#ffffff\"] ;\n",
        "    1 [label=\"< 10K\", fillcolor=\"#ffffff\"shape=\"none\"] ;\n",
        "    2 [label=\"[10K, 25K)\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    3 [label=\"[25K, 50K)\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    4 [label=\"[50K, 75K)\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    5 [label=\"> 75K\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    0 -> 1 ;\n",
        "    0 -> 2 ;\n",
        "    0 -> 3 ;\n",
        "    0 -> 4 ;\n",
        "    0 -> 5 ; }\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "\n",
        "Note that finding good partitions for $k$ nominal attributes can be expensive, $\\mathcal{O}(2^k)$, \n",
        "possibly involving combinatorial searching of groupings.  \n",
        "\n",
        "However for ordinal or continuous attributes, sweeping through a range of $n$\n",
        "threshold values can be more efficient if $n \\approx k$. $\\mathcal{O}(n)$ for a sorted list.\n",
        ":::\n",
        "\n",
        "## Selecting Attribute and Test Condition\n",
        "\n",
        "::: {.incremental}\n",
        "* Ideally, we want to pick attributes and test conditions that maximize the homogeneity of the splits.\n",
        "\n",
        "* We can use an **impurity index** to measure the homogeneity in a node.\n",
        "\n",
        "* We'll look at ways of measuring impurity of a node and then collective impurity of its child nodes.\n",
        ":::\n",
        "\n",
        "## Impurity Measures\n",
        "\n",
        "The following are three impurity indices:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\textnormal{Gini index} &= 1 - \\sum_{i=0}^{c-1}  p_i(t)^2 \\\\\n",
        "\\textnormal{Entropy} &= -\\sum_{i=0}^{c-1}  p_i(t) \\log_2 p_i(t) \\\\\n",
        "\\textnormal{Classification error} &= 1 - \\max_i p_i(t)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $p_i(t)$ is the **relative frequency** of training instances of class $i$ at a node $t$ and $c$ is the number of classes.\n",
        "\n",
        "::: {.callout-note}\n",
        "By convention, we set $0 \\log_2 0 = 0$ in entropy calculations.\n",
        ":::\n",
        "\n",
        "\n",
        "## Impurity Measures\n",
        "\n",
        "The following are three impurity indices:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\textnormal{Gini index} &= 1 - \\sum_{i=0}^{c-1}  p_i(t)^2 \\\\\n",
        "\\textnormal{Entropy} &= -\\sum_{i=0}^{c-1}  p_i(t) \\log_2 p_i(t) \\\\\n",
        "\\textnormal{Classification error} &= 1 - \\max_i p_i(t)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "All three impurity indices equal 0 when all the records at a node belong to the same class.\n",
        "\n",
        "All three impurity indices reach their maximum value when the classes are evenly distributed among the child nodes.\n",
        "\n",
        "---\n",
        "\n",
        "We can plot the three impurity indices to get a sense of how they behave for **binary classification** problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define probability values\n",
        "p = np.linspace(0, 1, 100)\n",
        "\n",
        "# Calculate impurity measures\n",
        "entropy = -p * np.log2(p + 1e-9) - (1 - p) * np.log2(1 - p + 1e-9)  # Adding small epsilon to avoid log(0)\n",
        "gini = 2 * p * (1 - p)\n",
        "misclassification_error = 1 - np.maximum(p, 1 - p)\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(p, entropy, label='Entropy', linewidth=2)\n",
        "plt.plot(p, gini, '--', label='Gini', linewidth=2)\n",
        "plt.plot(p, misclassification_error, '-.', label='Misclassification error', linewidth=2)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('p')\n",
        "plt.ylabel('Impurity')\n",
        "plt.title('Comparison among the impurity measures for binary classification problems')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "They all maintain the same ordering for every relative frequency, i.e., Entropy > Gini > Misclassification error.\n",
        "\n",
        "\n",
        "## Impurity Example 1\n",
        "\n",
        "\n",
        "\n",
        "| Node $N_1$ | Count | p |\n",
        "| --- | --- | --- |\n",
        "| Class=0 | 0 | $0/6 = 0$ |\n",
        "| Class=1 | 6 | $6/6 = 1$ |\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\textnormal{Gini} &= 1 - \\left(\\frac{0}{6}\\right)^2 - \\left(\\frac{6}{6}\\right)^2 = 0 \\\\\n",
        "\\textnormal{Entropy} &= -\\left(\\frac{0}{6} \\log_2 \\frac{0}{6} + \\frac{6}{6} \\log_2 \\frac{6}{6}\\right) = 0 \\\\\n",
        "\\textnormal{Error} &= 1 - \\max\\left[\\frac{0}{6}, \\frac{6}{6}\\right] = 0\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## Impurity Example 2\n",
        "\n",
        "\n",
        "| Node $N_2$ | Count | p |\n",
        "| --- | --- | --- |\n",
        "| Class=0 | 1 | $1/6 = 0.167$ |\n",
        "| Class=1 | 5 | $5/6 = 0.833$ |\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\textnormal{Gini} &= 1 - \\left(\\frac{1}{6}\\right)^2 - \\left(\\frac{5}{6}\\right)^2 = 0.278 \\\\\n",
        "\\textnormal{Entropy} &= -\\left(\\frac{1}{6} \\log_2 \\frac{1}{6} + \\frac{5}{6} \\log_2 \\frac{5}{6}\\right) = 0.650 \\\\\n",
        "\\textnormal{Error} &= 1 - \\max\\left[\\frac{1}{6}, \\frac{5}{6}\\right] = 0.167\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "## Impurity Example 3\n",
        "\n",
        "\n",
        "| Node $N_3$ | Count |\n",
        "| --- | --- |\n",
        "| Class=0 | 3 |\n",
        "| Class=1 | 3 |\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\textnormal{Gini} &= 1 - \\left(\\frac{3}{6}\\right)^2 - \\left(\\frac{3}{6}\\right)^2 = 0.5 \\\\\n",
        "\\textnormal{Entropy} &= -\\left(\\frac{3}{6} \\log_2 \\frac{3}{6} + \\frac{3}{6} \\log_2 \\frac{3}{6}\\right) = 1 \\\\\n",
        "\\textnormal{Error} &= 1 - \\max\\left[\\frac{3}{6}, \\frac{3}{6}\\right] = 0.5\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "## Impurity Class Exercise\n",
        "\n",
        "Calculate Gini impurity for the following node:\n",
        "\n",
        "<br>\n",
        "\n",
        "| Node $N_4$ | Count | p |\n",
        "| --- | --- | --- |\n",
        "| Class=0 | 3 |  |\n",
        "| Class=1 | 6 |  |\n",
        "| Class=2 | 1 |  |\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "\\textnormal{Gini} = \n",
        "$$\n",
        "\n",
        "<!--\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\textnormal{Gini} &= 1 - \\left(\\frac{3}{10}\\right)^2 - \\left(\\frac{6}{10}\\right)^2 - \\left(\\frac{1}{10}\\right)^2 \\\\\n",
        "&= 1 - \\left(\\frac{9}{100}\\right) - \\left(\\frac{36}{10}\\right) - \\left(\\frac{1}{100}\\right) \\\\\n",
        "&= 1 - 0.09 - 0.36 - 0.01 \\\\\n",
        "&= 0.54\n",
        "\\end{aligned}\n",
        "$$\n",
        "-->\n",
        "\n",
        "\n",
        "## Collective Impurity of Child Nodes\n",
        "\n",
        "We can compute the collective impurity of child nodes by taking a weighted sum of the impurities of the child nodes.\n",
        "\n",
        "$$\n",
        "I(\\textnormal{children}) = \\sum_{j=1}^{k} \\frac{N(v_j)}{N}\\; I(v_j)\n",
        "$$\n",
        "\n",
        "Here we split $N$ training instances into $k$ child nodes, $v_j$ for $j=1, \\ldots, k$.\n",
        "\n",
        "$N(v_j)$ is the number of training instances at child node $v_j$ and $I(v_j)$ is the impurity at child node $v_j$.\n",
        "\n",
        "## Impurity Example\n",
        "\n",
        "Let's compute collective impurity on our loans dataset to see which feature to split on.\n",
        "\n",
        "::: {layout-ncol=3}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-cap: '(a) Collective Entropy: 0.690'\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    node [shape=oval, style=\"filled\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"Home\\\\nOwner\", fillcolor=\"#ffffff\"] ;\n",
        "    1 [label=\"Yes: 0\\\\nNo: 3\", fillcolor=\"#ffffff\", shape=\"square\"] ;\n",
        "    2 [label=\"Yes: 3\\\\nNo: 4\", fillcolor=\"#ffffff\", shape=\"square\"] ;\n",
        "    0 -> 1 [labeldistance=2.5, labelangle=45, headlabel=\"Yes\"] ;\n",
        "    0 -> 2 [labeldistance=2.5, labelangle=-45, headlabel=\"No\"] ; }\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-cap: '(b) Collective Entropy: 0.686'\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    node [shape=oval, style=\"filled\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"Marital\\\\nStatus\", fillcolor=\"#ffffff\"] ;\n",
        "    1 [label=\"Yes: 2\\\\nNo: 3\", fillcolor=\"#ffffff\", shape=\"square\"] ;\n",
        "    2 [label=\"Yes: 0\\\\nNo: 3\", fillcolor=\"#ffffff\", shape=\"square\"] ;\n",
        "    3 [label=\"Yes: 1\\\\nNo: 1\", fillcolor=\"#ffffff\", shape=\"square\"] ;\n",
        "    0 -> 1 [xlabel=\"Single\"] ;\n",
        "    0 -> 2 [label=\"Married\"] ;\n",
        "    0 -> 3 [xlabel=\"Divorced\"] ; }\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-cap: '(c) Collective Entropy index: 0.00'\n",
        "dot_data = \"\"\"\n",
        "digraph Tree {\n",
        "    node [shape=oval, style=\"filled\", color=\"black\", fontname=\"helvetica\"] ;\n",
        "    edge [fontname=\"helvetica\"] ;\n",
        "    0 [label=\"ID\", fillcolor=\"#ffffff\"] ;\n",
        "    1 [label=\"Yes: 0\\\\nNo: 1\", fillcolor=\"#ffffff\", shape=\"square\"] ;\n",
        "    2 [label=\"...\", fillcolor=\"#ffffff\", shape=\"none\"] ;\n",
        "    3 [label=\"Yes: 1\\\\nNo: 0\", fillcolor=\"#ffffff\", shape=\"square\"] ;\n",
        "    4 [label=\"Yes: 0\\\\nNo: 1\", fillcolor=\"#ffffff\", shape=\"square\"] ;\n",
        "    5 [label=\"Yes: 1\\\\nNo: 0\", fillcolor=\"#ffffff\", shape=\"square\"] ;\n",
        "    0 -> 1 [xlabel=\"1\"] ;\n",
        "    0 -> 2 [color=\"white\"] ;\n",
        "    0 -> 3 [xlabel=\"8\"] ;\n",
        "    0 -> 4 [xlabel=\"9\"] ;\n",
        "    0 -> 5 [xlabel=\"10\"] ; }\n",
        "\"\"\"\n",
        "\n",
        "# Use graphviz to render the dot file\n",
        "graph = graphviz.Source(dot_data)  \n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.callout-tip}\n",
        "Try calculating the collective Entropy for (a) and (b) and see if you get the same values.\n",
        ":::\n",
        "\n",
        "::: {.callout-important}\n",
        "The collective entropy for (c) is 0. Why would we not want to use this node?\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "There are two ways to overcome this problem. \n",
        "\n",
        "1. One way is to _generate only binary decision trees_, thus avoiding the difficulty of handling attributes with varying\n",
        "   number of partitions. This strategy is employed by decision tree classifiers such as **CART**. \n",
        "2. Another way is to modify the splitting criterion to take into account the number of partitions produced by the\n",
        "   attribute. For example, in the **C4.5** decision tree algorithm, a measure known as **gain ratio** is used to compensate\n",
        "   for attributes that produce a large number of child nodes.\n",
        "\n",
        "::: aside\n",
        "CART stands for Classification And Regression Tree.\n",
        ":::\n",
        "\n",
        "## Gain Ratio\n",
        "\n",
        "See [@Tan2018, Chap. 3, p. 127]:\n",
        "\n",
        "* Having a low impurity value alone is insufficient to find a good attribute test condition for a node. \n",
        "* Having more child nodes can make a decision tree more complex and consequently more susceptible to overfitting. \n",
        "\n",
        "Hence, the number of children produced by the splitting attribute should also be taken into consideration while deciding the best attribute test condition. \n",
        "\n",
        "## Gain Ratio Formula\n",
        "\n",
        "$$\n",
        "\\text{Gain ratio} = \\frac{\\Delta_{\\text{info}}}{\\text{Split Info}} = \\frac{\\text{Entropy(Parent)} - \\sum_{i=1}^{k} \\frac{N(v_i)}{N} \\text{Entropy}(v_i)}{- \\sum_{i=1}^{k} \\frac{N(v_i)}{N} \\log_2 \\frac{N(v_i)}{N}}\n",
        "$$\n",
        "\n",
        "where $N(v_i)$ is the number of instances assigned to node $v_i$ and $k$ is the total number of splits. \n",
        "\n",
        "The split information measures the entropy of splitting a node into its child nodes and evaluates if the split results\n",
        "in a larger number of equally-sized child nodes or not.\n",
        "\n",
        "## Gain Ratio Example\n",
        "\n",
        "Let's compare the Gain Ratio for **Home Owner** and **Marital Status** attributes using the loans dataset.\n",
        "\n",
        "Recall from the earlier example that the parent node has 3 Yes and 7 No defaulters (10 total instances).\n",
        "\n",
        "**Parent node entropy:**\n",
        "\n",
        "$$\n",
        "\\text{Entropy(Parent)} = -\\frac{3}{10} \\log_2 \\frac{3}{10} - \\frac{7}{10} \\log_2 \\frac{7}{10} = 0.881\n",
        "$$\n",
        "\n",
        "## Ex: Gain Ratio for Home Owner Attribute\n",
        "\n",
        "The Home Owner attribute splits the data into 2 child nodes:\n",
        "\n",
        "- Yes: 0 Yes, 3 No (3 instances)\n",
        "- No: 3 Yes, 4 No (7 instances)\n",
        "\n",
        "From the earlier calculation, the Information Gain is:\n",
        "\n",
        "$$\n",
        "\\Delta_{\\text{info}} = 0.881 - \\left(\\frac{3}{10} \\times 0 + \\frac{7}{10} \\times 0.985\\right) = 0.192\n",
        "$$\n",
        "\n",
        "Now we calculate the Split Information:\n",
        "\n",
        "$$\n",
        "\\text{Split Info} = -\\frac{3}{10} \\log_2 \\frac{3}{10} - \\frac{7}{10} \\log_2 \\frac{7}{10} = 0.881\n",
        "$$\n",
        "\n",
        "Therefore, the Gain Ratio is:\n",
        "\n",
        "$$\n",
        "\\text{Gain Ratio(Home Owner)} = \\frac{0.192}{0.881} = 0.218\n",
        "$$\n",
        "\n",
        "## Ex: Gain Ratio for Marital Status Attribute\n",
        "\n",
        "The Marital Status attribute splits the data into 3 child nodes:\n",
        "\n",
        "- Single: 2 Yes, 3 No (5 instances)\n",
        "- Married: 0 Yes, 3 No (3 instances)\n",
        "- Divorced: 1 Yes, 1 No (2 instances)\n",
        "\n",
        "From the earlier calculation, the Information Gain is:\n",
        "\n",
        "$$\n",
        "\\Delta_{\\text{info}} = 0.881 - \\left(\\frac{5}{10} \\times 0.971 + \\frac{3}{10} \\times 0 + \\frac{2}{10} \\times 1\\right) = 0.195\n",
        "$$\n",
        "\n",
        "Now we calculate the Split Information:\n",
        "\n",
        "$$\n",
        "\\text{Split Info} = -\\frac{5}{10} \\log_2 \\frac{5}{10} - \\frac{3}{10} \\log_2 \\frac{3}{10} - \\frac{2}{10} \\log_2 \\frac{2}{10} = 1.486\n",
        "$$\n",
        "\n",
        "Therefore, the Gain Ratio is:\n",
        "\n",
        "$$\n",
        "\\text{Gain Ratio(Marital Status)} = \\frac{0.195}{1.486} = 0.131\n",
        "$$\n",
        "\n",
        "## Example: Comparison of Gain Ratios\n",
        "\n",
        "| Attribute | Information Gain | Split Info | Gain Ratio |\n",
        "|-----------|-----------------|------------|------------|\n",
        "| Home Owner | 0.192 | 0.881 | **0.218** |\n",
        "| Marital Status | 0.195 | 1.486 | 0.131 |\n",
        "\n",
        "Although Marital Status has a slightly higher Information Gain, it has a **lower Gain Ratio** because it splits the data into 3 child nodes rather than 2. \n",
        "\n",
        "The Gain Ratio penalizes attributes that create more splits, making **Home Owner** the preferred choice.\n",
        "\n",
        "## Identifying the Best Attribute Test Condition\n",
        "\n",
        "![](figs/L14-splitting-criteria-gini.png){width=\"70%\" fig-align=\"center\"}\n",
        "\n",
        "Here's an example of how to identify the best attribute test condition using the Collective Impurity of\n",
        "the Gini Impurity index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def gini_impurity(yes_count, no_count):\n",
        "    \"\"\"Calculate Gini impurity for a node\"\"\"\n",
        "    total = yes_count + no_count\n",
        "    if total == 0:\n",
        "        return 0\n",
        "    \n",
        "    p_yes = yes_count / total\n",
        "    p_no = no_count / total\n",
        "    \n",
        "    gini = 1 - (p_yes**2 + p_no**2)\n",
        "    return gini\n",
        "\n",
        "def weighted_gini(n1_yes, n1_no, n2_yes, n2_no):\n",
        "    \"\"\"Calculate weighted average Gini impurity after split\"\"\"\n",
        "    n1_total = n1_yes + n1_no\n",
        "    n2_total = n2_yes + n2_no\n",
        "    total = n1_total + n2_total\n",
        "    \n",
        "    gini_n1 = gini_impurity(n1_yes, n1_no)\n",
        "    gini_n2 = gini_impurity(n2_yes, n2_no)\n",
        "    \n",
        "    weighted_gini = (n1_total/total) * gini_n1 + (n2_total/total) * gini_n2\n",
        "    return weighted_gini\n",
        "\n",
        "# Parent node\n",
        "parent_yes = 3\n",
        "parent_no = 7\n",
        "parent_gini = gini_impurity(parent_yes, parent_no)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"PARENT NODE\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Yes: {parent_yes}, No: {parent_no}\")\n",
        "print(f\"Gini Impurity: {parent_gini:.3f}\")\n",
        "print()\n",
        "\n",
        "# Define all four splits\n",
        "splits = {\n",
        "    \"Home Owner\": {\n",
        "        \"N1\": {\"yes\": 0, \"no\": 3},  # Yes (owns home)\n",
        "        \"N2\": {\"yes\": 3, \"no\": 4}   # No (doesn't own home)\n",
        "    },\n",
        "    \"Marital Status (Split 1: Single vs Married+Divorced)\": {\n",
        "        \"N1\": {\"yes\": 2, \"no\": 3},  # Single\n",
        "        \"N2\": {\"yes\": 1, \"no\": 4}   # Married, Divorced\n",
        "    },\n",
        "    \"Marital Status (Split 2: Single+Married vs Divorced)\": {\n",
        "        \"N1\": {\"yes\": 2, \"no\": 6},  # Single, Married\n",
        "        \"N2\": {\"yes\": 1, \"no\": 1}   # Divorced\n",
        "    },\n",
        "    \"Marital Status (Split 3: Single+Divorced vs Married)\": {\n",
        "        \"N1\": {\"yes\": 3, \"no\": 4},  # Single, Divorced\n",
        "        \"N2\": {\"yes\": 0, \"no\": 3}   # Married\n",
        "    }\n",
        "}\n",
        "\n",
        "# Calculate for each split\n",
        "results = []\n",
        "for split_name, nodes in splits.items():\n",
        "    n1_yes = nodes[\"N1\"][\"yes\"]\n",
        "    n1_no = nodes[\"N1\"][\"no\"]\n",
        "    n2_yes = nodes[\"N2\"][\"yes\"]\n",
        "    n2_no = nodes[\"N2\"][\"no\"]\n",
        "    \n",
        "    gini_n1 = gini_impurity(n1_yes, n1_no)\n",
        "    gini_n2 = gini_impurity(n2_yes, n2_no)\n",
        "    weighted_gini_value = weighted_gini(n1_yes, n1_no, n2_yes, n2_no)\n",
        "    gini_gain = parent_gini - weighted_gini_value\n",
        "    \n",
        "    results.append({\n",
        "        'name': split_name,\n",
        "        'weighted_gini': weighted_gini_value,\n",
        "        'gini_gain': gini_gain,\n",
        "        'gini_n1': gini_n1,\n",
        "        'gini_n2': gini_n2,\n",
        "        'n1_yes': n1_yes,\n",
        "        'n1_no': n1_no,\n",
        "        'n2_yes': n2_yes,\n",
        "        'n2_no': n2_no\n",
        "    })\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(f\"SPLIT: {split_name}\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Node N1: Yes={n1_yes}, No={n1_no}, Total={n1_yes+n1_no}\")\n",
        "    print(f\"  Gini(N1) = {gini_n1:.3f}\")\n",
        "    print(f\"Node N2: Yes={n2_yes}, No={n2_no}, Total={n2_yes+n2_no}\")\n",
        "    print(f\"  Gini(N2) = {gini_n2:.3f}\")\n",
        "    print(f\"\\nWeighted Gini (after split): {weighted_gini_value:.3f}\")\n",
        "    print(f\"Gini Gain: {gini_gain:.3f}\")\n",
        "    print()\n",
        "\n",
        "# Summary: Find best split\n",
        "print(\"=\" * 60)\n",
        "print(\"SUMMARY - BEST SPLIT\")\n",
        "print(\"=\" * 60)\n",
        "results_sorted = sorted(results, key=lambda x: x['gini_gain'], reverse=True)\n",
        "for i, result in enumerate(results_sorted, 1):\n",
        "    print(f\"{i}. {result['name']}\")\n",
        "    print(f\"   Weighted Gini: {result['weighted_gini']:.3f}, Gini Gain: {result['gini_gain']:.3f}\")\n",
        "print()\n",
        "print(f\"Best split: {results_sorted[0]['name']}\")\n",
        "print(f\"Maximum Gini Gain: {results_sorted[0]['gini_gain']:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Splitting Continuous Attributes\n",
        "\n",
        "For quantitative attributes like _Annual Income_, we need to find some threshold $\\tau$ that\n",
        "minimizes the impurity index.\n",
        "\n",
        "The following table illustrates the process.\n",
        "\n",
        "![](figs/L14-splitting-continuous-attribs.png)\n",
        "\n",
        "\n",
        "**Procedure:**\n",
        "\n",
        "1. Sort all the training instances by _Annual Income_ in increasing order.\n",
        "2. Pick thresholds half way between consecutive values.\n",
        "3. Compute the Gini impurity index for each threshold.\n",
        "4. Select the threshold that minimizes the Gini impurity index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def gini_impurity(yes_count, no_count):\n",
        "    \"\"\"Calculate Gini impurity for a node\"\"\"\n",
        "    total = yes_count + no_count\n",
        "    if total == 0:\n",
        "        return 0\n",
        "    \n",
        "    p_yes = yes_count / total\n",
        "    p_no = no_count / total\n",
        "    \n",
        "    gini = 1 - (p_yes**2 + p_no**2)\n",
        "    return gini\n",
        "\n",
        "def weighted_gini_split(left_yes, left_no, right_yes, right_no):\n",
        "    \"\"\"Calculate weighted Gini impurity for a binary split\"\"\"\n",
        "    left_total = left_yes + left_no\n",
        "    right_total = right_yes + right_no\n",
        "    total = left_total + right_total\n",
        "    \n",
        "    if total == 0:\n",
        "        return 0\n",
        "    \n",
        "    gini_left = gini_impurity(left_yes, left_no)\n",
        "    gini_right = gini_impurity(right_yes, right_no)\n",
        "    \n",
        "    weighted_gini = (left_total/total) * gini_left + (right_total/total) * gini_right\n",
        "    return weighted_gini\n",
        "\n",
        "# Data from the table\n",
        "sorted_values = [60, 70, 75, 85, 90, 95, 100, 120, 125, 220]\n",
        "classes = ['No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No']\n",
        "\n",
        "# Calculate split positions (midpoints between consecutive values)\n",
        "split_positions = []\n",
        "for i in range(len(sorted_values) - 1):\n",
        "    split_pos = (sorted_values[i] + sorted_values[i+1]) / 2\n",
        "    split_positions.append(split_pos)\n",
        "\n",
        "# Parent node statistics\n",
        "parent_yes = classes.count('Yes')\n",
        "parent_no = classes.count('No')\n",
        "parent_gini = gini_impurity(parent_yes, parent_no)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CONTINUOUS ATTRIBUTE: Annual Income\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Parent Node: Yes={parent_yes}, No={parent_no}\")\n",
        "print(f\"Parent Gini Impurity: {parent_gini:.3f}\")\n",
        "print()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EVALUATING ALL POSSIBLE SPLITS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "best_split = None\n",
        "best_gini = float('inf')\n",
        "best_gain = -float('inf')\n",
        "\n",
        "results = []\n",
        "\n",
        "for i, split_pos in enumerate(split_positions):\n",
        "    # Count classes on left (<= split) and right (> split)\n",
        "    left_yes = 0\n",
        "    left_no = 0\n",
        "    right_yes = 0\n",
        "    right_no = 0\n",
        "    \n",
        "    for j, value in enumerate(sorted_values):\n",
        "        if value <= split_pos:\n",
        "            if classes[j] == 'Yes':\n",
        "                left_yes += 1\n",
        "            else:\n",
        "                left_no += 1\n",
        "        else:\n",
        "            if classes[j] == 'Yes':\n",
        "                right_yes += 1\n",
        "            else:\n",
        "                right_no += 1\n",
        "    \n",
        "    # Calculate weighted Gini\n",
        "    weighted_gini = weighted_gini_split(left_yes, left_no, right_yes, right_no)\n",
        "    gini_gain = parent_gini - weighted_gini\n",
        "    \n",
        "    results.append({\n",
        "        'split_position': split_pos,\n",
        "        'left_yes': left_yes,\n",
        "        'left_no': left_no,\n",
        "        'right_yes': right_yes,\n",
        "        'right_no': right_no,\n",
        "        'weighted_gini': weighted_gini,\n",
        "        'gini_gain': gini_gain\n",
        "    })\n",
        "    \n",
        "    print(f\"Split at {split_pos:6.1f}:\")\n",
        "    print(f\"  Left  (<=): Yes={left_yes}, No={left_no}, Total={left_yes+left_no}\")\n",
        "    print(f\"  Right ( >): Yes={right_yes}, No={right_no}, Total={right_yes+right_no}\")\n",
        "    print(f\"  Weighted Gini: {weighted_gini:.3f}\")\n",
        "    print(f\"  Gini Gain: {gini_gain:.3f}\")\n",
        "    \n",
        "    if weighted_gini < best_gini:\n",
        "        best_gini = weighted_gini\n",
        "        best_split = split_pos\n",
        "        best_gain = gini_gain\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"BEST SPLIT\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Best Split Position: {best_split}\")\n",
        "print(f\"Minimum Weighted Gini: {best_gini:.3f}\")\n",
        "print(f\"Maximum Gini Gain: {best_gain:.3f}\")\n",
        "\n",
        "# Show the split details\n",
        "best_result = [r for r in results if r['split_position'] == best_split][0]\n",
        "print(f\"\\nSplit Details (Annual Income <= {best_split}):\")\n",
        "print(f\"  Left  (<=): Yes={best_result['left_yes']}, No={best_result['left_no']}\")\n",
        "print(f\"  Right ( >): Yes={best_result['right_yes']}, No={best_result['right_no']}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Decision Tree on Loans Data Set\n",
        "\n",
        "Let's run the Scikit-learn Decision Tree, `sklearn.tree`, on the loans data set.\n",
        "\n",
        "`sklearn.tree` requires all fields to be numeric.\n",
        "\n",
        "So first we have to convert the categorical fields to category index numeric fields."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "loans['Defaulted Borrower'] = loans['Defaulted Borrower'].cat.codes\n",
        "loans['Home Owner'] = loans['Home Owner'].cat.codes\n",
        "loans['Marital Status'] = loans['Marital Status'].cat.codes\n",
        "loans.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Then the independent variables are all the fields except the \"Defaulted Borrower\" field, which we'll assign to `X`.\n",
        "\n",
        "The dependent variable is the \"Defaulted Borrower\" field, which we'll assign to `y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn import tree\n",
        "\n",
        "X = loans.drop('Defaulted Borrower', axis=1)\n",
        "y = loans['Defaulted Borrower']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<br>\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "`X`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "X.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "`y`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "y.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Let's fit a decision tree to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "clf = tree.DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf = clf.fit(X, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's plot the tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "annotations = tree.plot_tree(clf, \n",
        "               filled=True, \n",
        "               rounded=True,\n",
        "               # max_depth=2,\n",
        "               # impurity=False,\n",
        "               feature_names=loans.drop('Defaulted Borrower', axis=1).columns,\n",
        "               class_names=['No', 'Yes'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interestingly, the tree was built using only the Income field.\n",
        "\n",
        "That's arguably an advantage of Decision Trees: they automatically perform feature selection.\n",
        "\n",
        "\n",
        "## Ensemble Methods\n",
        "\n",
        "(See @Tan2018, Chapter 4)\n",
        "\n",
        "Motivated around the idea that combining several noisy classifiers can result in a better prediction\n",
        "under certain conditions.\n",
        "\n",
        "* The base classifiers are independent\n",
        "* The base classifiers are noisy (high variance)\n",
        "* The base classifiers are low (ideally zero) bias\n",
        "\n",
        "## Bias and Variance\n",
        "\n",
        "__Bias__\n",
        "\n",
        "* Definition: Error due to overly simplistic models.\n",
        "* High bias: Model underfits the data.\n",
        "    * Example: Shallow decision trees.\n",
        "* Low bias: Model accurately captures the underlying patterns in the data.\n",
        "    * Example: Deep decision trees.\n",
        "\n",
        "__Variance__\n",
        "\n",
        "* Definition: Error due to overly complex models.\n",
        "* High Variance: Model overfits the data.\n",
        "    * Example: Deep decision trees.\n",
        "* Low variance: Model predictions are stable and consistent across different training datasets.\n",
        "\n",
        "---\n",
        "\n",
        "![](figs/bias_variance_tradeoff.png){fig-align=\"center\" width=\"60%\"}\n",
        "\n",
        "## Bias Variance Trade-Off\n",
        "\n",
        "Goal: Find a balance to minimize total error.\n",
        "\n",
        "Bias-Variance Trade-off: Low bias and low variance are ideal but challenging to achieve simultaneously.\n",
        "\n",
        "![[Source](https://serokell.io/blog/bias-variance-tradeoff)](figs/bias_variance_tradeoff2.png){fig-align=\"center\" width=\"60%\"}\n",
        "\n",
        "## Random Forests\n",
        "\n",
        "Random forests are an ensemble of decision trees that:\n",
        "\n",
        "* Construct a set of base classifiers from random sub-samples of the training data.\n",
        "* Train each base classifier to completion.\n",
        "* Take a majority vote of the base classifiers to form the final prediction.\n",
        "\n",
        "## Titanic Example\n",
        "\n",
        "We'll use the [Titanic data set](https://www.kaggle.com/competitions/titanic) and\n",
        "excerpts of this [Kaggle tutorial](https://www.kaggle.com/code/jhoward/how-random-forests-really-work/)\n",
        "to illustrate the concepts of overfitting and random forests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "\n",
        "# Check if the directory exists, if not, create it\n",
        "if not os.path.exists('data/titanic'):\n",
        "    os.makedirs('data/titanic')\n",
        "\n",
        "if not os.path.exists('data/titanic/train.csv'):\n",
        "    url = 'https://raw.githubusercontent.com/tools4ds/DS701-Course-Notes/refs/heads/main/ds701_book/data/titanic/train.csv'\n",
        "    urllib.request.urlretrieve(url, 'data/titanic/train.csv')\n",
        "\n",
        "df_train = pd.read_csv('data/titanic/train.csv', index_col='PassengerId')\n",
        "\n",
        "if not os.path.exists('data/titanic/test.csv'):\n",
        "    url = 'https://raw.githubusercontent.com/tools4ds/DS701-Course-Notes/refs/heads/main/ds701_book/data/titanic/test.csv'\n",
        "    urllib.request.urlretrieve(url, 'data/titanic/test.csv')\n",
        "\n",
        "df_test = pd.read_csv('data/titanic/test.csv', index_col='PassengerId')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Let's look at the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that there are 891 entries with 11 fields. 'Age', 'Cabin', and 'Embarked' have missing values.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Let's look at the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_test.info()\n",
        "df_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are 418 entries in the test set with same fields except for 'Survived', which is what we need to predict.\n",
        "\n",
        "---\n",
        "\n",
        "We'll do some data cleaning and preparation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "\n",
        "def proc_data(df):\n",
        "    df['Fare'] = df.Fare.fillna(0)\n",
        "    df.fillna(modes, inplace=True) # Fill missing values with the mode\n",
        "    df['LogFare'] = np.log1p(df['Fare'])  # Create a new column for the log of the fare + 1\n",
        "    df['Embarked'] = pd.Categorical(df.Embarked)  # Convert to categorical\n",
        "    df['Sex'] = pd.Categorical(df.Sex)  # Convert to categorical\n",
        "\n",
        "modes = df_train.mode().iloc[0] # Get the mode for each column\n",
        "\n",
        "proc_data(df_train)\n",
        "proc_data(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Look at the dataframes again.\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: {.column width=\"50%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_test.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "We'll create lists of features by type."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "cats=[\"Sex\",\"Embarked\"]  # Categorical\n",
        "conts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]  # Continuous\n",
        "dep=\"Survived\"  # Dependent variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Let's explore some fields starting with survival rate by gender."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "fig,axs = plt.subplots(1,2, figsize=(11,5))\n",
        "sns.barplot(data=df_train, y='Survived', x=\"Sex\", ax=axs[0], hue=\"Sex\", palette=[\"#3374a1\",\"#e1812d\"]).set(title=\"Survival rate\")\n",
        "sns.countplot(data=df_train, x=\"Sex\", ax=axs[1], hue=\"Sex\", palette=[\"#3374a1\",\"#e1812d\"]).set(title=\"Histogram\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Indeed, \"women and children first\" was enforced on the Titanic.\n",
        "\n",
        "---\n",
        "\n",
        "Since we don't have labels for the test data, we'll split the training data into training and validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from numpy import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "random.seed(42)\n",
        "trn_df,val_df = train_test_split(df_train, test_size=0.25)\n",
        "\n",
        "# Replace categorical fields with numeric codes\n",
        "trn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\n",
        "val_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Let's split the independent (input) variables from the dependent (output) variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "def xs_y(df):\n",
        "    xs = df[cats+conts].copy()\n",
        "    return xs,df[dep] if dep in df else None\n",
        "\n",
        "trn_xs,trn_y = xs_y(trn_df)\n",
        "val_xs,val_y = xs_y(val_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Here's the predictions for our extremely simple model, where `female` is coded as `0`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "preds = val_xs.Sex==0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll use mean absolute error to measure how good this model is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mean_absolute_error(val_y, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Alternatively, we could try splitting on a continuous column. We have to use a somewhat different chart to see how this might work -- here's an example of how we could look at `LogFare`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_fare = trn_df[trn_df.LogFare>0]\n",
        "fig,axs = plt.subplots(1,2, figsize=(11,5))\n",
        "sns.boxenplot(data=df_fare, x=dep, y=\"LogFare\", ax=axs[0], hue=dep, palette=[\"#3374a1\",\"#e1812d\"])\n",
        "sns.kdeplot(data=df_fare, x=\"LogFare\", ax=axs[1]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The [boxenplot](https://seaborn.pydata.org/generated/seaborn.boxenplot.html) above shows quantiles of `LogFare` for each group of `Survived==0` and `Survived==1`. \n",
        "\n",
        "It shows that the average `LogFare` for passengers that didn't survive is around `2.5`, and for those that did it's around `3.2`. \n",
        "\n",
        "So it seems that people that paid more for their tickets were more likely to get put on a lifeboat.\n",
        "\n",
        "---\n",
        "\n",
        "Let's create a simple model based on this observation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "preds = val_xs.LogFare>2.7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "...and test it out:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "mean_absolute_error(val_y, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is quite a bit less accurate than our model that used `Sex` as the single binary split.\n",
        "\n",
        "## Full Decision Tree\n",
        "\n",
        "Ok. Let's build a decision tree model using all the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "clf = tree.DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf = clf.fit(trn_xs, trn_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's draw the tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "annotations = tree.plot_tree(clf, \n",
        "               filled=True, \n",
        "               rounded=True,\n",
        "               feature_names=trn_xs.columns,\n",
        "               class_names=['No', 'Yes'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Full Tree -- Evaluation Error\n",
        "\n",
        "Let's see how it does on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "preds = clf.predict(val_xs)\n",
        "mean_absolute_error(val_y, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::: {.fragment}\n",
        "That is quite a bit worse than splitting on `Sex` alone!!\n",
        "::::\n",
        "\n",
        "## Stopping Criteria -- Minimum Samples Split\n",
        "\n",
        "Let's train the decision tree again but with stopping criteria based on the number of samples in a node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "clf = tree.DecisionTreeClassifier(criterion='gini', random_state=42, min_samples_split=20)\n",
        "clf = clf.fit(trn_xs, trn_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's draw the tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "annotations = tree.plot_tree(clf, \n",
        "               filled=True, \n",
        "               rounded=True,\n",
        "               feature_names=trn_xs.columns,\n",
        "               class_names=['No', 'Yes'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Min Samples Split -- Evaluation Error\n",
        "\n",
        "Let's see how it does on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "preds = clf.predict(val_xs)\n",
        "mean_absolute_error(val_y, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decision Tree -- Maximum Depth\n",
        "\n",
        "Let's train the decision tree again but with a maximum depth of 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "clf = tree.DecisionTreeClassifier(criterion='gini', random_state=42, max_depth=3)\n",
        "clf = clf.fit(trn_xs, trn_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's draw the tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "annotations = tree.plot_tree(clf, \n",
        "               filled=True, \n",
        "               rounded=True,\n",
        "               feature_names=trn_xs.columns,\n",
        "               class_names=['No', 'Yes'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Maximum Depth -- Evaluation Error\n",
        "\n",
        "Let's see how it does on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "preds = clf.predict(val_xs)\n",
        "mean_absolute_error(val_y, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest\n",
        "\n",
        "Let's try a random forest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf = clf.fit(trn_xs, trn_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how it does on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "preds = clf.predict(val_xs)\n",
        "mean_absolute_error(val_y, preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "\n",
        "## Recap\n",
        "\n",
        "* Decision Trees\n",
        "* Impurity Measures\n",
        "* Avoiding Overfitting\n",
        "* Random Forests\n",
        "\n",
        ":::\n",
        "\n",
        "## References\n",
        "\n",
        "::: {#refs}\n",
        ":::\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}