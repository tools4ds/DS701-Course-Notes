{
  "hash": "1ee686719ebbc3dc39f97096110f967b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Hierarchical Clustering\njupyter: python3\nfig-align: center\n---\n\n## Moving away from strict partitions\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/08-Clustering-III-hierarchical.ipynb)\n\n::: {#dff381e7 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport sklearn.datasets as sk_data\nfrom sklearn.cluster import KMeans\n\nimport seaborn as sns\n```\n:::\n\n\n::: {.incremental}\n* Today we will look at a fairly different approach to clustering.\n* So far, we have been thinking of clustering as finding a __partition__ of our dataset.\n* That is, a set of nonoverlapping clusters, in which each data item is in one cluster.\n* However, in many cases, the notion of a strict partition is not as useful.\n:::\n\n# Example\n\n## How Many Clusters?\n\nHow many clusters would you say there are here?\n\n::: {#7e23ca30 .cell execution_count=3}\n``` {.python .cell-code}\nX_rand, y_rand = sk_data.make_blobs(\n    n_samples=[100, 100, 250, 70, 75, 80], \n    centers = [[1, 2], [1.5, 1], [3, 2], [1.75, 3.25], [2, 4], [2.25, 3.25]], \n    n_features = 2,\n    center_box = (-10.0, 10.0), \n    cluster_std = [.2, .2, .3, .1, .15, .15], \n    random_state = 0\n)\ndf_rand = pd.DataFrame(np.column_stack([X_rand[:, 0], X_rand[:, 1], y_rand]), columns = ['X', 'Y', 'label'])\ndf_rand = df_rand.astype({'label': 'int'})\ndf_rand['label3'] = [{0: 0, 1: 0, 2: 1, 3: 2, 4: 2, 5: 2}[x] for x in df_rand['label']]\ndf_rand['label4'] = [{0: 0, 1: 1, 2: 2, 3: 3, 4: 3, 5: 3}[x] for x in df_rand['label']]\ndf_rand['label5'] = [{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 3}[x] for x in df_rand['label']]\n\n# kmeans = KMeans(init = 'k-means++', n_clusters = 3, n_init = 100)\n# df_rand['label'] = kmeans.fit_predict(df_rand[['X', 'Y']])\n\ndf_rand.plot('X', 'Y', kind = 'scatter',  \n                   colorbar = False, figsize = (6, 6))\nplt.axis('square')\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-3-output-1.png){width=463 height=463 fig-align='center'}\n:::\n:::\n\n\n## Three clusters?\n\n::: {#f0d2dfd3 .cell execution_count=4}\n``` {.python .cell-code}\ndf_rand.plot('X', 'Y', kind = 'scatter', c = 'label3', colormap='viridis', \n                   colorbar = False, figsize = (6, 6))\nplt.axis('square')\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-4-output-1.png){width=463 height=463 fig-align='center'}\n:::\n:::\n\n\n## Four clusters?\n\n::: {#604063b2 .cell execution_count=5}\n``` {.python .cell-code}\ndf_rand.plot('X', 'Y', kind = 'scatter', c = 'label4', colormap='viridis', \n                   colorbar = False, figsize = (6, 6))\nplt.axis('square')\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-5-output-1.png){width=463 height=463 fig-align='center'}\n:::\n:::\n\n\n## Five clusters?\n\n::: {#b3df4b72 .cell execution_count=6}\n``` {.python .cell-code}\ndf_rand.plot('X', 'Y', kind = 'scatter', c = 'label5', colormap='viridis', \n                   colorbar = False, figsize = (6, 6))\nplt.axis('square')\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-6-output-1.png){width=463 height=463 fig-align='center'}\n:::\n:::\n\n\n## Six clusters?\n\n::: {#7e45be23 .cell execution_count=7}\n``` {.python .cell-code}\ndf_rand.plot('X', 'Y', kind = 'scatter', c = 'label', colormap='viridis', \n                   colorbar = False, figsize = (6, 6))\nplt.axis('square')\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-7-output-1.png){width=463 height=463 fig-align='center'}\n:::\n:::\n\n\n---\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n::: {#307e68af .cell fig-height='400px' fig-width='400px' execution_count=8}\n``` {.python .cell-code}\ndf_rand.plot('X', 'Y', kind = 'scatter', c = 'label', colormap='viridis', \n                   colorbar = False, figsize = (4, 4))\nplt.axis('square')\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-8-output-1.png){width=315 height=315 fig-align='left'}\n:::\n:::\n\n\n:::\n::: {.column width=\"60%\"}\n\nThis dataset shows clustering on __multiple scales.__\n\n:::: {.fragment}\nTo fully capture the structure in this dataset, two things are needed:\n::::\n\n::: {.incremental}\n1. Capturing the differing clusters depending on the scale\n2. Capturing the containment relations -- which clusters lie within other clusters\n:::\n\n:::\n::::\n\n:::: {.fragment}\nThese observations motivate the notion of __hierarchical__ clustering.\n\nIn hierarchical clustering, we move away from the __partition__ notion of $k$-means, \n\nand instead capture a more complex arrangement that includes containment of one cluster within another.\n\n> Note: GMMs relax the partitioning notion further!\n::::\n\n\n\n\n# Hierarchical Clustering\n\n## Hierarchical Clustering Example\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n::: {#abaa1fd7 .cell execution_count=9}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Create 5 points with labels\npoints = {\n    'A': (1.2, 5.2),\n    'B': (2.7, 4.2),\n    'C': (2, 6),\n    'D': (5, 3),\n    'E': (4, 2)\n}\n\n# Create the scatter plot\nplt.figure(figsize=(6, 6))\nfor label, (x, y) in points.items():\n    plt.scatter(x, y, s=100, alpha=0.7)\n    plt.annotate(label, (x, y), xytext=(5, 5), textcoords='offset points', \n                fontsize=12, fontweight='bold')\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('2D Scatter Plot with 5 Labeled Points')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 7)\nplt.ylim(0, 7)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-9-output-1.png){width=506 height=523}\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\n\n<br><br>\n\n* Goal: Combine closest points/clusters into a new cluster\n* Use distance metric to define closeness\n* So-called bottom-up approach or _agglomerative_ clustering\n* Which 2 points are closest?\n\n:::\n::::\n\n## Example, cont.\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n::: {#5868de7b .cell execution_count=10}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Ellipse\n\n# Create 5 points with labels (same as before)\npoints = {\n    'A': (1.2, 5.2),\n    'B': (2.7, 4.2),\n    'C': (2, 6),\n    'D': (5, 3),\n    'E': (4, 2)\n}\n\n# Create the scatter plot\nplt.figure(figsize=(6, 6))\nfor label, (x, y) in points.items():\n    plt.scatter(x, y, s=100, alpha=0.7)\n    plt.annotate(label, (x, y), xytext=(5, 5), textcoords='offset points', \n                fontsize=12, fontweight='bold')\n\n# Draw an ellipse around points A and C where A and C are the foci\nA_x, A_y = points['A']\nC_x, C_y = points['C']\n\n# Calculate the distance between foci (A and C)\nfocal_distance = np.sqrt((C_x - A_x)**2 + (C_y - A_y)**2)\n\n# Calculate center of ellipse (midpoint between foci)\ncenter_x = (A_x + C_x) / 2\ncenter_y = (A_y + C_y) / 2\n\n# Calculate the angle of the major axis\nangle = np.degrees(np.arctan2(C_y - A_y, C_x - A_x))\n\n# Set semi-major axis (a) and semi-minor axis (b)\n# For an ellipse: c = sqrt(a^2 - b^2) where c is half the focal distance\n# We'll set a = focal_distance/2 + padding, then calculate b\na = focal_distance/2 + 0.4  # semi-major axis with padding\nc = focal_distance/2  # half the focal distance\nb = np.sqrt(a**2 - c**2)  # semi-minor axis\n\n# Create and add the ellipse\nellipse = Ellipse((center_x, center_y), 2*a, 2*b, angle=angle,\n                  facecolor='none', edgecolor='red', linewidth=2, linestyle='--')\nplt.gca().add_patch(ellipse)\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('2D Scatter Plot - First Cluster (A & C)')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 7)\nplt.ylim(0, 7)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-10-output-1.png){width=506 height=523}\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\n\n<br><br>\n\n* Now we want to combine either the next closest pair of points or the point closest to a cluster\n* How do we measure closeness of points to clusters?\n* Or clusters to clusters?\n:::\n::::\n\n## Example, cont. (Part 2)\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n::: {#50d1db22 .cell execution_count=11}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Ellipse\n\n# Create 5 points with labels (same as before)\npoints = {\n    'A': (1.2, 5.2),\n    'B': (2.7, 4.2),\n    'C': (2, 6),\n    'D': (5, 3),\n    'E': (4, 2)\n}\n\n# Create the scatter plot\nplt.figure(figsize=(6, 6))\nfor label, (x, y) in points.items():\n    plt.scatter(x, y, s=100, alpha=0.7)\n    plt.annotate(label, (x, y), xytext=(5, 5), textcoords='offset points', \n                fontsize=12, fontweight='bold')\n\n# Draw an ellipse around points A and C where A and C are the foci\nA_x, A_y = points['A']\nC_x, C_y = points['C']\n\n# Calculate the distance between foci (A and C)\nfocal_distance = np.sqrt((C_x - A_x)**2 + (C_y - A_y)**2)\n\n# Calculate center of ellipse (midpoint between foci)\ncenter_x = (A_x + C_x) / 2\ncenter_y = (A_y + C_y) / 2\n\n# Calculate the angle of the major axis\nangle = np.degrees(np.arctan2(C_y - A_y, C_x - A_x))\n\n# Set semi-major axis (a) and semi-minor axis (b)\n# For an ellipse: c = sqrt(a^2 - b^2) where c is half the focal distance\n# We'll set a = focal_distance/2 + padding, then calculate b\na = focal_distance/2 + 0.4  # semi-major axis with padding\nc = focal_distance/2  # half the focal distance\nb = np.sqrt(a**2 - c**2)  # semi-minor axis\n\n# Create and add the ellipse for A and C\nellipse1 = Ellipse((center_x, center_y), 2*a, 2*b, angle=angle,\n                   facecolor='none', edgecolor='red', linewidth=2, linestyle='--')\nplt.gca().add_patch(ellipse1)\n\n# Draw an ellipse around points E and D where E and D are the foci\nE_x, E_y = points['E']\nD_x, D_y = points['D']\n\n# Calculate the distance between foci (E and D)\nfocal_distance_ED = np.sqrt((D_x - E_x)**2 + (D_y - E_y)**2)\n\n# Calculate center of ellipse (midpoint between foci)\ncenter_x_ED = (E_x + D_x) / 2\ncenter_y_ED = (E_y + D_y) / 2\n\n# Calculate the angle of the major axis\nangle_ED = np.degrees(np.arctan2(D_y - E_y, D_x - E_x))\n\n# Set semi-major axis (a) and semi-minor axis (b)\na_ED = focal_distance_ED/2 + 0.4  # semi-major axis with padding\nc_ED = focal_distance_ED/2  # half the focal distance\nb_ED = np.sqrt(a_ED**2 - c_ED**2)  # semi-minor axis\n\n# Create and add the ellipse for E and D\nellipse2 = Ellipse((center_x_ED, center_y_ED), 2*a_ED, 2*b_ED, angle=angle_ED,\n                   facecolor='none', edgecolor='blue', linewidth=2, linestyle='--')\nplt.gca().add_patch(ellipse2)\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('2D Scatter Plot - Two Clusters (A&C, E&D)')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 7)\nplt.ylim(0, 7)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-11-output-1.png){width=506 height=523}\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\n\n<br><br>\n\n* Assume D and E are the next closest pair of points\n* Now which cluster is B closest to?\n\n:::\n::::\n\n## Example, cont. (Part 3)\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n::: {#be268dbc .cell execution_count=12}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Ellipse\n\n# Create 5 points with labels (same as before)\npoints = {\n    'A': (1.2, 5.2),\n    'B': (2.7, 4.2),\n    'C': (2, 6),\n    'D': (5, 3),\n    'E': (4, 2)\n}\n\n# Create the scatter plot\nplt.figure(figsize=(6, 6))\nfor label, (x, y) in points.items():\n    plt.scatter(x, y, s=100, alpha=0.7)\n    plt.annotate(label, (x, y), xytext=(5, 5), textcoords='offset points', \n                fontsize=12, fontweight='bold')\n\n# Draw an ellipse around points A and C where A and C are the foci\nA_x, A_y = points['A']\nC_x, C_y = points['C']\n\n# Calculate the distance between foci (A and C)\nfocal_distance = np.sqrt((C_x - A_x)**2 + (C_y - A_y)**2)\n\n# Calculate center of ellipse (midpoint between foci)\ncenter_x = (A_x + C_x) / 2\ncenter_y = (A_y + C_y) / 2\n\n# Calculate the angle of the major axis\nangle = np.degrees(np.arctan2(C_y - A_y, C_x - A_x))\n\n# Set semi-major axis (a) and semi-minor axis (b)\n# For an ellipse: c = sqrt(a^2 - b^2) where c is half the focal distance\n# We'll set a = focal_distance/2 + padding, then calculate b\na = focal_distance/2 + 0.4  # semi-major axis with padding\nc = focal_distance/2  # half the focal distance\nb = np.sqrt(a**2 - c**2)  # semi-minor axis\n\n# Create and add the ellipse for A and C\nellipse1 = Ellipse((center_x, center_y), 2*a, 2*b, angle=angle,\n                   facecolor='none', edgecolor='red', linewidth=2, linestyle='--')\nplt.gca().add_patch(ellipse1)\n\n# Draw an ellipse around points E and D where E and D are the foci\nE_x, E_y = points['E']\nD_x, D_y = points['D']\n\n# Calculate the distance between foci (E and D)\nfocal_distance_ED = np.sqrt((D_x - E_x)**2 + (D_y - E_y)**2)\n\n# Calculate center of ellipse (midpoint between foci)\ncenter_x_ED = (E_x + D_x) / 2\ncenter_y_ED = (E_y + D_y) / 2\n\n# Calculate the angle of the major axis\nangle_ED = np.degrees(np.arctan2(D_y - E_y, D_x - E_x))\n\n# Set semi-major axis (a) and semi-minor axis (b)\na_ED = focal_distance_ED/2 + 0.4  # semi-major axis with padding\nc_ED = focal_distance_ED/2  # half the focal distance\nb_ED = np.sqrt(a_ED**2 - c_ED**2)  # semi-minor axis\n\n# Create and add the ellipse for E and D\nellipse2 = Ellipse((center_x_ED, center_y_ED), 2*a_ED, 2*b_ED, angle=angle_ED,\n                   facecolor='none', edgecolor='blue', linewidth=2, linestyle='--')\nplt.gca().add_patch(ellipse2)\n\n# Draw a circle around points A, B, and C\nA_x, A_y = points['A']\nB_x, B_y = points['B']\nC_x, C_y = points['C']\n\n# Calculate the center of the circle (centroid of the three points)\ncenter_x_ABC = (A_x + B_x + C_x) / 3\ncenter_y_ABC = (A_y + B_y + C_y) / 3\n\n# Calculate the radius as the maximum distance from center to any of the three points\ndist_A = np.sqrt((A_x - center_x_ABC)**2 + (A_y - center_y_ABC)**2)\ndist_B = np.sqrt((B_x - center_x_ABC)**2 + (B_y - center_y_ABC)**2)\ndist_C = np.sqrt((C_x - center_x_ABC)**2 + (C_y - center_y_ABC)**2)\nradius_ABC = max(dist_A, dist_B, dist_C) + 0.3  # Add some padding\n\n# Create and add the circle\ncircle = plt.Circle((center_x_ABC, center_y_ABC), radius_ABC,\n                   facecolor='none', edgecolor='green', linewidth=2, linestyle='--')\nplt.gca().add_patch(circle)\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('2D Scatter Plot - Three Clusters')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 7)\nplt.ylim(0, 7)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-12-output-1.png){width=506 height=523}\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\n\n<br><br>\n\n* We'll talk about measuring distances between clusters shortly\n* But let's just say that B is closest to the cluster A, C, E and forms a new cluster\n* and then finally...\n\n:::\n::::\n\n## Example, cont. (Part 4)\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n\n::: {#98e318a8 .cell execution_count=13}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Ellipse\n\n# Create 5 points with labels (same as before)\npoints = {\n    'A': (1.2, 5.2),\n    'B': (2.7, 4.2),\n    'C': (2, 6),\n    'D': (5, 3),\n    'E': (4, 2)\n}\n\n# Create the scatter plot\nplt.figure(figsize=(6, 6))\nfor label, (x, y) in points.items():\n    plt.scatter(x, y, s=100, alpha=0.7)\n    plt.annotate(label, (x, y), xytext=(5, 5), textcoords='offset points', \n                fontsize=12, fontweight='bold')\n\n# Draw an ellipse around points A and C where A and C are the foci\nA_x, A_y = points['A']\nC_x, C_y = points['C']\n\n# Calculate the distance between foci (A and C)\nfocal_distance = np.sqrt((C_x - A_x)**2 + (C_y - A_y)**2)\n\n# Calculate center of ellipse (midpoint between foci)\ncenter_x = (A_x + C_x) / 2\ncenter_y = (A_y + C_y) / 2\n\n# Calculate the angle of the major axis\nangle = np.degrees(np.arctan2(C_y - A_y, C_x - A_x))\n\n# Set semi-major axis (a) and semi-minor axis (b)\n# For an ellipse: c = sqrt(a^2 - b^2) where c is half the focal distance\n# We'll set a = focal_distance/2 + padding, then calculate b\na = focal_distance/2 + 0.4  # semi-major axis with padding\nc = focal_distance/2  # half the focal distance\nb = np.sqrt(a**2 - c**2)  # semi-minor axis\n\n# Create and add the ellipse for A and C\nellipse1 = Ellipse((center_x, center_y), 2*a, 2*b, angle=angle,\n                   facecolor='none', edgecolor='red', linewidth=2, linestyle='--')\nplt.gca().add_patch(ellipse1)\n\n# Draw an ellipse around points E and D where E and D are the foci\nE_x, E_y = points['E']\nD_x, D_y = points['D']\n\n# Calculate the distance between foci (E and D)\nfocal_distance_ED = np.sqrt((D_x - E_x)**2 + (D_y - E_y)**2)\n\n# Calculate center of ellipse (midpoint between foci)\ncenter_x_ED = (E_x + D_x) / 2\ncenter_y_ED = (E_y + D_y) / 2\n\n# Calculate the angle of the major axis\nangle_ED = np.degrees(np.arctan2(D_y - E_y, D_x - E_x))\n\n# Set semi-major axis (a) and semi-minor axis (b)\na_ED = focal_distance_ED/2 + 0.4  # semi-major axis with padding\nc_ED = focal_distance_ED/2  # half the focal distance\nb_ED = np.sqrt(a_ED**2 - c_ED**2)  # semi-minor axis\n\n# Create and add the ellipse for E and D\nellipse2 = Ellipse((center_x_ED, center_y_ED), 2*a_ED, 2*b_ED, angle=angle_ED,\n                   facecolor='none', edgecolor='blue', linewidth=2, linestyle='--')\nplt.gca().add_patch(ellipse2)\n\n# Draw a circle around points A, B, and C\nA_x, A_y = points['A']\nB_x, B_y = points['B']\nC_x, C_y = points['C']\n\n# Calculate the center of the circle (centroid of the three points)\ncenter_x_ABC = (A_x + B_x + C_x) / 3\ncenter_y_ABC = (A_y + B_y + C_y) / 3\n\n# Calculate the radius as the maximum distance from center to any of the three points\ndist_A = np.sqrt((A_x - center_x_ABC)**2 + (A_y - center_y_ABC)**2)\ndist_B = np.sqrt((B_x - center_x_ABC)**2 + (B_y - center_y_ABC)**2)\ndist_C = np.sqrt((C_x - center_x_ABC)**2 + (C_y - center_y_ABC)**2)\nradius_ABC = max(dist_A, dist_B, dist_C) + 0.3  # Add some padding\n\n# Create and add the circle\ncircle = plt.Circle((center_x_ABC, center_y_ABC), radius_ABC,\n                   facecolor='none', edgecolor='green', linewidth=2, linestyle='--')\nplt.gca().add_patch(circle)\n\n# Draw an ellipse around all 5 points\nall_x = [points[label][0] for label in points.keys()]\nall_y = [points[label][1] for label in points.keys()]\n\n# Calculate center of ellipse (centroid of all points)\ncenter_x_all = np.mean(all_x)\ncenter_y_all = np.mean(all_y)\n\n\n\n\nwidth_all, height_all = 3.2, 7\nangle_all = 40\n\nprint(width_all, height_all, angle_all, center_x_all, center_y_all)\n\n# Create and add the ellipse around all points\nellipse_all = Ellipse((center_x_all, center_y_all), width_all, height_all, angle=angle_all,\n                     facecolor='none', edgecolor='purple', linewidth=3, linestyle='-')\nplt.gca().add_patch(ellipse_all)\n\nplt.xlabel('Feature 1')\nplt.ylabel('Feature 2')\nplt.title('2D Scatter Plot - Final Cluster (All Points)')\nplt.grid(True, alpha=0.3)\nplt.xlim(0, 7)\nplt.ylim(0, 7)\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n3.2 7 40 2.98 4.08\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-13-output-2.png){width=506 height=523}\n:::\n:::\n\n\n:::\n::: {.column width=\"40%\"}\n\n<br><br>\n\n* We combine all points into a single cluster\n\n:::\n::::\n\n## Example: Dendrogram\n\n::: {#fa440b71 .cell execution_count=14}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Ellipse\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import pdist\n\n# Create 5 points with labels (same as before)\npoints = {\n    'A': (1.2, 5.2),\n    'B': (2.7, 4.2),\n    'C': (2, 6),\n    'D': (5, 3),\n    'E': (4, 2)\n}\n\n# Create figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\n# First subplot: Scatter plot with clusters\nfor label, (x, y) in points.items():\n    ax1.scatter(x, y, s=100, alpha=0.7)\n    ax1.annotate(label, (x, y), xytext=(5, 5), textcoords='offset points', \n                fontsize=12, fontweight='bold')\n\n# Draw an ellipse around points A and C where A and C are the foci\nA_x, A_y = points['A']\nC_x, C_y = points['C']\n\n# Calculate the distance between foci (A and C)\nfocal_distance = np.sqrt((C_x - A_x)**2 + (C_y - A_y)**2)\n\n# Calculate center of ellipse (midpoint between foci)\ncenter_x = (A_x + C_x) / 2\ncenter_y = (A_y + C_y) / 2\n\n# Calculate the angle of the major axis\nangle = np.degrees(np.arctan2(C_y - A_y, C_x - A_x))\n\n# Set semi-major axis (a) and semi-minor axis (b)\na = focal_distance/2 + 0.4  # semi-major axis with padding\nc = focal_distance/2  # half the focal distance\nb = np.sqrt(a**2 - c**2)  # semi-minor axis\n\n# Create and add the ellipse for A and C\nellipse1 = Ellipse((center_x, center_y), 2*a, 2*b, angle=angle,\n                   facecolor='none', edgecolor='red', linewidth=2, linestyle='--')\nax1.add_patch(ellipse1)\n\n# Draw an ellipse around points E and D where E and D are the foci\nE_x, E_y = points['E']\nD_x, D_y = points['D']\n\n# Calculate the distance between foci (E and D)\nfocal_distance_ED = np.sqrt((D_x - E_x)**2 + (D_y - E_y)**2)\n\n# Calculate center of ellipse (midpoint between foci)\ncenter_x_ED = (E_x + D_x) / 2\ncenter_y_ED = (E_y + D_y) / 2\n\n# Calculate the angle of the major axis\nangle_ED = np.degrees(np.arctan2(D_y - E_y, D_x - E_x))\n\n# Set semi-major axis (a) and semi-minor axis (b)\na_ED = focal_distance_ED/2 + 0.4  # semi-major axis with padding\nc_ED = focal_distance_ED/2  # half the focal distance\nb_ED = np.sqrt(a_ED**2 - c_ED**2)  # semi-minor axis\n\n# Create and add the ellipse for E and D\nellipse2 = Ellipse((center_x_ED, center_y_ED), 2*a_ED, 2*b_ED, angle=angle_ED,\n                   facecolor='none', edgecolor='blue', linewidth=2, linestyle='--')\nax1.add_patch(ellipse2)\n\n# Draw a circle around points A, B, and C\nA_x, A_y = points['A']\nB_x, B_y = points['B']\nC_x, C_y = points['C']\n\n# Calculate the center of the circle (centroid of the three points)\ncenter_x_ABC = (A_x + B_x + C_x) / 3\ncenter_y_ABC = (A_y + B_y + C_y) / 3\n\n# Calculate the radius as the maximum distance from center to any of the three points\ndist_A = np.sqrt((A_x - center_x_ABC)**2 + (A_y - center_y_ABC)**2)\ndist_B = np.sqrt((B_x - center_x_ABC)**2 + (B_y - center_y_ABC)**2)\ndist_C = np.sqrt((C_x - center_x_ABC)**2 + (C_y - center_y_ABC)**2)\nradius_ABC = max(dist_A, dist_B, dist_C) + 0.3  # Add some padding\n\n# Create and add the circle\ncircle = plt.Circle((center_x_ABC, center_y_ABC), radius_ABC,\n                   facecolor='none', edgecolor='green', linewidth=2, linestyle='--')\nax1.add_patch(circle)\n\n# Draw an ellipse around all 5 points\nall_x = [points[label][0] for label in points.keys()]\nall_y = [points[label][1] for label in points.keys()]\n\n# Calculate center of ellipse (centroid of all points)\ncenter_x_all = np.mean(all_x)\ncenter_y_all = np.mean(all_y)\n\nwidth_all, height_all = 3.2, 7\nangle_all = 40\n\n# Create and add the ellipse around all points\nellipse_all = Ellipse((center_x_all, center_y_all), width_all, height_all, angle=angle_all,\n                     facecolor='none', edgecolor='purple', linewidth=3, linestyle='-')\nax1.add_patch(ellipse_all)\n\nax1.set_xlabel('Feature 1')\nax1.set_ylabel('Feature 2')\nax1.set_title('Clusters at Different Levels')\nax1.grid(True, alpha=0.3)\nax1.set_xlim(0, 7)\nax1.set_ylim(0, 7)\n\n# Second subplot: Dendrogram\n# Prepare data for hierarchical clustering\npoints_array = np.array(list(points.values()))\npoint_labels = list(points.keys())\n\n# Calculate linkage matrix\nlinkage_matrix = linkage(points_array, method='ward')\n\n# Create dendrogram\ndendrogram(linkage_matrix, labels=point_labels, ax=ax2, \n          leaf_rotation=0, leaf_font_size=12)\nax2.set_title('Hierarchical Clustering Dendrogram')\nax2.set_xlabel('Points')\nax2.set_ylabel('Distance')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-14-output-1.png){width=1142 height=567}\n:::\n:::\n\n\n## Example: Dendrogram Explained\n\n::: {#a2b76b04 .cell execution_count=15}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Ellipse\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import pdist\n\n# Create 5 points with labels (same as before)\npoints = {\n    'A': (1.2, 5.2),\n    'B': (2.7, 4.2),\n    'C': (2, 6),\n    'D': (5, 3),\n    'E': (4, 2)\n}\n\n# Create figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n\n# First subplot: Scatter plot with clusters\nfor label, (x, y) in points.items():\n    ax1.scatter(x, y, s=100, alpha=0.7)\n    ax1.annotate(label, (x, y), xytext=(5, 5), textcoords='offset points', \n                fontsize=12, fontweight='bold')\n\n# Draw an ellipse around points A and C where A and C are the foci\nA_x, A_y = points['A']\nC_x, C_y = points['C']\n\n# Calculate the distance between foci (A and C)\nfocal_distance = np.sqrt((C_x - A_x)**2 + (C_y - A_y)**2)\n\n# Calculate center of ellipse (midpoint between foci)\ncenter_x = (A_x + C_x) / 2\ncenter_y = (A_y + C_y) / 2\n\n# Calculate the angle of the major axis\nangle = np.degrees(np.arctan2(C_y - A_y, C_x - A_x))\n\n# Set semi-major axis (a) and semi-minor axis (b)\na = focal_distance/2 + 0.4  # semi-major axis with padding\nc = focal_distance/2  # half the focal distance\nb = np.sqrt(a**2 - c**2)  # semi-minor axis\n\n# Create and add the ellipse for A and C\nellipse1 = Ellipse((center_x, center_y), 2*a, 2*b, angle=angle,\n                   facecolor='none', edgecolor='red', linewidth=2, linestyle='--')\nax1.add_patch(ellipse1)\n\n# Draw an ellipse around points E and D where E and D are the foci\nE_x, E_y = points['E']\nD_x, D_y = points['D']\n\n# Calculate the distance between foci (E and D)\nfocal_distance_ED = np.sqrt((D_x - E_x)**2 + (D_y - E_y)**2)\n\n# Calculate center of ellipse (midpoint between foci)\ncenter_x_ED = (E_x + D_x) / 2\ncenter_y_ED = (E_y + D_y) / 2\n\n# Calculate the angle of the major axis\nangle_ED = np.degrees(np.arctan2(D_y - E_y, D_x - E_x))\n\n# Set semi-major axis (a) and semi-minor axis (b)\na_ED = focal_distance_ED/2 + 0.4  # semi-major axis with padding\nc_ED = focal_distance_ED/2  # half the focal distance\nb_ED = np.sqrt(a_ED**2 - c_ED**2)  # semi-minor axis\n\n# Create and add the ellipse for E and D\nellipse2 = Ellipse((center_x_ED, center_y_ED), 2*a_ED, 2*b_ED, angle=angle_ED,\n                   facecolor='none', edgecolor='blue', linewidth=2, linestyle='--')\nax1.add_patch(ellipse2)\n\n# Draw a circle around points A, B, and C\nA_x, A_y = points['A']\nB_x, B_y = points['B']\nC_x, C_y = points['C']\n\n# Calculate the center of the circle (centroid of the three points)\ncenter_x_ABC = (A_x + B_x + C_x) / 3\ncenter_y_ABC = (A_y + B_y + C_y) / 3\n\n# Calculate the radius as the maximum distance from center to any of the three points\ndist_A = np.sqrt((A_x - center_x_ABC)**2 + (A_y - center_y_ABC)**2)\ndist_B = np.sqrt((B_x - center_x_ABC)**2 + (B_y - center_y_ABC)**2)\ndist_C = np.sqrt((C_x - center_x_ABC)**2 + (C_y - center_y_ABC)**2)\nradius_ABC = max(dist_A, dist_B, dist_C) + 0.3  # Add some padding\n\n# Create and add the circle\ncircle = plt.Circle((center_x_ABC, center_y_ABC), radius_ABC,\n                   facecolor='none', edgecolor='green', linewidth=2, linestyle='--')\nax1.add_patch(circle)\n\n# Draw an ellipse around all 5 points\nall_x = [points[label][0] for label in points.keys()]\nall_y = [points[label][1] for label in points.keys()]\n\n# Calculate center of ellipse (centroid of all points)\ncenter_x_all = np.mean(all_x)\ncenter_y_all = np.mean(all_y)\n\nwidth_all, height_all = 3.2, 7\nangle_all = 40\n\n# Create and add the ellipse around all points\nellipse_all = Ellipse((center_x_all, center_y_all), width_all, height_all, angle=angle_all,\n                     facecolor='none', edgecolor='purple', linewidth=3, linestyle='-')\nax1.add_patch(ellipse_all)\n\nax1.set_xlabel('Feature 1')\nax1.set_ylabel('Feature 2')\nax1.set_title('Clusters at Different Levels')\nax1.grid(True, alpha=0.3)\nax1.set_xlim(0, 7)\nax1.set_ylim(0, 7)\n\n# Second subplot: Dendrogram\n# Prepare data for hierarchical clustering\npoints_array = np.array(list(points.values()))\npoint_labels = list(points.keys())\n\n# Calculate linkage matrix\nlinkage_matrix = linkage(points_array, method='ward')\n\n# Create dendrogram\ndendrogram(linkage_matrix, labels=point_labels, ax=ax2, \n          leaf_rotation=0, leaf_font_size=12)\nax2.set_title('Hierarchical Clustering Dendrogram')\nax2.set_xlabel('Points')\nax2.set_ylabel('Distance')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-15-output-1.png){width=595 height=279 fig-align='center'}\n:::\n:::\n\n\n::: {.incremental}\n* In the dendrogram, the vertical axis is the distance between the clusters\n* The horizontal axis is the points\n* The clusters are merged from the bottom up\n:::\n\n## Example: Dendrogram cont.\n\n::: {#e89461d0 .cell execution_count=16}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom matplotlib.patches import Ellipse\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom scipy.spatial.distance import pdist\n\n# Create 5 points with labels (same as before)\npoints = {\n    'A': (1.2, 5.2),\n    'B': (2.7, 4.2),\n    'C': (2, 6),\n    'D': (5, 3),\n    'E': (4, 2)\n}\n\n# Create figure with two subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n\n# First subplot: Scatter plot with clusters\nfor label, (x, y) in points.items():\n    ax1.scatter(x, y, s=100, alpha=0.7)\n    ax1.annotate(label, (x, y), xytext=(5, 5), textcoords='offset points', \n                fontsize=12, fontweight='bold')\n\n# Draw an ellipse around points A and C where A and C are the foci\nA_x, A_y = points['A']\nC_x, C_y = points['C']\n\n# Calculate the distance between foci (A and C)\nfocal_distance = np.sqrt((C_x - A_x)**2 + (C_y - A_y)**2)\n\n# Calculate center of ellipse (midpoint between foci)\ncenter_x = (A_x + C_x) / 2\ncenter_y = (A_y + C_y) / 2\n\n# Calculate the angle of the major axis\nangle = np.degrees(np.arctan2(C_y - A_y, C_x - A_x))\n\n# Set semi-major axis (a) and semi-minor axis (b)\na = focal_distance/2 + 0.4  # semi-major axis with padding\nc = focal_distance/2  # half the focal distance\nb = np.sqrt(a**2 - c**2)  # semi-minor axis\n\n# Create and add the ellipse for A and C\nellipse1 = Ellipse((center_x, center_y), 2*a, 2*b, angle=angle,\n                   facecolor='none', edgecolor='red', linewidth=2, linestyle='--')\nax1.add_patch(ellipse1)\n\n# Draw an ellipse around points E and D where E and D are the foci\nE_x, E_y = points['E']\nD_x, D_y = points['D']\n\n# Calculate the distance between foci (E and D)\nfocal_distance_ED = np.sqrt((D_x - E_x)**2 + (D_y - E_y)**2)\n\n# Calculate center of ellipse (midpoint between foci)\ncenter_x_ED = (E_x + D_x) / 2\ncenter_y_ED = (E_y + D_y) / 2\n\n# Calculate the angle of the major axis\nangle_ED = np.degrees(np.arctan2(D_y - E_y, D_x - E_x))\n\n# Set semi-major axis (a) and semi-minor axis (b)\na_ED = focal_distance_ED/2 + 0.4  # semi-major axis with padding\nc_ED = focal_distance_ED/2  # half the focal distance\nb_ED = np.sqrt(a_ED**2 - c_ED**2)  # semi-minor axis\n\n# Create and add the ellipse for E and D\nellipse2 = Ellipse((center_x_ED, center_y_ED), 2*a_ED, 2*b_ED, angle=angle_ED,\n                   facecolor='none', edgecolor='blue', linewidth=2, linestyle='--')\nax1.add_patch(ellipse2)\n\n# Draw a circle around points A, B, and C\nA_x, A_y = points['A']\nB_x, B_y = points['B']\nC_x, C_y = points['C']\n\n# Calculate the center of the circle (centroid of the three points)\ncenter_x_ABC = (A_x + B_x + C_x) / 3\ncenter_y_ABC = (A_y + B_y + C_y) / 3\n\n# Calculate the radius as the maximum distance from center to any of the three points\ndist_A = np.sqrt((A_x - center_x_ABC)**2 + (A_y - center_y_ABC)**2)\ndist_B = np.sqrt((B_x - center_x_ABC)**2 + (B_y - center_y_ABC)**2)\ndist_C = np.sqrt((C_x - center_x_ABC)**2 + (C_y - center_y_ABC)**2)\nradius_ABC = max(dist_A, dist_B, dist_C) + 0.3  # Add some padding\n\n# Create and add the circle\ncircle = plt.Circle((center_x_ABC, center_y_ABC), radius_ABC,\n                   facecolor='none', edgecolor='green', linewidth=2, linestyle='--')\nax1.add_patch(circle)\n\n# Draw an ellipse around all 5 points\nall_x = [points[label][0] for label in points.keys()]\nall_y = [points[label][1] for label in points.keys()]\n\n# Calculate center of ellipse (centroid of all points)\ncenter_x_all = np.mean(all_x)\ncenter_y_all = np.mean(all_y)\n\nwidth_all, height_all = 3.2, 7\nangle_all = 40\n\n# Create and add the ellipse around all points\nellipse_all = Ellipse((center_x_all, center_y_all), width_all, height_all, angle=angle_all,\n                     facecolor='none', edgecolor='purple', linewidth=3, linestyle='-')\nax1.add_patch(ellipse_all)\n\nax1.set_xlabel('Feature 1')\nax1.set_ylabel('Feature 2')\nax1.set_title('Clusters at Different Levels')\nax1.grid(True, alpha=0.3)\nax1.set_xlim(0, 7)\nax1.set_ylim(0, 7)\n\n# Second subplot: Dendrogram\n# Prepare data for hierarchical clustering\npoints_array = np.array(list(points.values()))\npoint_labels = list(points.keys())\n\n# Calculate linkage matrix\nlinkage_matrix = linkage(points_array, method='ward')\n\n# Create dendrogram\ndendrogram(linkage_matrix, labels=point_labels, ax=ax2, \n          leaf_rotation=0, leaf_font_size=12)\nax2.set_title('Dendrogram with Splits')\nax2.set_xlabel('Points')\nax2.set_ylabel('Distance')\n\n# Add horizontal dashed red line at y=4 to show split level\nax2.axhline(y=4, color='red', linestyle='--', linewidth=2, alpha=0.8)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-16-output-1.png){width=566 height=279 fig-align='center'}\n:::\n:::\n\n\n* We can now split the dendrogram at a certain level to get a clustering\n\n\n\n\n## Hierarchical Clustering\n\nA hierarchical clustering produces a set of __nested__ clusters organized into a tree.\n\nA hierarchical clustering is visualized using a...\n\n __dendrogram__ \n\n* A tree-like diagram that records the containment relations among clusters.\n\n![](./figs/L08-dendrogram.png){width=\"600px\"}\n\n\n## Strengths of Hierarchical Clustering\n\nHierarchical clustering has a number of advantages:\n\n:::: {.incremental}\n* Encodes many __different__ clusterings.\n    * It does not itself decide on the correct number of clusters.\n* A clustering is obtained by \"cutting\" the dendrogram at some level.\n* You can make this crucial decision yourself, by inspecting the dendrogram.  \n* You can obtain a (somewhat) arbitrary number of clusters.\n::::\n\n![](./figs/L08-dendrogram-cut.png){width=\"600px\"}\n\n## Another advantage\n\nAnother advantage is that the dendrogram may itself correspond to a meaningful\nstructure, for example, a taxonomy.\n\n![](./figs/L08-animal-taxonomy.jpg){width=\"100%\"}\n\n## Yet another advantage\n\n* Many hierarchical clustering methods can be performed using either similarity (proximity) or dissimilarity (distance) metrics.\n* This can be very helpful! \n* Techniques like $k$-means rely on a specific way of measuring similarity or distance between data points. \n\n## Compared to $k$-means\n\nAnother aspect of hierachical clustering is that it can handle certain cases better than $k$-means.\n\nBecause of the nature of the $k$-means algorithm, $k$-means tends to produce:\n\n* Roughly spherical clusters\n* Clusters of approximately equal size\n* Non-overlapping clusters\n\nIn many real-world situations, clusters may not be round, they may be of\nunequal size, and they may overlap.\n\nHence we would like clustering algorithms that can work in those cases also.\n\n\n# Hierarchical Clustering Algorithms\n\n## Hierarchical Clustering Algorithms\n\nThere are two main approaches to hierarchical clustering: \"bottom-up\" and \"top-down.\"\n\n::: {.fragment}\n__Agglomerative__ Clustering (\"bottom-up\"):\n\n* Start by defining each point as its own cluster\n* At each successive step, merge the two clusters that are closest to each other\n* Repeat until only one cluster is left.\n:::\n\n::: {.fragment}\n__Divisive__ Clustering (\"top-down\"):\n    \n* Start with one, all-inclusive cluster\n* At each step, find the cluster split that creates the largest distance between resulting clusters\n* Repeat until each point is in its own cluster.\n:::\n\n## Some key points\n \n* Agglomerative techniques are by far the more common.\n* The key to both of these methods is defining __the distance between two clusters.__\n* Different definitions for the inter-cluster distance yield different clusterings.\n\n::: {.fragment}\nTo illustrate the impact of the choice of cluster distances, we'll focus on agglomerative clustering.\n:::\n\n## Hierarchical Clustering Algorithm Inputs\n\n1. __Input data__ as either\n    * __2-D sample/feature matrix__\n    * __1D condensed distance matrix__ -- upper triangular of pairwise distance matrix flattened\n2. The cluster __linkage__ method (__single__, complete, average, ward, ...)\n3. The __distance metric__ (_Euclidean_, manhattan, Jaccard, ...)\n\n## Hierarchical Clustering Algorithm Output\n\nAn $(n-1,4)$ shaped __linkage matrix__.\n\nWhere __each row__ is `[idx1, idx2, dist, sample_count]` is a single step in the clustering\nprocess and\n\n`idx1` and `idx2` are indices of the cluster being merged, where \n\n* indices $\\{ 0...n-1 \\}$ refer to the original data points and \n* indices $\\{n, n+1, ...\\}$ refer to each new cluster created\n\nAnd `sample_count` is the number of original data samples in the new cluster.\n\n\n## Hierarchical Clustering Algorithm Explained\n\n1. __Initialization__\n    * Start with each data point as its own cluster.\n    * Calculate the distance matrix if not provided\n2. __Iterative Clustering__\n    * At each step, the two closest clusters (according to linkage method and distance metric)\n      are merged into a new cluster.\n    * The distance between new cluster and the remaining clusters is added\n    * Repeat previous two steps until only one cluster remains.\n\n\n## Defining Cluster Proximity\n\nGiven two clusters, how do we define the _distance_ between them?\n\nHere are three natural ways to do it.\n\n## Cluster Proximity -- Single-Linkage\n\n![](./figs/L08-hierarchical-criteria-a.jpeg){height=\"300px\" fig-align=\"center\"}\n\n__Single-Linkage:__ the distance between two clusters is the distance between the\nclosest two points that are in different clusters.\n   \n$$\nD_\\text{single}(i,j) = \\min_{x, y}\\{d(x, y) \\,|\\, x \\in C_i, y \\in C_j\\}\n$$\n\n## Cluster Proximity -- Complete-Linkage\n\n![](./figs/L08-hierarchical-criteria-b.jpeg){height=\"300px\" fig-align=\"center\"}\n\n__Complete-Linkage:__ the distance between two clusters is the distance between\nthe farthest two points that are in different clusters.\n\n$$\nD_\\text{complete}(i,j) = \\max_{x, y}\\{d(x, y) \\,|\\, x \\in C_i, y \\in C_j\\}\n$$\n\n## Cluster Proximity -- Average-Linkage\n\n![](./figs/L08-hierarchical-criteria-c.jpeg){height=\"300px\" fig-align=\"center\"}\n\n__Average-Linkage:__ the distance between two clusters is the average distance between all pairs of points from different clusters.\n\n$$\nD_\\text{average}(i,j) = \\frac{1}{|C_i|\\cdot|C_j|}\\sum_{x \\in C_i,\\, y \\in C_j}d(x, y)\n$$\n\n## Cluster Proximity Example\n\nNotice that it is easy to express the definitions above in terms of similarity instead of distance.\n\nHere is a set of 6 points that we will cluster to show differences between distance metrics.\n\n::: {#7a9453df .cell fig-width='600px' execution_count=17}\n``` {.python .cell-code}\npt_x = [0.4, 0.22, 0.35, 0.26, 0.08, 0.45]\npt_y = [0.53, 0.38, 0.32, 0.19, 0.41, 0.30]\nplt.plot(pt_x, pt_y, 'o', markersize = 10, color = 'k')\nplt.ylim([.15, .60])\nplt.xlim([0.05, 0.70])\nfor i in range(6):\n    plt.annotate(f'{i}', (pt_x[i]+0.02, pt_y[i]-0.01), fontsize = 12)\nplt.axis('on')\nplt.xticks([])\nplt.yticks([])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-17-output-1.png){width=763 height=389 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Cluster Proximity Example, cont.\n:::\n\nWe can calculate the distance matrix\n\n::: {#b2ef8f4d .cell execution_count=18}\n``` {.python .cell-code}\nX = np.array([pt_x, pt_y]).T\nfrom scipy.spatial import distance_matrix\nlabels = ['p0', 'p1', 'p2', 'p3', 'p4', 'p5']\nD = pd.DataFrame(distance_matrix(X, X), index = labels, columns = labels)\nD.style.format('{:.2f}')\n```\n\n::: {.cell-output .cell-output-display execution_count=95}\n```{=html}\n<style type=\"text/css\">\n</style>\n<table id=\"T_62024\">\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_62024_level0_col0\" class=\"col_heading level0 col0\" >p0</th>\n      <th id=\"T_62024_level0_col1\" class=\"col_heading level0 col1\" >p1</th>\n      <th id=\"T_62024_level0_col2\" class=\"col_heading level0 col2\" >p2</th>\n      <th id=\"T_62024_level0_col3\" class=\"col_heading level0 col3\" >p3</th>\n      <th id=\"T_62024_level0_col4\" class=\"col_heading level0 col4\" >p4</th>\n      <th id=\"T_62024_level0_col5\" class=\"col_heading level0 col5\" >p5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_62024_level0_row0\" class=\"row_heading level0 row0\" >p0</th>\n      <td id=\"T_62024_row0_col0\" class=\"data row0 col0\" >0.00</td>\n      <td id=\"T_62024_row0_col1\" class=\"data row0 col1\" >0.23</td>\n      <td id=\"T_62024_row0_col2\" class=\"data row0 col2\" >0.22</td>\n      <td id=\"T_62024_row0_col3\" class=\"data row0 col3\" >0.37</td>\n      <td id=\"T_62024_row0_col4\" class=\"data row0 col4\" >0.34</td>\n      <td id=\"T_62024_row0_col5\" class=\"data row0 col5\" >0.24</td>\n    </tr>\n    <tr>\n      <th id=\"T_62024_level0_row1\" class=\"row_heading level0 row1\" >p1</th>\n      <td id=\"T_62024_row1_col0\" class=\"data row1 col0\" >0.23</td>\n      <td id=\"T_62024_row1_col1\" class=\"data row1 col1\" >0.00</td>\n      <td id=\"T_62024_row1_col2\" class=\"data row1 col2\" >0.14</td>\n      <td id=\"T_62024_row1_col3\" class=\"data row1 col3\" >0.19</td>\n      <td id=\"T_62024_row1_col4\" class=\"data row1 col4\" >0.14</td>\n      <td id=\"T_62024_row1_col5\" class=\"data row1 col5\" >0.24</td>\n    </tr>\n    <tr>\n      <th id=\"T_62024_level0_row2\" class=\"row_heading level0 row2\" >p2</th>\n      <td id=\"T_62024_row2_col0\" class=\"data row2 col0\" >0.22</td>\n      <td id=\"T_62024_row2_col1\" class=\"data row2 col1\" >0.14</td>\n      <td id=\"T_62024_row2_col2\" class=\"data row2 col2\" >0.00</td>\n      <td id=\"T_62024_row2_col3\" class=\"data row2 col3\" >0.16</td>\n      <td id=\"T_62024_row2_col4\" class=\"data row2 col4\" >0.28</td>\n      <td id=\"T_62024_row2_col5\" class=\"data row2 col5\" >0.10</td>\n    </tr>\n    <tr>\n      <th id=\"T_62024_level0_row3\" class=\"row_heading level0 row3\" >p3</th>\n      <td id=\"T_62024_row3_col0\" class=\"data row3 col0\" >0.37</td>\n      <td id=\"T_62024_row3_col1\" class=\"data row3 col1\" >0.19</td>\n      <td id=\"T_62024_row3_col2\" class=\"data row3 col2\" >0.16</td>\n      <td id=\"T_62024_row3_col3\" class=\"data row3 col3\" >0.00</td>\n      <td id=\"T_62024_row3_col4\" class=\"data row3 col4\" >0.28</td>\n      <td id=\"T_62024_row3_col5\" class=\"data row3 col5\" >0.22</td>\n    </tr>\n    <tr>\n      <th id=\"T_62024_level0_row4\" class=\"row_heading level0 row4\" >p4</th>\n      <td id=\"T_62024_row4_col0\" class=\"data row4 col0\" >0.34</td>\n      <td id=\"T_62024_row4_col1\" class=\"data row4 col1\" >0.14</td>\n      <td id=\"T_62024_row4_col2\" class=\"data row4 col2\" >0.28</td>\n      <td id=\"T_62024_row4_col3\" class=\"data row4 col3\" >0.28</td>\n      <td id=\"T_62024_row4_col4\" class=\"data row4 col4\" >0.00</td>\n      <td id=\"T_62024_row4_col5\" class=\"data row4 col5\" >0.39</td>\n    </tr>\n    <tr>\n      <th id=\"T_62024_level0_row5\" class=\"row_heading level0 row5\" >p5</th>\n      <td id=\"T_62024_row5_col0\" class=\"data row5 col0\" >0.24</td>\n      <td id=\"T_62024_row5_col1\" class=\"data row5 col1\" >0.24</td>\n      <td id=\"T_62024_row5_col2\" class=\"data row5 col2\" >0.10</td>\n      <td id=\"T_62024_row5_col3\" class=\"data row5 col3\" >0.22</td>\n      <td id=\"T_62024_row5_col4\" class=\"data row5 col4\" >0.39</td>\n      <td id=\"T_62024_row5_col5\" class=\"data row5 col5\" >0.00</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n## Single-Linkage Clustering\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n![](./figs/L08-singlelink-pointset.png){width=\"600px\"}\n\n:::\n::: {.column width=\"50%\"}\n\n::: {#be3f4c0d .cell execution_count=19}\n``` {.python .cell-code}\nimport scipy.cluster\nimport scipy.cluster.hierarchy as hierarchy\nZ = hierarchy.linkage(X, method='single', metric='euclidean')\nhierarchy.dendrogram(Z)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-19-output-1.png){width=802 height=414}\n:::\n:::\n\n\n:::\n::::\n\n## Single Linkage Clustering Advantages\n\nSingle-linkage clustering can handle non-elliptical shapes.\n\nHere we use SciPy's [fcluster](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.fcluster.html)\nto form flat custers from hierarchical clustering.\n\n::: {#73b85ac2 .cell execution_count=20}\n``` {.python .cell-code}\nX_moon_05, y_moon_05 = sk_data.make_moons(random_state = 0, noise = 0.05)\n\nZ = hierarchy.linkage(X_moon_05, method='single')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n\nplt.scatter(X_moon_05[:, 0], X_moon_05[:, 1], c = [['b','g'][i-1] for i in labels])\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-20-output-1.png){width=763 height=389 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Single Linkage Advantages, cont.\n:::\n\nSingle-Linkage can find different sized clusters:\n\n::: {#7e7f6dfc .cell execution_count=21}\n``` {.python .cell-code}\nX_rand_lo, y_rand_lo = sk_data.make_blobs(n_samples=[20, 200], centers = [[1, 1], [3, 1]], n_features = 2,\n                          center_box = (-10.0, 10.0), cluster_std = [.1, .5], random_state = 0)\n\nZ = hierarchy.linkage(X_rand_lo, method='single')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n\nplt.scatter(X_rand_lo[:, 0], X_rand_lo[:, 1], c = [['b','g'][i-1] for i in labels])\n# plt.title('Single-Linkage Can Find Different-Sized Clusters')\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-21-output-1.png){width=763 height=389}\n:::\n:::\n\n\n## Single Linkage Disadvantages\n\nSingle-linkage clustering can be sensitive to noise and outliers.\n\nThe results can change drastically on even slightly more noisy data.\n\n::: {#78470053 .cell execution_count=22}\n``` {.python .cell-code}\nX_moon_10, y_moon_10 = sk_data.make_moons(random_state = 0, noise = 0.1)\n\nZ = hierarchy.linkage(X_moon_10, method='single')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n\nplt.scatter(X_moon_10[:, 0], X_moon_10[:, 1], c = [['b','g'][i-1] for i in labels])\n# plt.title('Single-Linkage Clustering Changes Drastically on Slightly More Noisy Data')\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-22-output-1.png){width=763 height=389}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Single Linkage Disadvantages, cont.\n:::\n\nAnd here's another example where we bump the standard deviation on the clusters slightly.\n\n::: {#c1987c93 .cell execution_count=23}\n``` {.python .cell-code}\nX_rand_hi, y_rand_hi = sk_data.make_blobs(n_samples=[20, 200], centers = [[1, 1], [3, 1]], n_features = 2,\n                          center_box = (-10.0, 10.0), cluster_std = [.15, .6], random_state = 0)\n\nZ = hierarchy.linkage(X_rand_hi, method='single')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n\nplt.scatter(X_rand_hi[:, 0], X_rand_hi[:, 1], c = [['b','g'][i-1] for i in labels])\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-23-output-1.png){width=763 height=389}\n:::\n:::\n\n\n## Complete-Linkage Clustering\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n![](./figs/L08-completelink-pointset.png){width=\"100%\"}\n\n:::\n::: {.column width=\"50%\"}\n\n::: {#f03f59e9 .cell execution_count=24}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X, method='complete')\nhierarchy.dendrogram(Z)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-24-output-1.png){width=802 height=414}\n:::\n:::\n\n\n:::\n::::\n\n## Complete-Linkage Clustering Advantages\n\nProduces more-balanced clusters -- more-equal diameters\n\n::: {#6b0a2df0 .cell execution_count=25}\n``` {.python .cell-code}\nX_moon_05, y_moon_05 = sk_data.make_moons(random_state = 0, noise = 0.05)\n\nZ = hierarchy.linkage(X_moon_05, method='complete')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n\nplt.scatter(X_moon_05[:, 0], X_moon_05[:, 1], c = [['b','g'][i-1] for i in labels])\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-25-output-1.png){width=763 height=389}\n:::\n:::\n\n\n::: {#022205b0 .cell execution_count=26}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X_rand_hi, method='complete')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\n\nplt.scatter(X_rand_hi[:, 0], X_rand_hi[:, 1], c = [['b','g'][i-1] for i in labels])\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-26-output-1.png){width=763 height=389}\n:::\n:::\n\n\nLess susceptible to noise:\n\n::: {#b979becc .cell execution_count=27}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X_moon_10, method='complete')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\nplt.scatter(X_moon_10[:, 0], X_moon_10[:, 1], c = [['b','g'][i-1] for i in labels])\n\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-27-output-1.png){width=763 height=389}\n:::\n:::\n\n\n## Complete-Linkage Clustering Disadvantages\n\nSome disadvantages for complete-linkage clustering are:\n\n- Sensitivity to outliers\n- Tendency to compute more compact, spherical clusters\n- Computationally intensive for large datasets.\n\n## Average-Linkage Clustering\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n![](./figs/L08-averagelink-pointset.png){width=\"100%\"}\n\n:::\n::: {.column width=\"50%\"}\n\n::: {#500c1add .cell execution_count=28}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X, method='average')\nhierarchy.dendrogram(Z)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-28-output-1.png){width=802 height=414}\n:::\n:::\n\n\n:::\n::::\n\n## Average-Linkage Clustering Strengths and Limitations\n\nAverage-Linkage clustering is in some sense a compromise between Single-linkage and Complete-linkage clustering.\n\n__Strengths:__\n    \n* Less susceptible to noise and outliers\n\n__Limitations:__\n    \n* Biased toward elliptical clusters\n\nProduces more isotropic clusters.\n\n::: {#51787ad8 .cell execution_count=29}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X_moon_10, method='average')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\nplt.scatter(X_moon_10[:, 0], X_moon_10[:, 1], c = [['b','g'][i-1] for i in labels])\n\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-29-output-1.png){width=763 height=389}\n:::\n:::\n\n\nMore resistant to noise than Single-Linkage.\n\n::: {#1f344d53 .cell execution_count=30}\n``` {.python .cell-code}\nZ = hierarchy.linkage(X_rand_hi, method='average')\nlabels = hierarchy.fcluster(Z, 2, criterion = 'maxclust')\nplt.scatter(X_rand_hi[:, 0], X_rand_hi[:, 1], c = [['b','g'][i-1] for i in labels])\n\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-30-output-1.png){width=763 height=389}\n:::\n:::\n\n\n## All Three Compared\n\n::: {layout-ncol=\"3\"}\n\n![Single-Linkage](./figs/L08-singlelink-pointset.png){width=\"100%\"}\n\n![Complete-Linkage](./figs/L08-completelink-pointset.png){width=\"100%\"}\n\n![Average-Linkage](./figs/L08-averagelink-pointset.png){width=\"100%\"}\n\n:::\n\n## Ward's Distance\n\nFinally, we consider one more cluster distance.\n\nWard's distance asks \"What if we combined these two clusters -- how would clustering improve?\"\n\nTo define \"how would clustering improve?\" we appeal to the $k$-means criterion.\n\nSo:\n\n__Ward's Distance__ between clusters $C_i$ and $C_j$ is the difference between\nthe total within cluster sum of squares for the two clusters separately, \n__compared to__ the _within cluster sum of squares_ resulting from merging the two\nclusters into a new cluster $C_{i+j}$:\n\n$$\nD_\\text{Ward}(i, j) = \\sum_{x \\in C_i} (x - c_i)^2 + \\sum_{x \\in C_j} (x - c_j)^2  - \\sum_{x \\in C_{i+j}} (x - c_{i+j})^2 \n$$\n\nwhere $c_i, c_j, c_{i+j}$ are the corresponding cluster centroids.\n\n::: {.content-visible when-profile=\"slides\"}\n## Ward's Distance continued\n:::\n\nIn a sense, this cluster distance results in a hierarchical analog of $k$-means.\n\nAs a result, it has properties similar to $k$-means:\n    \n* Less susceptible to noise and outliers\n* Biased toward elliptical clusters\n\nHence it tends to behave more like average-linkage hierarchical clustering.\n\n\n# Hierarchical Clustering in Practice\n\n## Hierarchical Clustering In Practice\n\nNow we'll look at doing hierarchical clustering in practice.\n\nWe'll use the same synthetic data as we did in the k-means case -- i.e., three \"blobs\" living in 30 dimensions.\n\n::: {.content-visible when-profile=\"slides\"}\n## Hierarchical Clustering in Practice, cont.\n:::\n\n::: {#7a1cb5a3 .cell execution_count=31}\n``` {.python .cell-code}\nX, y = sk_data.make_blobs(n_samples=100, centers=3, n_features=30,\n                          center_box=(-10.0, 10.0),random_state=0)\n```\n:::\n\n\nThe raw data is shown in the following visualization: \n\n::: {#0e4b48ab .cell execution_count=32}\n``` {.python .cell-code}\nsns.heatmap(X, xticklabels=False, yticklabels=False, linewidths=0,cbar=False)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-32-output-1.png){width=763 height=389}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Hierarchical Clustering in Practice, cont.\n:::\n\n\nthen an embedding into 2-D (using MDS).\n\n::: {#a74e1c82 .cell execution_count=33}\n``` {.python .cell-code}\nimport sklearn.manifold\nimport sklearn.metrics as metrics\neuclidean_dists = metrics.euclidean_distances(X)\nmds = sklearn.manifold.MDS(n_components = 2, max_iter = 3000, eps = 1e-9, random_state = 0,\n                   dissimilarity = \"precomputed\", n_jobs = 1)\nfit = mds.fit(euclidean_dists)\npos = fit.embedding_\nplt.axis('equal')\nplt.scatter(pos[:, 0], pos[:, 1], s = 8)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-33-output-1.png){width=801 height=411}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Hierarchical Clustering in Practice, cont.\n:::\n\nHierarchical clustering is available in __`sklearn`__, but there is a much more\nfully developed set of \n[tools](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html) \nin the [scipy](https://docs.scipy.org/doc/scipy/index.html) package and that is the one to use.\n\nLet's run hierarchical clustering on our synthetic dataset.\n\n::: {.callout-tip}\nTry the other linkage methods and see how the clustering and dendrogram changes.\n:::\n\n::: {#6e478bc8 .cell execution_count=34}\n``` {.python .cell-code}\nimport scipy.cluster\nimport scipy.cluster.hierarchy as hierarchy\nimport scipy.spatial.distance\n\n# linkages = ['single','complete','average','weighted','ward']\nZ = hierarchy.linkage(X, method = 'single')\n```\n:::\n\n\nAnd draw the dendrogram.\n\n::: {#ed0638ee .cell execution_count=35}\n``` {.python .cell-code}\nR = hierarchy.dendrogram(Z)\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-35-output-1.png){width=790 height=407}\n:::\n:::\n\n\n## Hierarchical Clustering Real Data\n\nOnce again we'll use the\n[\"20 Newsgroup\"](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)\ndata provided as example data in sklearn.\n\nLoad three of the newsgroups.\n\n::: {#bc9b10c2 .cell execution_count=36}\n``` {.python .cell-code}\nfrom sklearn.datasets import fetch_20newsgroups\ncategories = ['comp.os.ms-windows.misc', 'sci.space','rec.sport.baseball']\nnews_data = fetch_20newsgroups(subset = 'train', categories = categories)\n```\n:::\n\n\nVectorize the data.\n\n::: {#310e2b45 .cell execution_count=37}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words='english', min_df = 4, max_df = 0.8)\ndata = vectorizer.fit_transform(news_data.data).todense()\ndata.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=114}\n```\n(1781, 9409)\n```\n:::\n:::\n\n\nCluster hierarchically and display dendrogram. Feel free to experiment with different metrics.\n\n::: {#eb864467 .cell execution_count=38}\n``` {.python .cell-code}\n# linkages are one of 'single','complete','average','weighted','ward'\n#\n# metrics can be braycurtis, canberra, chebyshev, cityblock, correlation, cosine, \n# dice, euclidean, hamming, jaccard, kulsinski, mahalanobis, matching, \n# minkowski, rogerstanimoto, russellrao, seuclidean, sokalmichener, sokalsneath, \n# sqeuclidean, yule.\n\nZ_20ng = hierarchy.linkage(data, method = 'ward', metric = 'euclidean')\nplt.figure(figsize=(14,4))\nR_20ng = hierarchy.dendrogram(Z_20ng, p=4, truncate_mode = 'level', show_leaf_counts=True)\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-38-output-1.png){width=1079 height=361}\n:::\n:::\n\n\n## Selecting the Number of Clusters\n\nLet's flatten the hierarchy to different numbers clusters and calculate the \n_Silhouette Score_.\n\n::: {#c49c47fb .cell execution_count=39}\n``` {.python .cell-code}\nmax_clusters = 20\ns = np.zeros(max_clusters+1)\n\nfor k in range(2, max_clusters+1):\n    clusters = hierarchy.fcluster(Z_20ng, k, criterion = 'maxclust')\n    s[k] = metrics.silhouette_score(np.asarray(data), clusters, metric = 'euclidean')\n\nplt.plot(range(2, len(s)), s[2:], '.-')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](08-Clustering-III-hierarchical_files/figure-revealjs/cell-39-output-1.png){width=829 height=429}\n:::\n:::\n\n\nWe see a first peak at 5.\n\nTop terms per cluster when we flatten to a depth of 5.\n\n::: {#b35ac9a1 .cell execution_count=40}\n``` {.python .cell-code}\nk = 5\nclusters = hierarchy.fcluster(Z_20ng, k, criterion = 'maxclust')\nfor i in range(1,k+1):\n    items = np.array([item for item,clust in zip(data, clusters) if clust == i])\n    centroids = np.squeeze(items).mean(axis = 0)\n    asc_order_centroids = centroids.argsort()#[:, ::-1]\n    order_centroids = asc_order_centroids[::-1]\n    terms = vectorizer.get_feature_names_out()\n    print(f'Cluster {i}:')\n    for ind in order_centroids[:10]:\n        print(f' {terms[ind]}')\n    print('')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster 1:\n space\n nasa\n edu\n henry\n gov\n alaska\n access\n com\n moon\n digex\n\nCluster 2:\n ax\n max\n b8f\n g9v\n a86\n 145\n 1d9\n pl\n 2di\n 0t\n\nCluster 3:\n edu\n com\n year\n baseball\n article\n writes\n cs\n team\n game\n university\n\nCluster 4:\n risc\n instruction\n ghhwang\n csie\n set\n nctu\n cisc\n tw\n reduced\n mq\n\nCluster 5:\n windows\n edu\n file\n dos\n com\n files\n card\n drivers\n driver\n use\n\n```\n:::\n:::\n\n\n## Comparison of Linkages\n\nScikit-Learn has a very nice [notebook](https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html)\nand plot, copied here, that shows the different clusters resulting from different\nlinkage methods.\n\n![](./figs/L08-sphx_glr_plot_linkage_comparison_001.png){width=\"100%\" fig-align=\"center\"}\n\n\n\n\n# Recap\n\n## Clustering Recap\n\nThis wraps up our _partitional_ Cluster topics. We covered:\n\n::: {.incremental}\n* What the clustering problem is\n* An overview of the $k$-means clustering algorithm including initialization with $k$-means++\n* Visualization techniques such as Multi-Dimensional Scaling\n* Cluster evaluation with (Adjusted) Rand Index and Silhouette Coefficient\n* Using evaluation to determine number of clusters\n* Hierarchical Clustering with different methods and metrics\n* Looked at applications of clustering on various types of synthetic data, image\n  color quantization, newsgroup clustering\n:::\n\n",
    "supporting": [
      "08-Clustering-III-hierarchical_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}