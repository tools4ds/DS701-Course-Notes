{
  "hash": "6efdd6e57bd4f2022761a6ffbbe63604",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Logistic Regression\njupyter: python3\n---\n\n## Introduction\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/18-Regression-II-Logistic.ipynb)\n\n\n\nSo far we have seen linear regression: \n\n* a continuous valued observation is estimated as a linear (or affine) function\n of the independent variables.\n\nNow we will look at the following situation.\n\n## Estimating a Probability\n\n::: {.incremental}\n\n* Imagine that you are observing a binary variable -- value 0 or 1.\n\n* That is, these could be pass/fail, admit/reject, Democrat/Republican, etc.\n\n* Assume there is some __probability__ of observing a 1, and that probability is a\nfunction of certain independent variables.\n\n* So the key properties of a problem that make it appropriate for logistic\nregression are:\n\n    * You are trying to predict a __categorical__ variable\n    * You want to estimate a __probability__ of seeing a particular value of the\n      categorical variable.\n\n:::\n\n## Example: Grad School Admission\n\n::: {.content-visible when-profile=\"web\"}\n\n::: {.callout-note}\nThe following example was adapted from this \n[URL](http://www.ats.ucla.edu/stat/r/dae/logit.htm) which seems to be no longer\navailable. There is an \n[archive of the page](https://web.archive.org/web/20161118221128/http://www.ats.ucla.edu/stat/r/dae/logit.htm)\nand an [archive of the dataset](https://web.archive.org/web/20161022051727/http://www.ats.ucla.edu/stat/data/binary.csv).\n:::\n:::\n\nLet's consider this question:\n\n> What is the probability I will be admitted to Grad School?\n\nLet's see how variables, such as,\n\n* _GRE_ (Graduate Record Exam scores), \n* _GPA_ (grade point average), and \n* prestige of the undergraduate institution\n\naffect admission into graduate school. \n\n::: {.content-visible when-profile=\"slides\"}\n## Example continued\n:::\n\nThe response variable, admit/don't admit, is a binary variable.\n\nSo there are three predictor variables: __gre,__ __gpa__ and __rank.__ \n\n* We will treat the variables _gre_ and _gpa_ as continuous. \n* The variable _rank_ takes on the values 1 through 4 with 1 being the highest prestige.\n\n::: {.content-visible when-profile=\"slides\"}\n## Example continued\n:::\n\nLet's look at 10 lines of the data:\n\n::: {#283679b1 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=113}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>admit</th>\n      <th>gre</th>\n      <th>gpa</th>\n      <th>rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>380</td>\n      <td>3.61</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>660</td>\n      <td>3.67</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>800</td>\n      <td>4.00</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>640</td>\n      <td>3.19</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>520</td>\n      <td>2.93</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1</td>\n      <td>760</td>\n      <td>3.00</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1</td>\n      <td>560</td>\n      <td>2.98</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>400</td>\n      <td>3.08</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1</td>\n      <td>540</td>\n      <td>3.39</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>700</td>\n      <td>3.92</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#181fd01c .cell execution_count=4}\n``` {.python .cell-code}\ndf.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=114}\n```\n(400, 4)\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Example continued\n:::\n\nand some summary statistics:\n\n::: {#606d826f .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\ndf.describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=115}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>admit</th>\n      <th>gre</th>\n      <th>gpa</th>\n      <th>rank</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>400.000000</td>\n      <td>400.000000</td>\n      <td>400.000000</td>\n      <td>400.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.317500</td>\n      <td>587.700000</td>\n      <td>3.389900</td>\n      <td>2.48500</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.466087</td>\n      <td>115.516536</td>\n      <td>0.380567</td>\n      <td>0.94446</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>220.000000</td>\n      <td>2.260000</td>\n      <td>1.00000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>520.000000</td>\n      <td>3.130000</td>\n      <td>2.00000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>580.000000</td>\n      <td>3.395000</td>\n      <td>2.00000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>660.000000</td>\n      <td>3.670000</td>\n      <td>3.00000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>800.000000</td>\n      <td>4.000000</td>\n      <td>4.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Example continued\n:::\n\nWe can also plot histograms of the variables:\n\n::: {#47b1b366 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\ndf.hist(figsize = (10, 4));\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-revealjs/cell-6-output-1.png){width=798 height=357}\n:::\n:::\n\n\n> Note how `df.hist()` automatically plots a histogram for each column in the\n> dataframe as a subplot.\n\n::: {.content-visible when-profile=\"slides\"}\n## Example continued\n:::\n\nLet's look at how each independent variable affects admission probability by\nplotting the mean admission probability as a function of the independent variable.\n\n> Note that there's a greatly expanded [`groupby`](https://tools4ds.github.io/DS701-Course-Notes/02B-Pandas.html#understanding-pandas-dataframe.groupby) section in the Pandas refresher.\n\nWe add error bars to the means to indicate the standard error of the mean.\n\n::: {.content-visible when-profile=\"slides\"}\n## Example continued\n:::\n\nFirst, __rank__:\n\n::: {#1b08c995 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\n\n# Calculate mean and standard error\ngrouped = df.groupby('rank')['admit']\nmeans = grouped.mean()\n\n# Compute 'standard error of the mean'\nerrors = grouped.std() / np.sqrt(grouped.count())\n\n# Plot with error bars\nax = means.plot(marker='o', yerr=errors, fontsize=12, capsize=5)\nax.set_ylabel('P[admit]', fontsize=16)\nax.set_xlabel('Rank', fontsize=16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-revealjs/cell-7-output-1.png){width=824 height=440}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Example continued\n:::\n\nNext, __GRE__:\n\n::: {#764510de .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\ngrouped_gre = df.groupby('gre')['admit']\nmeans_gre = grouped_gre.mean()\nerrors_gre = grouped_gre.std() / np.sqrt(grouped_gre.count())\n\nax = means_gre.plot(marker='o', yerr=errors_gre, fontsize=12, capsize=5)\nax.set_ylabel('P[admit]', fontsize=16)\nax.set_xlabel('GRE', fontsize=16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-revealjs/cell-8-output-1.png){width=824 height=440}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Example continued\n:::\n\nFinally, __GPA__ (for this visualization, we aggregate GPA into 8 bins):\n\n::: {#4a11753b .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\nbins = np.linspace(df.gpa.min(), df.gpa.max(), 8)\nbin_centers = (bins[:-1] + bins[1:]) / 2\ngrouped_gpa = df.groupby(np.digitize(df.gpa, bins)).mean()['admit']\nax = grouped_gpa.plot(marker='o', fontsize=12)\nax.set_ylabel('P[admit]', fontsize=16)\nax.set_xlabel('GPA', fontsize=16)\nax.set_xticks(range(1, len(bin_centers) + 1))\nax.set_xticklabels([f'{center:.2f}' for center in bin_centers], rotation=45);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-revealjs/cell-9-output-1.png){width=835 height=461}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Example continued\n:::\n\nFinally, we plot admission status versus GRE score for each data point for each of the four ranks:\n\n::: {#121a4c4d .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\ndf1 = df[df['rank']==1]\ndf2 = df[df['rank']==2]\ndf3 = df[df['rank']==3]\ndf4 = df[df['rank']==4]\n\nfig = plt.figure(figsize = (10, 5))\n\nax1 = fig.add_subplot(221)\ndf1.plot.scatter('gre','admit', ax = ax1)\nplt.title('Rank 1 Institutions')\n\nax2 = fig.add_subplot(222)\ndf2.plot.scatter('gre','admit', ax = ax2)\nplt.title('Rank 2 Institutions')\n\nax3 = fig.add_subplot(223, sharex = ax1)\ndf3.plot.scatter('gre','admit', ax = ax3)\nplt.title('Rank 3 Institutions')\n\nax4 = fig.add_subplot(224, sharex = ax2)\nplt.title('Rank 4 Institutions')\ndf4.plot.scatter('gre','admit', ax = ax4);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-revealjs/cell-10-output-1.png){width=812 height=449}\n:::\n:::\n\n\nWhat we want to do is to fit a model that predicts the probability of admission\nas a function of these independent variables.\n\n## Logistic Regression\n\n::: {.incremental}\n* Logistic regression is concerned with estimating a __probability.__\n\n* However, all that is available are categorical observations, which we will code as 0/1.\n\n* That is, these could be pass/fail, admit/reject, Democrat/Republican, etc.\n\n* Now, a linear function like $\\beta_0 + \\beta_1 x$ cannot be used to predict\nprobability directly, because \n    * the linear function takes on all values (from -$\\infty$ to +$\\infty$), \n    * and probability only ranges over $[0, 1]$.\n\n:::\n\n## Odds and Log-Odds\n\nHowever, there is a transformation of probability that works: it is called\n__log-odds__.\n\nFor any probabilty $p$, the __odds__ is defined as $p/(1-p)$, which is the ratio\nof the probability of an event to the probability of the non-event.\n\nNotice that odds vary from 0 to $\\infty$, and odds < 1 indicates that $p < 1/2$.\n\nNow, there is a good argument that to fit a linear function, instead of using\nodds, we should use log-odds. \n\n::: {.content-visible when-profile=\"slides\"}\n## Odds and Log-Odds\n:::\n\nThat is simply $\\log p/(1-p)$ which is also called the __logit__ function, which\nis an abbreviation for **log**istic un**it**.\n\n::: {#cb37ab48 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\npvec = np.linspace(0.01, 0.99, 100)\nax = plt.figure(figsize = (6, 4)).add_subplot()\nax.plot(pvec, np.log(pvec / (1-pvec)))\nax.tick_params(labelsize=12)\nax.set_xlabel('Probability', fontsize = 14)\nax.set_ylabel('Log-Odds', fontsize = 14)\nax.set_title('Logit Function: $\\log (p/1-p)$', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-revealjs/cell-11-output-1.png){width=522 height=389}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Odds and Log-Odds\n:::\n\nSo, logistic regression does the following: it does a linear regression of\n$\\beta_0 + \\beta_1 x$ against $\\log p/(1-p)$.\n\nThat is, it fits:\n\n$$\n\\begin{aligned}\n\\beta_0 + \\beta_1 x &= \\log \\frac{p(x)}{1-p(x)} \\\\\ne^{\\beta_0 + \\beta_1 x} &= \\frac{p(x)}{1-p(x)} \\quad \\text{(exponentiate both sides)} \\\\\ne^{\\beta_0 + \\beta_1 x} (1-p(x)) &= p(x) \\quad \\text{(multiply both sides by $1-p(x)$)} \\\\\ne^{\\beta_0 + \\beta_1 x}  &= p(x) + p(x)e^{\\beta_0 + \\beta_1 x} \\quad \\text{(distribute $p(x)$)} \\\\\n\\frac{e^{\\beta_0 + \\beta_1 x}}{1 +e^{\\beta_0 + \\beta_1 x}} &= p(x)\n\\end{aligned}\n$$\n\n::: {.content-visible when-profile=\"slides\"}\n## Odds and Log-Odds\n:::\n\nSo, logistic regression fits a probability of the following form:\n\n$$\np(x) = P(y=1\\mid x) = \\frac{e^{\\beta_0+\\beta_1 x}}{1+e^{\\beta_0+\\beta_1 x}}.\n$$\n\nThis is a **sigmoid function**; when $\\beta_1 > 0$, \n\n* as $x\\rightarrow \\infty$, then $p(x)\\rightarrow 1$ and \n* as $x\\rightarrow -\\infty$, then $p(x)\\rightarrow 0$.\n\n::: {.content-visible when-profile=\"slides\"}\n## Odds and Log-Odds\n:::\n\nHolding $\\beta_0$ constant, we see that as $\\beta_1$ increases, the logistic function becomes steeper.\n\n::: {#29464d31 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\nalphas = [-4, -8,-12,-20]\nalphas = [-8, -8, -8, -8]\nbetas = [0.2,0.4,0.6,1]\nx = np.arange(40)\nfig = plt.figure(figsize=(8, 6)) \nax = plt.subplot(111)\n\nfor i in range(len(alphas)):\n    a = alphas[i]\n    b = betas[i]\n    y = np.exp(a+b*x)/(1+np.exp(a+b*x))\n#     plt.plot(x,y,label=r\"$\\frac{e^{%d + %3.1fx}}{1+e^{%d + %3.1fx}}\\;\\beta_0=%d, \\beta_1=%3.1f$\" % (a,b,a,b,a,b))\n    ax.plot(x,y,label=r\"$\\beta_0=%d,$    $\\beta_1=%3.1f$\" % (a,b))\nax.tick_params(labelsize=12)\nax.set_xlabel('x', fontsize = 14)\nax.set_ylabel('$p(x)$', fontsize = 14)\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size': 16})\nax.set_title('Logistic Functions', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-revealjs/cell-12-output-1.png){width=965 height=536}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Odds and Log-Odds\n:::\n\nHolding $\\beta_1$ constant, we see that as $\\beta_0$ increases, the logistic function shifts to the right.\n\n::: {#16ed99f2 .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\nalphas = [-4, -8,-12,-20]\nbetas = [0.4, 0.4, 0.4, 0.4]\nx = np.arange(40)\nfig = plt.figure(figsize=(8, 6)) \nax = plt.subplot(111)\n\nfor i in range(len(alphas)):\n    a = alphas[i]\n    b = betas[i]\n    y = np.exp(a+b*x)/(1+np.exp(a+b*x))\n#     plt.plot(x,y,label=r\"$\\frac{e^{%d + %3.1fx}}{1+e^{%d + %3.1fx}}\\;\\beta_0=%d, \\beta_1=%3.1f$\" % (a,b,a,b,a,b))\n    ax.plot(x,y,label=r\"$\\beta_0=%d,$    $\\beta_1=%3.1f$\" % (a,b))\nax.tick_params(labelsize=12)\nax.set_xlabel('x', fontsize = 14)\nax.set_ylabel('$p(x)$', fontsize = 14)\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size': 16})\nax.set_title('Logistic Functions', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-revealjs/cell-13-output-1.png){width=979 height=536}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Odds and Log-Odds\n:::\n\nVarying both $\\beta_0$ and $\\beta_1$ gives us a more general logistic function.\n\n::: {#4cd01ca6 .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\nalphas = [-4, -8,-12,-20]\nbetas = [0.2,0.4,0.6,1]\nx = np.arange(40)\nfig = plt.figure(figsize=(8, 6)) \nax = plt.subplot(111)\n\nfor i in range(len(alphas)):\n    a = alphas[i]\n    b = betas[i]\n    y = np.exp(a+b*x)/(1+np.exp(a+b*x))\n#     plt.plot(x,y,label=r\"$\\frac{e^{%d + %3.1fx}}{1+e^{%d + %3.1fx}}\\;\\beta_0=%d, \\beta_1=%3.1f$\" % (a,b,a,b,a,b))\n    ax.plot(x,y,label=r\"$\\beta_0=%d,$    $\\beta_1=%3.1f$\" % (a,b))\nax.tick_params(labelsize=12)\nax.set_xlabel('x', fontsize = 14)\nax.set_ylabel('$p(x)$', fontsize = 14)\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5), prop={'size': 16})\nax.set_title('Logistic Functions', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-revealjs/cell-14-output-1.png){width=979 height=536}\n:::\n:::\n\n\nParameter $\\beta_1$ controls how fast $p(x)$ raises from $0$ to $1$\n\nThe value of -$\\beta_0$/$\\beta_1$ shows the value of $x$ for which $p(x)=0.5$\n\n::: {.content-visible when-profile=\"slides\"}\n## Odds and Log-Odds\n:::\n\nAnother interpretation of $\\beta_0$ is that it gives the __base rate__ -- the\nunconditional probability of a 1.   \n\nThat is, if you knew nothing about a\nparticular data item, then $p(x) = 1/(1+e^{-\\beta_0})$.\n\n::: {#1d7c5745 .cell execution_count=15}\n``` {.python .cell-code}\n# plot the base rate as a function of beta_0\nbeta_0_values = np.linspace(-10, 10, 100)\nbase_rate = 1 / (1 + np.exp(-beta_0_values))\nplt.figure(figsize=(6, 4))\nplt.plot(beta_0_values, base_rate)\nplt.xlabel('beta_0')\nplt.ylabel('Base Rate')\nplt.title('Base Rate as a function of beta_0')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-revealjs/cell-15-output-1.png){width=514 height=376 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Odds and Log-Odds\n:::\n\nThe function $f(x) = \\log (x/(1-x))$ is called the __logit__ function.\n\nSo a compact way to describe logistic regression is that it finds regression\ncoefficients $\\beta_0, \\beta_1$ to fit:\n\n$$\n\\text{logit}\\left(p(x)\\right)=\\log\\left(\\frac{p(x)}{1-p(x)} \\right) = \\beta_0 + \\beta_1 x.\n$$\n\nNote also that the __inverse__ logit function is:\n\n$$\n\\text{logit}^{-1}(x) = \\frac{e^x}{1 + e^x}.\n$$\n\nSomewhat confusingly, this is called the __logistic__ function.\n\n::: {.content-visible when-profile=\"slides\"}\n## Odds and Log-Odds\n:::\n\nSo, the best way to think of logistic regression is that we compute a linear function:\n    \n$$\n\\beta_0 + \\beta_1 x,\n$$\n    \nand then *map* that to a probability using the inverse $\\text{logit}$ function:\n\n$$\n\\frac{e^{\\beta_0+\\beta_1 x}}{1+e^{\\beta_0+\\beta_1 x}}.\n$$\n\n## Logistic vs Linear Regression\n\nLet's take a moment to compare linear and logistic regression.\n\nIn __Linear regression__ we fit \n\n$$\ny_i = \\beta_0 +\\beta_1 x_i + \\epsilon_i.\n$$\n\nWe do the fitting by minimizing the sum of squared errors $\\Vert\\epsilon\\Vert$.\nThis can be done in closed form using either geometric arguments or by calculus.\n\nNow, if $\\epsilon_i$ comes from a normal distribution with mean zero and some\nfixed variance, then minimizing the sum of squared errors is exactly the same as finding the\nmaximum likelihood of the data with respect to the probability of the errors.\n\nSo, in the case of linear regression, it is a lucky fact that the __MLE__ of\n$\\beta_0$ and $\\beta_1$ can be found by a __closed-form__ calculation.\n\n::: {.content-visible when-profile=\"slides\"}\n## Logistic vs Linear Regression\n:::\n\nIn __Logistic regression__ we fit \n\n$$\n\\text{logit}(p(x_i)) = \\beta_0 + \\beta_1 x_i.\n$$\n\n\nwith $\\text{P}(y_i=1\\mid x_i)=p(x_i).$\n\nHow should we choose parameters?   \n\nHere too, we use Maximum Likelihood Estimation of the parameters.\n\nThat is, we choose the parameter values that maximize the likelihood of the data given the model.\n\n$$\n\\text{P}(y_i \\mid x_i) = \n\\left\\{\\begin{array}{lr}\\text{logit}^{-1}(\\beta_0 + \\beta_1 x_i)& \\text{if } y_i = 1\\\\\n1 - \\text{logit}^{-1}(\\beta_0 + \\beta_1 x_i)& \\text{if } y_i = 0\\end{array}\\right.\n$$\n\n::: {.content-visible when-profile=\"slides\"}\n## Logistic vs Linear Regression\n:::\n\nWe can write this as a single expression:\n\n$$\n\\text{P}(y_i \\mid x_i) = \\text{logit}^{-1}(\\beta_0 + \\beta_1 x_i)^{y_i} (1-\\text{logit}^{-1}(\\beta_0 + \\beta_1 x_i))^{1-y_i},\n$$\n\nwhere we assume the parameters are fixed.\n\nWe can reinterpret this to express the __likelihood__ of parameters $\\beta_0$, $\\beta_1$:\n\n$$\nL(\\beta_0, \\beta_1 \\mid x_i, y_i) = \\text{logit}^{-1}(\\beta_0 + \\beta_1 x_i)^{y_i} (1-\\text{logit}^{-1}(\\beta_0 + \\beta_1 x_i))^{1-y_i},\n$$\n\ngiven that we have observed the data $(x_i, y_i)$.\n\n**This is our objective function to maximize.**\n\nHowever, there is no closed-form solution so we optimize it numerically with gradient descent.\n\n\n## How Gradient Descent Works\n\n**Algorithm:**\n\n1. **Initialize** parameters: Start with random values $\\beta^{(0)}$\n\n2. **Compute gradient**: Calculate $\\nabla \\ell(\\beta^{(t)})$ - the direction of steepest increase\n\n3. **Update parameters**: Take a step in that direction:\n   $$\\beta^{(t+1)} = \\beta^{(t)} + \\alpha \\nabla \\ell(\\beta^{(t)})$$\n   where $\\alpha$ is the **learning rate** (step size)\n\n4. **Repeat** steps 2-3 until convergence (gradient $\\approx 0$ or max iterations reached)\n\n**Result:** Parameters that (locally) maximize the likelihood of the observed data.\n\n## Logistic Regression In Practice\n\nSo, in summary, we have:\n\n**Input** pairs $(x_i,y_i)$\n\n**Output** parameters $\\widehat{\\beta_0}$ and $\\widehat{\\beta_1}$ that maximize the\nlikelihood of the data given these parameters for the logistic regression model.\n\n**Method** Maximum likelihood estimation, obtained by gradient descent.\n\nThe standard package will give us a coefficient $\\beta_i$ for each\nindependent variable (feature).\n\n::: {.content-visible when-profile=\"slides\"}\n## Logistic Regression in Practice\n:::\n\nIf we want to include a constant (i.e., $\\beta_0$) we need to add a column of 1s (just\nlike in linear regression).\n\n::: {#df17aabf .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\"}\ndf['intercept'] = 1.0\ntrain_cols = df.columns[1:]\ntrain_cols\n```\n\n::: {.cell-output .cell-output-display execution_count=126}\n```\nIndex(['gre', 'gpa', 'rank', 'intercept'], dtype='object')\n```\n:::\n:::\n\n\n::: {#4f4e8a15 .cell execution_count=17}\n``` {.python .cell-code code-fold=\"true\"}\nlogit = sm.Logit(df['admit'], df[train_cols])\n \n# fit the model\nresult = logit.fit() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.574302\n         Iterations 6\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Logistic Regression in Practice\n:::\n\nStatsmodels gives us a summary of the model fit.\n\n::: {#900da3e1 .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\nresult.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=128}\n```{=html}\n<table class=\"simpletable\">\n<caption>Logit Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>         <td>admit</td>      <th>  No. Observations:  </th>  <td>   400</td>  \n</tr>\n<tr>\n  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   396</td>  \n</tr>\n<tr>\n  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td>  \n</tr>\n<tr>\n  <th>Date:</th>            <td>Tue, 28 Oct 2025</td> <th>  Pseudo R-squ.:     </th>  <td>0.08107</td> \n</tr>\n<tr>\n  <th>Time:</th>                <td>10:40:08</td>     <th>  Log-Likelihood:    </th> <td> -229.72</td> \n</tr>\n<tr>\n  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -249.99</td> \n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>8.207e-09</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>gre</th>       <td>    0.0023</td> <td>    0.001</td> <td>    2.101</td> <td> 0.036</td> <td>    0.000</td> <td>    0.004</td>\n</tr>\n<tr>\n  <th>gpa</th>       <td>    0.7770</td> <td>    0.327</td> <td>    2.373</td> <td> 0.018</td> <td>    0.135</td> <td>    1.419</td>\n</tr>\n<tr>\n  <th>rank</th>      <td>   -0.5600</td> <td>    0.127</td> <td>   -4.405</td> <td> 0.000</td> <td>   -0.809</td> <td>   -0.311</td>\n</tr>\n<tr>\n  <th>intercept</th> <td>   -3.4495</td> <td>    1.133</td> <td>   -3.045</td> <td> 0.002</td> <td>   -5.670</td> <td>   -1.229</td>\n</tr>\n</table>\n```\n:::\n:::\n\n\nNotice that all of our independent variables are considered significant (no\nconfidence intervals contain zero).\n\n## Using the Model\n\nNote that by fitting a model to the data, we can make predictions for inputs that\nwere not in the training data.  \n\nFurthermore, we can make a prediction of a probability for cases where we don't\nhave enough data to estimate the probability directly -- e.g., for specific\nparameter values.\n\nLet's see how well the model fits the data.\n\n::: {.content-visible when-profile=\"slides\"}\n## Using the Model\n:::\n\nWe have three independent variables, so in each case we'll use average values\nfor the two that we aren't evaluating.\n\nGPA (GRE = 600, Rank = 2.5):\n\n::: {#7ffcb215 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\"}\nbins = np.linspace(df.gpa.min(), df.gpa.max(), 10)\ngroups = df.groupby(np.digitize(df.gpa, bins))\nprob = [result.predict([600, b, 2.5, 1.0]) for b in bins]\nax = plt.figure(figsize = (7, 4)).add_subplot()\nax.plot(bins, prob)\nax.plot(bins,groups.admit.mean(),'o')\nax.tick_params(labelsize=12)\nax.set_xlabel('gpa', fontsize = 14)\nax.set_ylabel('P[admit]', fontsize = 14)\nax.set_title('Marginal Effect of GPA', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-revealjs/cell-19-output-1.png){width=609 height=388 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Logistic Regression in Practice\n:::\n\nGRE Score (GPA = 3.4, Rank = 2.5):\n\n::: {#5e24bd1f .cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\"}\nprob = [result.predict([b, 3.4, 2.5, 1.0]) for b in sorted(df.gre.unique())]\nax = plt.figure(figsize = (7, 4)).add_subplot()\nax.plot(sorted(df.gre.unique()), prob)\nax.plot(df.groupby('gre').mean()['admit'],'o')\nax.tick_params(labelsize=12)\nax.set_xlabel('gre', fontsize = 14)\nax.set_ylabel('P[admit]', fontsize = 14)\nax.set_title('Marginal Effect of GRE', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-revealjs/cell-20-output-1.png){width=599 height=388 fig-align='center'}\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Logistic Regression in Practice\n:::\n\nInstitution Rank (GRE = 600, GPA = 3.4):\n\n::: {#9ec96b31 .cell execution_count=21}\n``` {.python .cell-code code-fold=\"true\"}\nprob = [result.predict([600, 3.4, b, 1.0]) for b in range(1,5)]\nax = plt.figure(figsize = (7, 4)).add_subplot()\nax.plot(range(1,5), prob)\nax.plot(df.groupby('rank').mean()['admit'],'o')\nax.tick_params(labelsize=12)\nax.set_xlabel('Rank', fontsize = 14)\nax.set_xlim([0.5,4.5])\nax.set_ylabel('P[admit]', fontsize = 14)\nax.set_title('Marginal Effect of Rank', fontsize = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-revealjs/cell-21-output-1.png){width=622 height=388 fig-align='center'}\n:::\n:::\n\n\n## Logistic Regression in Perspective\n\nAt the start of lecture we emphasized that logistic regression is concerned with\nestimating a __probability__ model for __discrete__ (0/1) data. \n\nHowever, it may well be the case that we want to do something with the\nprobability that amounts to __classification.__\n\nFor example, we may classify data items using a rule such as \"Assign item $x_i$\nto Class 1 if $p(x_i) > 0.5$\".\n\nFor this reason, logistic regression could be considered a classification method.\n\n::: {.content-visible when-profile=\"slides\"}\n## Logistic Regression in Perspective\n:::\n\nLet's use our logistic regression as a classifier.\n\nWe want to ask whether we can correctly predict whether a student gets admitted\nto graduate school.\n\nLet's separate our training and test data:\n\n::: {#82cea12d .cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\"}\nX_train, X_test, y_train, y_test = model_selection.train_test_split(\n        df[train_cols], df['admit'],\n        test_size=0.4, random_state=1)\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Logistic Regression in Perspective\n:::\n\nNow, there are some standard metrics used when evaluating a binary classifier.\n\nLet's say our classifier is outputting \"yes\" when it thinks the student will be admitted.\n\nThere are four cases:\n\n* Classifier says \"yes\", and student __is__ admitted:  __True Positive.__\n* Classifier says \"yes\", and student __is not__ admitted:  __False Positive.__\n* Classifier says \"no\", and student __is__ admitted:  __False Negative.__\n* Classifier says \"no\", and student __is not__ admitted:  __True Negative.__\n\n::: {.content-visible when-profile=\"slides\"}\n## Logistic Regression in Perspective\n:::\n\n__Precision__ is the fraction of \"yes\" classifications that are correct:\n\n$$\n\\mbox{Precision} = \\frac{\\mbox{True Positives}}{\\mbox{True Positives + False Positives}}.\n$$\n    \n__Recall__ is the fraction of admits that we say \"yes\" to:\n\n$$\n\\mbox{Recall} = \\frac{\\mbox{True Positives}}{\\mbox{True Positives + False Negatives}}.\n$$\n\n::: {#fda19ce5 .cell execution_count=23}\n``` {.python .cell-code code-fold=\"true\"}\ndef evaluate(y_train, X_train, y_test, X_test, threshold):\n\n    # learn model on training data\n    logit = sm.Logit(y_train, X_train)\n    result = logit.fit(disp=False)\n    \n    # make probability predictions on test data\n    y_pred = result.predict(X_test)\n    \n    # threshold probabilities to create classifications\n    y_pred = y_pred > threshold\n    \n    # report metrics\n    precision = metrics.precision_score(y_test, y_pred)\n    recall = metrics.recall_score(y_test, y_pred)\n    return precision, recall\n\nprecision, recall = evaluate(y_train, X_train, y_test, X_test, 0.5)\n\nprint(f'Precision: {precision:0.3f}, Recall: {recall:0.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPrecision: 0.586, Recall: 0.340\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Logistic Regression in Perspective\n:::\n\nNow, let's get a sense of average accuracy:\n\n::: {#f9439e11 .cell execution_count=24}\n``` {.python .cell-code code-fold=\"true\"}\nPR = []\nfor i in range(20):\n    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n            df[train_cols], df['admit'],\n            test_size=0.4)\n    PR.append(evaluate(y_train, X_train, y_test, X_test, 0.5))\n```\n:::\n\n\n::: {#a02b136e .cell execution_count=25}\n``` {.python .cell-code code-fold=\"true\"}\navgPrec = np.mean([f[0] for f in PR])\navgRec = np.mean([f[1] for f in PR])\nprint(f'Average Precision: {avgPrec:0.3f}, Average Recall: {avgRec:0.3f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAverage Precision: 0.571, Average Recall: 0.211\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Logistic Regression in Perspective\n:::\n\nSometimes we would like a single value that describes the overall performance of\nthe classifier.\n\nFor this, we take the harmonic mean of precision and recall, called __F1 Score__:\n\n$$\n\\mbox{F1 Score} = 2 \\;\\;\\frac{\\mbox{Precision} \\cdot \\mbox{Recall}}{\\mbox{Precision} + \\mbox{Recall}}.\n$$\n\n::: {.content-visible when-profile=\"slides\"}\n## F1 Score as a function of threshold\n:::\n\nUsing this, we can evaluate other settings for the threshold.\n\n::: {#db6d9681 .cell execution_count=26}\n``` {.python .cell-code code-fold=\"true\"}\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndef evalThresh(df, thresh):\n    PR = []\n    for i in range(20):\n        X_train, X_test, y_train, y_test = model_selection.train_test_split(\n                df[train_cols], df['admit'],\n                test_size=0.4)\n        PR.append(evaluate(y_train, X_train, y_test, X_test, thresh))\n    avgPrec = np.mean([f[0] for f in PR])\n    avgRec = np.mean([f[1] for f in PR])\n    return 2 * (avgPrec * avgRec) / (avgPrec + avgRec), avgPrec, avgRec\n\ntvals = np.linspace(0.05, 0.8, 50)\nf1vals = [evalThresh(df, tval)[0] for tval in tvals]\n```\n:::\n\n\n::: {#72912749 .cell execution_count=27}\n``` {.python .cell-code code-fold=\"true\"}\nplt.figure(figsize=(6, 3))\nplt.plot(tvals,f1vals)\nplt.ylabel('F1 Score')\nplt.xlabel('Threshold for Classification')\nplt.title('F1 as a function of Threshold');\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-revealjs/cell-27-output-1.png){width=514 height=302 fig-align='center'}\n:::\n:::\n\n\nBased on this plot, we can say that the best classification threshold appears to\nbe around 0.3, where precision and recall are:\n\n::: {#71c866c3 .cell execution_count=28}\n``` {.python .cell-code code-fold=\"true\"}\nF1, Prec, Rec = evalThresh(df, 0.3)\nprint('Best Precision: {:0.3f}, Best Recall: {:0.3f}'.format(Prec, Rec))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest Precision: 0.426, Best Recall: 0.677\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"web\"}\n\nThe example here is based on\nhttp://blog.yhathq.com/posts/logistic-regression-and-python.html\nwhere you can find additional details.\n\n:::\n\n## ROC-AUC Curve\n\n::: {.columns}\n::: {.column width=\"50%\"}\n\n<br><br>\n\n* The ROC-AUC curve is a plot of the true positive rate against the false positive rate at various threshold settings.\n\n* The AUC is the area under the ROC curve.\n\n* The AUC is a measure of the overall performance of the classifier.\n\n:::\n::: {.column width=\"50%\"}\n\n::: {#6c014538 .cell execution_count=29}\n``` {.python .cell-code code-fold=\"true\"}\n# Get predicted probabilities from the model\ny_pred_proba = result.predict(df[train_cols])\n\n# Compute ROC curve and AUC\nfpr, tpr, thresholds = metrics.roc_curve(df['admit'], y_pred_proba)\nauc_score = metrics.roc_auc_score(df['admit'], y_pred_proba)\n\n# Plot ROC curve\nfig, ax = plt.subplots(figsize=(4, 4))\nax.plot(fpr, tpr, linewidth=2, label=f'ROC curve (AUC = {auc_score:.3f})')\nax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random classifier')\nax.set_xlabel('False Positive Rate', fontsize=12)\nax.set_ylabel('True Positive Rate', fontsize=12)\nax.set_title('ROC Curve for Logistic Regression Model', fontsize=14)\nax.legend(loc='lower right', fontsize=11)\nax.grid(True, alpha=0.3)\nax.tick_params(labelsize=11)\nplt.tight_layout()\nplt.show()\n\nprint(f\"AUC Score: {auc_score:.4f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](18-Regression-II-Logistic_files/figure-revealjs/cell-29-output-1.png){width=414 height=373 fig-align='center'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nAUC Score: 0.6921\n```\n:::\n:::\n\n\n:::\n:::\n\n## Recap\n\n* Logistic regression is used to predict a probability.\n* It is a linear model for the log-odds.\n* It is fit by maximum likelihood.\n* It can be evaluated as a classifier.\n\n",
    "supporting": [
      "18-Regression-II-Logistic_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}