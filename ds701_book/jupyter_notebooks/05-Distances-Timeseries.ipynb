{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Distances and Time Series\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/05-Distances-Timeseries.ipynb)\n",
        "\n",
        "We will start building some tools for making comparisons of data objects with particular attention to time series.\n",
        "\n",
        "Working with data, we can encounter a wide variety of different data objects\n",
        "\n",
        "::: {.incremental}\n",
        "* records of users,\n",
        "* images,\n",
        "* videos,\n",
        "* text (webpages, books),\n",
        "* strings (DNA sequences), and\n",
        "* time series.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "How can we compare them?\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Lecture Overview\n",
        "\n",
        "We cover the following topics in today's lecture\n",
        "\n",
        ":::: {.incremental}\n",
        "- feature space and matrix representations of data,\n",
        "- metrics, norms, similarity, and dissimilarity,\n",
        "- bit vectors, sets, and time series.\n",
        "::::\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Time Series Data\n",
        "Some examples of time series data\n",
        "\n",
        ":::: {.incremental}\n",
        "- stock prices,\n",
        "- weather data,\n",
        "- electricity consumption,\n",
        "- website traffic,\n",
        "- retail sales, and\n",
        "- various economic indicators.\n",
        "::::\n",
        ":::\n",
        "\n",
        "## Feature space representation\n",
        "\n",
        "Usually a data object consists of a set of attributes.\n",
        "\n",
        "These are also commonly called __features.__\n",
        "\n",
        "* (\"J. Smith\", 25, \\$ 200,000)\n",
        "* (\"M. Jones\", 47, \\$ 45,000)\n",
        "\n",
        "If all $d$ dimensions are real-valued then we can visualize each data object as a point in a $d$-dimensional vector space.\n",
        " \n",
        "* `(25, USD 200000)` $\\rightarrow \\begin{bmatrix}25\\\\200000\\end{bmatrix}$.\n",
        "\n",
        "Likewise If all features are binary then we can think of each data object as a binary vector in vector space.\n",
        "\n",
        "The space is called __feature space.__\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## One-hot encoding\n",
        ":::\n",
        "Vector spaces are such a useful tool that we often use them even for non-numeric data.\n",
        "\n",
        "For example, consider a categorical variable that can be only one of \"house\", \"tree\", or \"moon\". For such a variable, we can use a __one-hot__ encoding.  \n",
        "\n",
        "::: {.content-hidden when-profile=\"slides\"}\n",
        "We would encode as follows:\n",
        ":::\n",
        "\n",
        "::: {.incremental}\n",
        "* `house`: $[1, 0, 0]$\n",
        "* `tree`:  $[0, 1, 0]$\n",
        "* `moon`:  $[0, 0, 1]$\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "So an encoding of `(25, USD 200000, 'house')` could be: \n",
        "$$\\begin{bmatrix}25\\\\200000\\\\1\\\\0\\\\0\\end{bmatrix}.$$\n",
        "::: \n",
        "\n",
        "## Why not alternative encoding?\n",
        "\n",
        "Why would we not encode as follows:\n",
        "\n",
        "\n",
        "* `house`: $[1]$\n",
        "* `tree`:  $[2]$\n",
        "* `moon`:  $[3]$\n",
        "\n",
        "::: {.fragment}\n",
        "So an encoding of `(25, USD 200000, 'house')` could be: \n",
        "$$\\begin{bmatrix}25\\\\200000\\\\1\\end{bmatrix}.$$\n",
        "::: \n",
        "\n",
        "::: {.fragment}\n",
        "This is not a good idea because it implicitly assumes that the categories are ordered.\n",
        "\n",
        "One hot encoding treats the categorical data as orthogonal from a vector space perspective.\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 🎯 Class Activity: Feature Encoding\n",
        "\n",
        "**Task**: Work in pairs to encode the following data into feature vectors.\n",
        "\n",
        "**Data to encode**:\n",
        "\n",
        "- Person A: (age=30, salary=$75,000, city=\"New York\", education=\"PhD\")\n",
        "- Person B: (age=25, salary=$50,000, city=\"Boston\", education=\"Masters\") \n",
        "- Person C: (age=35, salary=$90,000, city=\"New York\", education=\"Bachelors\")\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "1. Create one-hot encodings for categorical variables (city: NY, Boston, LA; education: Bachelors, Masters, PhD)\n",
        "2. Normalize the numerical features (age: divide by 100, salary: divide by 100,000)\n",
        "3. Write out the complete feature vectors for all three people\n",
        "4. Calculate the Euclidean distance between Person A and Person B\n",
        "\n",
        "**Discussion**: Why might we normalize numerical features? What does the distance tell us about similarity?\n",
        ":::\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Encodings\n",
        ":::\n",
        "\n",
        "We will see many other encodings that take non-numeric data and encode them into vectors or matrices.\n",
        "\n",
        "For example, there are vector or matrix encodings for\n",
        "\n",
        "::: {.incremental}\n",
        "* graphs,\n",
        "* images, and\n",
        "* text.\n",
        ":::\n",
        "\n",
        "## Matrix representation of data\n",
        "\n",
        "We generally store data in a matrix form as\n",
        "\n",
        "$$ \n",
        "\\mbox{$m$ data objects}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{ccccc}\n",
        "\\begin{array}{c} x_{11} \\\\ \\vdots \\\\ x_{i1} \\\\ \\vdots \\\\ x_{m1} \\end{array}&\n",
        "\\begin{array}{c} \\cdots  \\\\ \\ddots \\\\ \\cdots  \\\\ \\ddots \\\\ \\cdots  \\end{array}&\n",
        "\\begin{array}{c} x_{1j} \\\\ \\vdots \\\\ x_{ij} \\\\ \\vdots \\\\ x_{mj} \\end{array}&\n",
        "\\begin{array}{c} \\cdots  \\\\ \\ddots \\\\ \\cdots  \\\\ \\ddots \\\\ \\cdots  \\end{array}&\n",
        "\\begin{array}{c} x_{1n} \\\\ \\vdots \\\\ x_{in} \\\\ \\vdots \\\\ x_{mn} \\end{array}\n",
        "\\end{array}\\right]}^{\\mbox{$n$ features}} \n",
        "$$\n",
        "\n",
        "The number of rows is denoted by $m$ and the number of columns by $n$. The rows are instances or records of data and the columns are the features.\n",
        "\n",
        "## Metrics\n",
        "\n",
        "A metric is a function $d(x, y)$ that satisfies the following properties.\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"70%\"}\n",
        "::: {.incremental}\n",
        "* $d(x, x) = 0$\n",
        "* $d(x, y) > 0 \\hspace{1cm} \\forall x\\neq y$ (positivity)\n",
        "* $d(x, y) = d(y, x)$ (symmetry)\n",
        "* $d(x, y)\\leq d(x, z) + d(z, y)$ (triangle inequality)\n",
        ":::\n",
        ":::\n",
        "::: {.column width=\"30%\"}\n",
        ":::: {.fragment}\n",
        "![](figs/TriangleInequality.png){fig-align=\"center\"}\n",
        "::::\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: {.fragment}\n",
        "We can use a metric to determine how __similar__ or __dissimilar__ two objects are.\n",
        "\n",
        "A metric is a measure of the dissimilarity between two objects. The larger the\n",
        "measure, the more dissimilar the objects are.\n",
        "\n",
        "If the objects are vectors, then the metric is also commonly called a __distance__.\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "Sometimes we will use \"distance\" informally, i.e., to refer to a similarity or\n",
        "dissimilarity function even if we are not sure it is a metric.   \n",
        "\n",
        "We'll try to say \"dissimilarity\" in those cases though.\n",
        ":::\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Distance Matrices\n",
        ":::\n",
        "\n",
        "The distance matrix is defined as\n",
        "\n",
        "$$ \n",
        "\\mbox{$m$ data objects}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\n",
        "\\overbrace{\\left[\\begin{array}{ccccc}\n",
        "\\begin{array}{c} 0  \\\\  d(x_1, x_2) \\\\ d(x_1,x_3) \\\\ \\vdots \\\\ d(x_1,x_m)  \\end{array} &\n",
        "\\begin{array}{c} \\; \\\\  0      \\\\ d(x_2,x_3) \\\\ \\vdots \\\\ d(x_2,x_m)  \\end{array} &\n",
        "\\begin{array}{c} \\; \\\\ \\;      \\\\ 0      \\\\ \\vdots \\\\ \\cdots   \\end{array} &\n",
        "\\begin{array}{c} \\; \\\\ \\;      \\\\ \\;     \\\\ \\ddots \\\\ d(x_{m-1},x_m)   \\end{array}  &\n",
        "\\begin{array}{c} \\; \\\\ \\;      \\\\ \\;     \\\\ \\;     \\\\[6pt] 0 \\end{array} &\n",
        "\\end{array}\\right]}^{\\mbox{$m$ data objects}},\n",
        "$$\n",
        "\n",
        "where $x_i$ denotes the $i$-th column of the data matrix $X$.\n",
        "\n",
        "## Norms\n",
        "\n",
        "Let $\\mathbf{u}, \\mathbf{v}\\in\\mathbb{R}^{n}$ and $a\\in\\mathbb{R}$. The vector function $p(\\mathbf{v})$ is called a __norm__ if\n",
        "\n",
        "\n",
        "::: {.incremental}\n",
        "* $p(a\\mathbf{v}) = |a|p(\\mathbf{v})$,\n",
        "* $p(\\mathbf{u} + \\mathbf{v}) \\leq p(\\mathbf{u}) + p(\\mathbf{v})$,\n",
        "* $p(\\mathbf{v}) = 0$ if and only if $\\mathbf{v} = 0$.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "__Every norm defines a corresponding metric.__ \n",
        "\n",
        "In particular If $p()$ is a norm, then $d(\\mathbf{x}, \\mathbf{y}) = p(\\mathbf{x}-\\mathbf{y})$ is a metric.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "**But not every metric can be derived from a norm!**\n",
        "\n",
        "For example, the discrete metric is not a norm.\n",
        "$$\n",
        "d(x,y) =\n",
        "\\begin{cases}\n",
        "0 & x=y \\\\\n",
        "1 & x \\neq y\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "### $\\ell_p$ norm\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## $\\ell_p$ norm\n",
        ":::\n",
        "\n",
        "A general class of norms are called __$\\ell_p$__ norms, where $p \\geq 1.$\n",
        "\n",
        "$$\n",
        "\\Vert \\mathbf{x} \\Vert_p = \\left(\\sum_{i=1}^d |x_i|^p\\right)^{\\frac{1}{p}}.\n",
        "$$ \n",
        "\n",
        "The corresponding distance that an $\\ell_p$ norm defines is called the _Minkowski distance._\n",
        "\n",
        "$$\n",
        "\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_p = \\left(\\sum_{i=1}^d |x_i - y_i|^p\\right)^{\\frac{1}{p}}.\n",
        "$$\n",
        "\n",
        "::: {.fragment}\n",
        "The choice of $p$ affects what gets emphasized in the distance calculation:\n",
        "\n",
        "::: {.incremental}\n",
        "* For small $p$ (e.g. $p=1$):\n",
        "  * More emphasis on many small differences\n",
        "  * Less sensitive to outliers/large differences\n",
        "* For large $p$ (e.g. $p\\rightarrow \\infty$):\n",
        "  * More emphasis on the largest difference\n",
        "  * Very sensitive to outliers\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "### $\\ell_2$ norm\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## $\\ell_2$ norm\n",
        ":::\n",
        "\n",
        "A special -- but very important -- case is the $\\ell_2$ norm.\n",
        "\n",
        "$$\n",
        "\\Vert \\mathbf{x} \\Vert_2 = \\sqrt{\\sum_{i=1}^d |x_i|^2}.\n",
        "$$\n",
        "\n",
        "We've already mentioned it: it is the __Euclidean__ norm.\n",
        "\n",
        "The distance defined by the $\\ell_2$ norm is the same as the Euclidean distance between two vectors $\\mathbf{x}, \\mathbf{y}$.\n",
        "\n",
        "$$ \n",
        "\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2  = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}.\n",
        "$$\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "### $\\ell_1$ norm\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## $\\ell_1$ norm\n",
        ":::\n",
        "\n",
        "Another important special case is the $\\ell_1$ norm.\n",
        "\n",
        "$$ \\Vert \\mathbf{x} \\Vert_1 = \\sum_{i=1}^d |x_i|.$$\n",
        "\n",
        "This defines the __Manhattan__ distance, or (for binary vectors), the __Hamming__ distance:\n",
        "\n",
        "$$ \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_1 = \\sum_{i=1} |x_i - y_i|.$$\n",
        "\n",
        "![](figs/L05-manhattan-distance.png){fig-align=\"center\" width=\"400px\"}\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "### $\\ell_\\infty$ norm\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## $\\ell_\\infty$ norm\n",
        ":::\n",
        "\n",
        "If we take the limit of the $\\ell_p$ norm as $p$ gets large we get the $\\ell_\\infty$ norm.  \n",
        "\n",
        "We have that\n",
        "\n",
        "$$\n",
        "\\Vert \\mathbf{x} \\Vert_{\\infty} = \\max_{i} \\vert x_{i} \\vert .\n",
        "$$\n",
        "\n",
        "::: {.fragment}\n",
        "What is the metric that this norm induces?\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "Answer: The $\\ell_\\infty$ norm induces the __Chebyshev__ distance which is also known as the __maximum__ distance.\n",
        "\n",
        "$$\n",
        "\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_{\\infty} = \\max_{i} \\vert x_{i} - y_{i} \\vert .\n",
        "$$\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "## $\\ell_0$ norm\n",
        "\n",
        "Another related idea is the $\\ell_0$ \"norm,\" which is not a norm, but is in a sense what we get from the $p$-norm for $p = 0$.\n",
        "\n",
        "Note that this is __not__ a norm, but it gets called that anyway.   \n",
        "\n",
        "This \"norm\" simply counts the number of __nonzero__ elements in a vector.\n",
        "\n",
        "This is called the vector's __sparsity.__\n",
        ":::\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Visualizing norms\n",
        ":::\n",
        "\n",
        "Here is the notion of a \"circle\" under each of three norms.\n",
        "\n",
        "That is, for each norm, the set of vectors having norm 1, or distance 1 from the origin.\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/L5-Vector-Norms.png){fig-align=\"center\"}\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "<br><br>\n",
        "$|x_1| + |x_2| = 1$\n",
        "\n",
        "<br><br>\n",
        "$\\sqrt{x_1^2 + x_2^2} = 1$\n",
        "\n",
        "<br><br>\n",
        "$\\max(|x_1|, |x_2|) = 1$\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "[Source](https://commons.wikimedia.org/w/index.php?curid=678101)\n",
        "\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## What norm should I use?\n",
        "\n",
        "The choice of norm depends on the characteristics of your data and the problem you're trying to solve.\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"33%\"}\n",
        "__$\\ell_1$ norm__\n",
        "\n",
        "- Use when your data is sparse.\n",
        "- Robust to outliers.\n",
        ":::\n",
        "::: {.column width=\"33%\"}\n",
        "__$\\ell_2$ norm__\n",
        "\n",
        "- Use when measuring distances in Euclidean space.\n",
        "- Smooth and differentiable.\n",
        ":::\n",
        "::: {.column width=\"33%\"}\n",
        "__$\\ell_\\infty$ norm__\n",
        "\n",
        "- Use when you need uniform bounds.\n",
        "- Limits maximum deviation.\n",
        ":::\n",
        "::::\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 🎯 Class Activity: Norm Comparison\n",
        "\n",
        "**Task**: Compare how different norms measure distance between the same two points.\n",
        "\n",
        "**Given**: Two 2D points A = (3, 4) and B = (1, 1)\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "1. Calculate the distance between A and B using:\n",
        "   - $\\ell_1$ norm (Manhattan distance)\n",
        "   - $\\ell_2$ norm (Euclidean distance) \n",
        "   - $\\ell_\\infty$ norm (Chebyshev distance)\n",
        "\n",
        "2. Now consider points C = (0, 0) and D = (1, 0)\n",
        "   - Calculate all three distances again\n",
        "   - Which norm gives the smallest distance? Largest?\n",
        "\n",
        "3. **Think-pair-share**: \n",
        "   - In what real-world scenarios would you prefer each norm?\n",
        "   - How does the choice of norm affect outlier sensitivity?\n",
        "\n",
        "**Bonus**: Sketch the \"unit circles\" for each norm in 2D space.\n",
        ":::\n",
        "\n",
        "## Similarity and Dissimilarity Functions\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"44%\"}\n",
        "Similarity functions quantify how similar two objects are. The higher the similarity score, the more alike the objects.\n",
        "\n",
        ":::: {.fragment}\n",
        "__Examples__\n",
        "\n",
        ":::: {.incremental}\n",
        "- cosine similarity,\n",
        "- Jaccard similarity _(intersection over union)_.\n",
        "::::\n",
        "::::\n",
        ":::\n",
        "::: {.column width=\"55%\"}\n",
        "Dissimilarity functions quantifies the difference between two objects. The higher the dissimilarity score, the more different the objects are.\n",
        "\n",
        ":::: {.fragment}\n",
        "__Examples__\n",
        "\n",
        ":::: {.incremental}\n",
        "- Manhattan distance,\n",
        "- Hamming distance.\n",
        "::::\n",
        "::::\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "### Similarity\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Similarity\n",
        ":::\n",
        "\n",
        "We know that the inner product of two vectors can be used to compute the __cosine of the angle__ between them\n",
        "\n",
        "$$ \\cos(\\theta) = \\frac{\\mathbf{x}^T\\mathbf{y}}{\\Vert\\mathbf{x}\\Vert \\Vert\\mathbf{y}\\Vert} \\equiv \\cos(\\mathbf{x}, \\mathbf{y})  .$$\n",
        "\n",
        "This value is \n",
        "\n",
        "* close to 1 when $\\mathbf{x} \\approx \\mathbf{y}$ ($\\theta \\approx 0$) and \n",
        "* close to 0 when $\\mathbf{x}$ and $\\mathbf{y}$ are orthogonal ($\\theta \\approx 90^\\circ$). \n",
        "\n",
        "We can use this formula to define a __similarity__ function called the __cosine similarity__ $\\cos(\\mathbf{x}, \\mathbf{y})$.\n",
        "\n",
        "<br>\n",
        "\n",
        "Abuse of notation? 🤔\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "### Dissimilarity\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Dissimilarity\n",
        ":::\n",
        "\n",
        ":::: {.fragment}\n",
        "Given a similarity function $s(\\mathbf{x}, \\mathbf{y})$, how could we convert it to a dissimilarity function $d(\\mathbf{x}, \\mathbf{y})$?\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "Two straightforward ways of doing that are:\n",
        "\n",
        "$$d(\\mathbf{x},\\mathbf{y}) = 1\\,/\\,s(\\mathbf{x},\\mathbf{y})$$\n",
        "\n",
        "or \n",
        "\n",
        "$$d(\\mathbf{x},\\mathbf{y}) = k - s(\\mathbf{x},\\mathbf{y})$$\n",
        "\n",
        "for some properly chosen $k$.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "For cosine similarity, one often uses:\n",
        "    \n",
        "$$ d(\\mathbf{x}, \\mathbf{y}) = 1 - \\cos(\\mathbf{x}, \\mathbf{y})$$\n",
        "::::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "Note however that this is __not a metric!__\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 🎯 Class Activity: Similarity Calculations\n",
        "\n",
        "**Task**: Calculate different similarity measures between two documents represented as word frequency vectors.\n",
        "\n",
        "**Given**: Two documents with word counts:\n",
        "\n",
        "- Document A: [cat: 3, dog: 2, bird: 1, fish: 0]\n",
        "- Document B: [cat: 1, dog: 3, bird: 0, fish: 2]\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "1. Convert to normalized vectors (divide by total word count)\n",
        "2. Calculate cosine similarity between the documents\n",
        "3. Calculate the dissimilarity using: d = 1 - cosine_similarity\n",
        "4. **Quick discussion**: \n",
        "   - What does a cosine similarity of 0.5 mean?\n",
        "   - Why might cosine similarity be better than Euclidean distance for text documents?\n",
        ":::\n",
        "\n",
        "## Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def calculate_document_similarity():\n",
        "    \"\"\"\n",
        "    Calculate different similarity measures between two documents represented as word frequency vectors.\n",
        "    \"\"\"\n",
        "    print(\"Document Similarity Analysis\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Given word frequency vectors\n",
        "    doc_a = np.array([3, 2, 1, 0])  # [cat, dog, bird, fish]\n",
        "    doc_b = np.array([1, 3, 0, 2])  # [cat, dog, bird, fish]\n",
        "    \n",
        "    print(\"Original word frequency vectors:\")\n",
        "    print(f\"Document A: {doc_a} (cat: 3, dog: 2, bird: 1, fish: 0)\")\n",
        "    print(f\"Document B: {doc_b} (cat: 1, dog: 3, bird: 0, fish: 2)\")\n",
        "    print()\n",
        "    \n",
        "    # Step 1: Convert to normalized vectors (divide by total word count)\n",
        "    doc_a_normalized = doc_a / np.sum(doc_a)\n",
        "    doc_b_normalized = doc_b / np.sum(doc_b)\n",
        "    \n",
        "    print(\"Step 1: Normalized vectors (divided by total word count)\")\n",
        "    print(f\"Document A normalized: {doc_a_normalized}\")\n",
        "    print(f\"Document B normalized: {doc_b_normalized}\")\n",
        "    print(f\"Sum of normalized A: {np.sum(doc_a_normalized):.6f}\")\n",
        "    print(f\"Sum of normalized B: {np.sum(doc_b_normalized):.6f}\")\n",
        "    print()\n",
        "    \n",
        "    # Step 2: Calculate cosine similarity\n",
        "    # Reshape for sklearn's cosine_similarity function\n",
        "    doc_a_reshaped = doc_a_normalized.reshape(1, -1)\n",
        "    doc_b_reshaped = doc_b_normalized.reshape(1, -1)\n",
        "    \n",
        "    cosine_sim = cosine_similarity(doc_a_reshaped, doc_b_reshaped)[0][0]\n",
        "    \n",
        "    print(\"Step 2: Cosine similarity calculation\")\n",
        "    print(f\"Cosine similarity: {cosine_sim:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    # Manual calculation for verification\n",
        "    dot_product = np.dot(doc_a_normalized, doc_b_normalized)\n",
        "    norm_a = np.linalg.norm(doc_a_normalized)\n",
        "    norm_b = np.linalg.norm(doc_b_normalized)\n",
        "    cosine_sim_manual = dot_product / (norm_a * norm_b)\n",
        "    \n",
        "    print(\"Manual verification:\")\n",
        "    print(f\"Dot product: {dot_product:.6f}\")\n",
        "    print(f\"Norm of A: {norm_a:.6f}\")\n",
        "    print(f\"Norm of B: {norm_b:.6f}\")\n",
        "    print(f\"Cosine similarity (manual): {cosine_sim_manual:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    # Step 3: Calculate dissimilarity\n",
        "    dissimilarity = 1 - cosine_sim\n",
        "    \n",
        "    print(\"Step 3: Dissimilarity calculation\")\n",
        "    print(f\"Dissimilarity (1 - cosine_similarity): {dissimilarity:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    # Additional analysis\n",
        "    print(\"Additional Analysis:\")\n",
        "    print(\"-\" * 20)\n",
        "    \n",
        "    # Euclidean distance for comparison\n",
        "    euclidean_dist = np.linalg.norm(doc_a_normalized - doc_b_normalized)\n",
        "    print(f\"Euclidean distance: {euclidean_dist:.6f}\")\n",
        "    \n",
        "    # Manhattan distance\n",
        "    manhattan_dist = np.sum(np.abs(doc_a_normalized - doc_b_normalized))\n",
        "    print(f\"Manhattan distance: {manhattan_dist:.6f}\")\n",
        "    print()\n",
        "    \n",
        "    return {\n",
        "        'doc_a_normalized': doc_a_normalized,\n",
        "        'doc_b_normalized': doc_b_normalized,\n",
        "        'cosine_similarity': cosine_sim,\n",
        "        'dissimilarity': dissimilarity,\n",
        "        'euclidean_distance': euclidean_dist,\n",
        "        'manhattan_distance': manhattan_dist\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = calculate_document_similarity()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion points\n",
        "\n",
        "::: {.fragment}\n",
        "1. **What does a cosine similarity of 0.5 mean?**\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "A cosine similarity of 0.5 means the documents are moderately similar.\n",
        "The angle between their vectors is approximately 60 degrees.\n",
        "This indicates some overlap in word usage patterns but not complete similarity.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "2. **Why might cosine similarity be better than Euclidean distance for text documents?**\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "**Scale invariance:** Cosine similarity focuses on the direction of vectors,\n",
        "not their magnitude, so document length doesn't affect the comparison.\n",
        "\n",
        "**Normalization:** It naturally handles documents of different lengths.\n",
        "Word frequency interpretation: It measures the relative proportion of words,\n",
        "which is more meaningful for text analysis than absolute differences.\n",
        "\n",
        "**Range:** Cosine similarity ranges from -1 to 1, with 1 being identical,\n",
        "making it more interpretable for similarity measures.\n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "## Bit vectors and Sets\n",
        "\n",
        "When working with bit vectors, the $\\ell_1$ metric is commonly used and is called the __Hamming__ distance.\n",
        "\n",
        "![](figs/L5-hamming-1.png){fig-align=\"center\" width=\"500px\"}\n",
        "\n",
        "This has a natural interpretation: \"how well do the two vectors match?\"\n",
        "\n",
        "Or: \"What is the smallest number of bit flips that will convert one vector into the other?\"\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Hamming distance\n",
        ":::\n",
        "\n",
        "![](figs/L5-hamming-2.png){fig-align=\"center\" width=\"700px\"}\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "In other cases, the Hamming distance is not a very appropriate metric.\n",
        ":::\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Hamming distance with sets\n",
        ":::\n",
        "\n",
        "Consider the case in which the bit vector is being used to represent a _set_.\n",
        "\n",
        "**Definition**: A _set_ is an unordered collection of distinct elements.\n",
        "\n",
        "In that case, Hamming distance measures the __size of the set difference.__\n",
        "\n",
        "For example, consider two documents. We will use bit vectors to represent the sets of words in each document.\n",
        "\n",
        ":::: {.incremental}\n",
        "* Case 1: both documents are large, almost identical, but differ in 10 words.\n",
        "* Case 2: both documents are small, disjoint, have 5 words each.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "What is the Hamming distance for each case?\n",
        ":::\n",
        "\n",
        ":::: {.fragment}\n",
        "$L_1(\\text{Case 1}) = 10$ and $L_1(\\text{Case 2}) = 5$. Is case 1 really more different than case 2? \n",
        ":::\n",
        "\n",
        ":::: {.fragment}\n",
        "What matters is not just the size of the set difference, but the size of the _normalized_ intersection.\n",
        "::::\n",
        "\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Jaccard similarity\n",
        ":::\n",
        "This leads to the _Jaccard_ similarity or _Intersection over Union (IoU)_:\n",
        "\n",
        "$$\n",
        "J_{Sim}(\\mathbf{x}, \\mathbf{y}) = \\frac{|\\mathbf{x} \\cap \\mathbf{y}|}{|\\mathbf{x} \\cup \\mathbf{y}|}.\n",
        "$$\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"60%\"}\n",
        "This takes on values from 0 to 1, so a natural dissimilarity metric is $1 - J_{Sim}().$\n",
        "\n",
        "In fact, this is a __metric!__\n",
        "\n",
        "$$\n",
        "J_{Dist}(\\mathbf{x}, \\mathbf{y}) = 1- \\frac{|\\mathbf{x} \\cap \\mathbf{y}|}{|\\mathbf{x} \\cup \\mathbf{y}|}.\n",
        "$$\n",
        ":::\n",
        "::: {.column width=\"40%\"}\n",
        "![](figs/L5-jaccard-1.png){fig-align=\"center\"}\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Jaccard Similarity Example 1\n",
        ":::\n",
        "    \n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "Let's revisit the previously introduces cases comparing documents.\n",
        ":::\n",
        "\n",
        "Case 1: Very large almost identical documents.\n",
        "\n",
        "![](figs/L5-jaccard-2.png){fig-align=\"center\" width=\"500px\"}\n",
        "\n",
        "Here $J_{Sim}(\\mathbf{x}, \\mathbf{y})$ is almost 1.\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Jaccard Similarity Example 2\n",
        ":::\n",
        "\n",
        "Case 2: Very small disjoint documents.\n",
        "\n",
        "![](figs/L5-jaccard-3.png){fig-align=\"center\"}\n",
        "\n",
        "Here $J_{Sim}(\\mathbf{x}, \\mathbf{y})$ is 0.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 🎯 Class Activity: Jaccard Similarity Practice\n",
        "\n",
        "**Task**: Calculate Jaccard similarity for different document scenarios.\n",
        "\n",
        "**Given**: Three documents represented as sets of unique words:\n",
        "\n",
        "- Document 1: {apple, banana, cherry, date}\n",
        "- Document 2: {apple, banana, cherry, elderberry}  \n",
        "- Document 3: {grape, kiwi, lemon, mango}\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "1. Calculate Jaccard similarity between:\n",
        "   - Document 1 and Document 2\n",
        "   - Document 1 and Document 3\n",
        "   - Document 2 and Document 3\n",
        "\n",
        "2. **Think-pair-share**:\n",
        "   - Which pair is most similar? Least similar?\n",
        "   - How does this compare to what you'd expect from Hamming distance?\n",
        "   - When would you prefer Jaccard similarity over Hamming distance?\n",
        "\n",
        "**Real-world application**: Consider how this applies to comparing user preferences, shopping baskets, or social media interests.\n",
        ":::\n",
        "\n",
        "## Time Series\n",
        "\n",
        "A time series is a sequence of real numbers, representing the measurements of a real variable at (possibly equal) time intervals.\n",
        "\n",
        "Some examples are\n",
        "\n",
        "::: {.incremental}\n",
        "* stock prices,\n",
        "* the volume of sales over time, and\n",
        "* daily temperature readings.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "A time series database is a large collection of time series.\n",
        ":::\n",
        "\n",
        "## Similarity of Time Series\n",
        "\n",
        "Suppose we wish to compare the following time series.\n",
        "\n",
        "::: {.incremental}\n",
        "* Stock price movements for companies over a time interval.\n",
        "* The motion data of two people walking.\n",
        "* Credit usage patterns for bank clients.\n",
        ":::\n",
        "\n",
        ":::: {.fragment}\n",
        "How should we measure the \"similarity\" of these time series?\n",
        "::::\n",
        "\n",
        "::: {.fragment}\n",
        "There are two problems to address.\n",
        "\n",
        "::: {.incremental}\n",
        "1. Defining a meaningful similarity (or distance) function.\n",
        "2. Finding an efficient algorithm to compute it.\n",
        ":::\n",
        ":::\n",
        "\n",
        "## Norm-based Similarity Measures\n",
        "\n",
        "We could just view each sequence as a vector.\n",
        "\n",
        "Then we could use a $p$-norm, e.g., $\\ell_1, \\ell_2,$ or $\\ell_p$ to measure similarity.\n",
        "\n",
        "::: {.fragment}\n",
        "\n",
        "__Advantages__\n",
        "\n",
        "::: {.incremental}    \n",
        "1. Easy to compute - linear in the length of the time series (O(n)).\n",
        "2. It is a metric.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Beware of Norm-based Similarity\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        ":::: {.columns}\n",
        "::: {.column width=\"40%\"}\n",
        "__Disadvantage__\n",
        "\n",
        "1. May not be __meaningful!__\n",
        "\n",
        "We may believe that $\\mathbf{ts1}$ and $\\mathbf{ts2}$ are the most \"similar\" pair of time series.\n",
        ":::\n",
        "::: {.column width=\"60%\"}\n",
        "![](figs/L5-ts-euclidean.png){fig-align=\"center\"}\n",
        ":::\n",
        "::::\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "__Disadvantage__\n",
        "1. May not be __meaningful!__\n",
        "\n",
        "![](figs/L5-ts-euclidean.png){fig-align=\"center\"}\n",
        "\n",
        "We may believe that $\\mathbf{ts1}$ and $\\mathbf{ts2}$ are the most \"similar\" pair of time series.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "However, according to Euclidean distance: \n",
        "\n",
        "$$ \\Vert \\mathbf{ts1} - \\mathbf{ts2} \\Vert_2 = 26.9,$$\n",
        "\n",
        "while \n",
        "\n",
        "$$ \\Vert \\mathbf{ts1} - \\mathbf{ts3} \\Vert_2 = 23.2.$$\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "## Feature Engineering\n",
        "\n",
        "In general, there may be different aspects of a time series that are important in different settings.\n",
        "\n",
        "::: {.fragment}\n",
        "The first step therefore is to ask yourself \"what is important about time series in my application?\"\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "This is an example of __feature engineering.__\n",
        ":::\n",
        ":::\n",
        "\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Feature Engineering\n",
        ":::\n",
        "\n",
        "Feature engineering is the art of computing some derived measure from your data object that makes the important properties usable in a subsequent step.\n",
        "\n",
        "::: {.fragment}\n",
        "A reasonable approach is to\n",
        "\n",
        "::: {.incremental}    \n",
        "* extract the relevant features,\n",
        "* use a simple method (e.g., a norm) to define similarity over those features.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "In the case above, one might think of using \n",
        "\n",
        "::: {.incremental}\n",
        "* Fourier coefficients (to capture periodicity),\n",
        "* histograms,\n",
        "* or something else!\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "## Dynamic Time Warping\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Bump Hunting\n",
        ":::\n",
        "\n",
        "One case that arises often is something like the following:  \"bump hunting\"\n",
        "\n",
        "![](figs/L5-DTW-1.png){fig-align=\"center\"} \n",
        "\n",
        "Both time series have the same key characteristics: four bumps.\n",
        "\n",
        "But a one-to-one match (ala Euclidean distance) will not detect the similarity.\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "(Be sure to think about why Euclidean distance will fail here.)\n",
        ":::\n",
        "\n",
        "A solution to this is called __dynamic time warping.__\n",
        "\n",
        "\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Dynamic Time Warping\n",
        ":::\n",
        "\n",
        "The basic idea is to allow stretching or compressing of signals along the time dimension.\n",
        "\n",
        "::: {.fragment}\n",
        "__Classic applications__\n",
        "\n",
        "::: {.incremental}\n",
        "* speech recognition\n",
        "* handwriting recognition\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "Specifically\n",
        "\n",
        "::: {.incremental}\n",
        "* Consider $X = x_1, x_2, \\dots, x_n$ and $Y = y_1, y_2, \\dots, y_m$.\n",
        "* We are allowed to modify each sequence by duplicating, deleting, or matching elements to form $X'$ and $Y'$.\n",
        "* We then search over the space of all possible alignments to find the one that minimizes the distance between $X'$ and $Y'$.\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Visualizing DTW\n",
        ":::\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "There is a simple way to visualize this algorithm.\n",
        "\n",
        "Consider a matrix $M$ where $M_{ij} = |x_i - y_j|$.\n",
        "\n",
        "$M$ measures the amount of error we get if we match $x_i$ with $y_j$. \n",
        "\n",
        "So we seek a __path, starting from lower left, through $M$ that minimizes the total error.__\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/L5-DTW-2.png){fig-align=\"center\" width=\"550px\"}\n",
        ":::\n",
        "::::\n",
        "\n",
        "## DTW restrictions\n",
        "\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "The basic restrictions on path are:\n",
        "    \n",
        "Monotonicity\n",
        "\n",
        "* The path should not go down or to the left.\n",
        "\n",
        "Continuity\n",
        "\n",
        "* No elements may be skipped in a sequence.\n",
        "\n",
        "Restrict amount of deviation from diagonal (Sakoe-Chiba constraint)\n",
        "\n",
        "This can be solved via dynamic programming.\n",
        "\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "![](figs/L5-DTW-2.png){fig-align=\"center\" width=\"550px\"}\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Dynamic Time Warping Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sequences\n",
        "x = [1,1,2,2,3]\n",
        "y = [1,2,3,3,3]\n",
        "n, m = len(x), len(y)\n",
        "\n",
        "# build DTW table\n",
        "dtw = np.full((n+1,m+1), np.inf)\n",
        "dtw[0,0] = 0\n",
        "for i in range(1,n+1):\n",
        "    for j in range(1,m+1):\n",
        "        cost = abs(x[i-1]-y[j-1])\n",
        "        dtw[i,j] = cost + min(dtw[i-1,j], dtw[i,j-1], dtw[i-1,j-1])\n",
        "\n",
        "# backtrack with tie-breaking: prefer diagonal\n",
        "i,j = n,m\n",
        "path = [(i-1,j-1)]\n",
        "while (i>0 and j>0):\n",
        "    candidates = [(i-1,j-1,\"↖\"), (i-1,j,\"↑\"), (i,j-1,\"←\")]\n",
        "    i,j,_ = min(candidates, key=lambda s: (dtw[s[0],s[1]], [\"↖\",\"↑\",\"←\"].index(s[2])))\n",
        "    if i>0 or j>0:\n",
        "        path.append((i-1,j-1))\n",
        "path.reverse()\n",
        "\n",
        "# Print the fully aligned sequences\n",
        "print(\"Original sequences:\")\n",
        "print(f\"X = {x}\")\n",
        "print(f\"Y = {y}\")\n",
        "print(f\"\\nDTW distance = {dtw[n,m]:.1f}\")\n",
        "print(f\"\\nOptimal alignment path (0-indexed): {path}\")\n",
        "print(\"\\nFully aligned sequences:\")\n",
        "print(\"X_aligned: \", end=\"\")\n",
        "for i, j in path:\n",
        "    print(f\"{x[i]:2d}\", end=\" \")\n",
        "print()\n",
        "print(\"Y_aligned: \", end=\"\")\n",
        "for i, j in path:\n",
        "    print(f\"{y[j]:2d}\", end=\" \")\n",
        "print()\n",
        "print(\"\\nAlignment mapping:\")\n",
        "for k, (i, j) in enumerate(path):\n",
        "    print(f\"Step {k}: X[{i}] = {x[i]} ↔ Y[{j}] = {y[j]}\")\n",
        "\n",
        "# plot cost matrix + path\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(dtw[1:,1:], origin=\"lower\", cmap=\"Blues\", aspect=\"auto\")\n",
        "px, py = zip(*[(pi+1,pj+1) for pi,pj in path])\n",
        "plt.plot(np.array(py)-1, np.array(px)-1, 'r', linewidth=2)\n",
        "plt.colorbar(label=\"Cumulative Cost\")\n",
        "plt.title(\"DTW Cost Matrix with Optimal Path\")\n",
        "plt.xlabel(\"Y values\")\n",
        "plt.ylabel(\"X values\")\n",
        "\n",
        "# plot aligned sequences\n",
        "plt.subplot(1, 2, 2)\n",
        "x_aligned = [x[i] for i, j in path]\n",
        "y_aligned = [y[j] for i, j in path]\n",
        "plt.plot(range(len(x_aligned)), x_aligned, 'bo-', label='X aligned', linewidth=2)\n",
        "plt.plot(range(len(y_aligned)), y_aligned, 'ro-', label='Y aligned', linewidth=2)\n",
        "plt.xlabel('Alignment step')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Aligned Sequences')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## To explore further\n",
        "\n",
        "See \n",
        "\n",
        "T. Giorgino, \"Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package\", Journal of Statistical Software, 2003. [link](https://www.jstatsoft.org/article/view/v031i07)\n",
        "\n",
        "And python packages such as:\n",
        "\n",
        "* [dtw-python](https://dynamictimewarping.github.io/)\n",
        "* [dtaidistance](https://dtaidistance.readthedocs.io/en/latest/index.html)\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## 🎯 Class Activity: DTW Concept Exploration\n",
        "\n",
        "**Task**: Explore why DTW is better than Euclidean distance for time series with temporal shifts.\n",
        "\n",
        "**Given**: Two simple time series:\n",
        "\n",
        "- Series A: [1, 2, 3, 2, 1] (peak at position 3)\n",
        "- Series B: [0, 1, 2, 3, 2, 1, 0] (peak at position 4)\n",
        "\n",
        "**Instructions**:\n",
        "\n",
        "1. **Calculate Euclidean distance** between the series (pad with zeros if needed)\n",
        "2. **Sketch the DTW alignment** by hand:\n",
        "   - Expand series A to better align with series B\n",
        "3. **Calculate the Euclidean distance of the aligned series**\n",
        "3. **Group discussion**:\n",
        "   - Why does Euclidean distance of the original series fail here?\n",
        "   - What real-world scenarios would benefit from DTW?\n",
        "\n",
        "::: {.fragment}\n",
        "**Scenarios**:\n",
        "\n",
        "- Speech recognition (different speaking speeds)\n",
        "- Gesture recognition (different execution speeds)\n",
        "- Stock price patterns (different market timing)\n",
        ":::\n",
        "\n",
        "## From Time series to Strings\n",
        "\n",
        "A closely related idea concerns strings.\n",
        "\n",
        "The key point is that, like time series, strings are __sequences__.\n",
        "\n",
        "Given two strings, one way to define a 'distance' between them is:\n",
        "\n",
        "* the minimum number of __edit operations__ that are needed to transform one string into the other.\n",
        "\n",
        "Edit operations are insertion, deletion, and substitution of single characters.\n",
        "\n",
        "This is called __edit distance__ or __Levenshtein distance.__\n",
        "\n",
        "## Example\n",
        "\n",
        "For example, given strings: ``s = VIVALASVEGAS`` and ``t = VIVADAVIS``\n",
        "\n",
        "we would like to \n",
        "\n",
        "* compute the edit distance, and\n",
        "* obtain the optimal __alignment__.\n",
        "\n",
        "![](figs/viva-las-vegas.png){fig-align=\"center\" width=\"550px\"}\n",
        "\n",
        "\n",
        "A dynamic programming algorithm can also be used to find this distance, and it is __very similar to dynamic time-warping.__\n",
        "\n",
        "In bioinformatics this algorithm is called __\"Smith-Waterman\" sequence alignment.__\n",
        "\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Recap\n",
        "\n",
        "We covered the following topics\n",
        "\n",
        "- reviewed representations of data,\n",
        "- discussed metrics and norms,\n",
        "- discussed similarity and dissimilarity functions,\n",
        "- introduced time series, \n",
        "- feature engineering, and\n",
        "- dynamic time warping.\n",
        ":::"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}