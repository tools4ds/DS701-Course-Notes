{
  "hash": "82c805a58e288c81f52dbdb5d9f4a215",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"GMM Appendix B: Covariance Matrix Structure in Multivariate Gaussians\"\n---\n\n## The Multivariate Gaussian^[Courtesy of Claude.ai]\n\nFor a $d$-dimensional Gaussian distribution:\n\n$$\np(\\mathbf{x} \\mid \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{(2\\pi)^{d/2}|\\boldsymbol{\\Sigma}|^{1/2}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\\right)\n$$\n\nThe covariance matrix $\\boldsymbol{\\Sigma}$ is a $d \\times d$ positive definite matrix that completely determines the shape, orientation, and spread of the distribution.\n\n---\n\n## Different Forms of Covariance Matrices\n\n### 1. **Full Covariance Matrix** (Unrestricted)\n\n$$\n\\boldsymbol{\\Sigma}_{\\text{full}} = \\begin{pmatrix}\n\\sigma_1^2 & \\sigma_{12} & \\cdots & \\sigma_{1d} \\\\\n\\sigma_{21} & \\sigma_2^2 & \\cdots & \\sigma_{2d} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{d1} & \\sigma_{d2} & \\cdots & \\sigma_d^2\n\\end{pmatrix}\n$$\n\n**Properties:**\n\n- Symmetric: $\\sigma_{ij} = \\sigma_{ji}$\n- Number of free parameters: $\\frac{d(d+1)}{2}$\n- Can represent any orientation and shape\n\n**Geometric Interpretation:**\n\n- Ellipsoids can be oriented in any direction\n- Each dimension can have different variance\n- Dimensions can be correlated (non-axis-aligned)\n\n**Example (2D):**\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix}\n$$\n\nCreates an ellipse tilted at an angle, not aligned with coordinate axes.\n\n::: {#c0278f76 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import multivariate_normal\n\n# Full covariance matrix\nmean_full = [0, 0]\ncov_full = [[2, 1],\n            [1, 3]]\n\n# Create grid\nx = np.linspace(-5, 5, 100)\ny = np.linspace(-5, 5, 100)\nX, Y = np.meshgrid(x, y)\npos = np.dstack((X, Y))\n\n# Calculate PDF\nrv_full = multivariate_normal(mean_full, cov_full)\nZ_full = rv_full.pdf(pos)\n\n# Plot\nplt.figure(figsize=(8, 8))\nplt.contour(X, Y, Z_full, levels=10, cmap='viridis')\nplt.title('Full Covariance: Tilted Ellipse', fontsize=14)\nplt.xlabel('x₁')\nplt.ylabel('x₂')\nplt.axis('equal')\nplt.grid(True, alpha=0.3)\nplt.colorbar(label='Probability Density')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09B-Appendix-Cov-Matrix-in-MV-Gaussians_files/figure-html/cell-2-output-1.png){width=668 height=674}\n:::\n:::\n\n\n**Usage in GMMs:**\n\n- Most flexible, can fit complex cluster shapes\n- Most expensive: requires $O(d^2)$ parameters per component\n- Can overfit with limited data\n\n---\n\n### 2. **Diagonal Covariance Matrix**\n\n$$\n\\boldsymbol{\\Sigma}_{\\text{diag}} = \\begin{pmatrix}\n\\sigma_1^2 & 0 & \\cdots & 0 \\\\\n0 & \\sigma_2^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma_d^2\n\\end{pmatrix}\n$$\n\n**Properties:**\n\n- All off-diagonal elements are zero: $\\sigma_{ij} = 0$ for $i \\neq j$\n- Number of free parameters: $d$\n- Dimensions are **independent** (uncorrelated)\n\n**Geometric Interpretation:**\n\n- Ellipsoids are **axis-aligned** (principal axes parallel to coordinate axes)\n- Each dimension can have different spread\n- No rotation or tilt\n\n**Example (2D):**\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 4 \\end{pmatrix}\n$$\n\nCreates an axis-aligned ellipse (wider in the $y$-direction).\n\n::: {#59346095 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\n# Diagonal covariance matrix\nmean_diag = [0, 0]\ncov_diag = [[2, 0],\n            [0, 4]]\n\n# Calculate PDF\nrv_diag = multivariate_normal(mean_diag, cov_diag)\nZ_diag = rv_diag.pdf(pos)\n\n# Plot\nplt.figure(figsize=(8, 8))\nplt.contour(X, Y, Z_diag, levels=10, cmap='viridis')\nplt.title('Diagonal Covariance: Axis-Aligned Ellipse', fontsize=14)\nplt.xlabel('x₁')\nplt.ylabel('x₂')\nplt.axis('equal')\nplt.grid(True, alpha=0.3)\nplt.colorbar(label='Probability Density')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09B-Appendix-Cov-Matrix-in-MV-Gaussians_files/figure-html/cell-3-output-1.png){width=668 height=674}\n:::\n:::\n\n\n**Usage in GMMs:**\n\n- Good balance between flexibility and computational efficiency\n- Assumes features are independent within each cluster\n- Commonly used default in practice (e.g., scikit-learn's default)\n- Much faster than full covariance\n\n---\n\n### 3. **Spherical (Isotropic) Covariance Matrix**\n\n$$\n\\boldsymbol{\\Sigma}_{\\text{spherical}} = \\sigma^2 \\mathbf{I} = \\begin{pmatrix}\n\\sigma^2 & 0 & \\cdots & 0 \\\\\n0 & \\sigma^2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma^2\n\\end{pmatrix}\n$$\n\n**Properties:**\n\n- All dimensions have the **same variance**: $\\sigma_i^2 = \\sigma^2$ for all $i$\n- Number of free parameters: $1$\n- Special case of diagonal covariance\n\n**Geometric Interpretation:**\n- Contours are **hyperspheres** (circles in 2D, spheres in 3D)\n- Equal spread in all directions\n- No preferred direction\n\n**Example (2D):**\n$$\n\\boldsymbol{\\Sigma} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 2 \\end{pmatrix}\n$$\n\nCreates a perfect circle centered at the mean.\n\n::: {#f54b234a .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\n# Spherical covariance matrix\nmean_spher = [0, 0]\ncov_spher = [[2, 0],\n             [0, 2]]\n\n# Calculate PDF\nrv_spher = multivariate_normal(mean_spher, cov_spher)\nZ_spher = rv_spher.pdf(pos)\n\n# Plot\nplt.figure(figsize=(8, 8))\nplt.contour(X, Y, Z_spher, levels=10, cmap='viridis')\nplt.title('Spherical Covariance: Perfect Circle', fontsize=14)\nplt.xlabel('x₁')\nplt.ylabel('x₂')\nplt.axis('equal')\nplt.grid(True, alpha=0.3)\nplt.colorbar(label='Probability Density')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09B-Appendix-Cov-Matrix-in-MV-Gaussians_files/figure-html/cell-4-output-1.png){width=668 height=674}\n:::\n:::\n\n\n**Usage in GMMs:**\n\n- Most restrictive, assumes all dimensions have equal variance\n- Very efficient: only 1 parameter per component\n- Suitable when features are on similar scales and have similar variability\n- Often too restrictive for real data\n\n---\n\n### 4. **Tied (Shared) Covariance Matrix**\n\nAll mixture components share the **same** covariance matrix:\n\n$$\\boldsymbol{\\Sigma}_1 = \\boldsymbol{\\Sigma}_2 = \\cdots = \\boldsymbol{\\Sigma}_K = \\boldsymbol{\\Sigma}_{\\text{shared}}$$\n\n**Properties:**\n\n- Can be full, diagonal, or spherical\n- Number of parameters doesn't scale with $K$\n- All clusters have the same shape and orientation\n\n**Geometric Interpretation:**\n\n- All ellipsoids have the same shape, size, and orientation\n- Only the centers (means) differ between components\n- This is equivalent to **Linear Discriminant Analysis (LDA)** for classification\n\n**Usage in GMMs:**\n\n- Reduces overfitting when clusters have similar shapes\n- Much more parameter-efficient\n- Appropriate when clusters differ mainly in location, not shape\n\n---\n\n## Visual Comparison of All Types\n\n::: {#59485a22 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\n# Create a 2x2 subplot comparing all covariance types\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\n\n# Full covariance\nax1 = axes[0, 0]\nax1.contour(X, Y, Z_full, levels=10, cmap='viridis')\nax1.set_title('Full Covariance\\n(Tilted Ellipse)', fontsize=12, fontweight='bold')\nax1.set_xlabel('x₁')\nax1.set_ylabel('x₂')\nax1.axis('equal')\nax1.grid(True, alpha=0.3)\n\n# Diagonal covariance\nax2 = axes[0, 1]\nax2.contour(X, Y, Z_diag, levels=10, cmap='viridis')\nax2.set_title('Diagonal Covariance\\n(Axis-Aligned Ellipse)', fontsize=12, fontweight='bold')\nax2.set_xlabel('x₁')\nax2.set_ylabel('x₂')\nax2.axis('equal')\nax2.grid(True, alpha=0.3)\n\n# Spherical covariance\nax3 = axes[1, 0]\nax3.contour(X, Y, Z_spher, levels=10, cmap='viridis')\nax3.set_title('Spherical Covariance\\n(Perfect Circle)', fontsize=12, fontweight='bold')\nax3.set_xlabel('x₁')\nax3.set_ylabel('x₂')\nax3.axis('equal')\nax3.grid(True, alpha=0.3)\n\n# Tied covariance (example with 3 components)\nax4 = axes[1, 1]\nmeans_tied = [[-2, 0], [2, 0], [0, 2.5]]\ncov_tied = [[1.5, 0.5], [0.5, 1.5]]\nfor mean in means_tied:\n    rv_tied = multivariate_normal(mean, cov_tied)\n    Z_tied = rv_tied.pdf(pos)\n    ax4.contour(X, Y, Z_tied, levels=8, cmap='viridis', alpha=0.7)\nax4.set_title('Tied Covariance\\n(3 Components, Same Shape)', fontsize=12, fontweight='bold')\nax4.set_xlabel('x₁')\nax4.set_ylabel('x₂')\nax4.axis('equal')\nax4.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09B-Appendix-Cov-Matrix-in-MV-Gaussians_files/figure-html/cell-5-output-1.png){width=758 height=759}\n:::\n:::\n\n\n---\n\n## GMM Example with Different Covariance Types\n\n::: {#82f8117e .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.datasets import make_blobs\n\n# Generate synthetic data with 3 clusters using different covariance matrices\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# Create data with different covariance structures\nnp.random.seed(42)\n\n# Cluster 1: Spherical (circular)\nn1 = 167\nX1 = np.random.multivariate_normal([2, 2], [[1.0, 0], [0, 1.0]], n1)\ny1 = np.zeros(n1)\n\n# Cluster 2: Diagonal (axis-aligned ellipse, wider in y)\nn2 = 167  \nX2 = np.random.multivariate_normal([-2, 1], [[0.5, 0], [0, 2.0]], n2)\ny2 = np.ones(n2)\n\n# Cluster 3: Full covariance (tilted ellipse)\nn3 = 166\ncov3 = np.array([[1.2, 0.8], [0.8, 0.6]])  # Positive correlation\nX3 = np.random.multivariate_normal([0, -2], cov3, n3)\ny3 = np.full(n3, 2)\n\n# Combine all clusters\nX = np.vstack([X1, X2, X3])\ny_true = np.hstack([y1, y2, y3])\n\n# Fit GMMs with different covariance types\ncovariance_types = ['full', 'diag', 'spherical', 'tied']\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\n\nfor idx, (cov_type, ax) in enumerate(zip(covariance_types, axes.ravel())):\n    # Fit GMM\n    gmm = GaussianMixture(n_components=3, covariance_type=cov_type, random_state=42)\n    gmm.fit(X)\n    labels = gmm.predict(X)\n    \n    # Plot data points colored by cluster\n    scatter = ax.scatter(X[:, 0], X[:, 1], c=labels, s=20, cmap='viridis', alpha=0.6)\n    \n    # Plot cluster centers\n    centers = gmm.means_\n    ax.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.8, \n               marker='X', edgecolors='black', linewidths=2, label='Centers')\n    \n    # Draw confidence ellipses for each component\n    from matplotlib.patches import Ellipse\n    import matplotlib.transforms as transforms\n    \n    for i in range(3):\n        if cov_type == 'full':\n            covariance = gmm.covariances_[i]\n        elif cov_type == 'diag':\n            covariance = np.diag(gmm.covariances_[i])\n        elif cov_type == 'spherical':\n            covariance = gmm.covariances_[i] * np.eye(2)\n        elif cov_type == 'tied':\n            covariance = gmm.covariances_\n        \n        # Calculate eigenvalues and eigenvectors\n        v, w = np.linalg.eigh(covariance)\n        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)  # 95% confidence\n        angle = np.degrees(np.arctan2(w[1, 0], w[0, 0]))\n        \n        # Draw ellipse\n        ell = Ellipse(centers[i], v[0], v[1], angle=angle, \n                     edgecolor='red', facecolor='none', linewidth=2, linestyle='--')\n        ax.add_patch(ell)\n    \n    ax.set_title(f'{cov_type.capitalize()} Covariance', fontsize=14, fontweight='bold')\n    ax.set_xlabel('Feature 1')\n    ax.set_ylabel('Feature 2')\n    ax.set_xlim([-4, 6])\n    ax.set_ylim([-4, 6])\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n# Print BIC scores for comparison\nprint(\"\\nBIC Scores (lower is better):\")\nfor cov_type in covariance_types:\n    gmm = GaussianMixture(n_components=3, covariance_type=cov_type, random_state=42)\n    gmm.fit(X)\n    print(f\"  {cov_type.capitalize()}: {gmm.bic(X):.2f}\")\n```\n\n::: {.cell-output .cell-output-display}\n![](09B-Appendix-Cov-Matrix-in-MV-Gaussians_files/figure-html/cell-6-output-1.png){width=758 height=758}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nBIC Scores (lower is better):\n  Full: 3538.70\n  Diag: 3782.72\n  Spherical: 3846.10\n  Tied: 3815.55\n```\n:::\n:::\n\n\n---\n\n## Number of Parameters\n\nFor a GMM with $K$ components in $d$ dimensions:\n\n| Covariance Type | Parameters per Component | Total Covariance Parameters |\n|----------------|-------------------------|----------------------------|\n| Full | $\\frac{d(d+1)}{2}$ | $K \\cdot \\frac{d(d+1)}{2}$ |\n| Diagonal | $d$ | $K \\cdot d$ |\n| Spherical | $1$ | $K$ |\n| Tied Full | $\\frac{d(d+1)}{2}$ | $\\frac{d(d+1)}{2}$ |\n| Tied Diagonal | $d$ | $d$ |\n| Tied Spherical | $1$ | $1$ |\n\n::: {#8d097ba3 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\n# Calculate number of parameters for different scenarios\ndef count_parameters(K, d, cov_type, tied=False):\n    \"\"\"Count covariance parameters in a GMM\"\"\"\n    if cov_type == 'full':\n        params_per_component = d * (d + 1) // 2\n    elif cov_type == 'diag':\n        params_per_component = d\n    elif cov_type == 'spherical':\n        params_per_component = 1\n    \n    if tied:\n        return params_per_component\n    else:\n        return K * params_per_component\n\n# Example: K=5 components, d=10 dimensions\nK, d = 5, 10\n\nprint(f\"Parameter counts for K={K} components in d={d} dimensions:\\n\")\nprint(f\"  Full:           {count_parameters(K, d, 'full', tied=False)} parameters\")\nprint(f\"  Diagonal:       {count_parameters(K, d, 'diag', tied=False)} parameters\")\nprint(f\"  Spherical:      {count_parameters(K, d, 'spherical', tied=False)} parameters\")\nprint(f\"  Tied Full:      {count_parameters(K, d, 'full', tied=True)} parameters\")\nprint(f\"  Tied Diagonal:  {count_parameters(K, d, 'diag', tied=True)} parameters\")\nprint(f\"  Tied Spherical: {count_parameters(K, d, 'spherical', tied=True)} parameters\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nParameter counts for K=5 components in d=10 dimensions:\n\n  Full:           275 parameters\n  Diagonal:       50 parameters\n  Spherical:      5 parameters\n  Tied Full:      55 parameters\n  Tied Diagonal:  10 parameters\n  Tied Spherical: 1 parameters\n```\n:::\n:::\n\n\n---\n\n## Choosing the Right Covariance Structure\n\n### Use **Full Covariance** when:\n\n- You have lots of data relative to dimensionality\n- Clusters have different shapes and orientations\n- Features are correlated within clusters\n- Maximum flexibility is needed\n\n### Use **Diagonal Covariance** when:\n\n- Features are approximately independent\n- You want computational efficiency\n- Data is moderately sized\n- A good default choice for many applications\n\n### Use **Spherical Covariance** when:\n\n- Features are on similar scales\n- You have limited data\n- Clusters are roughly circular/spherical\n- Maximum computational efficiency needed\n\n### Use **Tied Covariance** when:\n\n- Clusters have similar shapes but different locations\n- You want to reduce overfitting\n- You have limited data\n- Similar to LDA assumptions\n\n---\n\n<!--\n## Mathematical Impact on the Gaussian\n\nThe quadratic form in the exponent:\n\n$$\nQ = (\\mathbf{x}-\\boldsymbol{\\mu})^T\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})\n$$\n\ndefines the **Mahalanobis distance** from $\\mathbf{x}$ to $\\boldsymbol{\\mu}$.\n\n- **Full $\\boldsymbol{\\Sigma}$**: $\\boldsymbol{\\Sigma}^{-1}$ is a general positive definite matrix → ellipsoids in any orientation\n- **Diagonal $\\boldsymbol{\\Sigma}$**: $\\boldsymbol{\\Sigma}^{-1}$ is diagonal → $Q = \\sum_{i=1}^d \\frac{(x_i-\\mu_i)^2}{\\sigma_i^2}$ → axis-aligned ellipsoids\n- **Spherical $\\boldsymbol{\\Sigma}$**: $\\boldsymbol{\\Sigma}^{-1} = \\frac{1}{\\sigma^2}\\mathbf{I}$ → $Q = \\frac{1}{\\sigma^2}\\|\\mathbf{x}-\\boldsymbol{\\mu}\\|^2$ → hyperspheres\n\nThe level sets (contours of constant probability density) are defined by $Q = c$ for constants $c$, which explains the geometric shapes!\n\n::: {#c6a0ae6e .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\n# Demonstrate Mahalanobis distance for different covariance types\nfrom scipy.spatial.distance import mahalanobis\n\n# Test point\nx_test = np.array([1.5, 2.0])\nmean = np.array([0.0, 0.0])\n\n# Different covariance matrices\ncov_full = np.array([[2, 1], [1, 3]])\ncov_diag = np.array([[2, 0], [0, 3]])\ncov_spher = np.array([[2, 0], [0, 2]])\n\n# Calculate Mahalanobis distances\ndist_full = mahalanobis(x_test, mean, np.linalg.inv(cov_full))\ndist_diag = mahalanobis(x_test, mean, np.linalg.inv(cov_diag))\ndist_spher = mahalanobis(x_test, mean, np.linalg.inv(cov_spher))\n\nprint(f\"Mahalanobis distances from origin to point {x_test}:\")\nprint(f\"  Full covariance:      {dist_full:.4f}\")\nprint(f\"  Diagonal covariance:  {dist_diag:.4f}\")\nprint(f\"  Spherical covariance: {dist_spher:.4f}\")\nprint(f\"\\nEuclidean distance:     {np.linalg.norm(x_test):.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMahalanobis distances from origin to point [1.5 2. ]:\n  Full covariance:      1.3229\n  Diagonal covariance:  1.5679\n  Spherical covariance: 1.7678\n\nEuclidean distance:     2.5000\n```\n:::\n:::\n\n\n-->\n\n",
    "supporting": [
      "09B-Appendix-Cov-Matrix-in-MV-Gaussians_files"
    ],
    "filters": [],
    "includes": {}
  }
}