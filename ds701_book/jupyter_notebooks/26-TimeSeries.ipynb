{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'Time Series Analysis'\n",
        "jupyter: python3\n",
        "bibliography: references.bib\n",
        "---\n",
        "\n",
        "## Colab\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/26-TimeSeries.ipynb)\n",
        "\n",
        "\n",
        "# Introduction to Time Series\n",
        "\n",
        "## What is a Time Series?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import os\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "# Define the file path for cached data\n",
        "aapl_file_path = os.path.join('data', 'aapl_stock_data.csv')\n",
        "\n",
        "# Check if the file exists and load it, otherwise download\n",
        "if os.path.exists(aapl_file_path):\n",
        "    # Load data from file - yfinance creates multi-level headers, skip rows 1,2\n",
        "    aapl_data = pd.read_csv(aapl_file_path, header=0, skiprows=[1, 2], index_col=0, parse_dates=True)\n",
        "    aapl_close_px = aapl_data['Close']\n",
        "else:\n",
        "    # Download data from yfinance\n",
        "    try:\n",
        "        aapl_data = yf.download('AAPL', start='2022-01-01', end='2024-11-01', progress=False)\n",
        "        \n",
        "        # Check if download was successful (non-empty dataframe)\n",
        "        if aapl_data.empty:\n",
        "            raise ValueError(\"Downloaded data is empty. Please check the ticker symbol and date range.\")\n",
        "        \n",
        "        # Flatten columns if MultiIndex (yfinance >= 0.2.40)\n",
        "        if isinstance(aapl_data.columns, pd.MultiIndex):\n",
        "            aapl_data.columns = aapl_data.columns.get_level_values(0)\n",
        "        \n",
        "        # Save to file for future use (with flat headers)\n",
        "        aapl_data.to_csv(aapl_file_path)\n",
        "        aapl_close_px = aapl_data['Close']\n",
        "        \n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Error downloading data from yfinance: {str(e)}\")\n",
        "\n",
        "# Plot the closing prices\n",
        "aapl_close_px.plot(label='AAPL', figsize=(10 ,3), ylabel='Price', title='AAPL Stock Price')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Time series** = data points indexed by time (hourly, daily, monthly...)\n",
        "\n",
        "Two key goals:\n",
        "\n",
        "1. **Analysis**: What patterns exist? Trends? Seasonality? Anomalies?\n",
        "2. **Forecasting**: What comes next?\n",
        "\n",
        "## Time Series Are Everywhere"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| fig-align: center\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Check if the 'data' directory exists, and if not, create it\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "if not os.path.exists(os.path.join('data', 'weather_data_filtered_2022.csv')):\n",
        "  # Load the dataset into a dataframe\n",
        "  url = 'https://raw.githubusercontent.com/tools4ds/DS701-Course-Notes/refs/heads/main/ds701_book/data/weather_data_filtered_2022.csv'\n",
        "  weather_data_filtered = pd.read_csv(url)\n",
        "else:\n",
        "  # Read the filtered weather data from the CSV file into a dataframe\n",
        "  weather_data_filtered = pd.read_csv(os.path.join('data', 'weather_data_filtered_2022.csv'), parse_dates=['Date'])\n",
        "\n",
        "# Plot the daily maximum temperature\n",
        "plt.figure(figsize=(10, 3))\n",
        "plt.plot(weather_data_filtered['Date'], weather_data_filtered['Data_Value'] / 10, label='Daily Max Temperature (Â°C)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Temperature (Â°C)')\n",
        "plt.title('Daily Maximum Temperature in 2022 - New York Central Park')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Domain | Example Questions |\n",
        "|--------|-------------------|\n",
        "| **Finance** | Will this stock go up? What's my portfolio risk? |\n",
        "| **Healthcare** | Is this patient's vitals trending abnormally? |\n",
        "| **Climate** | How much warmer will next decade be? |\n",
        "| **Retail** | How much inventory do I need for Black Friday? |\n",
        "\n",
        "## The Four Components\n",
        "\n",
        "Every time series can be decomposed into:\n",
        "\n",
        "| Component | What it captures | Example |\n",
        "|-----------|------------------|---------|\n",
        "| **Trend** | Long-term direction | \"Are EV sales growing?\" |\n",
        "| **Seasonality** | Fixed-period cycles | \"Ice cream sales spike in summer\" |\n",
        "| **Cyclic** | Variable-length oscillations | \"Business cycles\" |\n",
        "| **Noise** | Random variation | \"Day-to-day fluctuations\" |\n",
        "\n",
        "## Today's Roadmap\n",
        "\n",
        "1. **Visualize** â€” See patterns before modeling\n",
        "2. **Decompose** â€” Separate trend, seasonality, noise\n",
        "3. **Test** â€” Is it stationary? (Spoiler: it matters)\n",
        "4. **Model** â€” AR, MA, ARIMA, SARIMA\n",
        "5. **Forecast** â€” Predict the future (with uncertainty)\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Time and Date in Python\n",
        "\n",
        "> See course notes for datetime/pandas manipulation details \n",
        "> (based on [Python for Data Analysis, 3rd Ed.](https://wesmckinney.com/book/time-series))\n",
        ":::\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "# Time and Date Manipulation\n",
        "\n",
        "Many time series data sets are indexed by date or time. The python `datetime`\n",
        "library and the `pandas` library provide a powerful set of tools for manipulating\n",
        "time series data.\n",
        "\n",
        "The [Time Series](https://wesmckinney.com/book/time-series) chapter of the book\n",
        "[Python for Data Analysis, 3rd Ed.](https://wesmckinney.com/book/time-series)\n",
        "provides a good overview of these tools. We'll share a few excerpts here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "print(f\"Date and time when this cell was executed: {now}\")\n",
        "print(f\"Year: {now.year}, month: {now.month}, day: {now.day}\")\n",
        "\n",
        "delta = now - datetime(2024, 1, 1)\n",
        "print(f\"Since beginning of 2024 till when this cell was run there were {delta.days} days and {delta.seconds} seconds.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also convert between strings and datetime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# string to datetime\n",
        "date_string = \"2024-01-01\"\n",
        "date_object = datetime.strptime(date_string, \"%Y-%m-%d\")\n",
        "print(date_object)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also format datetime objects as strings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# datetime to string\n",
        "now_str = now.strftime(\"%Y-%m-%d\")\n",
        "print(now_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "See Table 11.2 in the [book](https://wesmckinney.com/book/time-series) for a list of formatting codes.\n",
        "\n",
        "Let's explore some of the pandas time series tools.\n",
        "\n",
        "Create a time series with a datetime index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "longer_ts = pd.Series(np.random.standard_normal(1000),\n",
        "                      index=pd.date_range(\"2022-01-01\", periods=1000))\n",
        "print(type(longer_ts))\n",
        "longer_ts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can access just the samples from 2023 with simply:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "longer_ts[\"2023\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or the month of September 2023:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "longer_ts[\"2023-09\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Or slice by date range:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "longer_ts[\"2023-03-01\":\"2023-03-10\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "or:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "longer_ts[\"2023-09-15\":]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are many more time series tools available that let you do things like:\n",
        "\n",
        "- Shifting and setting frequencies of date ranges\n",
        "- Time zone handling\n",
        "- Time series resampling\n",
        "- Time series rolling and expanding windows\n",
        "\n",
        "## Moving Window Functions\n",
        "\n",
        "Let's dive into the moving window functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import os\n",
        "\n",
        "# Define the file path for cached data (10-year history)\n",
        "aapl_10yr_file_path = os.path.join('data', 'aapl_stock_data_10yr.csv')\n",
        "\n",
        "# Check if the file exists and load it, otherwise download\n",
        "import warnings\n",
        "\n",
        "aapl_data = None\n",
        "if os.path.exists(aapl_10yr_file_path):\n",
        "    # Load data from file - yfinance creates multi-level headers, skip rows 1,2\n",
        "    aapl_data = pd.read_csv(aapl_10yr_file_path, header=0, skiprows=[1, 2], index_col=0, parse_dates=True)\n",
        "else:\n",
        "    # Download data from yfinance\n",
        "    try:\n",
        "        aapl_data = yf.download('AAPL', start='2012-01-01', end='2022-01-01', progress=False)\n",
        "        \n",
        "        # Check if download was successful (non-empty dataframe)\n",
        "        if aapl_data.empty:\n",
        "            warnings.warn(\"Downloaded data is empty. Please check the ticker symbol and date range.\")\n",
        "            aapl_data = None\n",
        "        else:\n",
        "            # Flatten columns if MultiIndex (yfinance >= 0.2.40)\n",
        "            if isinstance(aapl_data.columns, pd.MultiIndex):\n",
        "                aapl_data.columns = aapl_data.columns.get_level_values(0)\n",
        "            \n",
        "            # Save to file for future use\n",
        "            aapl_data.to_csv(aapl_10yr_file_path)\n",
        "        \n",
        "    except Exception as e:\n",
        "        warnings.warn(f\"Error downloading data from yfinance: {str(e)}\")\n",
        "        aapl_data = None\n",
        "\n",
        "if aapl_data is not None:\n",
        "    print(aapl_data.head())\n",
        "    aapl_close_px = aapl_data['Close']\n",
        "else:\n",
        "    print(\"AAPL data not available - skipping this example\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot the closing prices\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if aapl_data is not None:\n",
        "    ax = aapl_close_px.plot(label='AAPL')\n",
        "    aapl_close_px.rolling(window=250).mean().plot(label='250d MA', ax=ax)\n",
        "    ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "# Visualization\n",
        "\n",
        "Always visualize first! Let's explore patterns in a classic dataset.\n",
        "\n",
        "## Air Passengers Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import os\n",
        "\n",
        "# Check if the 'data' directory exists, and if not, create it\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "# Check if the 'air_passengers_1949_1960.csv' file exists, and if not, download it\n",
        "if not os.path.exists(os.path.join('data', 'air_passengers_1949_1960.csv')):\n",
        "    url = 'https://raw.githubusercontent.com/tools4ds/DS701-Course-Notes/refs/heads/main/ds701_book/data/air_passengers_1949_1960.csv'\n",
        "    air_passengers_data = pd.read_csv(url)\n",
        "    air_passengers_data.to_csv(os.path.join('data', 'air_passengers_1949_1960.csv'), index=False)\n",
        "    print(\"Air passengers data has been downloaded and saved to 'data/air_passengers_1949_1960.csv'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We're going to use a dataset of air passengers per month from 1949 to 1960."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "path = os.path.join('data', 'air_passengers_1949_1960.csv')\n",
        "air_passengers = pd.read_csv(path, index_col='Date', parse_dates=True)\n",
        "air_passengers.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Time Series Plot\n",
        "\n",
        "Let's look at the time series plot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "ts = air_passengers['Number of Passengers']\n",
        "ts.plot(ylabel='Number of Passengers', title='Air Passengers 1949-1960', figsize=(10, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Clearly there are some trends and seasonality in the data.\n",
        "\n",
        "Clearly: **trend** (going up) + **seasonality** (annual pattern) + **growing amplitude**.\n",
        "\n",
        "---\n",
        "\n",
        "Let's visualize that seasonal pattern more explicitly:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "# Seasonal plot of air_passengers\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Extract month and year from the index\n",
        "air_passengers['Month'] = air_passengers.index.month\n",
        "air_passengers['Year'] = air_passengers.index.year\n",
        "\n",
        "# Create a seasonal plot\n",
        "#plt.figure(figsize=(10, 4))\n",
        "sns.lineplot(data=air_passengers, x='Month', y='Number of Passengers', hue='Year', palette='tab10')\n",
        "# plt.title('Seasonal Plot of Air Passengers')\n",
        "# plt.ylabel('Number of Passengers')\n",
        "# plt.xlabel('Month')\n",
        "# plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice: the seasonal amplitude is **growing** over time â€” important for model choice!\n",
        "\n",
        "---\n",
        "\n",
        "Let's look at the distribution across years and months:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "#| echo: false\n",
        "# Add normalized column for later use\n",
        "import matplotlib.pyplot as plt\n",
        "air_passengers['Normalized_Passengers'] = air_passengers.groupby('Year')['Number of Passengers'].transform(lambda x: x / x.iloc[0])\n",
        "\n",
        "# Create side-by-side plots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "# Year-wise box plot\n",
        "sns.boxplot(data=air_passengers, x='Year', y='Number of Passengers', palette='tab10', ax=ax1)\n",
        "ax1.set_title('By Year: Clear Upward Trend')\n",
        "ax1.set_ylabel('Passengers')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Month-wise box plot  \n",
        "sns.boxplot(data=air_passengers, x='Month', y='Number of Passengers', palette='tab10', hue='Month', legend=False, ax=ax2)\n",
        "ax2.set_title('By Month: Summer Peak (Jul-Aug)')\n",
        "ax2.set_ylabel('Passengers')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Left**: Trend is obvious. **Right**: July-August are peak travel months.\n",
        "\n",
        "## Autocorrelation: The Key Diagnostic\n",
        "\n",
        "**Autocorrelation** = correlation between $y_t$ and $y_{t-k}$ (lagged by $k$ steps)\n",
        "\n",
        "$$\n",
        "\\rho_k = \\frac{\\sum_{t=k+1}^{T} (y_t - \\bar{y})(y_{t-k} - \\bar{y})}{\\sum_{t=1}^{T} (y_t - \\bar{y})^2}\n",
        "$$\n",
        "\n",
        "**Why it matters:**\n",
        "\n",
        "- High autocorrelation (for $k>0$) â†’ values depend on past â†’ predictable!\n",
        "- Peaks at regular lags â†’ seasonality\n",
        "- No autocorrelation â†’ white noise (unpredictable)\n",
        "\n",
        "## ACF of Air Passengers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "\n",
        "plot_acf(air_passengers['Number of Passengers'], lags=48)\n",
        "plt.title('Autocorrelation Plot of Air Passengers')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reading the ACF**: Blue shading = 95% CI. Peak at lag 12 is significant â†’ yearly seasonality confirmed!\n",
        "\n",
        "# Time Series Decomposition\n",
        "\n",
        "## The Goal: Separate Signal from Noise\n",
        "\n",
        "We want to break apart: **Observed = Trend + Seasonality + Residual**\n",
        "\n",
        "Why?\n",
        "\n",
        "- **Understand** the underlying patterns\n",
        "- **Remove** seasonality for cleaner modeling  \n",
        "- **Detect** anomalies in the residuals\n",
        "- **Forecast** each component separately\n",
        "\n",
        "## Additive vs Multiplicative\n",
        "\n",
        "**How do the components combine?**\n",
        "\n",
        "| Model | Formula | When to use |\n",
        "|-------|---------|-------------|\n",
        "| **Additive** | $Y = T + S + \\epsilon$ | Seasonal amplitude is constant |\n",
        "| **Multiplicative** | $Y = T \\times S \\times \\epsilon$ | Seasonal amplitude grows with trend |\n",
        "\n",
        "Look back at air passengers: amplitude **grows** â†’ multiplicative is better!\n",
        "\n",
        "## Classical Decomposition: The Algorithm\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "1. **Estimate trend** â€” moving average smooths out seasonality\n",
        "2. **Remove trend** â€” detrended = observed - trend  \n",
        "3. **Estimate seasonality** â€” average each month's detrended values\n",
        "4. **Residual** â€” what's left after removing trend & seasonality\n",
        "\n",
        "---\n",
        "\n",
        "Let's do it step by step:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Trend via moving average\n",
        "trend = ts.rolling(window=12, center=True).mean()\n",
        "\n",
        "# Step 2: Detrend\n",
        "detrended_ts = ts - trend\n",
        "\n",
        "# Step 3: Seasonal pattern (average by month)\n",
        "seasonal_ts = detrended_ts.groupby(detrended_ts.index.month).mean()\n",
        "\n",
        "# Step 4: Residual\n",
        "irregular_ts = detrended_ts - np.tile(seasonal_ts, len(detrended_ts) // len(seasonal_ts))\n",
        "\n",
        "# Plot all components\n",
        "fig, axes = plt.subplots(4, 1, figsize=(10, 8), sharex=False)\n",
        "\n",
        "ts.plot(ax=axes[0], title='Original')\n",
        "trend.plot(ax=axes[1], title='Trend (12-month MA)', color='orange')\n",
        "detrended_ts.plot(ax=axes[2], title='Detrended', color='green')\n",
        "irregular_ts.plot(ax=axes[3], title='Residual', color='red')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Question**: Is the residual truly random? (Hint: look at the variance over time)\n",
        "\n",
        "\n",
        "## STL: A Better Decomposition\n",
        "\n",
        "**STL** = Seasonal-Trend decomposition using Loess [@cleveland1990stl]\n",
        "\n",
        "Why STL over classical?\n",
        "\n",
        "- âœ… Handles **changing seasonality** (amplitude can vary)\n",
        "- âœ… Robust to **outliers**\n",
        "- âœ… Works with **any period** (not just 12 months)\n",
        "\n",
        "> See course notes for details on Loess (locally weighted regression)\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "\n",
        "## Locally Estimated Scatterplot Smoothing -- Loess[^gpt4o]\n",
        "\n",
        "[^gpt4o]: gpt-4o, personal communication, Nov 2024\n",
        "\n",
        "**Loess**, which stands for \"Locally Estimated Scatterplot Smoothing,\" is a non-parametric method used to estimate non-linear relationships in data. It is particularly useful for smoothing scatterplots and is a type of local regression.\n",
        "\n",
        "### Key Features of Loess:\n",
        "\n",
        "1. **Local Fitting**: Loess fits simple models to localized subsets of the data to build up a function that describes the deterministic part of the variation in the data, point by point.\n",
        "\n",
        "2. **Weighted Least Squares**: It uses weighted least squares to fit a polynomial surface to the data. The weights decrease with distance from the point of interest, giving more influence to points near the target point.\n",
        "\n",
        "3. **Flexibility**: Loess is flexible and can model complex relationships without assuming a specific global form for the data. It can adapt to various shapes and patterns in the data.\n",
        "\n",
        "4. **Smoothing Parameter**: The degree of smoothing is controlled by a parameter, often denoted as $\\alpha$ or the span. This parameter determines the proportion of data points used in each local fit. A smaller span results in a curve that follows the data more closely, while a larger span results in a smoother curve.\n",
        "\n",
        "5. **Polynomial Degree**: Loess can fit either linear or quadratic polynomials to the data. The choice of polynomial degree affects the smoothness and flexibility of the fit.\n",
        "\n",
        "### How Loess Works:\n",
        "\n",
        "- **Step 1**: For each point in the dataset, a neighborhood of points is selected based on the smoothing parameter.\n",
        "- **Step 2**: A weighted least squares regression is performed on the points in the neighborhood, with weights decreasing with distance from the target point.\n",
        "- **Step 3**: The fitted value at the target point is computed from the local regression model.\n",
        "- **Step 4**: This process is repeated for each point in the dataset, resulting in a smooth curve that captures the underlying trend.\n",
        "\n",
        "### Applications:\n",
        "\n",
        "Loess is widely used in exploratory data analysis to visualize trends and patterns in data. It is particularly useful when the relationship between variables is complex and not well-represented by a simple linear or polynomial model.\n",
        "\n",
        "### Example in Python:\n",
        "\n",
        "In Python, the `statsmodels` library provides a function for performing Loess smoothing:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "\n",
        "# Example data\n",
        "x = np.linspace(0, 10, 100)\n",
        "y = np.sin(x) + np.random.normal(0, 0.1, 100)\n",
        "\n",
        "# Apply Loess smoothing\n",
        "smoothed = lowess(y, x, frac=0.2)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(x, y, label='Data', alpha=0.5)\n",
        "plt.plot(smoothed[:, 0], smoothed[:, 1], color='red', label='Loess Smoothed')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, `frac` is the smoothing parameter that controls the amount of smoothing applied to the data.\n",
        ":::\n",
        "\n",
        "## In Practice: `statsmodels`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose, STL\n",
        "\n",
        "# Reload clean data\n",
        "data = pd.read_csv(os.path.join('data', 'air_passengers_1949_1960.csv'), index_col='Date', parse_dates=True)\n",
        "ts = data['Number of Passengers']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additive vs Multiplicative: See the Difference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "fig, axes = plt.subplots(2, 4, figsize=(14, 6))\n",
        "\n",
        "for i, model in enumerate(['additive', 'multiplicative']):\n",
        "    decomp = seasonal_decompose(ts, model=model)\n",
        "    decomp.observed.plot(ax=axes[i, 0], title='Observed' if i==0 else '')\n",
        "    decomp.trend.plot(ax=axes[i, 1], title='Trend' if i==0 else '')\n",
        "    decomp.seasonal.plot(ax=axes[i, 2], title='Seasonal' if i==0 else '')\n",
        "    decomp.resid.plot(ax=axes[i, 3], title='Residual' if i==0 else '')\n",
        "    axes[i, 0].set_ylabel(model.title())\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Key insight**: Multiplicative residuals are more uniform â€” better fit!\n",
        "\n",
        "## STL in Action"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center \n",
        "from statsmodels.tsa.seasonal import STL\n",
        "\n",
        "stl = STL(ts, period=12, robust=True)\n",
        "result = stl.fit()\n",
        "\n",
        "fig, axes = plt.subplots(4, 1, figsize=(10, 6), sharex=True)\n",
        "result.observed.plot(ax=axes[0], title='STL Decomposition')\n",
        "axes[0].set_ylabel('Observed')\n",
        "result.trend.plot(ax=axes[1])\n",
        "axes[1].set_ylabel('Trend')\n",
        "result.seasonal.plot(ax=axes[2])\n",
        "axes[2].set_ylabel('Seasonal')\n",
        "result.resid.plot(ax=axes[3])\n",
        "axes[3].set_ylabel('Residual')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stationarity: Why It Matters\n",
        "\n",
        "## The Stationarity Problem\n",
        "\n",
        "**Stationary** = statistical properties (mean, variance) don't change over time\n",
        "\n",
        "**Why do we care?**\n",
        "\n",
        "Most forecasting models **assume stationarity**. If your data has:\n",
        "\n",
        "- ðŸ“ˆ Trend â†’ mean is changing\n",
        "- ðŸ“Š Growing variance â†’ variance is changing  \n",
        "- ðŸ”„ Seasonality â†’ periodic non-stationarity\n",
        "\n",
        "...the model will fail. We must **transform** the data first.\n",
        "\n",
        "## Making Data Stationary\n",
        "\n",
        "| Problem | Solution |\n",
        "|---------|----------|\n",
        "| Trend | **Differencing**: $y'_t = y_t - y_{t-1}$ |\n",
        "| Growing variance | **Log transform**: $y'_t = \\log(y_t)$ |\n",
        "| Seasonality | **Seasonal differencing**: $y'_t = y_t - y_{t-12}$ |\n",
        "\n",
        "## Testing for Stationarity\n",
        "\n",
        "Two standard tests (both in `statsmodels`):\n",
        "\n",
        "| Test | Null Hypothesis | Reject means... |\n",
        "|------|-----------------|-----------------|\n",
        "| **ADF** | Has unit root (non-stationary) | Stationary âœ“ |\n",
        "| **KPSS** | Is stationary | Non-stationary âœ— |\n",
        "\n",
        "Use both â€” if they agree, you can trust the result!\n",
        "\n",
        "::: {.content-visible when-profile=\"web\"}\n",
        "\n",
        "## ADF Test Demonstration\n",
        "\n",
        "Let's apply the tests, starting with ADF:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "\n",
        "air_passengers = pd.read_csv(os.path.join('data', 'air_passengers_1949_1960.csv'))\n",
        "\n",
        "# Apply ADF and KPSS tests on the air passenger data\n",
        "\n",
        "# ADF Test\n",
        "result_adf = adfuller(air_passengers['Number of Passengers'])\n",
        "print('ADF Statistic:', result_adf[0])\n",
        "print('p-value:', result_adf[1])\n",
        "print('Critical Values:')\n",
        "for key, value in result_adf[4].items():\n",
        "    print('\\t%s: %.3f' % (key, value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation:\n",
        "\n",
        "1. **ADF Statistic**: The ADF statistic is 0.815, which is greater than all the critical values at the 1%, 5%, and 10% significance levels.\n",
        "\n",
        "2. **p-value**: The p-value is 0.991, which is significantly higher than common significance levels (e.g., 0.01, 0.05, 0.10).\n",
        "\n",
        "3. **Critical Values**:\n",
        "   - 1%: -3.482\n",
        "   - 5%: -2.884\n",
        "   - 10%: -2.579\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "- **Fail to Reject the Null Hypothesis**: Since the ADF statistic (0.815) is greater than the critical values and the p-value (0.991) is much higher than typical significance levels, you fail to reject the null hypothesis. This suggests that the time series has a unit root and is non-stationary.\n",
        "\n",
        "- **Implication**: The time series data for the number of passengers is non-stationary, indicating that it may have a trend or other non-stationary components. To make the series stationary, you might consider differencing the data or applying other transformations, such as detrending or seasonal adjustment, before proceeding with further analysis or modeling.\n",
        "\n",
        "## KPSS Test Demonstration\n",
        "\n",
        "Let's apply the KPSS test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# KPSS Test\n",
        "result_kpss = kpss(air_passengers['Number of Passengers'], regression='c')\n",
        "print('\\nKPSS Statistic:', result_kpss[0])\n",
        "print('p-value:', result_kpss[1])\n",
        "print('Critical Values:')\n",
        "for key, value in result_kpss[3].items():\n",
        "    print('\\t%s: %.3f' % (key, value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The KPSS (Kwiatkowski-Phillips-Schmidt-Shin) test is another test used to assess the stationarity of a time series, but it has a different null hypothesis compared to the ADF test.\n",
        "\n",
        "### KPSS Test Interpretation:\n",
        "\n",
        "1. **Null Hypothesis (\\(H_0\\))**: The null hypothesis of the KPSS test is that the time series is stationary around a deterministic trend (i.e., it does not have a unit root).\n",
        "\n",
        "2. **Alternative Hypothesis (\\(H_1\\))**: The alternative hypothesis is that the time series is not stationary (i.e., it has a unit root).\n",
        "\n",
        "### Given Results:\n",
        "\n",
        "- **KPSS Statistic**: 1.651\n",
        "- **p-value**: 0.01\n",
        "- **Critical Values**:\n",
        "  - 10%: 0.347\n",
        "  - 5%: 0.463\n",
        "  - 2.5%: 0.574\n",
        "  - 1%: 0.739\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "- **Reject the Null Hypothesis**: The KPSS statistic (1.651) is greater than all the critical values at the 10%, 5%, 2.5%, and 1% significance levels. This, along with the low p-value (0.01), suggests that you reject the null hypothesis of stationarity.\n",
        "\n",
        "- **Implication**: The time series is likely non-stationary according to the KPSS test. This aligns with the ADF test results, which also indicated non-stationarity.\n",
        "\n",
        "### Overall Interpretation:\n",
        "\n",
        "Both the ADF and KPSS tests suggest that the time series is non-stationary. This consistent result from both tests strengthens the conclusion that the series may need differencing or other transformations to achieve stationarity before further analysis or modeling.\n",
        "\n",
        ":::\n",
        "\n",
        "# Time Series Models\n",
        "\n",
        "## The ARIMA Family\n",
        "\n",
        "Building blocks for forecasting:\n",
        "\n",
        "| Model | Idea | Parameters |\n",
        "|-------|------|------------|\n",
        "| **AR(p)** | Past **values** predict future | $y_t = c + \\phi_1 y_{t-1} + ... + \\epsilon_t$ |\n",
        "| **MA(q)** | Past **errors** predict future | $y_t = \\mu + \\theta_1 \\epsilon_{t-1} + ... + \\epsilon_t$ |\n",
        "| **ARIMA(p,d,q)** | AR + MA + **differencing** | Combines both, d = differencing order |\n",
        "| **SARIMA** | ARIMA + **seasonality** | (p,d,q) Ã— (P,D,Q,s) |\n",
        "\n",
        "## ARIMA: The Workhorse\n",
        "\n",
        "$$\n",
        "y_t = c + \\underbrace{\\phi_1 y_{t-1} + ... + \\phi_p y_{t-p}}_{\\text{AR: past values}} + \\underbrace{\\theta_1 \\epsilon_{t-1} + ... + \\theta_q \\epsilon_{t-q}}_{\\text{MA: past errors}} + \\epsilon_t\n",
        "$$\n",
        "\n",
        "**ARIMA(p, d, q)**:\n",
        "\n",
        "- **p** = AR order (how many past values?)\n",
        "- **d** = differencing order (how many times to difference?)\n",
        "- **q** = MA order (how many past errors?)\n",
        "\n",
        "## SARIMA: Adding Seasonality\n",
        "\n",
        "For data with seasonal patterns, add seasonal terms:\n",
        "\n",
        "**SARIMA(p,d,q) Ã— (P,D,Q,s)**\n",
        "\n",
        "- Lowercase (p,d,q) = short-term dynamics\n",
        "- Uppercase (P,D,Q) = seasonal dynamics  \n",
        "- **s** = seasonal period (12 for monthly, 4 for quarterly)\n",
        "\n",
        "Example: SARIMA(1,1,1)Ã—(1,1,1,12) for monthly data with yearly seasonality\n",
        "\n",
        "\n",
        "## SARIMA in Practice\n",
        "\n",
        "Let's forecast air passengers with SARIMA(1,1,1)Ã—(1,1,1,12):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# Prepare data\n",
        "path = os.path.join('data', 'air_passengers_1949_1960.csv')\n",
        "data = pd.read_csv(path)\n",
        "data['Month'] = pd.date_range(start='1949-01', periods=len(data), freq='ME')\n",
        "data.set_index('Month', inplace=True)\n",
        "\n",
        "# Log transform stabilizes growing variance\n",
        "data['Log_Passengers'] = np.log(data['Number of Passengers'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Fit SARIMA model\n",
        "model = SARIMAX(data['Log_Passengers'], \n",
        "                order=(1, 1, 1),           # (p,d,q) \n",
        "                seasonal_order=(1, 1, 1, 12),  # (P,D,Q,s)\n",
        "                freq='ME')\n",
        "results = model.fit()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Diagnostics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "results.plot_diagnostics(figsize=(12, 6))\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What to check:**\n",
        "\n",
        "- **Residuals**: Should look like white noise (no pattern)\n",
        "- **Histogram**: Should be roughly normal\n",
        "- **Q-Q plot**: Points should follow the diagonal\n",
        "- **ACF**: No significant spikes (we captured all structure)\n",
        "\n",
        "---\n",
        "\n",
        "Finally, let's make a forecast for 2 years out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Forecasting\n",
        "forecast = results.get_forecast(steps=24)\n",
        "forecast_index = pd.date_range(data.index[-1] + pd.DateOffset(months=1), periods=24, freq='ME')\n",
        "forecast_values = np.exp(forecast.predicted_mean)  # Convert back from log\n",
        "confidence_intervals = np.exp(forecast.conf_int())\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(data['Number of Passengers'], label='Observed')\n",
        "plt.plot(forecast_index, forecast_values, label='Forecast', color='red')\n",
        "plt.fill_between(forecast_index, confidence_intervals.iloc[:, 0], confidence_intervals.iloc[:, 1], color='pink', alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The light pink area shows the 95% confidence interval for the forecast.\n",
        "\n",
        "**Question**: What observations do you have about the prediction?\n",
        "\n",
        "\n",
        "# Model Selection & Evaluation\n",
        "\n",
        "## How Do We Choose Parameters?\n",
        "\n",
        "**AIC & BIC**: Balance fit vs. complexity\n",
        "\n",
        "$$\\text{AIC} = T\\log\\left(\\frac{\\text{SSE}}{T}\\right) + 2k \\quad \\text{(lower is better)}$$\n",
        "\n",
        "| Criterion | Penalizes complexity... |\n",
        "|-----------|------------------------|\n",
        "| **AIC** | Moderately |\n",
        "| **BIC** | More heavily (prefers simpler models) |\n",
        "\n",
        "In practice: try multiple (p,d,q) combinations, pick lowest AIC/BIC.\n",
        "\n",
        "## Time Series Cross-Validation\n",
        "\n",
        "âš ï¸ **Can't shuffle!** Must respect temporal order.\n",
        "\n",
        "**Rolling forecast origin:**\n",
        "\n",
        "```\n",
        "Train: [----] Test: [.]\n",
        "Train: [-----] Test: [.]\n",
        "Train: [------] Test: [.]\n",
        "```\n",
        "\n",
        "Always train on past, test on future â€” no data leakage!\n",
        "\n",
        "# Summary\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Visualize first** â€” trends, seasonality, autocorrelation\n",
        "2. **Decompose** â€” separate signal from noise (STL > classical)\n",
        "3. **Check stationarity** â€” transform if needed (differencing, log)\n",
        "4. **Model** â€” ARIMA for non-seasonal, SARIMA for seasonal\n",
        "5. **Validate** â€” diagnostics + time-aware cross-validation\n",
        "\n",
        "## References\n",
        "\n",
        "- [Forecasting: Principles & Practice](https://otexts.com/fpp3/) â€” the definitive free textbook\n",
        "- [statsmodels documentation](https://www.statsmodels.org/stable/tsa.html)\n",
        "\n",
        "## Bibliography\n",
        "\n",
        "::: {#refs}\n",
        ":::"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}