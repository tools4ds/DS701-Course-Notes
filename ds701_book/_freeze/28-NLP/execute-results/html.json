{
  "hash": "3151dd6f58743a73c5184787eb21a253",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Natural Language Processing'\njupyter: python3\nbibliography: references.bib\n---\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/27-RNN.ipynb)\n\nNatural language processing (NLP) is a subfield of artificial intelligence that allows computers to understand, process, and manipulate human language.\n\n# Brief History of NLP\n\n## NLP Evolution Timeline\n\n::: {.columns}\n::: {.column width=\"50%\"}\n**1950s-1970s: Rule-Based Era**\n\n- Turing Test (1950), formal grammars, hand-crafted rules\n- Systems like ELIZA (1966) - pattern matching chatbot\n- Limitations: brittle, didn't scale, couldn't handle ambiguity\n\n**1980s-2000s: Statistical Revolution**\n\n- Shift from rules to learning from data\n- N-grams, Hidden Markov Models, machine learning\n- IBM alignment models for machine translation\n:::\n::: {.column width=\"50%\"}\n**2006-2017: Deep Learning**\n\n- Neural networks applied to NLP\n- Word2Vec (2013), RNNs, LSTMs for sequence modeling\n- Attention mechanism (2014) - precursor to Transformers\n\n**2017-Present: Transformer Era**\n\n- Attention is All You Need (2017) - Transformer architecture\n- BERT (2018), GPT-2/3 (2019/2020), ChatGPT (2022)\n- LLMs with billions of parameters, human-level performance\n:::\n:::\n\n# Lecture Outline\n\nFor the rest of this lecture we will cover:\n\n- Text preprocessing and tokenization\n- Numerical representations of words\n  - Sparse representations (One-Hot, BoW, TF-IDF)\n  - Word2Vec and static embeddings\n  - Contextual embeddings\n- Language models overview\n  - N-gram models\n  - Transformers (high-level)\n- **Named Entity Recognition (NER)**\n  - NER with NLTK\n  - NER with spaCy\n  - NER with BERT\n- **Topic Modeling**\n  - Latent Dirichlet Allocation (LDA)\n  - BERTopic\n\n\n# Numerical Representation of Words\n\n## Numerical Representations of Words\n\nMachine learning models for NLP are not able to process text in the form of characters and strings. Characters and strings must be converted to numbers in order to train our language models.\n\nThere are a number of ways to do this. These include\n\n- sparse representations, like one-hot encodings and TF-IDF encodings\n- word embeddings.\n\nHowever, prior to creating a numerical representation of text, we need to **tokenize** the text.\n\n## Tokenization\n\nTokenization is the process of splitting raw text into smaller pieces, called (drum-roll please), *tokens*. Tokens can be individual characters, words, or sentences.\n\nExamples of character and word tokenization are shown for the following raw text\n\n```Show me the money```\n\nCharacter tokenization:\n\n```['S', 'h', 'o', 'w', 'm', 'e', 't', 'h', 'e', 'm', 'o', 'n', 'e', 'y']```.\n\nWord tokenization:\n\n```['Show', 'me', 'the', 'money'] ```\n\n---\n\nThis code block demonstrates both of these tokenization techniques.\n\n::: {#e76266e3 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\n# Character and word tokenization\n\nsentence = \"Show me the money\"\nword_tokens = sentence.split()\nprint(word_tokens)\ncharacter_tokens = [char for char in sentence if char != ' ']\nprint(character_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Show', 'me', 'the', 'money']\n['S', 'h', 'o', 'w', 'm', 'e', 't', 'h', 'e', 'm', 'o', 'n', 'e', 'y']\n```\n:::\n:::\n\n\n---\n\nThere are advantages and disadvantages to different tokenization methods. We showed two very simple strategies. \n\nHowever, there are other strategies, such as subword and sentence tokenization,\nsee for example [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte-pair_encoding),\nand [SentencePiece](https://github.com/google/sentencepiece).\n\nWith tokenization, our goal is to not lose meaning with the tokens. With character based tokenization, especially for English (non-character based languages) we certainly lose meaning. \n\nHere is a demo of how to tokenize using the [transformers](https://huggingface.co/docs/transformers/en/index) package from Huggingface.\n\n::: {#88a9df29 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\nfrom transformers import AutoTokenizer, logging\n\nlogging.set_verbosity_warning()\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\ntokens = tokenizer.tokenize(sentence)\nprint(tokens)\n\n# Try a more advanced sentence\nsentence2 = \"Let's try to see if we can get this transformer to tokenize.\"\ntokens2 = tokenizer.tokenize(sentence2)\nprint(tokens2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Show', 'me', 'the', 'money']\n['Let', \"'\", 's', 'try', 'to', 'see', 'if', 'we', 'can', 'get', 'this', 'transform', '##er', 'to', 'token', '##ize', '.']\n```\n:::\n:::\n\n\n## Tokens and Token IDs\n\nAssociated to each token is a unique token ID. The total number of unique tokens that a model can recognize and process is the *vocabulary size*. The *vocabulary* is the collection of all the unique tokens.\n\nThe tokens (and token ids) alone hold no (semantic) information. What is needed is a numerical representation that *encodes* this information. \n\nThere are different ways to achieve this. One encoding technique that we already considered is one-hot encodings. Another more powerful encoding method, is the creation of word embeddings.\n\n## Sparse Representations\n\nSparse representations encode text as high-dimensional vectors with mostly zeros.\n\n**Common approaches:**\n\n- **One-Hot Encoding**: Each word is a vector with a single 1 and rest 0s\n  - Example: cat = [1,0,0], dog = [0,1,0], emu = [0,0,1]\n- **Bag of Words (BoW)**: Count word frequencies, ignore order\n- **TF-IDF**: Weight words by importance across documents\n\n**Limitations:**\n\n- High dimensionality (vocabulary size)\n- No semantic meaning (cat and kitten are equally distant)\n- Cannot capture word relationships\n\n## Word Embeddings\n\nWord embeddings represent words as dense vectors in high-dimensional spaces.\n\nThe individual values of the vector may be difficult to interpret, but the \noverall pattern is that _words with similar meanings are close to each other_, \nin the sense that their vectors have small angles with each other.\n\nThe similarity of two word embeddings is the cosine of the angle between the two\nvectors. Recall that for two vectors $v_1, v_2\\in\\mathbb{R}^{n}$, the formula \nfor the cosine of the angle between them is\n\n$$ \n\\cos{(\\theta)} = \\frac{v_1 \\cdot v_2}{\\Vert v_1 \\Vert_2 \\Vert v_2 \\Vert_2}.\n$$\n\nWord embeddings can be static or contextual. A static embedding is when each word has a single embedding, e.g., Word2Vec. A contextual embedding (used by more complex language model embedding algorithms) allows the embedding for a word to change depending on its context in a sentence.\n\n## Word2Vec: Static Embeddings\n\n**Word2Vec** [@mikolov2013efficient] is a technique to learn static word embeddings from large text corpora.\n\n**Key characteristics:**\n\n- Each word has a **single fixed vector** representation\n- Trained to predict context (CBOW) or predict word from context (Skip-gram)\n- Captures semantic relationships: *king - man + woman ≈ queen*\n- Fast to train and use\n- Popular pre-trained models: Google News (300d), GloVe\n\n**Advantages:**\n\n- Efficient for large vocabularies\n- Good for similarity tasks, clustering\n- Pre-trained embeddings available\n\n## Contextual vs Static Embeddings\n\n| Feature | Static (Word2Vec, GloVe) | Contextual (BERT, GPT) |\n|---------|--------------------------|------------------------|\n| **Representation** | One vector per word | Different vectors per context |\n| **Example** | \"bank\" always same vector | \"bank\" differs in \"river bank\" vs \"bank account\" |\n| **Model Type** | Shallow neural network | Deep transformer model |\n| **Training** | Fast, lightweight | Slow, resource-intensive |\n| **Best For** | Similarity, clustering, simple tasks | Complex understanding, ambiguity resolution |\n| **Dimensionality** | 100-300 dimensions | 768-1024+ dimensions |\n\n**When to use Word2Vec:**\n\n- Limited computational resources\n- Task doesn't require deep context understanding\n- Working with domain-specific corpora (train custom embeddings)\n\n**When to use Contextual Embeddings:**\n\n- Need to handle polysemy (words with multiple meanings)\n- Complex NLP tasks (NER, question answering, translation)\n- Have access to GPU resources\n\n# Language Models\n\n\n## Language Models\n\nA language model is a statistical tool that predicts the probability of a sequence of words. It helps in understanding and generating human language by learning patterns and structures from large text corpora.\n\n1. **N-gram Models**:\n   - Predict the next word based on the previous  $n-1$ words.\n   - Simple and effective for many tasks but limited by fixed context size.\n\n1. **Neural Language Models**:\n   - Use neural networks to capture more complex patterns.\n   - Examples include *RNNs*, *LSTMs*, and **Transformers**.\n\nSee [RNNs and LSTMs](#27-RNN.qmd) for more details.\n\nWe'll discuss N-grams briefly followed by a deep dive on Transformers.\n\n# N-gram Models\n\n\n## N-gram models\n\n- **Definition**: An n-gram model is a type of probabilistic language model used in natural language processing.\n- **Purpose**: It predicts the next item in a sequence based on the previous \\( n-1 \\) items.\n- **Types**:\n  - **Unigram (n=1)**: Considers each word independently.\n  - **Bigram (n=2)**: Considers pairs of consecutive words.\n  - **Trigram (n=3)**: Considers triples of consecutive words.\n\n## How N-gram Models Work\n\n- **Example**: Let's consider a bigram model.\n- **Training Data**: \"I love machine learning. Machine learning is fun.\"\n- **Bigrams**: \n  - \"I love\"\n  - \"love machine\"\n  - \"machine learning\"\n  - \"learning Machine\"\n  - \"Machine learning\"\n  - \"learning is\"\n  - \"is fun\"\n\n- **Probability Calculation**:\n  - P(\"learning\" | \"machine\") = Count(\"machine learning\") / Count(\"machine\")\n\n\n## Example of N-gram Model in Action\n\n- **Sentence Completion**:\n  - Given the sequence \"machine learning\", predict the next word.\n  - Using the bigram model:\n    - P(\"is\" | \"learning\") = Count(\"learning is\") / Count(\"learning\")\n    - P(\"fun\" | \"learning\") = Count(\"learning fun\") / Count(\"learning\")\n\n- **Prediction**:\n  - If \"learning is\" appears more frequently than \"learning fun\" in the training data, the model predicts \"is\" as the next word.\n\n# Transformers\n\n\n## Transformers: High-Level Overview\n\nTransformers [@vaswani2017attention] are the foundation of modern NLP systems.\n\n**Key innovations:**\n\n- Use **attention mechanism** to process entire sequences in parallel\n- Revolutionized NLP and enabled LLMs (ChatGPT, BERT, GPT-4)\n- Scalable across GPUs for massive models\n\n## Transformer Architecture\n\n![](drawio/Transformer_Enc_Dec.png){fig-align=\"center\"}\n\n**Components:**\n\n- **Encoder**: Processes input text, creates rich representations\n- **Decoder**: Generates output text using encoder representations\n- **Attention**: Allows model to focus on relevant parts of input\n\n## The Attention Mechanism\n\nAttention allows models to understand which words are most relevant to each other.\n\n**Example sentence:**\n\n> \"The elephant didn't cross the river because **it** was tired.\"\n\n**Question:** What does \"it\" refer to?\n\n- Attention helps the model determine that **\"it\" = \"elephant\"** (not \"river\")\n- The model \"attends\" more strongly to \"elephant\" when processing \"it\"\n\n## How Attention Works (Simplified)\n\nFor each word in a sentence:\n\n1. **Calculate relevance scores** with all other words\n2. **Apply softmax** to get attention weights (sum to 1)\n3. **Compute weighted sum** of word representations\n4. Result: each word's representation includes context from relevant words\n\n**Multi-head attention**: Run multiple attention operations in parallel to capture different types of relationships\n\n## Attention Visualized\n\n:::: {.columns}\n::: {.column width=\"60%\"}\nDarker connections show stronger attention.\n\nWhen processing \"it\", the model attends most to \"elephant\".\n\nThis is how transformers handle long-range dependencies and ambiguity.\n:::\n::: {.column width=\"40%\"}\n![](drawio/VisualizedAttention.png)\n:::\n::::\n\n# Transformer Architectures\n\n## 3 Types of Transformer Models\n\n1. **Encoder-Decoder** – Sequence-to-sequence tasks (e.g., machine translation)\n   - Example: Original Transformer, T5\n\n2. **Encoder-Only** – Understanding and classification tasks\n   - Example: **BERT** (sentiment analysis, NER, classification)\n\n3. **Decoder-Only** – Text generation and completion\n   - Example: **GPT** (ChatGPT, text generation, AI assistants)\n\n## BERT: Encoder Model\n\n**BERT** (Bidirectional Encoder Representations from Transformers) [@devlin2019bert]\n\n**Key features:**\n\n- Reads text **bidirectionally** (considers context from both directions)\n- Pre-trained on masked language modeling (predict hidden words)\n- 340M parameters (BERT-Large), 110M (BERT-Base)\n- Fine-tuned for specific tasks with one additional layer\n\n**Common uses:** Text classification, NER, question answering, sentiment analysis\n\n## GPT: Decoder Model\n\n**GPT** (Generative Pre-trained Transformer) [@brown2020language]\n\n**Key features:**\n\n- Reads text **left-to-right** (autoregressive)\n- Predicts next token: $P(t_1, t_2, \\ldots, t_N) = P(t_1)\\prod_{n=2}^{N} P(t_n | t_1, \\ldots t_{n-1})$\n- GPT-3: 175 billion parameters\n- Excellent at text generation\n\n**Common uses:** Text completion, creative writing, chatbots, code generation\n\n## Transformer Applications\n\nTransformers enable powerful NLP applications:\n\n- **Machine Translation**: Translate between languages\n- **Text Summarization**: Generate concise summaries\n- **Question Answering**: Answer questions from context\n- **Named Entity Recognition**: Identify entities in text\n- **Sentiment Analysis**: Determine sentiment/emotion\n- **Text Generation**: Create human-like text\n\n# Named Entity Recognition (NER)\n\n## What is Named Entity Recognition?\n\n**Named Entity Recognition (NER)** is the task of identifying and classifying named entities in text into predefined categories.\n\n**Common entity types:**\n\n- **PERSON**: Names of people (e.g., \"Barack Obama\")\n- **ORGANIZATION**: Companies, agencies (e.g., \"Google\", \"FBI\")\n- **LOCATION**: Cities, countries, landmarks (e.g., \"Paris\", \"Mount Everest\")\n- **DATE**: Dates and times (e.g., \"January 1, 2024\")\n- **MONEY**: Monetary values (e.g., \"$100\")\n- **GPE**: Geopolitical entities (countries, cities, states)\n\n**Applications:** Information extraction, content classification, question answering, knowledge graphs\n\n## NER with NLTK\n\nNLTK (Natural Language Toolkit) provides basic NER capabilities using the `ne_chunk()` function.\n\n::: {#440263d0 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\nimport nltk\nfrom nltk import word_tokenize, pos_tag, ne_chunk\n\n# Download required NLTK data (run once)\n# nltk.download('punkt')\n# nltk.download('averaged_perceptron_tagger')\n# nltk.download('maxent_ne_chunker')\n# nltk.download('words')\n\ntext = \"Apple Inc. was founded by Steve Jobs in Cupertino, California. In 2024, the company is worth over $3 trillion dollars.\"\n\n# Tokenize, POS tag, then NER\ntokens = word_tokenize(text)\npos_tags = pos_tag(tokens)\nnamed_entities = ne_chunk(pos_tags)\n\nprint(named_entities)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n(S\n  (PERSON Apple/NNP)\n  (ORGANIZATION Inc./NNP)\n  was/VBD\n  founded/VBN\n  by/IN\n  (PERSON Steve/NNP Jobs/NNP)\n  in/IN\n  (GPE Cupertino/NNP)\n  ,/,\n  (GPE California/NNP)\n  ./.\n  In/IN\n  2024/CD\n  ,/,\n  the/DT\n  company/NN\n  is/VBZ\n  worth/JJ\n  over/IN\n  $/$\n  3/CD\n  trillion/CD\n  dollars/NNS\n  ./.)\n```\n:::\n:::\n\n\n## NLTK NER Results\n\nNLTK uses a **rule-based and statistical approach** trained on Penn Treebank corpus.\n\n**Advantages:**\n\n- Easy to use, no GPU required\n- Good for simple, well-formed text\n- Lightweight\n\n**Limitations:**\n\n- Lower accuracy than modern methods\n- Limited entity types\n- Requires POS tagging first\n- Poor with informal text or domain-specific entities\n\n## NER with spaCy\n\nspaCy is a modern, production-ready NLP library with excellent NER capabilities.\n\n::: {#45aa7ee9 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\nimport spacy\n\n# Load pre-trained model (run: python -m spacy download en_core_web_sm)\nnlp = spacy.load(\"en_core_web_sm\")\n\ntext = \"Apple Inc. was founded by Steve Jobs in Cupertino, California. In 2024, the company is worth over $3 trillion dollars.\"\n\n# Process text\ndoc = nlp(text)\n\n# Extract named entities\nprint(\"Entities found:\")\nfor ent in doc.ents:\n    print(f\"  {ent.text:20} -> {ent.label_:15} ({spacy.explain(ent.label_)})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nEntities found:\n  Apple Inc.           -> ORG             (Companies, agencies, institutions, etc.)\n  Steve Jobs           -> PERSON          (People, including fictional)\n  Cupertino            -> GPE             (Countries, cities, states)\n  California           -> GPE             (Countries, cities, states)\n  2024                 -> DATE            (Absolute or relative dates or periods)\n  over $3 trillion dollars -> MONEY           (Monetary values, including unit)\n```\n:::\n:::\n\n\n## spaCy NER Visualization\n\nspaCy provides built-in visualization with `displacy`:\n\n::: {#8e8c2ea3 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"false\"}\nfrom spacy import displacy\n\ntext = \"Apple Inc. was founded by Steve Jobs in Cupertino, California. In 2024, the company is worth over $3 trillion dollars.\"\ndoc = nlp(text)\n\n# Visualize entities\ndisplacy.render(doc, style=\"ent\", jupyter=True)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Apple Inc.\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n was founded by \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Steve Jobs\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n in \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Cupertino\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n, \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    California\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n. In \n<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    2024\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n</mark>\n, the company is worth \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    over $3 trillion dollars\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n</mark>\n.</div></span>\n```\n:::\n:::\n\n\n**spaCy advantages:**\n\n- Fast and accurate\n- Multiple pre-trained models (small, medium, large)\n- Easy visualization\n- Production-ready\n\n## NER with BERT\n\nBERT-based NER models achieve state-of-the-art results using Hugging Face transformers.\n\n::: {#034ed673 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"false\"}\nfrom transformers import pipeline\n\n# Load pre-trained BERT NER model\nner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n\ntext = \"Apple Inc. was founded by Steve Jobs in Cupertino, California. In 2024, the company is worth over $3 trillion dollars.\"\n\n# Perform NER\nentities = ner_pipeline(text)\n\n# Display results\nprint(\"BERT NER Results:\")\nfor entity in entities:\n    print(f\"  {entity['word']:20} -> {entity['entity_group']:10} (score: {entity['score']:.3f})\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBERT NER Results:\n  Apple Inc            -> ORG        (score: 0.999)\n  Steve Jobs           -> PER        (score: 0.989)\n  Cupertino            -> LOC        (score: 0.998)\n  California           -> LOC        (score: 1.000)\n```\n:::\n:::\n\n\n## BERT NER Details\n\n**How BERT NER works:**\n\n- Pre-trained BERT model fine-tuned on labeled NER datasets (e.g., CoNLL-2003)\n- Predicts entity label for each token\n- Aggregation combines sub-word tokens into complete entities\n\n**Advantages:**\n\n- State-of-the-art accuracy\n- Handles context and ambiguity well\n- Multiple pre-trained models available\n- Can be fine-tuned on custom data\n\n**Limitations:**\n\n- Requires GPU for fast inference\n- Larger model size\n- More complex setup\n\n## NER Tools Comparison\n\n| Feature | NLTK | spaCy | BERT (Transformers) |\n|---------|------|-------|---------------------|\n| **Accuracy** | Moderate | High | Very High |\n| **Speed** | Fast | Very Fast | Moderate (needs GPU) |\n| **Ease of Use** | Moderate | Easy | Moderate |\n| **GPU Required** | No | No | Recommended |\n| **Model Size** | Small | Small-Medium | Large |\n| **Customization** | Limited | Good | Excellent |\n| **Best For** | Simple tasks, learning | Production, most use cases | Highest accuracy, custom domains |\n\n**Recommendation:** Start with spaCy for most applications, use BERT when accuracy is critical.\n\n# Topic Modeling\n\n## What is Topic Modeling?\n\n**Topic modeling** is an unsupervised learning technique that discovers abstract \"topics\" in a collection of documents.\n\n**Key concepts:**\n\n- **Topic**: A distribution over words (e.g., \"sports\" → {game, team, player, score, ...})\n- **Document**: A mixture of topics\n- **Goal**: Automatically discover topics and their distributions\n\n**Applications:**\n\n- Content recommendation\n- Document organization\n- Trend analysis\n- Search and discovery\n- Summarization\n\n## Latent Dirichlet Allocation (LDA)\n\n**LDA** [@blei2003latent] is the most popular classical topic modeling algorithm.\n\n**How LDA works (simplified):**\n\n1. Assume each document is a mixture of topics\n2. Assume each topic is a mixture of words\n3. Use statistical inference to discover these mixtures\n\n**Key hyperparameters:**\n\n- `n_topics`: Number of topics to discover\n- `alpha`: Document-topic density (higher = more topics per document)\n- `beta`: Topic-word density (higher = more words per topic)\n\n## LDA with Scikit-Learn\n\n::: {#8c149591 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n# Load sample data (subset for speed)\ncategories = ['sci.space', 'rec.sport.baseball', 'comp.graphics']\nnewsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\ndocuments = newsgroups.data[:300]  # Use subset for demo\n\n# Convert to document-term matrix\nvectorizer = CountVectorizer(max_features=1000, stop_words='english', max_df=0.7)\ndoc_term_matrix = vectorizer.fit_transform(documents)\n\n# Train LDA model\nn_topics = 3\nlda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=20)\nlda_model.fit(doc_term_matrix)\n\nprint(f\"LDA model trained with {n_topics} topics\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLDA model trained with 3 topics\n```\n:::\n:::\n\n\n## LDA Topics Visualization\n\n::: {#be7f0980 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"false\"}\nimport numpy as np\n\n# Display top words for each topic\nfeature_names = vectorizer.get_feature_names_out()\nn_top_words = 10\n\nprint(\"\\nTop words per topic:\")\nfor topic_idx, topic in enumerate(lda_model.components_):\n    top_words_idx = topic.argsort()[-n_top_words:][::-1]\n    top_words = [feature_names[i] for i in top_words_idx]\n    print(f\"\\nTopic {topic_idx}: {', '.join(top_words)}\")\n\n# Show topic distribution for a sample document\ndoc_idx = 0\ntopic_dist = lda_model.transform(doc_term_matrix[doc_idx:doc_idx+1])[0]\nprint(f\"\\n\\nSample document topic distribution:\")\nfor i, prob in enumerate(topic_dist):\n    print(f\"  Topic {i}: {prob:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTop words per topic:\n\nTopic 0: launch, space, satellite, commercial, data, satellites, market, 000, orbit, year\n\nTopic 1: space, 03, 02, year, 04, 01, won, lost, team, 05\n\nTopic 2: like, think, don, just, know, does, good, need, time, program\n\n\nSample document topic distribution:\n  Topic 0: 0.012\n  Topic 1: 0.011\n  Topic 2: 0.977\n```\n:::\n:::\n\n\n## LDA Strengths and Limitations\n\n**Strengths:**\n\n- Interpretable topics (distributions over words)\n- Well-established, proven method\n- Fast training on moderate-sized corpora\n- Works well on homogeneous documents\n\n**Limitations:**\n\n- Bag-of-words assumption (loses word order, context)\n- Requires careful hyperparameter tuning\n- Struggles with short documents\n- Topics can be noisy or incoherent\n- Doesn't leverage modern pre-trained embeddings\n\n## BERTopic\n\n**BERTopic** [@grootendorst2022bertopic] is a modern topic modeling technique using transformer embeddings.\n\n**How BERTopic works:**\n\n1. **Embed** documents using BERT (contextual embeddings)\n2. **Reduce** dimensionality with UMAP\n3. **Cluster** documents with HDBSCAN\n4. **Extract** topic words using class-based TF-IDF (c-TF-IDF)\n\n**Advantages over LDA:**\n\n- Uses semantic embeddings (better captures meaning)\n- No need to specify number of topics upfront\n- Better with short documents\n- More coherent topics\n\n## BERTopic Implementation\n\n::: {#42b1297d .cell execution_count=10}\n``` {.python .cell-code code-fold=\"false\"}\nfrom bertopic import BERTopic\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Load data\ncategories = ['sci.space', 'rec.sport.baseball', 'comp.graphics']\nnewsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\ndocuments = newsgroups.data[:300]\n\n# Configure vectorizer to remove stop words\nvectorizer_model = CountVectorizer(stop_words=\"english\")\n\n# Train BERTopic model with proper configuration\ntopic_model = BERTopic(vectorizer_model=vectorizer_model, verbose=False, language=\"english\")\ntopics, probabilities = topic_model.fit_transform(documents)\n\nprint(f\"BERTopic discovered {len(set(topics))} topics (including -1 for outliers)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBERTopic discovered 3 topics (including -1 for outliers)\n```\n:::\n:::\n\n\n## BERTopic Results\n\n::: {#cb040b91 .cell execution_count=11}\n``` {.python .cell-code code-fold=\"false\"}\n# Show topic information\ntopic_info = topic_model.get_topic_info()\nprint(\"\\nTopic Overview:\")\nprint(topic_info.head(10))\n\n# Show top words for each topic\nprint(\"\\n\\nTop words per topic:\")\nfor topic_num in sorted(set(topics)):\n    if topic_num == -1:  # Skip outlier topic\n        continue\n    words = topic_model.get_topic(topic_num)\n    if words:\n        top_words = ', '.join([word for word, score in words[:10]])\n        print(f\"\\nTopic {topic_num}: {top_words}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTopic Overview:\n   Topic  Count                              Name  \\\n0     -1      7  -1_sherzer_methodology_keywords_   \n1      0    204  0_space_launch_program_satellite   \n2      1     89                1_won_lost_year_05   \n\n                                      Representation  \\\n0     [sherzer, methodology, keywords, , , , , , , ]   \n1  [space, launch, program, satellite, like, data...   \n2  [won, lost, year, 05, 04, game, team, 03, game...   \n\n                                 Representative_Docs  \n0  [\\n, Keywords: , \\n\\nSherzer Methodology!!!!!!...  \n1  [ETHER IMPLODES 2 EARTH CORE, IS GRAVITY!!!\\n\\...  \n2  [ \\nFor your information, Lankford is injured ...  \n\n\nTop words per topic:\n\nTopic 0: space, launch, program, satellite, like, data, dont, just, think, commercial\n\nTopic 1: won, lost, year, 05, 04, game, team, 03, games, league\n```\n:::\n:::\n\n\n## BERTopic Visualization\n\n```python\n#| code-fold: false\n# Visualize topics (interactive plot)\nfig = topic_model.visualize_topics()\nfig.show()\n\n# Visualize topic hierarchy\nfig_hierarchy = topic_model.visualize_hierarchy()\nfig_hierarchy.show()\n```\n\n## LDA vs BERTopic\n\n| Feature | LDA | BERTopic |\n|---------|-----|----------|\n| **Embeddings** | Bag-of-words (TF-IDF) | Contextual (BERT) |\n| **Num Topics** | Must specify | Auto-detected |\n| **Interpretability** | Good (probability distributions) | Excellent (semantic coherence) |\n| **Speed** | Fast | Slower (embeddings + clustering) |\n| **Short Documents** | Poor | Good |\n| **Computational Cost** | Low | High (needs embeddings) |\n| **Tunability** | Many hyperparameters | Fewer hyperparameters |\n| **Best For** | Large corpora, speed matters | Quality topics, semantic understanding |\n\n**Recommendation:** Use BERTopic for better quality topics when resources allow; use LDA for speed and large-scale applications.\n\n# Summary\n\n## NLP Tools and Applications Recap\n\n**What we covered:**\n\n1. **Text representation**: Sparse (TF-IDF) vs embeddings (Word2Vec vs BERT)\n2. **Language models**: N-grams → Transformers\n3. **Transformers**: Attention mechanism, BERT (encoder), GPT (decoder)\n4. **Named Entity Recognition**: NLTK → spaCy → BERT NER\n5. **Topic Modeling**: LDA → BERTopic\n\n**Key takeaways:**\n\n- Modern NLP relies heavily on transformers and contextual embeddings\n- Choose tools based on accuracy needs, resources, and task complexity\n- spaCy is excellent for production NER; BERT for highest accuracy\n- BERTopic produces better topics than LDA but requires more resources\n\n## References\n\n",
    "supporting": [
      "28-NLP_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}