{
  "hash": "95bafd13f95b9a84641f2f5a096c9d5f",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Clustering In Practice\n---\n\n...featuring $k$-means\n\n\nToday we'll do an extended example showing $k$-means clustering in practice and in the context of the python libraries\n__`scikit-learn.`__\n\n`scikit-learn` is the main python library for machine learning functions.\n\nOur goals are to learn:\n* How clustering is used in practice\n* Tools for evaluating the quality of a clustering\n* Tools for assigning meaning or labels to a cluster\n* Important visualizations\n* A little bit about feature extraction for text\n\n## Training wheels: Synthetic data\n\nGenerally, when learning about or developing a new unsupervised method, it's a good idea to try it out on a dataset in which you already know the \"right\" answer.\n\nOne way to do that is to generate synthetic data that has some known properties.\n\nAmong other things, `scikit-learn` contains tools for generating synthetic data for testing.\n\n\n. . .\n\n::: {#a959634c .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=1}\n``` {.python .cell-code}\nimport sklearn.datasets as sk_data\nX, y = sk_data.make_blobs(n_samples=100, centers=3, n_features=30,\n                          center_box=(-10.0, 10.0), random_state=0)\n```\n:::\n\n\nTo get a sense of the raw data we can inspect it.\n\nFor statistical visualization, a good library is `seaborn`, imported as `sns`.\n\n::: {#5d74e8f1 .cell slideshow='{\"slide_type\":\"-\"}' execution_count=2}\n``` {.python .cell-code}\nimport seaborn as sns\nsns.heatmap(X, xticklabels = False, yticklabels = False, linewidths = 0, cbar = False);\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-3-output-1.png){width=763 height=389}\n:::\n:::\n\n\nThat plot shows all the data.   \n\nAs usual, each row is a data item and the columns correspond to features (which are simply coordinates here).\n\nGeometrically, these points live in a __30 dimensional__ space, so we cannot directly visualize their geometry.  \n\nThis is a __big problem__ that you will run into time and again!\n\nWe will discuss methods for visualizing high dimensional data later on.\n\nFor now, we will use a method that can turn a set of pairwise distances into an approximate 2-D representation __in some cases.__\n\nSo let's compute the pairwise distances for visualization purposes.\n\nWe can compute all pairwise distances in a single step using a `scikit-learn` function:\n\n\n. . .\n\n::: {#001c504b .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=3}\n``` {.python .cell-code}\nimport sklearn.metrics as metrics\neuclidean_dists = metrics.euclidean_distances(X)\neuclidean_dists\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\narray([[ 0.        , 47.73797008, 45.18787978, ..., 47.87535624,\n        49.64694402, 45.58307694],\n       [47.73797008,  0.        , 43.66760596, ...,  7.3768511 ,\n         7.36794305, 43.51069074],\n       [45.18787978, 43.66760596,  0.        , ..., 42.55609472,\n        43.80829605,  9.31642449],\n       ...,\n       [47.87535624,  7.3768511 , 42.55609472, ...,  0.        ,\n         8.19377462, 41.81523421],\n       [49.64694402,  7.36794305, 43.80829605, ...,  8.19377462,\n         0.        , 43.41205895],\n       [45.58307694, 43.51069074,  9.31642449, ..., 41.81523421,\n        43.41205895,  0.        ]])\n```\n:::\n:::\n\n\n### Visualizing with Multidimensional Scaling\n\nThe idea behind Multidimensional Scaling (MDS) is: \n\n* given a distance (or dissimilarity) matrix, \n* find a set of coordinates for the points that approximates those distances as well as possible.\n\nUsually we are looking for points in 2D or maybe 3D for visualization purposes.\n\nThe algorithm works using an algorithm that starts with random positions, and then moves points in a way that reduces the disparity between true distance and euclidean distance.\n\nNote that there are many ways that this can fail!  \n\n* Perhaps the dissimilarities are not well modeled as euclidean distances\n* It may be necessary to use more than 2 dimensions to capture any clustering via euclidean distances\n\n\n. . .\n\n::: {#e3027efd .cell slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=4}\n``` {.python .cell-code}\nimport sklearn.manifold\nimport matplotlib.pyplot as plt\nmds = sklearn.manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=0,\n                   dissimilarity = \"precomputed\", n_jobs = 1)\nfit = mds.fit(euclidean_dists)\npos = fit.embedding_\nplt.scatter(pos[:, 0], pos[:, 1], s=8)\nplt.axis('square');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-5-output-1.png){width=426 height=411}\n:::\n:::\n\n\nSo we can see that, although the data lives in 30 dimensions, we can get a sense of how the points are clustered by approximately placing the points into two dimensions.\n\nA second way to visualize the data is by using a heatmap on the set of pairwise distances.\n\n::: {#fc9eb29e .cell slideshow='{\"slide_type\":\"-\"}' execution_count=5}\n``` {.python .cell-code}\nsns.heatmap(euclidean_dists, xticklabels=False, yticklabels=False, linewidths=0, \n            square=True );\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-6-output-1.png){width=471 height=397}\n:::\n:::\n\n\n## Applying  $k$-Means \n\nThe Python package `scikit-learn` has a huge set of tools for unsupervised learning generally, and clustering specifically.  \n\nThese are in __`sklearn.cluster.`__\n\nThere are 3 functions in all the clustering classes, \n\n* __`fit()`__, \n* __`predict()`__, and \n* __`fit_predict()`__. \n\n`fit()` builds the model from the training data (e.g. for $k$-means, it finds the\n            centroids), \n            \n`predict()` assigns labels to the data after building\n            the model, and\n            \n`fit_predict()` does both in a single step.\n\n\n. . .\n\n::: {#13224f45 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(init = 'k-means++', n_clusters = 3, n_init = 100)\nkmeans.fit_predict(X)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\narray([0, 2, 1, 1, 2, 0, 2, 2, 0, 2, 1, 0, 1, 0, 2, 1, 1, 0, 1, 2, 1, 2,\n       1, 0, 2, 2, 0, 1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 2, 1, 2, 2, 2, 0, 1,\n       0, 1, 0, 2, 0, 1, 1, 0, 0, 0, 0, 1, 0, 2, 2, 1, 0, 1, 2, 0, 0, 2,\n       1, 0, 0, 2, 2, 2, 0, 0, 1, 0, 2, 0, 2, 0, 2, 2, 2, 0, 1, 1, 2, 0,\n       0, 1, 0, 2, 2, 1, 0, 1, 1, 2, 2, 1], dtype=int32)\n```\n:::\n:::\n\n\nAll the tools in `scikit-learn` are implemented as python objects.\n\nThus, the general sequence for using a tool from `scikit-learn` is:\n\n* Create the object, probably with some hyperparameter settings or intialization,\n* Run the method, generally by using the `fit()` function, and\n* Examine the results, which are generally property variables of the object.\n\n\n. . .\n\n::: {#98b27e8f .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=7}\n``` {.python .cell-code}\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_\nerror = kmeans.inertia_\n```\n:::\n\n\n\n. . .\n\n::: {#b18b92d1 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=8}\n``` {.python .cell-code}\nprint(f'The total error of the clustering is: {error:0.1f}.')\nprint('\\nCluster labels:')\nprint(labels)\nprint('\\nCluster centroids:')\nprint(centroids)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe total error of the clustering is: 2733.8.\n\nCluster labels:\n[0 2 1 1 2 0 2 2 0 2 1 0 1 0 2 1 1 0 1 2 1 2 1 0 2 2 0 1 1 1 1 0 1 0 1 2 1\n 2 1 2 2 2 0 1 0 1 0 2 0 1 1 0 0 0 0 1 0 2 2 1 0 1 2 0 0 2 1 0 0 2 2 2 0 0\n 1 0 2 0 2 0 2 2 2 0 1 1 2 0 0 1 0 2 2 1 0 1 1 2 2 1]\n\nCluster centroids:\n[[ 0.88697885  4.29142902  1.93200132  1.10877989 -1.55994342  2.80616392\n  -1.11495818  7.74595341  8.92512875 -2.29656298  6.09588722  0.47062896\n   1.36408008  8.63168509 -8.54512921 -8.59161818 -9.64308952  6.92270491\n   5.65321496  7.29061444  9.58822315  5.79602014 -0.84970449  5.46127493\n  -7.77730238  2.75092191 -7.17026663  9.07475984  0.04245798 -1.98719465]\n [-4.7833887   5.32946939 -0.87141823  1.38900567 -9.59956915  2.35207348\n   2.22988468  2.03394692  8.9797878   3.67857655 -2.67618716 -1.17595897\n   3.76433199 -8.46317271  3.28114395  3.73803392 -5.73436869 -7.0844462\n  -3.75643598 -3.07904369  1.36974653 -0.95918462  9.91135428 -8.17722281\n  -5.8656831  -6.76869078  3.12196673 -4.85745245 -0.70449349 -4.94582258]\n [-7.0489904  -7.92501873  2.89710462 -7.17088692 -6.01151677 -2.66405834\n   6.43970052 -8.20341647  6.54146052 -7.92978843  9.56983319 -0.86327902\n   9.25897119  1.73061823  4.84528928 -9.26418246 -4.54021612 -7.47784575\n  -4.15060719 -7.85665458 -3.76688414 -1.6692291  -8.78048843  3.78904162\n   1.24247168 -4.73618733  0.27327032 -7.93180624  1.59974866  8.78601576]]\n```\n:::\n:::\n\n\n### Visualizing the Results of Clustering\n\nLet's visualize the results.  We'll do that by reordering the data items according to their cluster.\n\n\n. . .\n\n::: {#6c776d2b .cell slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=9}\n``` {.python .cell-code}\nimport numpy as np\nidx = np.argsort(labels)\nrX = X[idx, :]\nsns.heatmap(rX, xticklabels = False, yticklabels = False, linewidths = 0);\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-10-output-1.png){width=708 height=389}\n:::\n:::\n\n\n\n. . .\n\n::: {#1b7233bc .cell slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=10}\n``` {.python .cell-code}\nrearranged_dists = euclidean_dists[idx,:][:,idx]\nsns.heatmap(rearranged_dists, xticklabels = False, yticklabels = False, linewidths = 0, square = True);\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-11-output-1.png){width=471 height=397}\n:::\n:::\n\n\n## Cluster Evaluation\n\nHow do we know whether the clusters we get represent \"real\" structure in our data?\n\nConsider this dataset, which we have clustered using $k$-means:\n\n::: {#2341697a .cell tags='[\"hide-input\"]' execution_count=11}\n``` {.python .cell-code}\nimport pandas as pd\nunif_X = np.random.default_rng().uniform(0, 1, 100)\nunif_Y = np.random.default_rng().uniform(0, 1, 100)\ndf = pd.DataFrame(np.column_stack([unif_X, unif_Y]), columns = ['X', 'Y'])\nkmeans = KMeans(init = 'k-means++', n_clusters = 3, n_init = 100)\ndf['label'] = kmeans.fit_predict(df[['X', 'Y']])\ndf.plot('X', 'Y', kind = 'scatter', c = 'label', colormap='viridis', colorbar = False)\nplt.axis('equal')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-12-output-1.png){width=763 height=389}\n:::\n:::\n\n\nIn fact, this dataset was generated using uniform random numbers for the coordinates.\n\nSo ... it truly has __no__ clusters.\n\nThe point is: any clustering algorithm will always output some \"clustering\" of the data.\n\nThe question is, does the clustering reflect __real__ structure?\n\nGenerally we encounter two problems:\n* Are there \"real\" clusters in the data?\n* if so, __how many__ clusters are there?\n\nThere is often no definitive answer to either of these questions.\n\nYou will often need to use your judgment in answering them.\n\n### Rand Index\n\nOne tool we may be able to use in some settings is __external__ information about the data.\n\nIn particular, we may have knowledge from some other source about the nature of the clusters in the data.\n\nIn that case, what we need is a way to compare a proposed clustering with some externally-known, \"ground truth\" clustering.\n\nThe Rand Index is a __similarity measure__ for __clusterings.__   We can use it to compare two clusterings.\n\nOr, if we are testing an algorithm on data for which we know ground truth, we can use it to assess the algorithm's accuracy.\n\n__Defining the Rand Index.__\n\nEach item in our dataset is assumed to have two labelings, one for each clustering.\n\nFor example, we could have a ground truth label assignment $T$ and a comparison clustering $C$. \n\n\n. . .\n\n::: {#bd81c798 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=13}\n``` {.python .cell-code}\nfigs, axs = plt.subplots(1, 2, figsize = (12, 5))\ndf_rand_gt.plot('X', 'Y', kind = 'scatter', c = 'label', colormap='viridis', ax = axs[0],\n                   colorbar = False)\naxs[0].set_title('Ground Truth (T)')\naxs[0].set_axis_off()\ndf_rand_clust.plot('X', 'Y', kind = 'scatter', c = 'label', colormap='viridis', ax = axs[1],\n                  colorbar = False)\naxs[1].set_title('Clustering (C)')\naxs[1].set_axis_off();\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-13-output-1.png){width=912 height=409}\n:::\n:::\n\n\nIntuitively, the idea behind Rand Index is to consider __pairs__ of points, and ask whether pairs that fall into the same cluster in $T$ also fall into the same cluster in $C$.\n\nSpecifically:\n\nLet $a$ be the number of pairs that have the same label in $T$ and the same label in $C$. \n\nLet $b$ be: the number of pairs that have different labels in $T$ and different labels in $C$. \n\nThen the Rand Index is: \n\n$$ \\text{RI}(T,C) = \\frac{a+b}{n \\choose 2} $$\n\nHow do we know whether a particular Rand Index (RI) score is significant?\n\nWe might compare it to the RI for a __random__ assignment of points to labels.\n\nThis leads to the __Adjusted Rand Index.__\n\n__Definition of Adjusted Rand Index.__\n\nTo \"calibrate\" the Rand Index this way, we use the expected Rand Index of random labelings, denoted $E[\\text{RI}]$.   \n\nThe Expected Rand Index considers $C$ to be a clustering that has the same cluster sizes as $T$, but labels are assigned at random.\n\nUsing that, we define the adjusted Rand index as a simple __rescaling__ of RI:\n\n\\begin{equation}\n\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}\n\\end{equation}\n\nThe computation of the $E[\\text{RI}]$ and the $\\max(\\text{RI})$ are simple combinatorics (we'll omit the derivation).\n\n__Example.__\n\nLet's consider again our 3-cluster dataset with known labels `y`.\n\n\n. . .\n\n::: {#3642d8cb .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=14}\n``` {.python .cell-code}\nsns.heatmap(rX, xticklabels = False, yticklabels = False, linewidths = 0);\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-14-output-1.png){width=708 height=389}\n:::\n:::\n\n\nHere is the adjusted Rand Index, when using $k$-means to cluster this dataset for 1 to 10 clusters:\n\n::: {#1da20fa0 .cell hide_input='false' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=15}\n``` {.python .cell-code}\ndef ri_evaluate_clusters(X,max_clusters,ground_truth):\n    ri = np.zeros(max_clusters+1)\n    ri[0] = 0;\n    for k in range(1,max_clusters+1):\n        kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10)\n        kmeans.fit_predict(X)\n        ri[k] = metrics.adjusted_rand_score(kmeans.labels_,ground_truth)\n    return ri\n    \nri = ri_evaluate_clusters(X, 10, y)\nplt.plot(range(1,len(ri)), ri[1:], 'o-')\nplt.xlabel('Number of clusters')\nplt.title('$k$-means Clustering Compared to Known Labels')\nplt.ylabel('Adjusted Rand Index');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-15-output-1.png){width=812 height=450}\n:::\n:::\n\n\n## Deciding on the Number of Clusters\n\nThe second question we face in evaluating a clustering is how many clusters are present.\n\nIn practice, to use $k$-means or most other clustering methods, one must choose $k$, the number of clusters, via some process.\n\n### Inspecting Clustering Error\n\nThe first thing you might do is to look at the $k$-means objective function  and see if it levels off after a certain point.\n\nRecall that the $k$-means objective can be considered the clustering \"error\".\n\nIf the error stops going down, that would suggest that the clustering is not improving as the number of clusters is increased.\n\n\n. . .\n\n::: {#f0372a34 .cell slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=16}\n``` {.python .cell-code}\nerror = np.zeros(11)\nfor k in range(1,11):\n    kmeans = KMeans(init='k-means++', n_clusters = k, n_init = 10)\n    kmeans.fit_predict(X)\n    error[k] = kmeans.inertia_\n```\n:::\n\n\nFor our synthetic data, here is the $k$-means objective, as a function of $k$:\n\n\n. . .\n\n::: {#d05f2bf4 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=17}\n``` {.python .cell-code}\nplt.plot(range(1, len(error)), error[1:], 'o-')\nplt.xlabel('Number of clusters')\nplt.title(r'$k$-means clustering performance of synthetic data')\nplt.ylabel('Error');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-17-output-1.png){width=833 height=450}\n:::\n:::\n\n\n__Warning__: This synthetic data is not at all typical.   You will almost never see such a sharp change in the error function as we see here.\n\nLet's create a function for later use.\n\n### Silhouette Coefficient\n\nOf course, normally, the ground truth labels are not known.\n\nIn that case, evaluation must be performed using the model itself. |\n\nRecall our definition of clustering: \n\n> a grouping of data objects, such that the objects within a group are similar (or near) to one another and dissimilar (or far) from the objects in other groups.\n\nThis suggests a metric that could evaluate a clustering: comparing the distances between points within a cluster, to the distances between points in different clusters.\n\nThe Silhouette Coefficient is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with \"better defined\" clusters. \n\n(__`sklearn.metrics.silhouette_score`__)\n\nLet $a$ be the mean distance between a data point and all other points in the same cluster.\n\nLet $b$ be the mean distance between a data point and all other points in the next nearest cluster. \n\nThen the \n**Silhouette Coefficient** for a clustering is:\n\n$$ \ns = \\frac{b-a}{\\max(a, b)} \n$$\n\n\n. . .\n\n::: {#13ad7059 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=19}\n``` {.python .cell-code}\nsc = metrics.silhouette_score(X, labels, metric='euclidean')\nprint(sc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.8319348841402534\n```\n:::\n:::\n\n\n\n. . .\n\n::: {#717363ea .cell hide_input='false' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=20}\n``` {.python .cell-code}\ndef sc_evaluate_clusters(X, max_clusters, n_init, seed):\n    s = np.zeros(max_clusters+1)\n    s[0] = 0;\n    s[1] = 0;\n    for k in range(2, max_clusters+1):\n        kmeans = KMeans(init='k-means++', n_clusters = k, n_init = n_init, random_state = seed)\n        kmeans.fit_predict(X)\n        s[k] = metrics.silhouette_score(X, kmeans.labels_, metric = 'euclidean')\n    return s\n\ns = sc_evaluate_clusters(X, 10, 10, 1)\nplt.plot(range(2, len(s)), s[2:], 'o-')\nplt.xlabel('Number of Clusters')\nplt.title('$k$-means clustering performance on synthetic data')\nplt.ylabel('Silhouette Score');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-19-output-1.png){width=812 height=450}\n:::\n:::\n\n\nAgain, these results are more perfect than typical. \n\nBut the general idea is to look for a local maximum in the Silhouette Coefficient as the potential number of clusters.\n\n## Taking the Training Wheels Off: Real Data\n\nAs a \"real world\" example, we'll use the \"20 Newsgroup\" data provided as example data in sklearn.\n\n(http://scikit-learn.org/stable/datasets/twenty_newsgroups.html).\n\n::: {#c3d64e81 .cell execution_count=21}\n``` {.python .cell-code}\nfrom sklearn.datasets import fetch_20newsgroups\n\n\"\"\"\ncategories = [\n 'comp.windows.x',\n 'misc.forsale',\n 'rec.autos',\n 'rec.motorcycles',\n 'rec.sport.baseball',\n 'rec.sport.hockey',\n 'talk.religion.misc',\n 'comp.graphics',\n 'sci.space',\n 'rec.autos',\n 'rec.sport.baseball'\n]\n\"\"\"\ncategories = ['comp.os.ms-windows.misc', 'sci.space', 'rec.sport.baseball']\nnews_data = fetch_20newsgroups(subset = 'train', categories = categories)\nprint(news_data.target, len(news_data.target))\nprint(news_data.target_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[2 0 0 ... 2 1 2] 1781\n['comp.os.ms-windows.misc', 'rec.sport.baseball', 'sci.space']\n```\n:::\n:::\n\n\n\n. . .\n\n::: {#87ac12ef .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=22}\n``` {.python .cell-code}\nnews_data.data[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\n'From: aws@iti.org (Allen W. Sherzer)\\nSubject: Re: DC-X update???\\nOrganization: Evil Geniuses for a Better Tomorrow\\nLines: 122\\n\\nIn article <ugo62B8w165w@angus.mi.org> dragon@angus.mi.org writes:\\n\\n>Exactly when will the hover test be done, \\n\\nEarly to mid June.\\n\\n>and will any of the TV\\n>networks carry it.  I really want to see that...\\n\\nIf they think the public wants to see it they will carry it. Why not\\nwrite them and ask? You can reach them at:\\n\\n\\n                          F: NATIONAL NEWS MEDIA\\n\\n\\nABC \"World News Tonight\"                 \"Face the Nation\"\\n7 West 66th Street                       CBS News\\nNew York, NY 10023                       2020 M Street, NW\\n212/887-4040                             Washington, DC 20036\\n                                         202/457-4321\\n\\nAssociated Press                         \"Good Morning America\"\\n50 Rockefeller Plaza                     ABC News\\nNew York, NY 10020                       1965 Broadway\\nNational Desk (212/621-1600)             New York, NY 10023\\nForeign Desk (212/621-1663)              212/496-4800\\nWashington Bureau (202/828-6400)\\n                                         Larry King Live TV\\n\"CBS Evening News\"                       CNN\\n524 W. 57th Street                       111 Massachusetts Avenue, NW\\nNew York, NY 10019                       Washington, DC 20001\\n212/975-3693                             202/898-7900\\n\\n\"CBS This Morning\"                       Larry King Show--Radio\\n524 W. 57th Street                       Mutual Broadcasting\\nNew York, NY 10019                       1755 So. Jefferson Davis Highway\\n212/975-2824                             Arlington, VA 22202\\n                                         703/685-2175\\n\"Christian Science Monitor\"\\nCSM Publishing Society                   \"Los Angeles Times\"\\nOne Norway Street                        Times-Mirror Square\\nBoston, MA 02115                         Los Angeles, CA 90053\\n800/225-7090                             800/528-4637\\n\\nCNN                                      \"MacNeil/Lehrer NewsHour\"\\nOne CNN Center                           P.O. Box 2626\\nBox 105366                               Washington, DC 20013\\nAtlanta, GA 30348                        703/998-2870\\n404/827-1500\\n                                         \"MacNeil/Lehrer NewsHour\"\\nCNN                                      WNET-TV\\nWashington Bureau                        356 W. 58th Street\\n111 Massachusetts Avenue, NW             New York, NY 10019\\nWashington, DC 20001                     212/560-3113\\n202/898-7900\\n\\n\"Crossfire\"                              NBC News\\nCNN                                      4001 Nebraska Avenue, NW\\n111 Massachusetts Avenue, NW             Washington, DC 20036\\nWashington, DC 20001                     202/885-4200\\n202/898-7951                             202/362-2009 (fax)\\n\\n\"Morning Edition/All Things Considered\"  \\nNational Public Radio                    \\n2025 M Street, NW                        \\nWashington, DC 20036                     \\n202/822-2000                             \\n\\nUnited Press International\\n1400 Eye Street, NW\\nWashington, DC 20006\\n202/898-8000\\n\\n\"New York Times\"                         \"U.S. News & World Report\"\\n229 W. 43rd Street                       2400 N Street, NW\\nNew York, NY 10036                       Washington, DC 20037\\n212/556-1234                             202/955-2000\\n212/556-7415\\n\\n\"New York Times\"                         \"USA Today\"\\nWashington Bureau                        1000 Wilson Boulevard\\n1627 Eye Street, NW, 7th Floor           Arlington, VA 22229\\nWashington, DC 20006                     703/276-3400\\n202/862-0300\\n\\n\"Newsweek\"                               \"Wall Street Journal\"\\n444 Madison Avenue                       200 Liberty Street\\nNew York, NY 10022                       New York, NY 10281\\n212/350-4000                             212/416-2000\\n\\n\"Nightline\"                              \"Washington Post\"\\nABC News                                 1150 15th Street, NW\\n47 W. 66th Street                        Washington, DC 20071\\nNew York, NY 10023                       202/344-6000\\n212/887-4995\\n\\n\"Nightline\"                              \"Washington Week In Review\"\\nTed Koppel                               WETA-TV\\nABC News                                 P.O. Box 2626\\n1717 DeSales, NW                         Washington, DC 20013\\nWashington, DC 20036                     703/998-2626\\n202/887-7364\\n\\n\"This Week With David Brinkley\"\\nABC News\\n1717 DeSales, NW\\nWashington, DC 20036\\n202/887-7777\\n\\n\"Time\" magazine\\nTime Warner, Inc.\\nTime & Life Building\\nRockefeller Center\\nNew York, NY 10020\\n212/522-1212\\n\\n-- \\n+---------------------------------------------------------------------------+\\n| Lady Astor:   \"Sir, if you were my husband I would poison your coffee!\"   |\\n| W. Churchill: \"Madam, if you were my wife, I would drink it.\"             |\\n+----------------------57 DAYS TO FIRST FLIGHT OF DCX-----------------------+\\n'\n```\n:::\n:::\n\n\n### Feature Extraction\n\nWe've discussed a bit the challenges of feature engineering.   \n\nOne of the most basic issues concerns how to encode categorical or text data in a form usable by algorithms that expect numeric input.\n\nThe starting point is to note that one can encode a set using a binary vector with one component for each potential set member.  \n\nThe so-called _bag of words_ encoding for a document is to treat the document as a **multi**set of words.\n\nThat is, we simply count how many times each word occurs.   It is a \"bag\" because all the order of the words in the document is lost.\n\nSurprisingly, we can still tell a lot about the document even without knowing its word ordering.\n\nCounting the number of times each word occurs in a document yields a vector of __term frequencies.__\n\nHowever, simply using the \"bag of words\" directly has a number of drawbacks.   First of all, large documents will have more words than small documents.   \n\nHence it often makes sense to normalize the frequency vectors.\n\n$\\ell_1$ or $\\ell_2$ normalization are common.\n\nNext, as noted in __scikit-learn__:\n\n>In a large text corpus, some words will be very [frequent] (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. \n\n>If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.\n\n\n>In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform.\n\n>__Tf__ means __term-frequency__ while __tf–idf__ means __term-frequency times inverse document-frequency.__\n\n>This is a originally a term weighting scheme developed for information retrieval (as a ranking function for search engines results), that has also found good use in document classification and clustering.\n\nThe idea is that rare words are more informative than common words.  \n\n(This has connections to information theory).\n\nHence, the definition of tf-idf is as follows.\n\nFirst:\n\n$$\\text{tf}(t,d) = \\text{Number of times term }t \\text{ occurs in document } d$$\n\nNext, if $N$ is the total number of documents in the corpus $D$ then:\n\n$$\\text{idf}(t,D)=\\frac{N}{|\\{d\\in D : t\\in d \\}|}$$\n\nwhere the denominator is the number of documents in which the term $t$ appears.\n\nAnd finally:\n\n$$\\text{tf-idf}(t,d)=\\text{tf}(t,d)\\times \\text{idf}(t,D)$$\n\n\n. . .\n\n::: {#de31df9e .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=23}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words = 'english', min_df = 4, max_df = 0.8)\ndata = vectorizer.fit_transform(news_data.data)\n```\n:::\n\n\n### Getting to know the Data\n\n\n. . .\n\n::: {#6164e19b .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=24}\n``` {.python .cell-code}\nprint(type(data), data.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'scipy.sparse._csr.csr_matrix'> (1781, 9409)\n```\n:::\n:::\n\n\n\n. . .\n\n::: {#5dfef1ed .cell slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=25}\n``` {.python .cell-code}\nfig, ax1 = plt.subplots(1,1,figsize=(15,10))\ndum = sns.heatmap(data[1:100,1:200].todense(), xticklabels=False, yticklabels=False, \n            linewidths=0, cbar=False, ax=ax1)\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-24-output-1.png){width=1135 height=758}\n:::\n:::\n\n\n\n. . .\n\n::: {#44eb9147 .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=26}\n``` {.python .cell-code}\nprint(news_data.target)\nprint(news_data.target_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[2 0 0 ... 2 1 2]\n['comp.os.ms-windows.misc', 'rec.sport.baseball', 'sci.space']\n```\n:::\n:::\n\n\n### Selecting the Number of Clusters\n\n\n. . .\n\n::: {#e2749201 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=27}\n``` {.python .cell-code}\nerror = evaluate_clusters(data, 10)\nplt.plot(range(1, len(error)), error[1:])\nplt.title('$k$-means Clustering Performance on Newsgroup Articles')\nplt.xlabel('Number of clusters')\nplt.ylabel('Error');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-26-output-1.png){width=825 height=450}\n:::\n:::\n\n\n\n. . .\n\n::: {#ce16b848 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=28}\n``` {.python .cell-code}\nri = ri_evaluate_clusters(data, 10, news_data.target)\nplt.plot(range(1, len(ri)), ri[1:], 'o-')\nplt.xlabel('Number of clusters')\nplt.title('$k$-means Clustering Compared to Known Labels\\nNewsgroup Articles')\nplt.ylabel('Adjusted Rand Index');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-27-output-1.png){width=812 height=469}\n:::\n:::\n\n\n\n. . .\n\n::: {#ce81d95b .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=29}\n``` {.python .cell-code}\ns = sc_evaluate_clusters(data, 10, 100, 3)\nplt.plot(range(2, len(s)), s[2:], 'o-')\nplt.xlabel('Number of Clusters')\nplt.title('$k$-means clustering performance on Newsgroup Articles')\nplt.ylabel('Silhouette Score');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-28-output-1.png){width=829 height=450}\n:::\n:::\n\n\n### Looking into the clusters\n\n\n. . .\n\n::: {#934b28fc .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=30}\n``` {.python .cell-code}\nk = 4\nkmeans = KMeans(n_clusters = k, init = 'k-means++', max_iter = 100, n_init = 25, random_state = 3)\nkmeans.fit_predict(data)\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\narray([3, 2, 2, ..., 0, 3, 0], dtype=int32)\n```\n:::\n:::\n\n\n\n. . .\n\n::: {#46a325ab .cell slideshow='{\"slide_type\":\"fragment\"}' execution_count=31}\n``` {.python .cell-code}\nprint('Top terms per cluster:')\nasc_order_centroids = kmeans.cluster_centers_.argsort()#[:, ::-1]\norder_centroids = asc_order_centroids[:,::-1]\nterms = vectorizer.get_feature_names_out()\nfor i in range(k):\n    print(f'Cluster {i}:')\n    for ind in order_centroids[i, :10]:\n        print(f' {terms[ind]}')\n    print('')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTop terms per cluster:\nCluster 0:\n henry\n alaska\n toronto\n edu\n zoo\n aurora\n nsmca\n spencer\n umd\n space\n\nCluster 1:\n space\n nasa\n gov\n access\n com\n digex\n pat\n edu\n orbit\n jpl\n\nCluster 2:\n windows\n edu\n file\n dos\n com\n files\n driver\n use\n card\n drivers\n\nCluster 3:\n edu\n com\n baseball\n year\n team\n game\n writes\n article\n cs\n university\n\n```\n:::\n:::\n\n\n\n. . .\n\n::: {#9ca234d0 .cell slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=32}\n``` {.python .cell-code}\neuclidean_dists = metrics.euclidean_distances(data)\nlabels = kmeans.labels_\nidx = np.argsort(labels)\nclustered_dists = euclidean_dists[idx][:,idx]\nfig, ax1 = plt.subplots(1,1,figsize=(15,15))\ndum = sns.heatmap(clustered_dists, xticklabels=False, yticklabels=False, linewidths=0, square=True,cbar=False, ax=ax1)\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-31-output-1.png){width=1128 height=1128}\n:::\n:::\n\n\nLet's visualize with MDS.   Note that MDS is a slow algorithm and we can't do all 1700+ data points quickly, so we will take a random sample.\n\n\n. . .\n\n::: {#1602e846 .cell slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=33}\n``` {.python .cell-code}\nimport random\nn_items = euclidean_dists.shape[0]\nsubset = random.sample(range(n_items),500)\n\nfit = mds.fit(euclidean_dists[subset][:,subset])\npos = fit.embedding_\n```\n:::\n\n\n::: {#7362c5b9 .cell execution_count=34}\n``` {.python .cell-code}\nlabels\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\narray([3, 2, 2, ..., 0, 3, 0], dtype=int32)\n```\n:::\n:::\n\n\n\n. . .\n\n::: {#92769e5f .cell slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=35}\n``` {.python .cell-code}\ncols = [['y', 'b', 'g', 'r', 'c'][l] for l in labels[subset]]\nplt.scatter(pos[:, 0], pos[:, 1], s = 12, c = cols)\nplt.title('MDS Embedding of Newsgroup Articles');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-34-output-1.png){width=805 height=431}\n:::\n:::\n\n\n",
    "supporting": [
      "07-Clustering-II-in-practice_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}