{
  "hash": "66a5a6103a4c0b737db8db83523dbdea",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Soft Clustering with Gaussian Mixture Models\njupyter: python3\nfig-align: center\n---\n\n## From Hard to Soft Clustering\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/09-Clustering-IV-GMM-EM.ipynb)\n\n\n\n<!--\n![](figs/L09-MultivariateNormal.png){fig-align=\"center\" width=\"60%\"}\n-->\n\n\n__So far,__ we have seen how to cluster objects using $k$-means: \n\n1. Start with an initial set of cluster centers,\n1. Assign each object to its closest cluster center \n1. Recompute the centers of the new clusters\n1. Repeat 2 $\\rightarrow$ 3 until convergence\n\nIn $k$-means clustering every object is assigned to a **single** cluster. \n\nThis is called __hard__ assignment.\n\nHowever, there may be cases where we either __cannot__ use hard assignments or we do not __want__ to do it.\n\nIn particular, we might have reason to believe that the best description of the data is a set of __overlapping__ clusters.\n\n# Overlapping Clusters\n\n## Income Level Example\n\nConsider a population in terms of a simple __binary__ income level: higher or lower than some threshold.\n\nHow might we model such a population in terms of the single feature __age__?\n\n## Income Level Sampling\n\nLet's sample 20,000 _above income threshold_ individuals and 20,000 _below income threshold_ individuals. \n\nFrom this sample we get have the following histograms.\n\n::: {#856ead94 .cell execution_count=3}\n``` {.python .cell-code}\n# original inspiration for this example from\n# https://www.cs.cmu.edu/~./awm/tutorials/gmm14.pdf\nfrom scipy.stats import multivariate_normal\nnp.random.seed(4)\ndf = pd.DataFrame(multivariate_normal.rvs(mean = np.array([37, 45]), cov = np.diag([196, 121]), size = 20000),\n                   columns = ['below', 'above'])\ndf.hist(bins = range(80), sharex = True, sharey=True, figsize=(12, 2))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-3-output-1.png){width=947 height=209 fig-align='center'}\n:::\n:::\n\n\nWe find that ages of the below income threshold set have mean $\\mu=37$ with standard deviation $\\sigma=14$, while the ages of the above income threshold set have mean $\\mu=45$ with standard deviation $\\sigma=11$.\n\n## Income Level by Age\n\nWe can plot gaussian distributions for the two income levels.\n\n::: {#a2ee061b .cell execution_count=4}\n``` {.python .cell-code}\nfrom scipy.stats import norm\nplt.figure(figsize=(7, 4.5))\nx = np.linspace(norm.ppf(0.001, loc = 37, scale = 14), norm.ppf(0.999, loc = 37, scale = 14), 100)\nplt.plot(x, norm.pdf(x, loc = 37, scale = 14),'b-', lw = 5, alpha = 0.6, label = 'below')\nx = np.linspace(norm.ppf(0.001, loc = 45, scale = 11), norm.ppf(0.999, loc = 45, scale = 11), 100)\nplt.plot(x, norm.pdf(x, loc = 45, scale = 11),'g-', lw = 5, alpha = 0.6, label = 'above')\nplt.xlim([15, 70])\nplt.xlabel('Age', size=14)\nplt.legend(loc = 'best')\nplt.title('Age Distributions')\nplt.ylabel(r'$p(x)$', size=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-4-output-1.png){width=622 height=418 fig-align='center'}\n:::\n:::\n\n\n## Soft Clustering\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {.incremental}\n* two overlapping clusters.\n\n* given some particular individual at a given age, say 25, we cannot say for sure which cluster they belong to.  \n\n* use _probability_ to quantify our uncertainty about cluster membership.\n\n* Individual belongs to the cluster 1 with some probability $p_1$ and the cluster 2 with a different probability $p_2$.\n:::\n\n:::\n::: {.column width=\"50%\"}\n\n::: {#092ca380 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-5-output-1.png){width=399 height=381 fig-align='center'}\n:::\n:::\n\n\n:::\n::::\n\n::: {.fragment}\n* Naturally we expect the probabilities to sum up to 1.\n\n* This is called __soft assignment,__ and a clustering using this principle is called __soft clustering.__\n:::\n\n## Conditional Probability\n\nMore formally, we say that an object can belong to each particular cluster with some probability, such that the sum of the probabilities adds up to 1 for each object. \n\n::: {.fragment}\nFor example, assuming that we have two clusters $C_1$ and $C_2$, we can have that an object $x_1$ belongs to $C_1$ with probability $0.3$ and to $C_2$ with probability $0.7$.\n\nNote that the distribution over $C_1$ and $C_2$ only refers to object $x_1$.\n:::\n\n::: {.fragment}\nThus, it is a __conditional__ probability:\n\n$$\n\\begin{align*}\nP(C_1 \\,|\\, x_1 = 25) &= 0.3, \\\\\nP(C_2 \\,|\\, x_1 = 25) &= 0.7.\n\\end{align*}\n$$\n\n:::\n\n::: {.fragment}\n\nAnd to return to our previous example\n\n$$\nP(\\text{above income threshold}\\,|\\,\\text{age 25}) + P(\\text{below income threshold}\\,|\\,\\text{age 25}) = 1.\n$$\n\n:::\n\n# GMM Overview\n\nOur goal with Gaussian mixture models (GMMs) is to show how to compute the probabilities that a data point belongs to a particular cluster.\n\nThe main ideas behind GMMs are\n\n- Assume each cluster is normally distributed\n- Use the Expectation Maximization (EM) algorithm to iteratively determine:\n    * the parameters of the Gaussian clusters using _MLE_, and\n    * the probabilities that a data point belongs to a particular cluster\n\nWe will see that GMMs are better suited for clustering non-spherical distributions of points.\n\nTo help us in understanding GMMs we will first review _maximum likelihood estimation_.\n\n# Maximum Likelihood Estimation (MLE)\n\n## Motivation\n\nProbability distributions are specified by their parameters.\n\nThe Gaussian (Normal) distribution is determined by the parameters $\\mu$ and $\\sigma^{2}$, i.e.,\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n$$\n\\begin{align*}\nf(x\\vert\\mu, \\sigma^{2}) & = \\mathcal{N}(x_n\\vert \\mu, \\sigma^{2}) \\\\\n & = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^{2}}{2\\sigma^2}}.\n\\end{align*}\n$$\n\n:::\n::: {.column width=\"60%\"}\n\n::: {#2716f1d4 .cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-6-output-1.png){width=449 height=302}\n:::\n:::\n\n\n:::\n::::\n\n**Maximum likelihood estimation** is a method to estimate the parameters of a probability distribution given a sample of observed data that _best fits the data_.\n\n## Likelihood Function\n\nThe likelihood function $L(\\boldsymbol{\\theta}, x)$ represents the probability\nof observing the given data $x$ as a function of the parameters $\\boldsymbol{\\theta}$ of the distribution.\n\nThe primary purpose of the likelihood function is to estimate the parameters that make the observed data $x$ most probable.\n\n:::: {.fragment}\nThe likelihood function for a set of samples $x_{n}~\\text{for}~n=1, \\ldots, N$ drawn from an independent and identically distributed (i.i.d.) Gaussian distribution is\n\n$$\nL(\\mu, \\sigma^{2}, x_1, \\ldots, x_n) \\\n= \\prod_{n=1}^{N}\\mathcal{N}(x_n\\vert \\mu, \\sigma^{2}) \\\n= \\prod_{n=1}^{N}\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x_n-\\mu)^{2}}{2\\sigma^2}}.\n$$\n\n::::\n\n## Maximizing the Likelihood\n\nFor a particular set of parameters $\\mu, \\sigma^{2}$\n\n- **large** values of $L(\\mu, \\sigma^{2}, x_1, \\ldots, x_n)$ indicate the observed data is very probable (high likelihood) and thus _well modeled by the parameters_\n- **small** values of $L(\\mu, \\sigma^{2}, x_1, \\ldots, x_n)$ indicate the observed data is very improbable (low likelihood) and thus _poorly modeled by the parameters_\n\n:::: {.fragment}\nThe parameters that _maximize_ the likelihood function are called the **maximum likelihood estimates**.\n::::\n\n## Log-likelihood\n\nA common manipulation to obtain a more useful form of the likelihood function is to take its natural logarithm.\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n::: {#9d036ead .cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-7-output-1.png){width=364 height=302}\n:::\n:::\n\n\n:::\n::: {.column width=\"60%\"}\n\nAdvantages of the log-likelihood:\n\n* The log function is monotonically increasing, so the MLE is the same as the log-likelihood estimate\n* The product of probabilities becomes a sum of logarithms, which is more numerically stable\n* The log-likelihood is easier to work with\n\n:::\n::::\n\n## Applying the Log\n\n$$\n\\begin{align*}\n\\ell(\\mu, \\sigma^{2}, x_1, \\ldots, x_n) \\\n& = \\log{\\left(L(\\mu, \\sigma^{2}, x_1, \\ldots, x_n)\\right)} \\\\\n& = \\log{\\left(\\prod_{n=1}^{N}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x_n-\\mu)^{2}}{2\\sigma^2}}\\right)} \\\\\n& = \\sum_{n=1}^{N}\\log{\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x_n-\\mu)^{2}}{2\\sigma^2}}\\right)} \\\\\n& = \\sum_{n=1}^{N}\\left(\\log{\\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)} + \\log{\\left(e^{-\\frac{(x_n-\\mu)^{2}}{2\\sigma^2}}\\right)}\\right) \\\\\n& = -\\frac{N}{2}\\log{(2\\pi\\sigma^2)} - \\frac{1}{2\\sigma^{2}}\\sum_{n=1}^{N}(x_n -\\mu)^{2}.\n\\end{align*}\n$$\n\nUsing the log-likelihood we will be able to derive formulas for the maximum likelihood estimates. \n\n## Maximizing the log-likelihood\n\n:::: {.fragment}\nHow do we maximize (optimize) a function of parameters?\n::::\n\n:::: {.fragment}\nTo find the optimal parameters of a function, we compute partial derivatives of the function and set them equal to zero. The solution to these equations gives us a local optimal value for the parameters.\n\nFor the case of the Gaussian we compute\n\n$$\n\\begin{align*}\n\\nabla_{\\mu} \\ell(\\mu, \\sigma^{2}, x_1, \\ldots, x_n) &= 0, \\\\\n\\nabla_{\\sigma} \\ell(\\mu, \\sigma^{2}, x_1, \\ldots, x_n) &= 0. \\\\\n\\end{align*}\n$$\n::::\n\n## Gaussian MLEs\n\nThe maximum log-likelihood estimates for a Gaussian distribution are given by\n\n$$\n\\begin{align*}\n\\bar{\\mu} &= \\frac{1}{N}\\sum_{n=1}^{N} x_{n}, \\\\\n\\bar{\\sigma}^2 &= \\frac{1}{N}\\sum_{n=1}^{N}(x_{n} - \\bar{\\mu})^{2}.\n\\end{align*}\n$$\n\nThe full derivation of these results is provided [here](09A-Appendix-Derive-MLE-params-for-gaussian.qmd).\n\n> Tip: Try deriving these results yourself!\n\n## Summary of MLE\n\nGiven samples $x_{1}, \\ldots, x_n$ drawn from a Gaussian distribution, we compute the maximum likelihood estimates by maximizing the log likelihood function\n\n$$\n\\ell (\\mu, \\sigma^{2}, x_1, \\ldots, x_n) = -\\frac{N}{2}\\log{2\\pi} - N\\log{\\sigma} - \\frac{1}{2\\sigma^{2}}\\sum_{n=1}^{N}(x_n -\\mu)^{2}.\n$$\n\nThis gives us the maximum likelihood estimates\n\n$$\n\\begin{align*}\n\\bar{\\mu} &= \\frac{1}{N}\\sum_{n=1}^{N} x_{n}, \\\\\n\\bar{\\sigma}^2 &= \\frac{1}{N}\\sum_{n=1}^{N}(x_{n} - \\bar{\\mu})^{2}.\n\\end{align*}\n$$\n\nWe use this information to help our understanding of Gaussian Mixture models.\n\n# Gaussian Mixture Models\n\n## Univariate Gaussians\n\nIn GMMs we assume that each of our clusters follows a Gaussian (normal) distribution with their own parameters. \n\n::: {#004525a7 .cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the parameters for the 4 Gaussians\nparams = [\n    {\"mean\": 0, \"variance\": 1, \"color\": \"#377eb8\"},  # Blue\n    {\"mean\": 2, \"variance\": 0.5, \"color\": \"#4daf4a\"},  # Green\n    {\"mean\": -2, \"variance\": 1.5, \"color\": \"#e41a1c\"},  # Red\n    {\"mean\": 1, \"variance\": 2, \"color\": \"#984ea3\"}  # Purple\n]\n\n# Generate x values\nx = np.linspace(-10, 10, 1000)\n\n# Plot each Gaussian\nfor param in params:\n    mean = param[\"mean\"]\n    variance = param[\"variance\"]\n    color = param[\"color\"]\n    \n    # Calculate the Gaussian\n    y = (1 / np.sqrt(2 * np.pi * variance)) * np.exp(- (x - mean)**2 / (2 * variance))\n    \n    # Plot the Gaussian\n    plt.plot(x, y, color=color, label=f\"$\\\\mu$: {mean}, $\\\\sigma^2$: {variance}\")\n\n# Add legend\nplt.legend()\n\n# Add title and labels\nplt.title(\"1D Gaussians with Different Means and Variances\")\nplt.xlabel(\"x\")\nplt.ylabel(\"Probability Density\")\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-8-output-1.png){width=589 height=449}\n:::\n:::\n\n\n::: {.notes}\nThis is a similar situation to our previous example where we considered labeling a person as above or below an income threshold based on the single feature age.\n\nThis could be a hypothetical situation with $K=4$ normally distributed clusters.\n:::\n\n## Multivariate Gaussians\n\nGiven data points $\\mathbf{x}\\in\\mathbb{R}^{d}$, i.e., $d$-dimensional points, then we use the multivariate Gaussian to describe the clusters. The formula for the multivariate Gaussian is\n\n$$\nf(\\mathbf{x}\\vert \\boldsymbol{\\mu}, \\Sigma) = \\frac{1}{(2\\pi)^{d/2}\\vert \\Sigma \\vert^{1/2}}e^{-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})},\n$$\n\nwhere $\\Sigma\\in\\mathbb{R}^{d\\times d}$ is the covariance matrix, $\\vert \\Sigma\\vert$, denote the determinant of $\\Sigma$, and $\\boldsymbol{\\mu}$ is the $d$-dimensional vector of expected values.\n\nThe covariance matrix is the multidimensional analog of the variance. It \ndetermines the extent to which vector components are correlated.\n\nSee [here](09B-Appendix-Cov-Matrix-in-MV-Gaussians.qmd) for a refresher on the covariance matrix.\n\n## Notation\n\nThe notation $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is used to denote a _univariate_ random variable that is normally \ndistributed with expected value $\\mu$ and variance $\\sigma^{2}.$\n\nThe notation $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\Sigma)$ is used to denote a _multivariate_\nrandom variable that is normally distributed with mean $\\boldsymbol{\\mu}$ and covariance matrix $\\Sigma$.\n\n## Multivariate Example: Cars\n\nTo illustrate a particular model, let us consider the properties of cars produced in the US, Europe, and Asia.\n\nFor example, let's say we are looking to classify cars based on their model year and miles per gallon (mpg).  \n\n<!-- image credit: https://www.cs.cmu.edu/~./awm/tutorials/gmm14.pdf -->\n![](figs/L09-multivariate-example.png){width=\"90%}\n\nIt seems that the data can be described (roughly) as a mixture of __three__ __multivariate__ Gaussian distributions.\n\n# Expectation-Maximization for Gaussian Mixture Models\n\n## The Clustering Problem\nSuppose we observe $n$ data points $X = \\{x_1, x_2, \\ldots, x_n\\}$ in $\\mathbb{R}^d$ that appear to come from multiple clusters. \n\nWe want to model this data as a mixture of $K$ Gaussian distributions, each representing a different cluster.\n\n## The Gaussian Mixture Model\n\nA Gaussian Mixture Model (GMM) assumes each data point is generated by the following process:\n\n1. Choose a cluster $k \\in \\{1, 2, \\ldots, K\\}$ with probability $\\pi_k$ (where $\\sum_{k=1}^K \\pi_k = 1$)\n2. Generate the point from a Gaussian distribution: $x_i \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)$\n\nThe probability density of observing point $x_i$ is:\n\n$$\np(x_i \\mid \\theta) = \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\n$$\n\nwhere $\\theta = \\{\\pi_k, \\mu_k, \\Sigma_k\\}_{k=1}^K$ are the parameters we need to estimate.\n\n## The Likelihood Function\n\nAssuming independent and identically distributed (i.i.d.) data, the likelihood function is:\n\n$$\nL(X, \\theta) = \\prod_{i=1}^{n} p(x_i \\mid \\theta) = \\prod_{i=1}^{n} \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)\n$$\n\nThe log-likelihood function is:\n\n$$\n\\ell(X, \\theta) = \\log L(X, \\theta) = \\sum_{i=1}^{n} \\log \\left( \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n$$\n\n## Why is This Hard?\n\nAssume $Z = \\{z_1, z_2, \\ldots, z_n\\}$ are the cluster assignments for each $i=1, \\ldots, n$.\n\nIf we knew which cluster generated each point, finding the parameters of the MLE would be straightforward:\n\n- $\\mu_k$ = mean of points in cluster $k$\n- $\\Sigma_k$ = covariance of points in cluster $k$\n- $\\pi_k$ = fraction of the total points in cluster $k$\n\nBut we don't know the cluster assignments! \n\nAnd if we knew the parameters, we could infer the cluster assignments. \n\nThis is a classic 🐓 and 🥚 problem.\n\n## The Log-Likelihood Challenge\nThe log-likelihood we want to maximize is:\n\n$$\n\\ell(X,\\theta) = \\log p(X \\mid \\theta) = \\sum_{i=1}^{n} \\log \\left( \\sum_{k=1}^{K} \\pi_k \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right)\n$$\n\nThe sum inside the logarithm makes this difficult to optimize directly. \n\nWe can't separate the terms, and taking derivatives leads to a system of coupled equations with no closed-form solution.\n\n## EM to the Rescue\nEM treats the cluster assignments $Z = \\{z_1, z_2, \\ldots, z_n\\}$ as latent (hidden) variables. \n\nInstead of solving for a hard assignment of points to clusters, EM finds the parameters of the MLE by iterating between two steps:\n\n1. **E-Step**: Computes soft assignments (responsibilities) - the probability each point belongs to each cluster given current parameters\n2. **M-Step**: Updates parameters assuming these soft assignments are the true (weighted) cluster memberships\n\nBy iterating between these steps, EM monotonically increases the likelihood until convergence to a local maximum.\n\n## The Complete Data Likelihood\nIf we knew the cluster assignments $z_i \\in \\{1, \\ldots, K\\}$ for each point, the complete data log-likelihood would be:\n\n$$\n\\log p(X, Z \\mid \\theta) = \\sum_{i=1}^{n} \\sum_{k=1}^{K} \\mathbb{1}(z_i = k) \\left[ \\log \\pi_k + \\log \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k) \\right]\n$$\n\nThis is much easier to work with because the log operates on individual Gaussian densities rather than on a sum of densities.\n\nSee [here](09C-Appendix-complete-data-likelihood.qmd) for a detailed derivation.\n\n> Tip: Try deriving this yourself!\n\n## Why EM Works for GMMs\n- **E-Step**: Since we don't know $z_i$, we compute the expected value of the indicator $\\mathbb{1}(z_i = k)$, which is the posterior probability $\\gamma_{ik} = p(z_i = k \\mid x_i, \\theta^{(t)})$, where $\\theta^{(t)}$ is the parameter vector at iteration $t$.\n- **M-Step**: With these soft assignments, maximizing the expected complete log-likelihood yields closed-form updates that look like weighted versions of the standard MLE formulas\n\nThis elegant approach transforms an intractable optimization problem into a simple iterative procedure with intuitive updates at each step.\n\nSee [here](09D-Appendix-Derive-the-E-step.qmd) for a detailed derivation.\n\n> Tip: Try deriving this yourself!\n\n# EM Algorithm for Gaussian Mixture Models\n\n## Setup and Goal\n- We observe data points $x_1, x_2, \\ldots, x_n$ in $d$-dimensional space\n- We assume the data comes from $K$ Gaussian distributions, each with mean $\\mu_k$, covariance $\\Sigma_k$, and mixing weight $\\pi_k$\n- The latent variables are the cluster assignments $z_i$ (which component generated each point)\n- Goal: Find parameters $\\theta = \\{\\mu_k, \\Sigma_k, \\pi_k\\}_{k=1}^K$ that maximize the likelihood of the observed data\n\n## Initialization\n- Randomly initialize the $K$ means $\\mu_k$ (or use k-means clustering)\n- Initialize covariances $\\Sigma_k$ (often as identity matrices or sample covariance)\n- Initialize mixing weights $\\pi_k = \\frac{1}{K}$ (uniform distribution over components)\n\n## E-Step: Compute Responsibilities\n- For each data point $x_i$ and each component $k$, compute the responsibility $\\gamma_{ik}$ (the probability that point $i$ belongs to component $k$ given current parameters)\n\n$$\n\\gamma_{ik} = \\frac{\\pi_k \\cdot \\mathcal{N}(x_i \\mid \\mu_k, \\Sigma_k)}{\\sum_{j=1}^{K} \\pi_j \\cdot \\mathcal{N}(x_i \\mid \\mu_j, \\Sigma_j)}\n$$\n\n- This is a soft assignment: each point gets a fractional membership in each cluster\n- Responsibilities sum to 1 for each data point: $\\sum_{k=1}^{K} \\gamma_{ik} = 1$\n\n## M-Step: Update Parameters\n- Update means as weighted averages:\n\n$$\\mu_k = \\frac{\\sum_{i=1}^{n} \\gamma_{ik} x_i}{\\sum_{i=1}^{n} \\gamma_{ik}}$$\n\n- Update covariances using weighted samples:\n\n$$\\Sigma_k = \\frac{\\sum_{i=1}^{n} \\gamma_{ik} (x_i - \\mu_k)(x_i - \\mu_k)^T}{\\sum_{i=1}^{n} \\gamma_{ik}}$$\n\n- Update mixing weights:\n\n$$\\pi_k = \\frac{\\sum_{i=1}^{n} \\gamma_{ik}}{n}$$\n\n- Each parameter update has an intuitive interpretation as a weighted MLE\n\n## Iteration and Convergence\n- Alternate between E and M steps until convergence\n- Monitor the log-likelihood: it's guaranteed to increase (or stay the same) at each iteration\n- Stop when log-likelihood change falls below a threshold or after a maximum number of iterations\n- Note: EM finds a local maximum, so results depend on initialization (run multiple times with different initializations)\n\n## Key Intuition\n- **E-step**: Given current cluster parameters, probabilistically assign points to clusters\n- **M-step**: Given these soft assignments, update cluster parameters as if they were the true assignments\n- This 🐓 and 🥚 problem is resolved through iteration\n\n\n<!--\n\n## Intuition\n\nGiven $K$ clusters, the goal with a GMM is to determine the probability for whether a point $\\mathbf{x}\\in\\mathbb{R}^{d}$ belongs to a particular cluster. \n\nWe assume that each cluster is distributed normally, according to some (unknown) parameters $\\boldsymbol{\\mu}_i, \\Sigma_i$. We also assume that the probability that a point belongs to a particular cluster is given by $w_i$.\n\nA Gaussian mixture model is defined by these parameters $w_i, \\boldsymbol{\\mu}_i, \\Sigma_i$ for $i=1, \\ldots, K$. \n\nWe can use these parameters to compute the probability that a point $\\mathbf{x}$ belongs to a particular cluster $C_k$.\n\nSimilar to MLE, we must compute the parameters $w_i, \\boldsymbol{\\mu}_i, \\Sigma_i$.\n\n## Learning the Parameters of a GMM\n\nGiven some data how do we estimate the \n\n* cluster probability $w_i$,\n* cluster mean $\\boldsymbol{\\mu}_i$, and \n* cluster covariance  $\\Sigma_i$\n\nfor each $i=1, \\ldots, K$?\n\n:::: {.fragment}\nWe will formulate a likelihood function for these parameters $\\boldsymbol{\\theta}_i = (w_i, \\boldsymbol{\\mu}_i, \\Sigma_i)$ and then optimize this function.\n::::\n\n## Latent Variables\n\nWe assume that each data point $x$ is produced by a **latent variable** $z$.\n\nThis variable is called latent because it is never actually observed. It is used to help indicate cluster membership and is helpful for the derivation of the likelihood function.\n\nWe can view this as a one-hot-encoding that identifies a probability of membership to one of the $K$ clusters.\n\nWe impose a distribution over $z$ representing a soft assignment\n\n$$\np(z) = (w_1, \\cdots, w_K)^{T}, \\quad \\text{with} \\quad 0\\leq w_i \\leq 1 \\quad \\text{and} \\quad \\sum_{i=1}^{K}w_i = 1.\n$$\n\n## PDF of a GMM\n\nWith our distribution weights $w_i$ for the $K$ clusters, we can compute the probability density for a GMM at any point $\\mathbf{x}$ via the formula\n\n$$\np(\\mathbf{x}) = \\sum_{i=1}^{K} w_i \\cdot f(\\mathbf{x}\\vert \\mu_i, \\Sigma_i).\n$$\n\nIf this is feeling a bit abstract, let's see how this applies to the initial example where we were clustering a people by income level based on their age.\n-->\n\n## Income Level Example\n\nWe have \n\n* Our single data point $x=25$, and \n* We have 2 Gaussians representing our clusters, \n    * $C_1$ with parameters $\\mu_1 = 37, \\sigma_1^{2}=14$ and \n    * $C_2$ with parameters $\\mu_2 = 45, \\sigma_2^{2}=11$, \n\nOur latent variables are the probabilities that we classify someone as either\nabove or below an income threshold, i.e., \n$$\n\\gamma_1 = P(z = 1) = P(\\text{above income threshold})\n$$\nand\n$$\n\\gamma_2 = P(z = 2) = P(\\text{below income threshold}).\n$$\n\n## Income Level Example, cont.\n\nOur interest in this problem is computing the posterior probability, which is,\n\n$$\nP(C_1 | x) = P(z=1 \\vert x) = P(\\text{above income threshold} \\vert \\text{age 25}).\n$$\n\n:::: {.fragment}\nHow do we determine these posterior probabilities $P(C_i \\vert x) = P(z=1 \\vert x)$?\n::::\n\n:::: {.fragment}\nThe answer is Bayes' Rule.\n::::\n\n\n## Income Level Example, cont.\n\nIf we know the parameters of the Gaussians, we can determine the value of $P(x | C_1)$ when $x=25$. This is shown with the red dot.\n\n::: {#3788fb07 .cell execution_count=9}\n``` {.python .cell-code}\nx = np.linspace(norm.ppf(0.001, loc = 37, scale = 14), norm.ppf(0.999, loc = 37, scale = 14), 100)\nplt.plot(x, norm.pdf(x, loc = 37, scale = 14),'b-', lw = 5, alpha = 0.6, label = 'below')\nx = np.linspace(norm.ppf(0.001, loc = 45, scale = 11), norm.ppf(0.999, loc = 45, scale = 11), 100)\nplt.plot(x, norm.pdf(x, loc = 45, scale = 11),'g-', lw = 5, alpha = 0.6, label = 'above')\nplt.plot(25, norm.pdf(25, loc = 45, scale = 11), 'ro')\nplt.xlim([15, 70])\nplt.xlabel('Age', size=14)\nplt.legend(loc = 'best')\nplt.title('Age Distributions')\nplt.ylabel(r'$p(x)$', size=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-9-output-1.png){width=622 height=455}\n:::\n:::\n\n\n---\n\nBayes' Rule then allows us to compute \n\n$$\nP(C_1 \\vert x)=\\frac{P(x\\vert C_1)}{P(x)}P(C_1).\n$$\n\nWe can always use the law of total probability to compute\n\n$$\n\\begin{align*}\nP(x) &= P(x \\vert C_1)P(C_1) + P(x\\vert C_2)P(C_2), \\\\\n&=  P(z=1) P(x \\vert z=1) + P(z=2) P(x\\vert z=2), \\\\\n&=  \\gamma_1 P(x \\vert z=1) + \\gamma_2 P(x\\vert z=2), \\\\\n&= \\sum_{i=1}^{2} \\gamma_i \\cdot f(\\mathbf{x}\\vert \\mu_i, \\sigma_i).\n\\end{align*}\n$$\n\nThe final formula is illustrated in the following figure.\n\n\n---\n\n::: {#e9a98801 .cell execution_count=10}\n``` {.python .cell-code}\nplt.figure()\nx = np.linspace(norm.ppf(0.001, loc = 37, scale = 14), norm.ppf(0.999, loc = 37, scale = 14), 100)\nplt.plot(x, norm.pdf(x, loc = 37, scale = 14),'b-', lw = 5, alpha = 0.6, label = 'below')\nplt.plot(25, norm.pdf(25, loc = 37, scale = 14), 'ko', markersize = 8)\nx = np.linspace(norm.ppf(0.001, loc = 45, scale = 11), norm.ppf(0.999, loc = 45, scale = 11), 100)\nplt.plot(x, norm.pdf(x, loc = 45, scale = 11),'g-', lw = 5, alpha = 0.6, label = 'above')\nplt.plot(25, norm.pdf(25, loc = 45, scale = 11), 'ro', markersize = 8)\nplt.xlim([15, 70])\nplt.xlabel('Age', size=14)\nplt.legend(loc = 'best')\nplt.title('Age Distributions')\nplt.ylabel(r'$p(x)$', size=14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-10-output-1.png){width=622 height=455}\n:::\n:::\n\n\n$$\nP(\\text{above}\\,|\\,\\text{age 25}) = \\frac{\\text{red}}{\\text{red} \\cdot P(\\text{above}) + \\text{black} \\cdot P(\\text{below})} \\cdot P(\\text{above}).\n$$\n\nWhere:\n\n* \"red\" = $P(\\text{age 25} | \\text{above})$ and \"black\" = $P(\\text{age 25} | \\text{below})$, and\n* $P(\\text{above})$ and $P(\\text{below})$ are the prior probabilities (mixing weights) that must sum to 1: $P(\\text{above}) + P(\\text{below}) = 1$, \n    * e.g. 0.5 in this example since we split the 20K points equally.\n\n<!--\n## GMM Likelihood\n\nLet's gather for all $i=1, \\ldots, K$, the coefficients $w_i,\\boldsymbol{\\mu}_i, \\Sigma_i$ into a vector $\\boldsymbol{\\theta}$.\n\nThe likelihood function for a Gaussian mixture model with $N$ data points\n\n$$\nL(\\boldsymbol{\\theta}, x_1, \\ldots, x_n) = \\prod_{n=1}^{N}\\sum_{k=1}^{K}w_k \\frac{1}{(2\\pi)^{d/2}\\vert \\Sigma_k \\vert^{1/2}}e^{-\\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)^T\\Sigma_{k}^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_{k})}\n$$\n\nThe log-likelihood is\n\n$$\n\\log{\\left(L(\\boldsymbol{\\theta}, x_1, \\ldots, x_n)\\right)} = \\sum_{n=1}^{N}\\log{\\left(\\sum_{k=1}^{K}w_k \\frac{1}{(2\\pi)^{d/2}\\vert \\Sigma_k \\vert^{1/2}}e^{-\\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)^T\\Sigma_{k}^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_{k})}\\right)}.\n$$\n\nUnlike the log of a product, the log of a sum does not simplify nicely. As a result, the partial derivatives with respect to $\\boldsymbol{\\mu}_k$, depend on the covariances and mixture weights (similarly for the other partial derivatives of $\\Sigma_k$ and $w_k$).\n\nTo solve this problem we turn to __Expectation Maximization__.\n\n# Expectation Maximization\n\nThis is another famous algorithm, in the same \"super-algorithm\" league as $k$-means.\n\nEM is formulated using a probabilistic model for data. It can solve a problem like\n\n> Given a set of data points and a parameter $K$, find the $(w_k, \\mu_k, \\Sigma_k)~k = 1,\\dots,K$ that __maximizes the likelihood of the data__ assuming a GMM with those parameters.\n\nIt can also solve lots of other problems involving maximizing the likelihood of data under a different model.\n\nSimilar to $k$-means, this problem is NP-hard.  \n\nFurthermore, EM only guarantees that we will find a __local__ optimum of the objective function.\n\n## EM Algorithm for GMM\n\nHere is the EM algorithm.\n\n__Step 1 Initialization__\n\nInitialize the parameters $w_k, \\boldsymbol{\\mu}_k, \\Sigma_k$ for $k=1, \\dots, K$. \nThe final result will be sensitive to this choice, so a good (and fast) initialization procedure is $k$-means.\n\n__Step 2 Expectation__ \n\nUse the current values for $w_k, \\boldsymbol{\\mu}_k, \\Sigma_k$ and for each of the $N$ data points $x_n$, compute the posterior probabilities\n\n$$\nr_{nk} = \\frac{w_k f(\\mathbf{x}_n \\vert \\boldsymbol{\\mu}_{k}, \\Sigma_k)}{\\sum_{i=1}^{K}w_i f(\\mathbf{x}_n \\vert \\boldsymbol{\\mu}_{i}, \\Sigma_i)},\n$$\n\nwhere $f(\\mathbf{x}_n \\vert \\boldsymbol{\\mu}_{k}, \\Sigma_k)$ is the multivariate Gaussian.\n\n## EM Algorithm for GMM, cont.\n\n__Step 3 Maximization__ \n\nUsing the values $r_{nk}$ for $n=1,\\ldots, N$ and $k=1, \\ldots, K$. First compute $N_{k} = \\sum_{n=1}^{N} r_{nk}$. Then compute updated $w_k, \\boldsymbol{\\mu}_k, \\Sigma_k$ according to the formulas\n\n$$\n\\boldsymbol{\\mu}_{k}= \\frac{1}{N_k}\\sum_{n=1}^{N} r_{nk} \\mathbf{x}_n, \\quad \n\\Sigma_{k} = \\frac{1}{N_k}\\sum_{n=1}^{N}  r_{nk}(\\mathbf{x}_n - \\boldsymbol{\\mu}_k)(\\mathbf{x}_n -\\boldsymbol{\\mu}_k)^{T}, \\quad\nw_k = \\frac{N_k}{N}.\n$$\n\n__Step 4__ Stop if **convergence criterion** is satisfied. Otherwise repeat Steps 2 and 3.\n-->\n\n## Convergence criteria\n\nThe convergence criteria for the Expectation-Maximization (EM) algorithm \nassess the _change across iterations_ of either the \n\n* likelihood function, or\n* model parameters\n\nwith usually a _limit on the maximum number of iterations_.\n\nHere are the common convergence criteria used:\n\n## Log-Likelihood Convergence\n\n$$\n|\\mathcal{L}^{(t)} - \\mathcal{L}^{(t-1)}| < \\text{tol}\n$$\n\nWhere:\n\n* $\\mathcal{L}^{(t)}$ is the log-likelihood at iteration $t$,\n* $\\text{tol}$ is a small positive number (e.g., $10^{-4}$).\n\n## Parameter Convergence\n\n$$\n||\\theta^{(t)} - \\theta^{(t-1)}||_2 < \\text{tol}\n$$\n\nWhere:\n\n* $\\theta^{(t)}$ represents the model parameters (means, covariances, and weights) at iteration $t$,\n* $||\\cdot||_2$ is the Euclidean (L2) norm,\n* $\\text{tol}$ is a small positive number.\n\n## Maximum Number of Iterations\n\nThe EM algorithm is typically capped at a maximum number of iterations to avoid\nlong runtimes in cases where the log-likelihood or parameters converge very\nslowly or never fully stabilize.\n\n$$\nt > \\text{max\\_iterations}\n$$\n\nWhere:\n\n* $\\text{max\\_iterations}$ is a predefined limit (e.g., 100 or 500 iterations).\n\n## Storage and Computational Costs\n\nIt is important to be aware of the computational costs of our algorithm.\n\nStorage costs:\n\n- There are $N$ $d$-dimensional points\n- There are $K$ clusters\n- There are $N\\times K$ coefficients $r_{nk}$\n- There are $K$ $d$-dimensional cluster centers $\\boldsymbol{\\mu}_k$\n- There are $K$ $d\\times d$ covariance matrices $\\Sigma_k$\n- There are $K$ weights $w_k$\n\nComputational costs:\n\n- Computing each $r_{nk}$ requires a sum of $K$ evaluations of the Gaussian PDF.\n- Updating $\\boldsymbol{\\mu}_k$ requires $N$ vector summations\n- Updating $\\Sigma_k$ requires $N$ outer products\n- Updating $\\gamma_k$ requires a division (though we must compute $N_k$)\n\n## k-means vs GMMs\n\nLet's pause for a minute and compare GMM/EM with $k$-means.\n\n__GMM/EM__\n\n1. Initialize randomly or using some rule\n1. Compute the probability that each point belongs in each cluster\n1. Update the clusters (weights, means, and variances).\n1. Repeat 2-3 until convergence.\n\n__$k$-means__\n\n1. Initialize randomly or using some rule\n1. Assign each point to a single cluster\n1. Update the clusters (means).\n1. Repeat 2-3 until convergence.\n\n--- \n\nFrom a practical standpoint, the main difference is that in GMMs, data points do not belong to a __single__ cluster, but have some probability of belonging to __each__ cluster.\n\nIn other words, as stated previously, GMMs use soft assignment.\n\nFor that reason, GMMs are also sometimes called __soft $k$-means.__\n\n---\n\nHowever, there is also an important conceptual difference. \n\nThe GMM starts by making an __explicit assumption__ about how the data were generated.  \n\nIt says: \"the data came from a collection of multivariate Gaussians.\"\n\nWe made no such assumption when we came up with the $k$-means problem. In that case, we simply defined an objective function and declared that it was a good one.\n\nNonetheless, it appears that we were making a sort of Gaussian assumption when we formulated the $k$-means objective function. However, __it wasn't explicitly stated.__\n\nThe point is that because the GMM makes its assumptions explicit, we can\n\n* examine them and think about whether they are valid, and\n* replace them with different assumptions if we wish.\n\nFor example, it is perfectly valid to replace the Gaussian assumption with some other probability distribution.  As long as we can estimate the parameters of such distributions from data (e.g., have MLEs), we can use EM in that case as well.\n\n## Versatility of EM\n\nA final statement about EM generally. EM is a versatile algorithm that can be used in many other settings.  What is the main idea behind it?\n\nNotice that the problem definition only required that we find the clusters, $C_i$, meaning that we were to find the $(\\mu_i, \\Sigma_i)$.\n\nHowever, the EM algorithm posited that we should find as well the $P(C_j|x_i) = P(z=j | x_i)$, that is, the probability that each point is a member of each cluster.\n\nThis is the true heart of what EM does.\n\nBy __adding parameters__ to the problem, it actually finds a way to make the problem solvable.\n\nThese are the latent parameters we introduced earlier. Latent parameters don't show up in the solution.\n\n# Examples\n\n## Example 1\n\nHere is an example using **GMM**.\n\n\nWe're going to create two clusters, one spherical, and one highly skewed.\n\n::: {#864160f4 .cell execution_count=11}\n``` {.python .cell-code}\n# Number of samples of larger component\nn_samples = 1000\n\n# C is a transfomation that will make a heavily skewed 2-D Gaussian\nC = np.array([[0.1, -0.1], [1.7, .4]])\n\nprint(f'The covariance matrix of our skewed cluster will be:\\n {C.T@C}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe covariance matrix of our skewed cluster will be:\n [[2.9  0.67]\n [0.67 0.17]]\n```\n:::\n:::\n\n\n::: {#dfd83f72 .cell execution_count=12}\n``` {.python .cell-code}\nimport warnings\nwarnings.filterwarnings('ignore')\n\nrng = np.random.default_rng(0)\n\n# now we construct a data matrix that has n_samples from the skewed distribution,\n# and n_samples/2 from a symmetric distribution offset to position (-4, 2)\nX = np.r_[(rng.standard_normal((n_samples, 2)) @ C),\n          .7 * rng.standard_normal((n_samples//2, 2)) + np.array([-4, 2])]\n```\n:::\n\n\n::: {#3d8749eb .cell execution_count=13}\n``` {.python .cell-code}\nplt.scatter(X[:, 0], X[:, 1], s = 10, alpha = 0.8)\nplt.axis('equal')\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-13-output-1.png){width=540 height=389}\n:::\n:::\n\n\n--- \n\n::: {#b3246e54 .cell execution_count=14}\n``` {.python .cell-code}\n# Fit a mixture of Gaussians with EM using two components\nimport sklearn.mixture\ngmm = sklearn.mixture.GaussianMixture(n_components=2, \n                                      covariance_type='full', \n                                      init_params = 'kmeans')\ny_pred = gmm.fit_predict(X)\n```\n:::\n\n\n::: {#7bfa1bf3 .cell execution_count=15}\n``` {.python .cell-code}\ncolors = ['bg'[p] for p in y_pred]\nplt.title('Clustering via GMM')\nplt.axis('off')\nplt.axis('equal')\nplt.scatter(X[:, 0], X[:, 1], color = colors, s = 10, alpha = 0.8)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-15-output-1.png){width=540 height=409}\n:::\n:::\n\n\n::: {#51594ac6 .cell execution_count=16}\n``` {.python .cell-code}\nfor clus in range(2):\n    print(f'Cluster {clus}:')\n    print(f' weight: {gmm.weights_[clus]:0.3f}')\n    print(f' mean: {gmm.means_[clus]}')\n    print(f' cov: \\n{gmm.covariances_[clus]}\\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster 0:\n weight: 0.667\n mean: [-0.04719455 -0.00741777]\n cov: \n[[2.78299944 0.64624931]\n [0.64624931 0.16589271]]\n\nCluster 1:\n weight: 0.333\n mean: [-4.03209555  1.96770685]\n cov: \n[[ 0.46440554 -0.01263282]\n [-0.01263282  0.48113597]]\n\n```\n:::\n:::\n\n\n## Comparison with k-means\n\n::: {#1ce16d02 .cell execution_count=17}\n``` {.python .cell-code}\nimport sklearn.cluster\nkmeans = sklearn.cluster.KMeans(init = 'k-means++', n_clusters = 2, n_init = 100)\ny_pred_kmeans = kmeans.fit_predict(X)\ncolors = ['bg'[p] for p in y_pred_kmeans]\nplt.title('Clustering via $k$-means\\n$k$-means centers: red, GMM centers: black')\nplt.axis('off')\nplt.axis('equal')\nplt.scatter(X[:, 0], X[:, 1], color = colors, s = 10, alpha = 0.8)\nplt.plot(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], 'ro')\nplt.plot(gmm.means_[:,0], gmm.means_[:, 1], 'ko')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-17-output-1.png){width=540 height=429}\n:::\n:::\n\n\n::: {#76039379 .cell execution_count=18}\n``` {.python .cell-code}\nfor clus in range(2):\n    print(f'Cluster {clus}:')\n    print(f' center: {kmeans.cluster_centers_[clus]}\\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster 0:\n center: [0.20678138 0.04903169]\n\nCluster 1:\n center: [-3.91417527  1.61676661]\n\n```\n:::\n:::\n\n\n## Overlapping Clusters\n\nNow, let's construct __overlapping__ clusters.  What will happen?\n\n::: {#a3e37012 .cell quarto-private-1='{\"key\":\"slideshow\",\"value\":{\"slide_type\":\"fragment\"}}' execution_count=19}\n``` {.python .cell-code}\nX = np.r_[(rng.standard_normal((n_samples, 2)) @ C),\n          .7 * rng.standard_normal((n_samples//2, 2))]\ngmm = sklearn.mixture.GaussianMixture(n_components=2, covariance_type='full')\ny_pred_over = gmm.fit_predict(X)\n```\n:::\n\n\n::: {#10d4d64c .cell execution_count=20}\n``` {.python .cell-code}\ncolors = ['bgrky'[p] for p in y_pred_over]\nplt.title('GMM for overlapping clusters\\nNote they have nearly the same center')\nplt.scatter(X[:, 0], X[:, 1], color = colors, s = 10, alpha = 0.8)\nplt.axis('equal')\nplt.axis('off')\nplt.plot(gmm.means_[:,0], gmm.means_[:,1], 'ro')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-20-output-1.png){width=540 height=427}\n:::\n:::\n\n\n::: {#953a0719 .cell execution_count=21}\n``` {.python .cell-code}\nfor clus in range(2):\n    print(f'Cluster {clus}:')\n    print(f' weight: {gmm.weights_[clus]:0.3f}')\n    print(f' mean: {gmm.means_[clus]}\\n')\n    # print(f' cov: \\n{gmm.covariances_[clus]}\\n')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster 0:\n weight: 0.647\n mean: [0.10975427 0.0237165 ]\n\nCluster 1:\n weight: 0.353\n mean: [-0.02170599  0.0046345 ]\n\n```\n:::\n:::\n\n\n## How many parameters are estimated?\n\nMost of the parameters in the model are contained in the covariance matrices.\n\nIn the most general case, for $K$ clusters of points in $d$ dimensions, there are $K$ covariance matrices each of size $d \\times d$.   \n\nSo we need $Kd^2$ parameters to specify this model.\n\nIt can happen that you may not have enough data to estimate so many parameters.\n\nAlso, it can happen that you believe that clusters should have some constraints on their shapes.\n\nHere is where the GMM assumptions become __really__ useful.\n\n## Clusters with Equal Variance\n\nLet's say you believe all the clusters should have the same shape, but the shape can be arbitrary. \n\nThen you only need to estimate __one__ covariance matrix - just $d^2$ parameters.\n\nThis is specified by the GMM parameter `covariance_type='tied'`.\n\n---\n\n::: {#2143261c .cell execution_count=22}\n``` {.python .cell-code}\nX = np.r_[np.dot(rng.standard_normal((n_samples, 2)), C),\n          0.7 * rng.standard_normal((n_samples, 2))]\ngmm = sklearn.mixture.GaussianMixture(n_components=2, covariance_type='tied')\ny_pred = gmm.fit_predict(X)\n```\n:::\n\n\n::: {#eb318dca .cell execution_count=23}\n``` {.python .cell-code}\ncolors = ['bgrky'[p] for p in y_pred]\nplt.scatter(X[:, 0], X[:, 1], color=colors, s=10, alpha=0.8)\nplt.title('Covariance type = tied')\nplt.axis('equal')\nplt.axis('off')\nplt.plot(gmm.means_[:,0],gmm.means_[:,1], 'ok')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-23-output-1.png){width=540 height=409}\n:::\n:::\n\n\n## Non-Skewed Clusters\n\nPerhaps you believe in even more restricted shapes: all clusters should have their axes aligned with the coordinate axes.\n\nThat is, clusters are not skewed.\n\nThen you only need to estimate the diagonals of the covariance matrices - just $Kd$ parameters.\n\nThis is specified by the GMM parameter `covariance_type='diag'`.\n\n--- \n\n::: {#e3f9bc20 .cell execution_count=24}\n``` {.python .cell-code}\nX = np.r_[np.dot(rng.standard_normal((n_samples, 2)), C),\n          0.7 * rng.standard_normal((n_samples, 2))]\ngmm = sklearn.mixture.GaussianMixture(n_components=4, covariance_type='diag')\ny_pred = gmm.fit_predict(X)\n```\n:::\n\n\n::: {#591b00de .cell execution_count=25}\n``` {.python .cell-code}\ncolors = ['bgrky'[p] for p in y_pred]\nplt.scatter(X[:, 0], X[:, 1], color = colors, s = 10, alpha = 0.8)\nplt.axis('equal')\nplt.axis('off')\nplt.plot(gmm.means_[:,0], gmm.means_[:,1], 'oc')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-25-output-1.png){width=540 height=389}\n:::\n:::\n\n\n## Round Clusters\n\nFinally, if you believe that all clusters should be round, then you only need to estimate the $K$ variances.  \n\nThis is specified by the GMM parameter `covariance_type='spherical'`.\n\n--- \n\n::: {#aa1862b0 .cell execution_count=26}\n``` {.python .cell-code}\nX = np.r_[np.dot(rng.standard_normal((n_samples, 2)), C),\n          0.7 * rng.standard_normal((n_samples, 2))]\ngmm = sklearn.mixture.GaussianMixture(n_components=2, covariance_type='spherical')\ny_pred = gmm.fit_predict(X)\n```\n:::\n\n\n::: {#a63a4cf7 .cell execution_count=27}\n``` {.python .cell-code}\ncolors = ['bgrky'[p] for p in y_pred]\nplt.scatter(X[:, 0], X[:, 1], color = colors, s = 10, alpha = 0.8)\nplt.axis('equal')\nplt.axis('off')\nplt.plot(gmm.means_[:,0], gmm.means_[:,1], 'oc')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](09-Clustering-IV-GMM-EM_files/figure-html/cell-27-output-1.png){width=540 height=389}\n:::\n:::\n\n\n::: {.content-hidden when-profile=\"web\"}\n# Summary\n\nToday we covered:\n\n- Maximum likelihood estimators\n- Gaussian mixture models\n- Expectation Maximization\n\nA major benefit of GMMs is they are soft clustering technique that allows you to capture non-spherical clusters.\n:::\n\n",
    "supporting": [
      "09-Clustering-IV-GMM-EM_files"
    ],
    "filters": [],
    "includes": {}
  }
}