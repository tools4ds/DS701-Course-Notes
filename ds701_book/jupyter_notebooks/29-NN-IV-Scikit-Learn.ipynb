{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: 'NN IV -- Neural Networks in Scikit-Learn'\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/29-NN-IV-Scikit-Learn.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.display import Image, HTML\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In previous lectures, we built neural networks from scratch and used PyTorch. Now we'll explore scikit-learn's neural network capabilities, which provide a simpler, high-level interface for many common tasks.\n",
        "\n",
        "::: {.callout-note}\n",
        "While PyTorch and TensorFlow are more powerful for complex deep learning tasks, scikit-learn's `MLPClassifier` and `MLPRegressor` are excellent for:\n",
        "\n",
        "* Rapid prototyping\n",
        "* Small to medium-sized datasets\n",
        "* Integration with scikit-learn pipelines\n",
        "* Cases where you need a simple, fast neural network solution\n",
        ":::\n",
        "\n",
        "# Multi-Layer Perceptron in Scikit-Learn\n",
        "\n",
        "## MLPClassifier and MLPRegressor\n",
        "\n",
        "Scikit-learn provides two main classes for neural networks:\n",
        "\n",
        "* **`MLPClassifier`**: Multi-layer Perceptron classifier\n",
        "* **`MLPRegressor`**: Multi-layer Perceptron regressor\n",
        "\n",
        "Both use the same underlying architecture but differ in their output layer and loss function.\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## MLPClassifier and MLPRegressor\n",
        ":::\n",
        "\n",
        "Key features:\n",
        "\n",
        "* **Multiple hidden layers**: Specify architecture with a tuple\n",
        "* **Various activation functions**: `'relu'`, `'tanh'`, `'logistic'`, `'identity'`\n",
        "* **Multiple solvers**: `'adam'`, `'sgd'`, `'lbfgs'`\n",
        "* **Regularization**: L2 penalty parameter `alpha`\n",
        "* **Early stopping**: Automatic validation-based stopping\n",
        "\n",
        "## Basic Architecture\n",
        "\n",
        "The architecture is specified as a tuple of hidden layer sizes:\n",
        "\n",
        "```python\n",
        "# Single hidden layer with 100 neurons\n",
        "hidden_layer_sizes=(100,)\n",
        "\n",
        "# Two hidden layers with 100 and 50 neurons\n",
        "hidden_layer_sizes=(100, 50)\n",
        "\n",
        "# Three hidden layers\n",
        "hidden_layer_sizes=(128, 64, 32)\n",
        "```\n",
        "\n",
        "The input and output layers are automatically determined from the data.\n",
        "\n",
        "# Classification Example: MNIST Digits\n",
        "\n",
        "## Load and Explore the Data\n",
        "\n",
        "Let's classify handwritten digits using the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load MNIST data (this may take a moment)\n",
        "print(\"Loading MNIST dataset...\")\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False, parser='auto')\n",
        "\n",
        "# Convert labels to integers (they come as strings from fetch_openml)\n",
        "y = y.astype(int)\n",
        "\n",
        "# Use a subset for faster training in this demo\n",
        "# Remove this line to use the full dataset\n",
        "X, _, y, _ = train_test_split(X, y, train_size=10000, stratify=y, random_state=42)\n",
        "\n",
        "print(f\"Dataset shape: {X.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(y))}\")\n",
        "print(f\"Label type: {y.dtype}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Load and Explore the Data\n",
        ":::\n",
        "\n",
        "Split into training and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Load and Explore the Data\n",
        ":::\n",
        "\n",
        "Visualize some examples:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(X_train[i].reshape(28, 28), cmap='gray')\n",
        "    ax.set_title(f'Label: {y_train[i]}')\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing\n",
        "\n",
        "Neural networks work best with normalized data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Scale features to [0, 1] range (pixels are already in [0, 255])\n",
        "X_train_scaled = X_train / 255.0\n",
        "X_test_scaled = X_test / 255.0\n",
        "\n",
        "print(f\"Feature range: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-tip}\n",
        "Alternatively, you could use `StandardScaler()` to normalize to zero mean and unit variance, which is often preferred for neural networks.\n",
        ":::\n",
        "\n",
        "## Create and Train the MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Create MLP with 2 hidden layers\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(128, 64),  # Two hidden layers\n",
        "    activation='relu',              # ReLU activation\n",
        "    solver='adam',                  # Adam optimizer\n",
        "    alpha=0.0001,                   # L2 regularization\n",
        "    batch_size=64,                  # Mini-batch size\n",
        "    learning_rate_init=0.001,       # Initial learning rate\n",
        "    max_iter=20,                    # Number of epochs\n",
        "    random_state=42,\n",
        "    verbose=True                    # Print progress\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"Training MLP...\")\n",
        "mlp.fit(X_train_scaled, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Create and Train the MLP\n",
        ":::\n",
        "\n",
        "The `verbose=True` parameter shows the loss at each iteration, similar to what we saw in our custom implementation and PyTorch.\n",
        "\n",
        "## Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Make predictions\n",
        "y_pred = mlp.predict(X_test_scaled)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nTest Accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Evaluate the Model\n",
        ":::\n",
        "\n",
        "Detailed classification report:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Evaluate the Model\n",
        ":::\n",
        "\n",
        "Visualize the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=mlp.classes_)\n",
        "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
        "plt.title('Confusion Matrix for MNIST Classification')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize Training Progress\n",
        "\n",
        "Scikit-learn's MLP stores the loss at each iteration:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "#| fig-align: center\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(mlp.loss_curve_)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss Curve')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-note}\n",
        "The loss curve shows how the model's error decreases during training. A smooth decreasing curve indicates good convergence.\n",
        ":::\n",
        "\n",
        "## Visualize Predictions\n",
        "\n",
        "Let's look at some predictions and their confidence:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "# Get prediction probabilities\n",
        "y_pred_proba = mlp.predict_proba(X_test_scaled)\n",
        "\n",
        "# Visualize some predictions\n",
        "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(X_test[i].reshape(28, 28), cmap='gray')\n",
        "    pred_label = y_pred[i]\n",
        "    true_label = y_test[i]\n",
        "    confidence = y_pred_proba[i].max()\n",
        "    \n",
        "    color = 'green' if pred_label == true_label else 'red'\n",
        "    ax.set_title(f'True: {true_label}, Pred: {pred_label}\\nConf: {confidence:.2f}', \n",
        "                 color=color)\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Regression Example: California Housing\n",
        "\n",
        "## Load and Prepare Data\n",
        "\n",
        "Now let's use `MLPRegressor` for a regression task:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X_housing = housing.data\n",
        "y_housing = housing.target\n",
        "\n",
        "print(f\"Dataset shape: {X_housing.shape}\")\n",
        "print(f\"Features: {housing.feature_names}\")\n",
        "print(f\"Target: Median house value (in $100,000s)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Load and Prepare Data\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Split the data\n",
        "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n",
        "    X_housing, y_housing, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale the features (important for neural networks!)\n",
        "scaler = StandardScaler()\n",
        "X_train_h_scaled = scaler.fit_transform(X_train_h)\n",
        "X_test_h_scaled = scaler.transform(X_test_h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train MLP Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "mlp_reg = MLPRegressor(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    activation='relu',\n",
        "    solver='adam',\n",
        "    alpha=0.001,\n",
        "    batch_size=32,\n",
        "    learning_rate_init=0.001,\n",
        "    max_iter=100,\n",
        "    random_state=42,\n",
        "    verbose=False,\n",
        "    early_stopping=True,        # Use validation set for early stopping\n",
        "    validation_fraction=0.1,    # 10% of training data for validation\n",
        "    n_iter_no_change=10         # Stop if no improvement for 10 iterations\n",
        ")\n",
        "\n",
        "print(\"Training MLP Regressor...\")\n",
        "mlp_reg.fit(X_train_h_scaled, y_train_h)\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Train MLP Regressor\n",
        ":::\n",
        "\n",
        "The `early_stopping=True` parameter automatically reserves some training data for validation and stops training when the validation score stops improving.\n",
        "\n",
        "## Evaluate Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "\n",
        "# Make predictions\n",
        "y_pred_h = mlp_reg.predict(X_test_h_scaled)\n",
        "\n",
        "# Calculate metrics\n",
        "mse = mean_squared_error(y_test_h, y_pred_h)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(y_test_h, y_pred_h)\n",
        "r2 = r2_score(y_test_h, y_pred_h)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"Root Mean Squared Error: {rmse:.4f}\")\n",
        "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Evaluate Regression Model\n",
        ":::\n",
        "\n",
        "Visualize predictions vs actual values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Scatter plot\n",
        "axes[0].scatter(y_test_h, y_pred_h, alpha=0.5)\n",
        "axes[0].plot([y_test_h.min(), y_test_h.max()], \n",
        "             [y_test_h.min(), y_test_h.max()], 'r--', lw=2)\n",
        "axes[0].set_xlabel('Actual Values')\n",
        "axes[0].set_ylabel('Predicted Values')\n",
        "axes[0].set_title('Predicted vs Actual House Prices')\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Residual plot\n",
        "residuals = y_test_h - y_pred_h\n",
        "axes[1].scatter(y_pred_h, residuals, alpha=0.5)\n",
        "axes[1].axhline(y=0, color='r', linestyle='--', lw=2)\n",
        "axes[1].set_xlabel('Predicted Values')\n",
        "axes[1].set_ylabel('Residuals')\n",
        "axes[1].set_title('Residual Plot')\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Evaluate Regression Model\n",
        ":::\n",
        "\n",
        "Training and validation loss curves:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "#| fig-align: center\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(mlp_reg.loss_curve_, label='Training Loss')\n",
        "plt.plot(mlp_reg.validation_scores_, label='Validation Score (R²)')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Training Progress with Early Stopping')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "## Grid Search\n",
        "\n",
        "One of the advantages of scikit-learn is easy integration with hyperparameter tuning tools:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "#| warning: false\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid (simplified for faster execution)\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "    'activation': ['relu'],\n",
        "    'alpha': [0.0001, 0.001]\n",
        "}\n",
        "\n",
        "# Create MLP with fewer iterations for faster grid search\n",
        "mlp_grid = MLPClassifier(\n",
        "    max_iter=20,\n",
        "    random_state=42,\n",
        "    early_stopping=True,\n",
        "    validation_fraction=0.1,\n",
        "    n_iter_no_change=5,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"Running Grid Search (this may take a while)...\")\n",
        "# Use a smaller subset for the grid search demo\n",
        "X_grid = X_train_scaled[:1500]\n",
        "y_grid = y_train[:1500]\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    mlp_grid, \n",
        "    param_grid, \n",
        "    cv=3,           # 3-fold cross-validation\n",
        "    n_jobs=2,       # Limit parallel jobs for better stability\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "grid_search.fit(X_grid, y_grid)\n",
        "print(\"\\nBest parameters:\", grid_search.best_params_)\n",
        "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Grid Search\n",
        ":::\n",
        "\n",
        "Visualize grid search results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "#| fig-align: center\n",
        "#| warning: false\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "\n",
        "# Get top configurations\n",
        "n_configs = min(10, len(results_df))\n",
        "top_results = results_df.nlargest(n_configs, 'mean_test_score')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(range(len(top_results)), top_results['mean_test_score'])\n",
        "plt.yticks(range(len(top_results)), \n",
        "           [f\"Config {i+1}\" for i in range(len(top_results))])\n",
        "plt.xlabel('Mean CV Score')\n",
        "plt.title('Top Hyperparameter Configurations')\n",
        "plt.grid(True, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop configurations:\")\n",
        "print(top_results[['params', 'mean_test_score', 'std_test_score']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparison: Scikit-Learn vs PyTorch\n",
        "\n",
        "## When to Use Each\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "### Scikit-Learn MLP\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Simple, high-level API\n",
        "* Easy integration with scikit-learn pipelines\n",
        "* Built-in cross-validation and grid search\n",
        "* Good for small to medium datasets\n",
        "* Minimal boilerplate code\n",
        "\n",
        "**Best for:**\n",
        "\n",
        "* Rapid prototyping\n",
        "* Standard ML workflows\n",
        "* Small to medium datasets (< 100K samples)\n",
        "* When you need scikit-learn compatibility\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "### PyTorch\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "* Full control over architecture\n",
        "* GPU acceleration\n",
        "* Advanced architectures (CNNs, RNNs, Transformers)\n",
        "* Dynamic computation graphs\n",
        "* Production deployment tools\n",
        "\n",
        "**Best for:**\n",
        "\n",
        "* Large datasets (> 100K samples)\n",
        "* Complex architectures\n",
        "* GPU-accelerated training\n",
        "* Research and experimentation\n",
        "* Production deep learning systems\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Code Comparison\n",
        "\n",
        "Let's compare the code for creating a simple MLP:\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "### Scikit-Learn\n",
        "\n",
        "```python\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Define and train\n",
        "mlp = MLPClassifier(\n",
        "    hidden_layer_sizes=(128, 64),\n",
        "    activation='relu',\n",
        "    max_iter=100\n",
        ")\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "predictions = mlp.predict(X_test)\n",
        "```\n",
        ":::\n",
        "\n",
        "::: {.column width=\"50%\"}\n",
        "### PyTorch\n",
        "\n",
        "```python\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(784, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 10)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Training loop required...\n",
        "```\n",
        ":::\n",
        "::::\n",
        "\n",
        "::: {.callout-tip}\n",
        "For most standard tasks with moderate-sized datasets, scikit-learn's MLP is perfectly adequate and much simpler to use. Save PyTorch for when you need more power and flexibility.\n",
        ":::\n",
        "\n",
        "# Advanced Features\n",
        "\n",
        "## Learning Rate Schedules\n",
        "\n",
        "Scikit-learn supports adaptive learning rates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Adaptive learning rate\n",
        "mlp_adaptive = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    learning_rate='adaptive',      # Decrease learning rate when loss plateaus\n",
        "    learning_rate_init=0.01,\n",
        "    max_iter=50,\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "mlp_adaptive.fit(X_train_scaled[:5000], y_train[:5000])\n",
        "print(f\"Final accuracy: {mlp_adaptive.score(X_test_scaled, y_test):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Warm Start\n",
        "\n",
        "You can continue training from where you left off:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Initial training\n",
        "mlp_warm = MLPClassifier(\n",
        "    hidden_layer_sizes=(100,),\n",
        "    max_iter=10,\n",
        "    warm_start=True,    # Allow continued training\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"Initial training (10 iterations)...\")\n",
        "mlp_warm.fit(X_train_scaled[:5000], y_train[:5000])\n",
        "print(f\"Accuracy after 10 iterations: {mlp_warm.score(X_test_scaled, y_test):.4f}\")\n",
        "\n",
        "# Continue training\n",
        "print(\"\\nContinued training (10 more iterations)...\")\n",
        "mlp_warm.set_params(max_iter=20)\n",
        "mlp_warm.fit(X_train_scaled[:5000], y_train[:5000])\n",
        "print(f\"Accuracy after 20 iterations: {mlp_warm.score(X_test_scaled, y_test):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Partial Fit for Online Learning\n",
        "\n",
        "For large datasets that don't fit in memory, use `partial_fit`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Create model\n",
        "mlp_online = MLPClassifier(\n",
        "    hidden_layer_sizes=(100,),\n",
        "    random_state=42,\n",
        "    warm_start=True\n",
        ")\n",
        "\n",
        "# Train in batches\n",
        "batch_size = 1000\n",
        "n_batches = len(X_train_scaled) // batch_size\n",
        "\n",
        "print(\"Training with partial_fit...\")\n",
        "for i in range(min(n_batches, 5)):  # Just 5 batches for demo\n",
        "    start_idx = i * batch_size\n",
        "    end_idx = start_idx + batch_size\n",
        "    \n",
        "    X_batch = X_train_scaled[start_idx:end_idx]\n",
        "    y_batch = y_train[start_idx:end_idx]\n",
        "    \n",
        "    # For first batch, need to specify classes\n",
        "    if i == 0:\n",
        "        mlp_online.partial_fit(X_batch, y_batch, classes=np.unique(y_train))\n",
        "    else:\n",
        "        mlp_online.partial_fit(X_batch, y_batch)\n",
        "    \n",
        "    if (i + 1) % 2 == 0:\n",
        "        score = mlp_online.score(X_test_scaled, y_test)\n",
        "        print(f\"  Batch {i+1}/{n_batches}: Test accuracy = {score:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Best Practices\n",
        "\n",
        "## 1. Data Preprocessing\n",
        "\n",
        "Always scale your features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create a pipeline with scaling and MLP\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('mlp', MLPClassifier(hidden_layer_sizes=(100,), random_state=42))\n",
        "])\n",
        "\n",
        "# The pipeline handles scaling automatically\n",
        "pipeline.fit(X_train[:1000], y_train[:1000])\n",
        "accuracy = pipeline.score(X_test, y_test)\n",
        "print(f\"Pipeline accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Cross-Validation\n",
        "\n",
        "Use cross-validation to get robust performance estimates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "mlp_cv = MLPClassifier(\n",
        "    hidden_layer_sizes=(50,),\n",
        "    max_iter=20,\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# 5-fold cross-validation\n",
        "cv_scores = cross_val_score(\n",
        "    mlp_cv, \n",
        "    X_train_scaled[:2000], \n",
        "    y_train[:2000],\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(f\"CV Scores: {cv_scores}\")\n",
        "print(f\"Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Monitor for Overfitting\n",
        "\n",
        "Use early stopping and regularization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# With early stopping and regularization\n",
        "mlp_reg = MLPClassifier(\n",
        "    hidden_layer_sizes=(100, 50),\n",
        "    alpha=0.01,              # L2 regularization\n",
        "    early_stopping=True,\n",
        "    validation_fraction=0.2,\n",
        "    n_iter_no_change=10,\n",
        "    max_iter=100,\n",
        "    random_state=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "mlp_reg.fit(X_train_scaled[:5000], y_train[:5000])\n",
        "print(f\"Training stopped at iteration: {mlp_reg.n_iter_}\")\n",
        "print(f\"Best validation score: {mlp_reg.best_validation_score_:.4f}\")\n",
        "print(f\"Test accuracy: {mlp_reg.score(X_test_scaled, y_test):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Practical Tips\n",
        "\n",
        "## Architecture Selection\n",
        "\n",
        "::: {.callout-tip}\n",
        "### Rules of Thumb\n",
        "\n",
        "1. **Start simple**: Try a single hidden layer first\n",
        "2. **Layer size**: Start with layer sizes between input and output dimensions\n",
        "3. **Deeper vs wider**: More layers can learn more complex patterns, but may overfit\n",
        "4. **Typical architectures**:\n",
        "   - Small datasets: (100,) or (50, 50)\n",
        "   - Medium datasets: (100, 50) or (128, 64, 32)\n",
        "   - Large datasets: Consider PyTorch instead\n",
        ":::\n",
        "\n",
        "## Solver Selection\n",
        "\n",
        "Different solvers work better in different scenarios:\n",
        "\n",
        "| Solver | Best For | Notes |\n",
        "|--------|----------|-------|\n",
        "| `'adam'` | Most cases | Good default, fast convergence |\n",
        "| `'sgd'` | Large datasets | Need to tune learning rate carefully |\n",
        "| `'lbfgs'` | Small datasets | Faster for small datasets, more memory |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "# Example comparing solvers\n",
        "solvers = ['adam', 'sgd', 'lbfgs']\n",
        "results = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    mlp = MLPClassifier(\n",
        "        hidden_layer_sizes=(50,),\n",
        "        solver=solver,\n",
        "        max_iter=50,\n",
        "        random_state=42,\n",
        "        verbose=False\n",
        "    )\n",
        "    mlp.fit(X_train_scaled[:2000], y_train[:2000])\n",
        "    score = mlp.score(X_test_scaled, y_test)\n",
        "    results[solver] = score\n",
        "    print(f\"{solver:10s}: {score:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Common Issues and Solutions\n",
        "\n",
        "::: {.callout-warning}\n",
        "### Convergence Warnings\n",
        "\n",
        "If you see `ConvergenceWarning`, try:\n",
        "1. Increase `max_iter`\n",
        "2. Decrease `learning_rate_init`\n",
        "3. Enable `early_stopping=True`\n",
        "4. Check if data is properly scaled\n",
        ":::\n",
        "\n",
        "::: {.callout-warning}\n",
        "### Poor Performance\n",
        "\n",
        "If accuracy is low, check:\n",
        "1. Is the data scaled/normalized?\n",
        "2. Is the architecture appropriate for the problem?\n",
        "3. Is the learning rate too high or low?\n",
        "4. Do you need more training iterations?\n",
        "5. Is regularization (`alpha`) too strong?\n",
        ":::\n",
        "\n",
        "# Summary\n",
        "\n",
        "## Recap\n",
        "\n",
        "We covered:\n",
        "\n",
        "* Scikit-learn's `MLPClassifier` and `MLPRegressor`\n",
        "* Classification example with MNIST\n",
        "* Regression example with California Housing\n",
        "* Hyperparameter tuning with Grid Search\n",
        "* Comparison with PyTorch\n",
        "* Advanced features: adaptive learning, warm start, partial fit\n",
        "* Best practices and practical tips\n",
        "\n",
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Recap\n",
        ":::\n",
        "\n",
        "**Key Takeaways:**\n",
        "\n",
        "1. Scikit-learn's MLP is great for standard ML tasks with moderate data\n",
        "2. Always preprocess/scale your data\n",
        "3. Use cross-validation and early stopping to avoid overfitting\n",
        "4. Start with simple architectures and gradually increase complexity\n",
        "5. For large-scale or complex tasks, consider PyTorch\n",
        "\n",
        "## Resources\n",
        "\n",
        "* [Scikit-learn Neural Network Documentation](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
        "* [MLPClassifier API Reference](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)\n",
        "* [MLPRegressor API Reference](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)\n",
        "* [Neural Network Models in Scikit-learn](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
        "\n",
        "## Exercise\n",
        "\n",
        "Try the following on your own:\n",
        "\n",
        "1. Train an MLP on the Iris dataset and compare with other classifiers\n",
        "2. Experiment with different architectures on MNIST\n",
        "3. Use Grid Search to find optimal hyperparameters for a regression task\n",
        "4. Build a pipeline that includes feature engineering and MLP\n",
        "5. Compare training time and accuracy between scikit-learn and PyTorch on the same task\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}