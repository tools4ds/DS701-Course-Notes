{
  "hash": "493df68f453b06ed427568b918f8ffdb",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: SVD - Low Rank Approximations\njupyter: python3\n---\n\n# Low Rank Approximations\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/10-Low-Rank-and-SVD.ipynb)\n\nWe now consider applications of the Singular Value Decomposition (SVD).\n\n> SVD is \"the Swiss Army Knife of Numerical Linear Algebra.”\n\nDianne O’Leary, MMDS ’06 (Workshop on Algorithms for Modern Massive Data Sets)\n\n## Applications\n\nWe will see how the SVD is used for\n\n:::: {.incremental}\n- low rank approximations\n- dimensionality reduction\n::::\n\n\n## Singular Vectors and Values\n\nFor \n\n$$\nA\\in\\mathbb{R}^{m\\times n} \\text{ with } m>n \\text{ and rank } k,\n$$\n\nthere exists a set of orthogonal vectors $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n\\}$ with $\\mathbf{v}_i \\in \\mathbb{R}^n$\n\nand a set of orthogonal vectors $\\{\\mathbf{u}_1, \\mathbf{u}_2, \\dots, \\mathbf{u}_n\\}$ with $\\mathbf{u}_i \\in \\mathbb{R}^m$\n\nsuch that\n\n$$\nA\\mathbf{v}_1 = \\sigma_1 \\mathbf{u}_1 \\quad\\cdots\\quad A\\mathbf{v}_k = \\sigma_k \\mathbf{u}_k, \\quad A\\mathbf{v}_{k+1} = 0 \\quad\\cdots\\quad A\\mathbf{v}_n = 0.\n$$\n\nwhere $\\sigma_1, \\sigma_2, \\dots, \\sigma_k$ are the _singular values_ of $A$.\n\n::: {.fragment}\n* $\\mathbf{u}_{k+1}, \\ldots, \\mathbf{u}_n$ are any orthonormal vectors that complete the orthonormal basis for $\\mathbb{R}^m$ and \n* $\\mathbf{v}_{k+1}, \\ldots, \\mathbf{v}_n$ are any orthonormal vectors that complete the orthonormal basis for $\\mathbb{R}^n$ and are in the null space of $A$.\n:::\n\n## Singular Value Decomposition\n\nWe can collect the vectors $\\mathbf{v}_i$ into a matrix $V$ and the vectors $\\mathbf{u}_i$ into a matrix $U$.\n$$\nA\n\\begin{bmatrix}\n\\vert & \\vert &   & \\vert \\\\\n\\mathbf{v}_1   & \\mathbf{v}_2  & \\dots  & \\mathbf{v}_n  \\\\\n\\vert & \\vert &  & \\vert\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\vert & \\vert &   & \\vert \\\\\n\\mathbf{u}_1   & \\mathbf{u}_2  & \\dots  & \\mathbf{u}_m  \\\\\n\\vert & \\vert &  & \\vert\n\\end{bmatrix}\n\\left[\n\\begin{array}{c|c}\n\\begin{matrix}\n\\sigma_1 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\sigma_k\n\\end{matrix}\n&\n\\mathbf{0}\n\\\\\n\\hline\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\n\\right]\n.\n$$\n\nWe call the $\\mathbf{v}_i$ the __right singular vectors__ and the $\\mathbf{u}_i$ the __left singular vectors__.\n\n\n## Singular Value Decomposition\n\n\nAnd because $V$ is an orthogonal matrix, we have $V V^T = I$, so we can right multiply both sides by $V^T$ to get\n\n$$\nA\n =\n\\begin{bmatrix}\n\\vert & \\vert &   & \\vert \\\\\n\\mathbf{u}_1   & \\mathbf{u}_2  & \\dots  & \\mathbf{u}_m  \\\\\n\\vert & \\vert &  & \\vert\n\\end{bmatrix}\n\\left[\n\\begin{array}{c|c}\n\\begin{matrix}\n\\sigma_1 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\sigma_k\n\\end{matrix}\n&\n\\mathbf{0}\n\\\\\n\\hline\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\n\\right]\n\\begin{bmatrix}\n\\vert & \\vert &   & \\vert \\\\\n\\mathbf{v}_1   & \\mathbf{v}_2  & \\dots  & \\mathbf{v}_n  \\\\\n\\vert & \\vert &  & \\vert\n\\end{bmatrix}^T.\n$$\n\n\n\nWe can write this as\n\n$$\nA = U\\Sigma V^{T}.\n$$\n\n\n## Singular Value Decomposition\n\nThe SVD of a matrix $A\\in\\mathbb{R}^{m\\times n}$ (where $m>n$) is\n\n$$\nA = U\\Sigma V^{T},\n$$\n\nwhere\n\n:::: {.incremental}\n- $U$ has dimension $m\\times n$. The columns of $U$ are orthogonal. The columns of $U$ are the __left singular vectors__.\n- $\\Sigma$ has dimension $n\\times n$. The only non-zero values are on the main diagonal and they are nonnegative real numbers  $\\sigma_1\\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_k$ and $\\sigma_{k+1} = \\ldots = \\sigma_n = 0$. These are called the __singular values__ of $A$.\n- $V$ has dimension $n \\times n$. The columns of $V$ are orthogonal. The columns of $V$ are the __right singular vectors__.\n::::\n\n## SVD Matrix Shapes\n\n::: {#670c110a .cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n# Create a figure and axis\nfig, ax = plt.subplots()\n\n# Draw matrix A\nrect_A = patches.Rectangle((0, 0), 2, 3, linewidth=1, edgecolor='black', facecolor='none')\nax.add_patch(rect_A)\nax.text(1, 1.5, r'$A$', fontsize=20, ha='center', va='center')\nax.text(1, -0.5, r'$(m \\times n)$', fontsize=12, ha='center', va='center')\n\n# Draw equal sign\nax.text(2.5, 1.5, r'$=$', fontsize=20, ha='center', va='center')\n\n# Draw matrix U\nrect_U = patches.Rectangle((3, 0), 2, 3, linewidth=1, edgecolor='black', facecolor='none')\nax.add_patch(rect_U)\nax.text(4, 1.5, r'$U$', fontsize=20, ha='center', va='center')\nax.text(4, -0.5, r'$(m \\times n)$', fontsize=12, ha='center', va='center')\n\n# Draw Sigma\nrect_Sigma = patches.Rectangle((5.5, 1), 2, 2, linewidth=1, edgecolor='black', facecolor='none')\nax.add_patch(rect_Sigma)\nax.text(6.5, 2, r'$\\Sigma$', fontsize=20, ha='center', va='center')\nax.text(6.5, 0.5, r'$(n \\times n)$', fontsize=12, ha='center', va='center')\n\n# Draw matrix V^T with the same dimensions as Sigma\nrect_VT = patches.Rectangle((8, 1), 2, 2, linewidth=1, edgecolor='black', facecolor='none')\nax.add_patch(rect_VT)\nax.text(9, 2, r'$V^T$', fontsize=20, ha='center', va='center')\nax.text(9, 0.5, r'$(n \\times n)$', fontsize=12, ha='center', va='center')\n\n# Set limits and remove axes\nax.set_xlim(-1, 11)\nax.set_ylim(-2, 4)\nax.axis('off')\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-2-output-1.png){width=763 height=389 fig-align='center'}\n:::\n:::\n\n\n## SVD Properties\n\n<br><br>\n\n* The SVD of a matrix always exists.\n\n    * The existence of the SVD was proven in 1936 by [Carl Eckart and Gale Young](https://www.cambridge.org/core/journals/psychometrika/article/abs/approximation-of-one-matrix-by-another-of-lower-rank/B29672E1EDD0FA1B7611D4DFAFC321B3?utm_campaign=shareaholic&utm_medium=copy_link&utm_source=bookmark).\n\n* The singular values are uniquely determined.\n\n* The left and right singular vectors are uniquely determined up to $\\pm 1$.\n\n## Outer Products\n\nThe SVD can also be represented as a sum of outer products\n\n$$ \nA = \\sum_{i=1}^{n} \\sigma_{i}\\mathbf{u}_i\\mathbf{v}_{i}^{T},\n$$\n\nwhere $\\mathbf{u}_i, \\mathbf{v}_{i}$ are the $i$-th columns of $U$ and $V$, respectively.\n\n## Outer Products, continued\n\nAn outer product of a $m\\times 1$ vector and a $1\\times n$ vector is a $m\\times n$ matrix.\n\n$$\\mathbf{u}_i\\mathbf{v}_{i}^{T}=\n\\begin{bmatrix}\nu_{i1} \\\\\nu_{i2} \\\\\n\\vdots \\\\\nu_{im} \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nv_{i1} & v_{i2} & \\cdots & v_{in} \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\nu_{i1}v_{i1} & u_{i1}v_{i2} & \\cdots & u_{i1}v_{in} \\\\\nu_{i2}v_{i1} & u_{i2}v_{i2}  & \\cdots & u_{i2}v_{in} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nu_{im}v_{i1} &  u_{im}v_{i2}  & \\cdots & u_{im}v_{in} \\\\\n\\end{bmatrix}\n$$\n\nIt is a rank-1 matrix.  How can you tell?\n\n:::: {.fragment}\n**Alternate Interpretation:**\n\nThe SVD decomposes $A$ into a linear combination of rank-1 matrices. \n\nThe singular value tells us the weight (contribution) of each rank-1 matrix to the matrix $A$.\n::::\n\n# Lecture Organization\n\nIn this lecture we first discuss:\n\n* Theoretical properties of the SVD related to\n    * matrix rank\n    * determining the best low rank approximations to a matrix\n\n:::: {.fragment}\n* We will then apply these results when we consider data matrices from the following applications \n    * internet traffic data\n    * social media data\n    * image data\n    * movie data\n\n:::\n\n# SVD Properties\n\n## Matrix Rank\n\n::: {.content-visible when-profile=\"web\"}\nLet's review some definitions.\n:::\n\nLet $A\\in\\mathbb{R}^{m\\times n}$ be a real matrix such that with $m>n$.\n\n:::: {.fragment}\nThe __rank__ of $A$ is the number of linearly independent rows or columns of the matrix. \n::::\n\n:::: {.fragment}\nThe largest value that a matrix rank can take is $\\min(m,n)$. Since we assumed $m>n$, the largest value of the rank is $n$.\n::::\n\n:::: {.fragment}\nIf the matrix $A$ has rank equal to $n$, then we say it is **full rank**.\n::::\n\n:::: {.fragment}\nHowever, it can happen that the rank of a matrix is __less__ than $\\min(m,n)$. In this case we say that $A$ is **rank-deficient**.\n::::\n\n## Matrix Rank and Column Space\n\nThe dimension of the column space of $A$ is the **smallest number of vectors that suffice to construct the columns of $A$.**\n\nAnd it is equal to the rank of the matrix.\n\n:::: {.fragment}\nIf the dimension of the column spaces is $k$, then there exists a set of vectors $\\{\\mathbf{c}_1, \\mathbf{c}_2, \\dots, \\mathbf{c}_k\\}$ such that every column $\\mathbf{a}_i$ of $A$ can be expressed as:\n\n$$\\mathbf{a}_i = r_{1i}\\mathbf{c}_1 + r_{2i}\\mathbf{c}_2 + \\dots + r_{ki}\\mathbf{c}_k\\quad i=1,\\ldots,n.$$\n::::\n\n\n---\n\nTo store a matrix $A \\in \\mathbb{R}^{m\\times n}$ we need to store $mn$ values.\n\nHowever, if $A$ has rank $k$, it can be factored as $A = CR$,\n$$\nA =\n\\begin{bmatrix} \n\\bigg| & \\bigg| &   & \\bigg| \\\\\n\\mathbf{c}_1   & \\mathbf{c}_2  & \\dots  & \\mathbf{c}_k  \\\\\n\\bigg| & \\bigg| &  & \\bigg|\n\\end{bmatrix}\n\\begin{bmatrix}\n\\vert & \\vert &   & \\vert \\\\\n\\mathbf{r}_1   & \\mathbf{r}_2  & \\dots  & \\mathbf{r}_n  \\\\\n\\vert & \\vert &  & \\vert\n\\end{bmatrix}\n$$\n\nwhere $C \\in \\mathbb{R}^{m\\times k}$ and $R \\in \\mathbb{R}^{k \\times n}$.\n\nThis requires $k(m+n)$ values, which could be much smaller than $mn$ when $k$ is small, e.g. $k < \\frac{mn}{m+n}$.\n\n\n\n## Low Effective Rank\n\nIn many situations we want to __approximate__ a matrix $A$ with a low-rank matrix $A^{(k)}.$\n\n:::: {.fragment}\nTo talk about when one matrix *approximates* another, we need a norm for matrices.  \n::::\n\n:::: {.fragment}\nWe will use the __Frobenius norm__, which is defined as\n\n$$\\Vert A\\Vert_F = \\sqrt{\\sum a_{ij}^2}.$$\n::::\n\n:::: {.fragment}\nObserve that this is the $\\ell_2$ norm for a vectorized matrix, i.e., by  stacking the columns of the matrix $A$ to form a vector of length $mn$. \n::::\n\n---\n\nTo quantify when one matrix is *close* to another, we define the distance function:\n\n$$ \n\\operatorname{dist}(A,B) = \\Vert A-B\\Vert_F. \n$$\n\nThis can be viewed as Euclidean distance between $mn$-dimensional vectors.\n\nWe define the optimal __rank-$k$ approximation__ to $A$ as \n\n$$\nA^{(k)} =\\mathop{\\arg\\min}\\limits_{\\{B~|~\\operatorname{Rank} B = k\\}} \\Vert A-B\\Vert_F.\n$$\n\nIn other words, $A^{(k)}$ is the closest rank-$k$ matrix to $A$.\n\n## Finding Rank-$k$ Approximations\n\nHow can we find the optimal rank-$k$ approximation to a matrix $A$?\n\n:::: {.fragment}\nThe __Singular Value Decomposition (SVD).__\n::::\n\n:::: {.fragment}\nWhy?\n::::\n\n:::: {.fragment}\nThe SVD  gives the best rank-$k$ approximation to $A$ for __every__ $k$ up to the rank of $A$.\n::::\n\n---\n\nTo form the best rank-$k$ approximation to using the SVD you calculate\n\n$$ A^{(k)} = U'\\Sigma'(V')^T,$$\n\nwhere\n\n:::: {.incremental}\n* $U'$ are the $k$ leftmost columns of $U$, \n* $\\Sigma'$ is the $k\\times k$ upper left sub-matrix of $\\Sigma$, and \n* $V'$ is the $k$ leftmost columns of $V$ (or the $k$ topmost rows of $V^T$)\n::::\n\n---\n\nFull SVD:\n\n$$\nA =\n\\begin{bmatrix}\n\\vert & \\vert &   & \\vert \\\\\n\\mathbf{u}_1   & \\mathbf{u}_2  & \\dots  & \\mathbf{u}_m  \\\\\n\\vert & \\vert &  & \\vert\n\\end{bmatrix}\n\\left[\n\\begin{array}{c|c}\n\\begin{matrix}\n\\sigma_1 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\sigma_k\n\\end{matrix}\n&\n\\mathbf{0}\n\\\\\n\\hline\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\n\\right]\n\\begin{bmatrix}\n- & \\mathbf{v}_1^T & - \\\\\n- & \\mathbf{v}_2^T & - \\\\\n  & \\vdots &   \\\\\n- & \\mathbf{v}_n^T & -\n\\end{bmatrix}\n$$\n\nRank-$k$ SVD:\n\n$$\nA =\n\\begin{bmatrix}\n\\vert & \\vert &   & \\vert \\\\\n\\mathbf{u}_1   & \\mathbf{u}_2  & \\dots  & \\mathbf{u}_k  \\\\\n\\vert & \\vert &  & \\vert\n\\end{bmatrix}\n\\left[\n\\begin{array}{c|c}\n\\begin{matrix}\n\\sigma_1 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\sigma_k\n\\end{matrix}\n&\n\\mathbf{0}\n\\\\\n\\hline\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\n\\right]\n\\begin{bmatrix}\n- & \\mathbf{v}_1^T & - \\\\\n- & \\mathbf{v}_2^T & - \\\\\n  & \\vdots &   \\\\\n- & \\mathbf{v}_k^T & -\n\\end{bmatrix}\n$$\n\n---\n\nFor a matrix $A$ of rank $n$, we can prove that\n\n$$\\Vert A-A^{(k)}\\Vert_F^2 = \\sum_{i=k+1}^n\\sigma^2_i.$$\n\nThis means that the distance (in Frobenius norm) of the best rank-$k$ approximation $A^{(k)}$ from $A$ is equal to $\\sqrt{\\sum_{i=k+1}^n\\sigma^2_i}$.\n\n# Low Rank Approximations in Practice \n\n## Models are simplifications\n\n::: {.fragment}\nOne way of thinking about modeling or clustering is that we are building a \n__simplification__ of the data. \n:::\n\n:::: {.fragment}\nThat is, a model is a description of the data, that is simpler than the data.\n:::\n\n:::: {.fragment}\nIn particular, instead of thinking of the data as thousands or millions of \nindividual data points, we might think of it in terms of a small number of \nclusters, or a parametric distribution, etc.\n:::\n\n:::: {.fragment}\nFrom this simpler description, we hope to gain __insight.__\n:::\n\n:::: {.fragment}\nThere is an interesting question here:  __why__ does this process often lead to insight?   \n:::\n\n:::: {.fragment}\nThat is, why does it happen so often that a large dataset can be described in\nterms of a much simpler model?\n:::\n\n## William of Ockham\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n![](figs/L10-William-of-Ockham.png){width=80%}\n\n[Source](https://commons.wikimedia.org/w/index.php?curid=5523066)\n:::\n::: {.column width=\"60%\"}\nWilliam of Ockham (c. 1300 AD) said:\n\n:::: {.fragment}\n\n> Non sunt multiplicanda entia sine necessitate\n::::\n\n:::: {.fragment}\nor, in other words:\n\n> Entities must not be multiplied beyond necessity.\n::::\n\n:::: {.fragment}\nby which he meant:\n\n> Among competing hypotheses, the one with the fewest assumptions should be selected.\n::::\n\n:::\n::::\n\n::: aside\nThis has come to be known as \"Occam's razor.\"\n:::\n\n\n## Occam's Razor\n\nWilliam was saying that it is more common for a set of observations to be determined by a simple process than a complex process.\n\n:::: {.fragment}\nIn other words, the world is full of simple (but often hidden) patterns.\n::::\n\n:::: {.fragment}\nFrom which one can justify the observation that *modeling works surprisingly often*.\n::::\n\n## Low Effective Rank of Data Matrices\n\nIn general, a data matrix $A\\in\\mathbb{R}^{m\\times n}$  is usually __full rank__, meaning that $\\operatorname{Rank}(A)\\equiv p = \\min(m, n)$.\n\n:::: {.fragment}\nHowever, it is possible to encounter data matrices that have __low effective rank__.\n::::\n\n:::: {.fragment}\nThis means that we can approximate $A$ by some $A^{(k)}$ for which $k \\ll p$.\n::::\n\n:::: {.fragment}\nFor any data matrix, we can judge when this is the case by looking at its singular values, because the singular values tell us the distance to the nearest rank-$k$ matrix.\n::::\n\n## Traffic Data\n\nLet's see how this theory can be used in practice  and investigate some real data.\n\nWe'll look at data traffic on the Abilene network:\n\n![](figs/L10-Abilene-map.png){fig-align=\"center\" width=\"200px}\n\nSource: Internet2, circa 2005\n\n---\n\n::: {#c77a83b7 .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nwith open('data/net-traffic/AbileneFlows/odnames','r') as f:\n    odnames = [line.strip() for line in f]\ndates = pd.date_range('9/1/2003', freq = '10min', periods = 1008)\nAtraf = pd.read_table('data/net-traffic/AbileneFlows/X', sep='  ', header=None, names=odnames, engine='python')\nAtraf.index = dates\nAtraf.head(4)\n```\n\n::: {.cell-output .cell-output-display execution_count=114}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ATLA-ATLA</th>\n      <th>ATLA-CHIN</th>\n      <th>ATLA-DNVR</th>\n      <th>ATLA-HSTN</th>\n      <th>ATLA-IPLS</th>\n      <th>ATLA-KSCY</th>\n      <th>ATLA-LOSA</th>\n      <th>ATLA-NYCM</th>\n      <th>ATLA-SNVA</th>\n      <th>ATLA-STTL</th>\n      <th>...</th>\n      <th>WASH-CHIN</th>\n      <th>WASH-DNVR</th>\n      <th>WASH-HSTN</th>\n      <th>WASH-IPLS</th>\n      <th>WASH-KSCY</th>\n      <th>WASH-LOSA</th>\n      <th>WASH-NYCM</th>\n      <th>WASH-SNVA</th>\n      <th>WASH-STTL</th>\n      <th>WASH-WASH</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2003-09-01 00:00:00</th>\n      <td>8466132.0</td>\n      <td>29346537.0</td>\n      <td>15792104.0</td>\n      <td>3646187.0</td>\n      <td>21756443.0</td>\n      <td>10792818.0</td>\n      <td>14220940.0</td>\n      <td>25014340.0</td>\n      <td>13677284.0</td>\n      <td>10591345.0</td>\n      <td>...</td>\n      <td>53296727.0</td>\n      <td>18724766.0</td>\n      <td>12238893.0</td>\n      <td>52782009.0</td>\n      <td>12836459.0</td>\n      <td>31460190.0</td>\n      <td>105796930.0</td>\n      <td>13756184.0</td>\n      <td>13582945.0</td>\n      <td>120384980.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-01 00:10:00</th>\n      <td>20524567.0</td>\n      <td>28726106.0</td>\n      <td>8030109.0</td>\n      <td>4175817.0</td>\n      <td>24497174.0</td>\n      <td>8623734.0</td>\n      <td>15695839.0</td>\n      <td>36788680.0</td>\n      <td>5607086.0</td>\n      <td>10714795.0</td>\n      <td>...</td>\n      <td>68413060.0</td>\n      <td>28522606.0</td>\n      <td>11377094.0</td>\n      <td>60006620.0</td>\n      <td>12556471.0</td>\n      <td>32450393.0</td>\n      <td>70665497.0</td>\n      <td>13968786.0</td>\n      <td>16144471.0</td>\n      <td>135679630.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-01 00:20:00</th>\n      <td>12864863.0</td>\n      <td>27630217.0</td>\n      <td>7417228.0</td>\n      <td>5337471.0</td>\n      <td>23254392.0</td>\n      <td>7882377.0</td>\n      <td>16176022.0</td>\n      <td>31682355.0</td>\n      <td>6354657.0</td>\n      <td>12205515.0</td>\n      <td>...</td>\n      <td>67969461.0</td>\n      <td>37073856.0</td>\n      <td>15680615.0</td>\n      <td>61484233.0</td>\n      <td>16318506.0</td>\n      <td>33768245.0</td>\n      <td>71577084.0</td>\n      <td>13938533.0</td>\n      <td>14959708.0</td>\n      <td>126175780.0</td>\n    </tr>\n    <tr>\n      <th>2003-09-01 00:30:00</th>\n      <td>10856263.0</td>\n      <td>32243146.0</td>\n      <td>7136130.0</td>\n      <td>3695059.0</td>\n      <td>28747761.0</td>\n      <td>9102603.0</td>\n      <td>16200072.0</td>\n      <td>27472465.0</td>\n      <td>9402609.0</td>\n      <td>10934084.0</td>\n      <td>...</td>\n      <td>66616097.0</td>\n      <td>43019246.0</td>\n      <td>12726958.0</td>\n      <td>64027333.0</td>\n      <td>16394673.0</td>\n      <td>33440318.0</td>\n      <td>79682647.0</td>\n      <td>16212806.0</td>\n      <td>16425845.0</td>\n      <td>112891500.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows × 121 columns</p>\n</div>\n```\n:::\n:::\n\n\n---\n\n::: {#63ce0ed2 .cell execution_count=4}\n``` {.python .cell-code code-fold=\"false\"}\nAtraf.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=115}\n```\n(1008, 121)\n```\n:::\n:::\n\n\nAs we would expect, our traffic matrix has rank 121:\n\n::: {#139dc903 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\nnp.linalg.matrix_rank(Atraf)\n```\n\n::: {.cell-output .cell-output-display execution_count=116}\n```\nnp.int64(121)\n```\n:::\n:::\n\n\nHowever -- perhaps it has low __effective__ rank.\n\nThe `numpy` routine for computing the SVD is `np.linalg.svd`:\n\n::: {#60454301 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"false\"}\nu, s, vt = np.linalg.svd(Atraf)\n```\n:::\n\n\n---\n\nNow let's look at the singular values of `Atraf` to see if it can be usefully approximated as a low-rank matrix:\n\n::: {#45b2f8e0 .cell execution_count=7}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(5, 3))\nplt.plot(range(1, 1+len(s)), s)\nplt.xlabel(r'$k$', size=20)\nplt.ylabel(r'$\\sigma_k$', size=20)\nplt.ylim(ymin=0)\nplt.xlim(xmin=-1)\nplt.title(r'Singular Values of $A$', size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-7-output-1.png){width=453 height=324 fig-align='center'}\n:::\n:::\n\n\nThis classic, sharp-elbow tells us that a few singular values are very large, and most singular values are quite small.\n\n---\n\nZooming in for just small $k$ values, we can see that the elbow is around 4 - 6 singular values:\n\n::: {#0f9cbc3b .cell execution_count=8}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(5, 3))\nAnorm = np.linalg.norm(Atraf)\nplt.plot(range(1, 21), s[0:20]/Anorm, '.-')\nplt.xlim([0.5, 20])\nplt.ylim([0, 1])\nplt.xlabel(r'$k$', size=20)\nplt.xticks(range(1, 21))\nplt.ylabel(r'$\\sigma_k$', size=20);\nplt.title(r'Singular Values of $A$',size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-8-output-1.png){width=462 height=324 fig-align='center'}\n:::\n:::\n\n\nThis pattern of singular values suggests __low effective rank.__\n\n---\n\nLet's use the formula above to compute the relative error of a rank-$k$ approximation to $A$:\n\n::: {#cad3ca67 .cell execution_count=9}\n``` {.python .cell-code}\nfig = plt.figure(figsize=(5, 3))\nAnorm = np.linalg.norm(Atraf)\nerr = np.cumsum(s[::-1]**2)\nerr = np.sqrt(err[::-1])\nplt.plot(range(0, 20), err[:20]/Anorm, '.-')\nplt.xlim([0, 20])\nplt.ylim([0, 1])\nplt.xticks(range(1, 21))\nplt.xlabel(r'$k$', size = 16)\nplt.ylabel(r'relative F-norm error', size=16)\nplt.title(r'Relative Error of rank-$k$ approximation to $A$', size=16)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-9-output-1.png){width=492 height=315 fig-align='center'}\n:::\n:::\n\n\nRemarkably, we are down to 9% relative error using only a rank 20 approximation to $A$.\n\n---\n\nSo instead of storing \n\n* $mn =$ (1008 $\\cdot$ 121) = 121,968 values, \n\nwe only need to store \n\n* $k(m+n)$ = 20 $\\cdot$ (1008 + 121) = 22,580 values, \n\nwhich is an 81% reduction in size.\n\n## Low Effective Rank is Common\n\nIn practice __many__ datasets have low effective rank.   \n\nWe consider the following examples:\n\n:::: {.incremental}\n* Likes on Facebook,\n* Yelp reviews and Tweets (the site formerly known as Twitter),\n* User preferences over time,\n* Images.\n::::\n\n## Likes on Facebook\n\nHere, the matrices are \n\n:::: {.incremental}\n1. Number of likes:  Timebins $\\times$ Users\n2. Number of likes:  Users $\\times$ Page Categories\n3. Entropy of likes across categories:  Timebins $\\times$ Users\n::::\n\n:::: {.fragment}\n![](figs/L10-facebook.png){fig-align=\"center\" width=\"50%\"}\n\nSource: [Viswanath et al., Usenix Security, 2014]\n::::\n\n## Social Media Activity\n\nHere, the matrices are \n\n:::: {.incremental}\n1. Number of Yelp reviews:  Timebins $\\times$ Users\n2. Number of Yelp reviews:  Users $\\times$ Yelp Categories\n3. Number of Tweets:  Users $\\times$ Topic Categories\n::::\n\n:::: {.fragment}\n![](figs/L10-yelp-twitter.png){fig-align=\"center\" width=\"50%\"}\n\nSource: [Viswanath et al., Usenix Security, 2014]\n::::\n\n\n## Netflix\n\nExample: the Netflix prize worked with partially-observed (sparse) rating matrices like this:\n\n::: {.columns}\n::: {.column width=\"50%\"}\n\n\n$$\n\\begin{bmatrix}\n & & & \\vdots & & & \\\\\n & & 3 & 2 & & 1 &\\\\\n & 1 & & 1 & & & \\\\\n\\dots & & 2 & & 4 & & \\dots\\\\\n & 5 & 5 & & 4 & & \\\\\n & 1 & & & 1 & 5 & \\\\\n & & & \\vdots & & & \\\\\n\\end{bmatrix},\n$$\n\n:::\n::: {.column width=\"50%\"}\n\n\n\n:::: {.fragment}\nwhere the rows correspond to users, the columns to movies, and the entries are ratings.\n::::\n\n:::: {.fragment}\nAlthough the problem matrix was of size 500,000 $\\times$ 18,000, the winning approach modeled the matrix as having __rank 20 to 40.__\n\nSource: [Koren et al, IEEE Computer, 2009]\n::::\n\n:::\n::: \n\n## Images\n\nImage data often shows low effective rank.\n\nFor example, here is an original $512 \\times 512$ photo:\n\n::: {#b6e1719f .cell execution_count=10}\n``` {.python .cell-code}\nboat = np.loadtxt('data/images/boat/boat.dat')\nimport matplotlib.cm as cm\nplt.figure()\nplt.imshow(boat, cmap=cm.Greys_r)\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-10-output-1.png){width=389 height=389 fig-align='center'}\n:::\n:::\n\n\n---\n\nTreat the image as a matrix and then compute the singular values.\n\n::: {#9c28e5f1 .cell execution_count=11}\n``` {.python .cell-code}\nu, s, vt = np.linalg.svd(boat, full_matrices=False)\nplt.plot(s)\nplt.xlabel('$k$', size=16)\nplt.ylabel(r'$\\sigma_k$', size=16)\nplt.title('Singular Values of Boat Image', size=16)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-11-output-1.png){width=841 height=462 fig-align='center'}\n:::\n:::\n\n\n---\n\nThis image is 512 $\\times$ 512. As a matrix, it has rank of 512.   \n\nBut its _effective_ rank is low.\n\nBased on the previous plot, its effective rank is perhaps 40.\n\nLet's find the closest rank-40 matrix and view it.\n\n::: {#39f8e419 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"false\"}\nu, s, vt = np.linalg.svd(boat, full_matrices=False)\ns[40:] = 0\nboatApprox = u @ np.diag(s) @ vt\n```\n:::\n\n\n::: {#aaa70159 .cell execution_count=13}\n``` {.python .cell-code}\nplt.figure(figsize=(9, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(boatApprox, cmap=cm.Greys_r)\nplt.axis('off')\nplt.title('Rank 40 Boat')\nplt.subplot(1, 2, 2)\nplt.imshow(boat, cmap=cm.Greys_r)\nplt.axis('off')\nplt.title('Full Rank 512 Boat')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-13-output-1.png){width=689 height=344 fig-align='center'}\n:::\n:::\n\n\n## Interpretations of Low Effective Rank\n\nHow can we understand the low-effective-rank phenomenon in general?\n\n:::: {.fragment}\nThere are two helpful interpretations:\n\n:::: {.incremental}\n1. Common Patterns\n2. Latent Factors\n::::\n::::\n\n## Low Rank Implies Common Patterns\n\nThe first interpretation of low-rank behavior is in answering the question:\n\n:::: {.fragment}\n\"What is the strongest pattern in the data?\"\n::::\n\n:::: {.fragment}\nRemember that using the SVD we form the low-rank approximation as\n\n$$ A^{(k)} =  U'\\Sigma'(V')^T$$\n\nand\n\n:::: {.incremental}\n\n- $U'$ are the $k$ leftmost columns of $U$, \n- $\\Sigma'$ is the $k\\times k$ upper left submatrix of $\\Sigma$, and \n- $V'$ are the $k$ leftmost columns of $V$.\n::::\n::::\n \n:::: {.fragment}\nIn this interpretation, we think of each column of $A^{(k)}$ as a combination of the columns of $U'$.\n::::\n\n:::: {.fragment}\nHow can this be helpful? \n::::\n\n\n## Common Patterns: Traffic Example\n\nConsider the set of traffic traces. There are clearly some common patterns. How can we find them?\n\n::: {#80cf7213 .cell execution_count=14}\n``` {.python .cell-code}\nwith open('data/net-traffic/AbileneFlows/odnames','r') as f:\n    odnames = [line.strip() for line in f]\ndates = pd.date_range('9/1/2003', freq='10min', periods=1008)\nAtraf = pd.read_table('data/net-traffic/AbileneFlows/X', sep='  ', header=None, names=odnames, engine='python')\nAtraf.index = dates\nplt.figure(figsize=(10, 8))\nfor i in range(1, 13):\n    ax = plt.subplot(4, 3, i)\n    Atraf.iloc[:, i-1].plot()\n    plt.title(odnames[i])\nplt.subplots_adjust(hspace=1)\nplt.suptitle('Twelve Example Traffic Traces', size=20)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-14-output-1.png){width=781 height=740 fig-align='center'}\n:::\n:::\n\n\n---\n\nLet's use as our example $\\mathbf{a}_1,$ the first column of $A$.\n\nThis happens to be the ATLA-CHIN flow.\n\nThe earlier SVD equation tells us that\n\n$$\\mathbf{a}_1 \\approx v_{11}\\sigma_1\\mathbf{u}_1 + v_{12}\\sigma_2\\mathbf{u}_2 + \\dots + v_{1k}\\sigma_k\\mathbf{u}_k.$$\n\nIn other words, $\\mathbf{u}_1$ (the first column of $U$) is the \"strongest\" pattern occurring in $A$, and its strength is measured by $\\sigma_1$.\n\n---\n\nHere is a view of the first 2 columns of $U\\Sigma$ for the traffic matrix data.\n\nThese are the strongest patterns occurring across all of the 121 traces.\n\n::: {#eff7bc93 .cell execution_count=15}\n``` {.python .cell-code}\nu, s, vt = np.linalg.svd(Atraf, full_matrices=False)\nuframe = pd.DataFrame(u @ np.diag(s), index=pd.date_range('9/1/2003', freq = '10min', periods = 1008))\nuframe[0].plot(color='r', label='Column 1')\nuframe[1].plot(label='Column 2')\nplt.legend(loc='best')\nplt.title('First Two Columns of $U$')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](10-Low-Rank-and-SVD_files/figure-revealjs/cell-15-output-1.png){width=792 height=461}\n:::\n:::\n\n\n## Low Rank Defines Latent Factors\n\nThe next interpretation of low-rank behavior is that it exposes \"latent factors\" that describe the data.\n\n:::: {.fragment}\nIn this interpretation, we think of each element of $A^{(k)}=U'\\Sigma'(V')^T$ as the inner product of a row of $U'\\Sigma'$ and a column of $(V')^{T}$ (equivalently a row of $V'$).\n::::\n\n:::: {.fragment}\nLet's say we are working with a matrix of users and items.\n::::\n\n:::: {.fragment}\nIn particular, let the items be movies and matrix entries be ratings, as in the Netflix prize.\n::::\n\n## Latent Factors: Netflix example\n\nRecall the structure from earlier:\n\n$$\n\\begin{bmatrix}\n\\vdots & \\vdots &  & \\vdots \\\\\n\\mathbf{a}_{1} & \\mathbf{a}_{2} & \\cdots & \\mathbf{a}_{n} \\\\\n\\vdots & \\vdots &  & \\vdots \\\\\n\\end{bmatrix} \n\\approx\n\\underbrace{\n\\begin{bmatrix}\n\\vdots &  \\vdots  \\\\\n\\sigma_1 \\mathbf{u}_1 & \\sigma_k \\mathbf{u}_{k} \\\\\n\\vdots& \\vdots  \\\\\n\\end{bmatrix}\n}_{\\tilde{U}\\in\\mathbb{R}^{m\\times k}}\n\\underbrace{\n\\begin{bmatrix}\n\\cdots & \\mathbf{v}_{1}^{T} & \\cdots   \\\\\n\\cdots & \\mathbf{v}_{k}^{T} & \\cdots   \\\\\n\\end{bmatrix}\n}_{\\tilde{V}\\in\\mathbb{R}^{k\\times n}},\n$$\n\nwhere the rows of $A$ are the users and the columns are movie ratings.\n\nThen the rating that a user gives a movie is the inner product of a $k$-element vector that corresponds to the user, and a $k$-element vector that corresponds to the movie.\n\nIn other words we have:\n    \n$$ \na_{ij} = \\sum_{p=1}^{k} \\tilde{U}_{ip} \\tilde{V}_{pj}.\n$$\n\n---\n\nWe can think of user $i$'s preferences as being captured by row $i$ of $\\tilde{U}$, which is a point in $\\mathbb{R}^k$.  \n\n:::: {.fragment}\nWe have described everything we need to know to predict user $i$'s ratings via a $k$-element vector.\n::::\n\n:::: {.fragment}\nThe $k$-element vector is called a __latent factor.__\n::::\n\n:::: {.fragment}\nLikewise, we can think of column $j$ of $\\tilde{V}$ as a \"description\" of movie $j$ (another latent factor).\n::::\n\n:::: {.fragment}\nThe value in using latent factors comes from the summarization of user preferences, and the predictive power one obtains.\n::::\n\n:::: {.fragment}\nFor example, the winning entry in the Netflix prize competition modeled user preferences with 20 latent factors.\n::::\n\n:::: {.fragment}\nThe remarkable thing is that a person's preferences for all 18,000 movies can be reasonably well captured in a vector of dimension 20!\n::::\n\n---\n\nHere is a figure from the paper that described the winning strategy in the Netflix prize.\n\nIt shows a hypothetical __latent space__ in which each user, and each movie, is represented by a latent vector.\n\n![](figs/L10-Movie-Latent-Space.png){fig-align=\"center\" width=\"45%\"}\n\nSource: Koren et al, IEEE Computer, 2009 \n\nIn practice, this is perhaps a 20- or 40-dimensional space.\n\n---\n\nHere are some representations of movies in that space (reduced to 2-D).\n\nNotice how the space seems to capture similarity among movies!\n\n![](figs/L10-Netflix-Latent-Factors.png){fig-align=\"center\" width=\"55%\"}\n\nSource: Koren et al, IEEE Computer, 2009 \n\n## Summary\n\n:::: {.incremental}\n* When we are working with data matrices, it is valuable to consider the __effective rank__.\n* Many (many) datasets in real life show __low effective rank__.\n* This property can be explored precisely using the Singular Value Decomposition of the matrix.\n* When low effective rank is present\n    * the matrix can be compressed with only small loss of accuracy,\n    * we can extract the *strongest* patterns in the data,\n    * we can describe each data item in terms of the inner product of __latent factors__.\n::::\n\n",
    "supporting": [
      "10-Low-Rank-and-SVD_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}