---
title: Distances and Time Series
jupyter: python3
---

[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/05-Distances-Timeseries.ipynb)

We will start building some tools for making comparisons of data objects with particular attention to time series.

Working with data, we can encounter a wide variety of different data objects

::: {.incremental}
* records of users,
* images,
* videos,
* text (webpages, books),
* strings (DNA sequences), and
* time series.
:::

::: {.fragment}
How can we compare them?
:::

::: {.content-visible when-profile="slides"}
## Lecture Overview

We cover the following topics in today's lecture

:::: {.incremental}
- feature space and matrix representations of data,
- metrics, norms, similarity, and dissimilarity,
- bit vectors, sets, and time series.
::::
:::

::: {.content-visible when-profile="slides"}
## Time Series Data
Some examples of time series data

:::: {.incremental}
- stock prices,
- weather data,
- electricity consumption,
- website traffic,
- retail sales, and
- various economic indicators.
::::
:::

## Feature space representation

Usually a data object consists of a set of attributes.

These are also commonly called __features.__

* ("J. Smith", 25, \$ 200,000)
* ("M. Jones", 47, \$ 45,000)

If all $d$ dimensions are real-valued then we can visualize each data object as a point in a $d$-dimensional vector space.
 
* `(25, USD 200000)` $\rightarrow \begin{bmatrix}25\\200000\end{bmatrix}$.

Likewise If all features are binary then we can think of each data object as a binary vector in vector space.

The space is called __feature space.__

::: {.content-hidden when-profile="web"}
## One-hot encoding
:::
Vector spaces are such a useful tool that we often use them even for non-numeric data.

For example, consider a categorical variable that can be only one of "house", "tree", or "moon". For such a variable, we can use a __one-hot__ encoding.  

::: {.content-hidden when-profile="slides"}
We would encode as follows:
:::

::: {.incremental}
* `house`: $[1, 0, 0]$
* `tree`:  $[0, 1, 0]$
* `moon`:  $[0, 0, 1]$
:::

::: {.fragment}
So an encoding of `(25, USD 200000, 'house')` could be: 
$$\begin{bmatrix}25\\200000\\1\\0\\0\end{bmatrix}.$$
::: 

## Why not alternative encoding?

Why would we not encode as follows:


* `house`: $[1]$
* `tree`:  $[2]$
* `moon`:  $[3]$

::: {.fragment}
So an encoding of `(25, USD 200000, 'house')` could be: 
$$\begin{bmatrix}25\\200000\\1\end{bmatrix}.$$
::: 

::: {.fragment}
This is not a good idea because it implicitly assumes that the categories are ordered.

One hot encoding treats the categorical data as orthogonal from a vector space perspective.
:::

::: {.content-hidden when-profile="web"}
## Encodings
:::

We will see many other encodings that take non-numeric data and encode them into vectors or matrices.

For example, there are vector or matrix encodings for

::: {.incremental}
* graphs,
* images, and
* text.
:::

## Matrix representation of data

We generally store data in a matrix form as

$$ 
\mbox{$m$ data objects}\left\{\begin{array}{c}\;\\\;\\\;\\\;\\\;\end{array}\right.\;\;\overbrace{\left[\begin{array}{ccccc}
\begin{array}{c} x_{11} \\ \vdots \\ x_{i1} \\ \vdots \\ x_{m1} \end{array}&
\begin{array}{c} \cdots  \\ \ddots \\ \cdots  \\ \ddots \\ \cdots  \end{array}&
\begin{array}{c} x_{1j} \\ \vdots \\ x_{ij} \\ \vdots \\ x_{mj} \end{array}&
\begin{array}{c} \cdots  \\ \ddots \\ \cdots  \\ \ddots \\ \cdots  \end{array}&
\begin{array}{c} x_{1n} \\ \vdots \\ x_{in} \\ \vdots \\ x_{mn} \end{array}
\end{array}\right]}^{\mbox{$n$ features}} 
$$

The number of rows is denoted by $m$ and the number of columns by $n$. The rows are instances or records of data and the columns are the features.

## Metrics

A metric is a function $d(x, y)$ that satisfies the following properties.

:::: {.columns}
::: {.column width="70%"}
::: {.incremental}
* $d(x, x) = 0$
* $d(x, y) > 0 \hspace{1cm} \forall x\neq y$ (positivity)
* $d(x, y) = d(y, x)$ (symmetry)
* $d(x, y)\leq d(x, z) + d(z, y)$ (triangle inequality)
:::
:::
::: {.column width="30%"}
:::: {.fragment}
![](figs/TriangleInequality.png){fig-align="center"}
::::
:::
::::

::: {.fragment}
We can use a metric to determine how __similar__ or __dissimilar__ two objects are.

A metric is a measure of the dissimilarity between two objects. The larger the
measure, the more dissimilar the objects are.

If the objects are vectors, then the metric is also commonly called a __distance__.
:::

::: {.content-visible when-profile="web"}
Sometimes we will use "distance" informally, i.e., to refer to a similarity or
dissimilarity function even if we are not sure it is a metric.   

We'll try to say "dissimilarity" in those cases though.
:::

::: {.content-hidden when-profile="web"}
## Distance Matrices
:::

The distance matrix is defined as

$$ 
\mbox{$m$ data objects}\left\{\begin{array}{c}\;\\\;\\\;\\\;\\\;\end{array}\right.\;\;
\overbrace{\left[\begin{array}{ccccc}
\begin{array}{c} 0  \\  d(x_1, x_2) \\ d(x_1,x_3) \\ \vdots \\ d(x_1,x_m)  \end{array} &
\begin{array}{c} \; \\  0      \\ d(x_2,x_3) \\ \vdots \\ d(x_2,x_m)  \end{array} &
\begin{array}{c} \; \\ \;      \\ 0      \\ \vdots \\ \cdots   \end{array} &
\begin{array}{c} \; \\ \;      \\ \;     \\ \ddots \\ d(x_{m-1},x_m)   \end{array}  &
\begin{array}{c} \; \\ \;      \\ \;     \\ \;     \\[6pt] 0 \end{array} &
\end{array}\right]}^{\mbox{$m$ data objects}},
$$

where $x_i$ denotes the $i$-th column of the data matrix $X$.

## Norms

Let $\mathbf{u}, \mathbf{v}\in\mathbb{R}^{n}$ and $a\in\mathbb{R}$. The vector function $p(\mathbf{v})$ is called a __norm__ if


::: {.incremental}
* $p(a\mathbf{v}) = |a|p(\mathbf{v})$,
* $p(\mathbf{u} + \mathbf{v}) \leq p(\mathbf{u}) + p(\mathbf{v})$,
* $p(\mathbf{v}) = 0$ if and only if $\mathbf{v} = 0$.
:::

::: {.fragment}
__Every norm defines a corresponding metric.__ 

In particular If $p()$ is a norm, then $d(\mathbf{x}, \mathbf{y}) = p(\mathbf{x}-\mathbf{y})$ is a metric.
:::

::: {.fragment}
**But not every metric can be derived from a norm!**

For example, the discrete metric is not a norm.
$$
d(x,y) =
\begin{cases}
0 & x=y \\
1 & x \neq y
\end{cases}
$$




:::

::: {.content-visible when-profile="web"}
### $\ell_p$ norm
:::

::: {.content-visible when-profile="slides"}
## $\ell_p$ norm
:::

A general class of norms are called __$\ell_p$__ norms, where $p \geq 1.$

$$
\Vert \mathbf{x} \Vert_p = \left(\sum_{i=1}^d |x_i|^p\right)^{\frac{1}{p}}.
$$ 

The corresponding distance that an $\ell_p$ norm defines is called the _Minkowski distance._

$$
\Vert \mathbf{x} - \mathbf{y} \Vert_p = \left(\sum_{i=1}^d |x_i - y_i|^p\right)^{\frac{1}{p}}.
$$

::: {.fragment}
The choice of $p$ affects what gets emphasized in the distance calculation:

::: {.incremental}
* For small $p$ (e.g. $p=1$):
  * More emphasis on many small differences
  * Less sensitive to outliers/large differences
* For large $p$ (e.g. $p\rightarrow \infty$):
  * More emphasis on the largest difference
  * Very sensitive to outliers
:::
:::


::: {.content-visible when-profile="web"}
### $\ell_2$ norm
:::

::: {.content-visible when-profile="slides"}
## $\ell_2$ norm
:::

A special -- but very important -- case is the $\ell_2$ norm.

$$
\Vert \mathbf{x} \Vert_2 = \sqrt{\sum_{i=1}^d |x_i|^2}.
$$

We've already mentioned it: it is the __Euclidean__ norm.

The distance defined by the $\ell_2$ norm is the same as the Euclidean distance between two vectors $\mathbf{x}, \mathbf{y}$.

$$ 
\Vert \mathbf{x} - \mathbf{y} \Vert_2  = \sqrt{\sum_{i=1}^d (x_i - y_i)^2}.
$$

::: {.content-visible when-profile="web"}
### $\ell_1$ norm
:::

::: {.content-visible when-profile="slides"}
## $\ell_1$ norm
:::

Another important special case is the $\ell_1$ norm.

$$ \Vert \mathbf{x} \Vert_1 = \sum_{i=1}^d |x_i|.$$

This defines the __Manhattan__ distance, or (for binary vectors), the __Hamming__ distance:

$$ \Vert \mathbf{x} - \mathbf{y} \Vert_1 = \sum_{i=1} |x_i - y_i|.$$

![](figs/L05-manhattan-distance.png){fig-align="center" width="400px"}

::: {.content-visible when-profile="web"}
### $\ell_\infty$ norm
:::

::: {.content-visible when-profile="slides"}
## $\ell_\infty$ norm
:::

If we take the limit of the $\ell_p$ norm as $p$ gets large we get the $\ell_\infty$ norm.  

We have that

$$
\Vert \mathbf{x} \Vert_{\infty} = \max_{i} \vert x_{i} \vert .
$$

::: {.fragment}
What is the metric that this norm induces?
:::

::: {.fragment}
Answer: The $\ell_\infty$ norm induces the __Chebyshev__ distance which is also known as the __maximum__ distance.

$$
\Vert \mathbf{x} - \mathbf{y} \Vert_{\infty} = \max_{i} \vert x_{i} - y_{i} \vert .
$$
:::

::: {.content-visible when-profile="web"}
## $\ell_0$ norm

Another related idea is the $\ell_0$ "norm," which is not a norm, but is in a sense what we get from the $p$-norm for $p = 0$.

Note that this is __not__ a norm, but it gets called that anyway.   

This "norm" simply counts the number of __nonzero__ elements in a vector.

This is called the vector's __sparsity.__
:::

::: {.content-hidden when-profile="web"}
## Visualizing norms
:::

Here is the notion of a "circle" under each of three norms.

That is, for each norm, the set of vectors having norm 1, or distance 1 from the origin.

:::: {.columns}
::: {.column width="50%"}
![](figs/L5-Vector-Norms.png){fig-align="center"}
:::
::: {.column width="50%"}

<br><br>
$|x_1| + |x_2| = 1$

<br><br>
$\sqrt{x_1^2 + x_2^2} = 1$

<br><br>
$\max(|x_1|, |x_2|) = 1$
:::
::::


[Source](https://commons.wikimedia.org/w/index.php?curid=678101)


::: {.content-visible when-profile="slides"}
## What norm should I use?

The choice of norm depends on the characteristics of your data and the problem you're trying to solve.

:::: {.columns}
::: {.column width="33%"}
__$\ell_1$ norm__

- Use when your data is sparse.
- Robust to outliers.
:::
::: {.column width="33%"}
__$\ell_2$ norm__

- Use when measuring distances in Euclidean space.
- Smooth and differentiable.
:::
::: {.column width="33%"}
__$\ell_\infty$ norm__

- Use when you need uniform bounds.
- Limits maximum deviation.
:::
::::

:::


## Similarity and Dissimilarity Functions

:::: {.columns}
::: {.column width="44%"}
Similarity functions quantify how similar two objects are. The higher the similarity score, the more alike the objects.

:::: {.fragment}
__Examples__

:::: {.incremental}
- cosine similarity,
- Jaccard similarity _(intersection over union)_.
::::
::::
:::
::: {.column width="55%"}
Dissimilarity functions quantifies the difference between two objects. The higher the dissimilarity score, the more different the objects are.

:::: {.fragment}
__Examples__

:::: {.incremental}
- Manhattan distance,
- Hamming distance.
::::
::::
:::
::::

::: {.content-visible when-profile="web"}
### Similarity
:::

::: {.content-visible when-profile="slides"}
## Similarity
:::

We know that the inner product of two vectors can be used to compute the __cosine of the angle__ between them

$$ \cos(\theta) = \frac{\mathbf{x}^T\mathbf{y}}{\Vert\mathbf{x}\Vert \Vert\mathbf{y}\Vert} \equiv \cos(\mathbf{x}, \mathbf{y})  .$$

This value is close to 1 when $\mathbf{x} \approx \mathbf{y}$. We can use this formula to define a __similarity__ function called the __cosine similarity__ $\cos(\mathbf{x}, \mathbf{y})$.

::: {.content-visible when-profile="web"}
### Dissimilarity
:::

::: {.content-visible when-profile="slides"}
## Dissimilarity
:::

:::: {.fragment}
Given a similarity function $s(\mathbf{x}, \mathbf{y})$, how could we convert it to a dissimilarity function $d(\mathbf{x}, \mathbf{y})$?
::::

:::: {.fragment}
Two straightforward ways of doing that are:

$$d(\mathbf{x},\mathbf{y}) = 1\,/\,s(\mathbf{x},\mathbf{y})$$

or 

$$d(\mathbf{x},\mathbf{y}) = k - s(\mathbf{x},\mathbf{y})$$

for some properly chosen $k$.
::::

:::: {.fragment}
For cosine similarity, one often uses:
    
$$ d(\mathbf{x}, \mathbf{y}) = 1 - \cos(\mathbf{x}, \mathbf{y})$$
::::

::: {.content-visible when-profile="web"}
Note however that this is __not a metric!__
:::


## Bit vectors and Sets

When working with bit vectors, the $\ell_1$ metric is commonly used and is called the __Hamming__ distance.

![](figs/L5-hamming-1.png){fig-align="center" width="500px"}

This has a natural interpretation: "how well do the two vectors match?"

Or: "What is the smallest number of bit flips that will convert one vector into the other?"

::: {.content-hidden when-profile="web"}
## Hamming distance
:::

![](figs/L5-hamming-2.png){fig-align="center" width="700px"}

::: {.content-visible when-profile="web"}
In other cases, the Hamming distance is not a very appropriate metric.
:::

::: {.content-hidden when-profile="web"}
## Hamming distance with sets
:::

Consider the case in which the bit vector is being used to represent a _set_.

**Definition**: A _set_ is an unordered collection of distinct elements.

In that case, Hamming distance measures the __size of the set difference.__

For example, consider two documents. We will use bit vectors to represent the sets of words in each document.

:::: {.incremental}
* Case 1: both documents are large, almost identical, but differ in 10 words.
* Case 2: both documents are small, disjoint, have 5 words each.
::::

:::: {.fragment}
What is the Hamming distance for each case?
:::

:::: {.fragment}
$L_1(\text{Case 1}) = 10$ and $L_1(\text{Case 2}) = 5$. Is case 1 really more different than case 2? 
:::

:::: {.fragment}
What matters is not just the size of the set difference, but the size of the intersection.
::::


::: {.content-hidden when-profile="web"}
## Jaccard similarity
:::
This leads to the _Jaccard_ similarity or _Intersection over Union (IoU)_:

$$
J_{Sim}(\mathbf{x}, \mathbf{y}) = \frac{|\mathbf{x} \cap \mathbf{y}|}{|\mathbf{x} \cup \mathbf{y}|}.
$$

:::: {.columns}
::: {.column width="60%"}
This takes on values from 0 to 1, so a natural dissimilarity metric is $1 - J_{Sim}().$

In fact, this is a __metric!__

$$
J_{Dist}(\mathbf{x}, \mathbf{y}) = 1- \frac{|\mathbf{x} \cap \mathbf{y}|}{|\mathbf{x} \cup \mathbf{y}|}.
$$
:::
::: {.column width="40%"}
![](figs/L5-jaccard-1.png){fig-align="center"}
:::
::::


::: {.content-hidden when-profile="web"}
## Jaccard Similarity Example 1
:::
    
::: {.content-visible when-profile="web"}
Let's revisit the previously introduces cases comparing documents.
:::

Case 1: Very large almost identical documents.

![](figs/L5-jaccard-2.png){fig-align="center" width="500px"}

Here $J_{Sim}(\mathbf{x}, \mathbf{y})$ is almost 1.

::: {.content-hidden when-profile="web"}
## Jaccard Similarity Example 2
:::

Case 2: Very small disjoint documents.

![](figs/L5-jaccard-3.png){fig-align="center"}

Here $J_{Sim}(\mathbf{x}, \mathbf{y})$ is 0.

## Time Series

A time series is a sequence of real numbers, representing the measurements of a real variable at (possibly equal) time intervals.

Some examples are

::: {.incremental}
* stock prices,
* the volume of sales over time, and
* daily temperature readings.
:::

::: {.fragment}
A time series database is a large collection of time series.
:::

## Similarity of Time Series

Suppose we wish to compare the following time series.

::: {.incremental}
* Stock price movements for companies over a time interval.
* The motion data of two people walking.
* Credit usage patterns for bank clients.
:::

:::: {.fragment}
How should we measure the "similarity" of these time series?
::::

::: {.fragment}
There are two problems to address.

::: {.incremental}
1. Defining a meaningful similarity (or distance) function.
2. Finding an efficient algorithm to compute it.
:::
:::

## Norm-based Similarity Measures

We could just view each sequence as a vector.

Then we could use a $p$-norm, e.g., $\ell_1, \ell_2,$ or $\ell_p$ to measure similarity.

::: {.fragment}

__Advantages__

::: {.incremental}    
1. Easy to compute - linear in the length of the time series (O(n)).
2. It is a metric.
:::
:::

::: {.content-hidden when-profile="web"}
## Beware of Norm-based Similarity
:::

::: {.content-visible when-profile="slides"}
:::: {.columns}
::: {.column width="40%"}
__Disadvantage__

1. May not be __meaningful!__

We may believe that $\mathbf{ts1}$ and $\mathbf{ts2}$ are the most "similar" pair of time series.
:::
::: {.column width="60%"}
![](figs/L5-ts-euclidean.png){fig-align="center"}
:::
::::
:::

::: {.content-visible when-profile="web"}
__Disadvantage__
1. May not be __meaningful!__

![](figs/L5-ts-euclidean.png){fig-align="center"}

We may believe that $\mathbf{ts1}$ and $\mathbf{ts2}$ are the most "similar" pair of time series.
:::

::: {.fragment}
However, according to Euclidean distance: 

$$ \Vert \mathbf{ts1} - \mathbf{ts2} \Vert_2 = 26.9,$$

while 

$$ \Vert \mathbf{ts1} - \mathbf{ts3} \Vert_2 = 23.2.$$
:::

::: {.content-visible when-profile="web"}
## Feature Engineering

In general, there may be different aspects of a time series that are important in different settings.

::: {.fragment}
The first step therefore is to ask yourself "what is important about time series in my application?"
:::

::: {.fragment}
This is an example of __feature engineering.__
:::
:::


::: {.content-visible when-profile="slides"}
## Feature Engineering
:::

Feature engineering is the art of computing some derived measure from your data object that makes the important properties usable in a subsequent step.

::: {.fragment}
A reasonable approach is to

::: {.incremental}    
* extract the relevant features,
* use a simple method (e.g., a norm) to define similarity over those features.
:::
:::

::: {.fragment}
In the case above, one might think of using 

::: {.incremental}
* Fourier coefficients (to capture periodicity),
* histograms,
* or something else!
:::
:::

::: {.content-visible when-profile="web"}
## Dynamic Time Warping
:::

::: {.content-visible when-profile="slides"}
## Bump Hunting
:::

One case that arises often is something like the following:  "bump hunting"

![](figs/L5-DTW-1.png){fig-align="center"} 

Both time series have the same key characteristics: four bumps.

But a one-to-one match (ala Euclidean distance) will not detect the similarity.

::: {.content-visible when-profile="web"}
(Be sure to think about why Euclidean distance will fail here.)
:::

A solution to this is called __dynamic time warping.__



::: {.content-hidden when-profile="web"}
## Dynamic Time Warping
:::

The basic idea is to allow stretching or compressing of signals along the time dimension.

::: {.fragment}
__Classic applications__

::: {.incremental}
* speech recognition
* handwriting recognition
:::
:::

::: {.fragment}
Specifically

::: {.incremental}
* Consider $X = x_1, x_2, \dots, x_n$ and $Y = y_1, y_2, \dots, y_m$.
* We are allowed to modify each sequence by duplicating, deleting, or matching elements to form $X'$ and $Y'$.
* We then search over the space of all possible alignments to find the one that minimizes the distance between $X'$ and $Y'$.
:::
:::

::: {.content-hidden when-profile="web"}
## Visualizing DTW
:::

:::: {.columns}
::: {.column width="50%"}

There is a simple way to visualize this algorithm.

Consider a matrix $M$ where $M_{ij} = |x_i - y_j|$.

$M$ measures the amount of error we get if we match $x_i$ with $y_j$. 

So we seek a __path, starting from lower left, through $M$ that minimizes the total error.__
:::

::: {.column width="50%"}
![](figs/L5-DTW-2.png){fig-align="center" width="550px"}
:::
::::

## DTW restrictions


:::: {.columns}
::: {.column width="50%"}

The basic restrictions on path are:
    
Monotonicity

* The path should not go down or to the left.

Continuity

* No elements may be skipped in a sequence.

Restrict amount of deviation from diagonal (Sakoe-Chiba constraint)

This can be solved via dynamic programming.

:::

::: {.column width="50%"}
![](figs/L5-DTW-2.png){fig-align="center" width="550px"}
:::
::::

## Dynamic Time Warping Example

```{python}
import numpy as np
import matplotlib.pyplot as plt

# sequences
x = [1,1,2,2,3]
y = [1,2,3,3,3]
n, m = len(x), len(y)

# build DTW table
dtw = np.full((n+1,m+1), np.inf)
dtw[0,0] = 0
for i in range(1,n+1):
    for j in range(1,m+1):
        cost = abs(x[i-1]-y[j-1])
        dtw[i,j] = cost + min(dtw[i-1,j], dtw[i,j-1], dtw[i-1,j-1])

# backtrack with tie-breaking: prefer diagonal
i,j = n,m
path = [(i-1,j-1)]
while (i>0 and j>0):
    candidates = [(i-1,j-1,"↖"), (i-1,j,"↑"), (i,j-1,"←")]
    i,j,_ = min(candidates, key=lambda s: (dtw[s[0],s[1]], ["↖","↑","←"].index(s[2])))
    if i>0 or j>0:
        path.append((i-1,j-1))
path.reverse()

# Print the fully aligned sequences
print("Original sequences:")
print(f"X = {x}")
print(f"Y = {y}")
print(f"\nDTW distance = {dtw[n,m]:.1f}")
print(f"\nOptimal alignment path (0-indexed): {path}")
print("\nFully aligned sequences:")
print("X_aligned: ", end="")
for i, j in path:
    print(f"{x[i]:2d}", end=" ")
print()
print("Y_aligned: ", end="")
for i, j in path:
    print(f"{y[j]:2d}", end=" ")
print()
print("\nAlignment mapping:")
for k, (i, j) in enumerate(path):
    print(f"Step {k}: X[{i}] = {x[i]} ↔ Y[{j}] = {y[j]}")

# plot cost matrix + path
plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.imshow(dtw[1:,1:], origin="lower", cmap="Blues", aspect="auto")
px, py = zip(*[(pi+1,pj+1) for pi,pj in path])
plt.plot(np.array(py)-1, np.array(px)-1, 'r', linewidth=2)
plt.colorbar(label="Cumulative Cost")
plt.title("DTW Cost Matrix with Optimal Path")
plt.xlabel("Y values")
plt.ylabel("X values")

# plot aligned sequences
plt.subplot(1, 2, 2)
x_aligned = [x[i] for i, j in path]
y_aligned = [y[j] for i, j in path]
plt.plot(range(len(x_aligned)), x_aligned, 'bo-', label='X aligned', linewidth=2)
plt.plot(range(len(y_aligned)), y_aligned, 'ro-', label='Y aligned', linewidth=2)
plt.xlabel('Alignment step')
plt.ylabel('Value')
plt.title('Aligned Sequences')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()
```

## To explore further

See 

T. Giorgino, "Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package", Journal of Statistical Software, 2003. [link](https://www.jstatsoft.org/article/view/v031i07)

And python packages such as:

* [dtw-python](https://dynamictimewarping.github.io/)
* [dtaidistance](https://dtaidistance.readthedocs.io/en/latest/index.html)

## From Time series to Strings

A closely related idea concerns strings.

The key point is that, like time series, strings are __sequences__.

Given two strings, one way to define a 'distance' between them is:

* the minimum number of __edit operations__ that are needed to transform one string into the other.

Edit operations are insertion, deletion, and substitution of single characters.

This is called __edit distance__ or __Levenshtein distance.__

## Example

For example, given strings: ``s = VIVALASVEGAS`` and ``t = VIVADAVIS``

we would like to 

* compute the edit distance, and
* obtain the optimal __alignment__.

![](figs/viva-las-vegas.png){fig-align="center" width="550px"}


A dynamic programming algorithm can also be used to find this distance, and it is __very similar to dynamic time-warping.__

In bioinformatics this algorithm is called __"Smith-Waterman" sequence alignment.__


::: {.content-visible when-profile="slides"}
## Recap

We covered the following topics

- reviewed representations of data,
- discussed metrics and norms,
- discussed similarity and dissimilarity functions,
- introduced time series, 
- feature engineering, and
- dynamic time warping.
:::