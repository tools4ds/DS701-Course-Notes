{
  "hash": "9f615e842cf69626869c2858117f7f13",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Decision Tree and Random Forest Regression\njupyter: python3\n---\n\n<!--\nAI-GENERATED CODE\nGenerated by: Cursor / Sonnet 4.5\nPrompt: \"write a short tutorial called 19-decision-tree-regression.md where you put lecture notes to explain how regression with decision trees and random foretsts work. Give intuitive explanations and use plenty of exals.\"\nDate: October 27, 2025\nPurpose: Used to create the markdown file for the decision tree and random forest regression lecture.\nThe code was reviewed and adapted.\n-->\n\n# Decision Tree and Random Forest Regression\n\n## Introduction\n\nWhile linear and logistic regression are powerful tools, they assume specific relationships between variables (linear for continuous outcomes, logistic for binary outcomes). What if the relationship is more complex? Enter **decision tree regression** and **random forest regression** - flexible methods that can capture non-linear patterns and interactions without explicit feature engineering.\n\n## What is Decision Tree Regression?\n\n### The Big Picture\n\nImagine you're estimating house prices. Instead of fitting a straight line (linear regression), you ask a series of yes/no questions:\n- Is the house bigger than 2000 sq ft?\n- Does it have more than 3 bedrooms?\n- Is it in neighborhood A or B?\n\nEach question splits your data into smaller groups, and at the end, you predict the **average price** of houses in each final group.\n\n### How It Works: The Mechanics\n\nA decision tree regression model works by:\n\n1. **Splitting the data**: Find the feature and threshold that best divides your data into two groups\n2. **Recursing**: Repeat the process on each group\n3. **Stopping**: When groups are \"pure enough\" or meet stopping criteria\n4. **Predicting**: For a new data point, follow the tree down to a leaf and return the average value of training points in that leaf\n\n### Example 1: Simple House Price Prediction\n\nLet's say we have this tiny dataset:\n\n| Square Feet | Bedrooms | Price ($1000s) |\n|-------------|----------|----------------|\n| 1200        | 2        | 200            |\n| 1400        | 2        | 220            |\n| 1600        | 3        | 280            |\n| 1800        | 3        | 300            |\n| 2000        | 4        | 380            |\n| 2200        | 4        | 400            |\n\nThe tree might look like:\n\n```\n           [Square Feet < 1700?]\n                /          \\\n              Yes           No\n              /              \\\n        [Avg: 230K]    [Bedrooms < 4?]\n                           /         \\\n                         Yes          No\n                         /             \\\n                   [Avg: 290K]    [Avg: 390K]\n```\n\n**Prediction for a 1500 sq ft, 2 BR house**: Follow left → predict $230K  \n**Prediction for a 1900 sq ft, 3 BR house**: Follow right → left → predict $290K\n\n### Example 2: Visualizing the Split\n\nConsider predicting a person's salary based on years of experience and education level.\n\n```\nData points (Years_Experience, Education_Years, Salary):\n(1, 12, 35K), (2, 12, 40K), (3, 16, 55K), (4, 16, 60K),\n(5, 18, 70K), (6, 18, 75K), (10, 18, 90K), (12, 20, 110K)\n```\n\nThe decision tree creates **rectangular regions** in the feature space:\n\n```\n        [Years_Experience < 4?]\n             /            \\\n           Yes             No\n           /                \\\n    [Edu_Years < 14?]   [Years_Exp < 8?]\n       /        \\           /         \\\n     Yes        No        Yes          No\n     /          \\         /             \\\n[Avg: 37.5K] [Avg: 57.5K] [Avg: 78.3K] [Avg: 100K]\n```\n\n**Key Insight**: Unlike linear regression which fits a single plane, decision trees partition the space into rectangles, each with its own prediction.\n\n### How Are Splits Chosen?\n\nThe algorithm minimizes **residual sum of squares (RSS)** at each split:\n\n$$RSS = \\sum_{i \\in \\text{left}} (y_i - \\bar{y}_{\\text{left}})^2 + \\sum_{j \\in \\text{right}} (y_j - \\bar{y}_{\\text{right}})^2$$\n\nWhere $\\bar{y}_{\\text{left}}$ and $\\bar{y}_{\\text{right}}$ are the average values in each resulting group.\n\n**Intuition**: Find the split that makes each side as \"homogeneous\" as possible - minimize the variance within each group.\n\n### Example 3: Temperature Prediction\n\nSuppose we're predicting daily high temperature based on the month:\n\n| Month | Temperature (°F) |\n|-------|------------------|\n| 1     | 35              |\n| 2     | 38              |\n| 3     | 45              |\n| 4     | 58              |\n| 5     | 70              |\n| 6     | 80              |\n| 7     | 85              |\n| 8     | 83              |\n| 9     | 75              |\n| 10    | 60              |\n| 11    | 48              |\n| 12    | 37              |\n\nA decision tree might split like this:\n\n```\n            [Month < 6.5?]\n               /        \\\n             Yes         No\n             /            \\\n      [Month < 3.5?]  [Month < 9.5?]\n         /      \\         /       \\\n       Yes      No      Yes        No\n       /         \\       /          \\\n   [Avg: 36°] [Avg: 58°] [Avg: 81°] [Avg: 48°]\n```\n\nThis captures the **non-linear** relationship (winter → spring → summer → fall) that linear regression would struggle with.\n\n## The Piecewise Constant Nature of Predictions\n\n### Understanding Discretized Outputs\n\nHere's a critical characteristic of decision tree regression: **outputs are always discretized**. \n\nWhen you train a tree, each leaf node stores a single prediction value (the mean of all training samples in that leaf). No matter how many different inputs you provide, the model can only output as many distinct values as it has leaf nodes.\n\n**Key Insight**: If your tree has 10 leaf nodes, it can only produce 10 unique predictions - ever.\n\n### Example: Sweeping Through Input Values\n\nLet's see this in action with a simple example:\n\n::: {#28792aa4 .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n\n# Simple quadratic relationship\nX_train = np.array([[1], [2], [3], [4], [5], [6], [7], [8]])\ny_train = np.array([10, 15, 25, 40, 60, 85, 115, 150])  # roughly x^2\n\n# Shallow tree with only 4 leaf nodes\ntree = DecisionTreeRegressor(max_depth=2, random_state=42)\ntree.fit(X_train, y_train)\n\n# Sweep through 100 input values\nX_test = np.linspace(0, 10, 100).reshape(-1, 1)\npredictions = tree.predict(X_test)\n\nprint(f\"Number of unique predictions: {len(np.unique(predictions))}\")\nprint(f\"Unique values: {np.unique(predictions)}\")\n\n# Output:\n# Number of unique predictions: 4\n# Unique values: [12.5, 32.5, 72.5, 132.5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of unique predictions: 4\nUnique values: [ 16.66666667  50.         100.         150.        ]\n```\n:::\n:::\n\n\n### Visualizing the Step Function\n\nWhen you plot predictions from a decision tree, you see a **step function**:\n\n::: {#52259105 .cell execution_count=3}\n``` {.python .cell-code}\nplt.figure(figsize=(10, 6))\n\n# Plot the step function\nplt.plot(X_test, predictions, 'r-', linewidth=2, label='Decision Tree', drawstyle='steps-post')\n\n# Plot training data\nplt.scatter(X_train, y_train, s=100, alpha=0.6, edgecolors='black', label='Training Data')\n\n# Plot the true function for comparison\ny_true = X_test**2\nplt.plot(X_test, y_true, 'g--', alpha=0.5, linewidth=2, label='True Function (x²)')\n\nplt.xlabel('Input (x)', fontsize=12)\nplt.ylabel('Prediction (y)', fontsize=12)\nplt.title('Decision Tree Creates Step Functions', fontsize=14)\nplt.legend(fontsize=11)\nplt.grid(alpha=0.3)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](18A-decision-tree-regression_files/figure-html/cell-3-output-1.png){width=819 height=528}\n:::\n:::\n\n\n**What You'll See**:\n- The red line jumps at specific thresholds (decision boundaries)\n- Between jumps, the prediction is constant\n- The output can never smoothly interpolate between values\n\n### Why Does This Happen?\n\nThe tree partitions the input space into regions:\n\n```\nRegion 1: x < 2.5  →  predict 12.5  (avg of x=1,2)\nRegion 2: 2.5 ≤ x < 5.5  →  predict 32.5  (avg of x=3,4)\nRegion 3: 5.5 ≤ x < 7.5  →  predict 72.5  (avg of x=5,6,7)\nRegion 4: x ≥ 7.5  →  predict 132.5  (avg of x=8)\n```\n\nAll inputs falling into Region 1 get the **exact same prediction**, regardless of whether x = 0.5 or x = 2.4.\n\n### Critical Implications\n\nThis discretization has several important consequences:\n\n1. **No True Extrapolation**: The model can't predict outside the range of training targets\n\n\n   ::: {#bde77e9e .cell execution_count=4}\n   ``` {.python .cell-code}\n   print(f\"Min training value: {y_train.min()}\")  # 10\n   print(f\"Max training value: {y_train.max()}\")  # 150\n   print(f\"Min prediction: {predictions.min()}\")  # 12.5 (close to 10)\n   print(f\"Max prediction: {predictions.max()}\")  # 132.5 (close to 150)\n   # Can NEVER predict 200, even if x=20!\n   ```\n   \n   ::: {.cell-output .cell-output-stdout}\n   ```\n   Min training value: 10\n   Max training value: 150\n   Min prediction: 16.666666666666668\n   Max prediction: 150.0\n   ```\n   :::\n   :::\n   \n   \n2. **Step Functions Only**: Predictions jump discontinuously at decision boundaries\n   - Input: x = 2.49 → output: 12.5\n   - Input: x = 2.51 → output: 32.5\n   - A tiny change in input causes a large jump in output\n\n3. **Limited Resolution**: With k leaf nodes, you get exactly k possible outputs\n   - Shallow tree (depth=2): ~4 leaves → 4 unique predictions\n   - Medium tree (depth=5): ~32 leaves → 32 unique predictions\n   - Deep tree (depth=10): ~1024 leaves → up to 1024 unique predictions\n\n4. **Can't Capture Smooth Trends**: Even if the true function is smooth (like sin(x) or x²), the prediction will be jagged\n\n### How Random Forests Help\n\nRandom forests partially address this limitation by **averaging many step functions**:\n\n::: {#65796961 .cell execution_count=5}\n``` {.python .cell-code}\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Random forest with 100 trees\nrf = RandomForestRegressor(n_estimators=100, max_depth=2, random_state=42)\nrf.fit(X_train, y_train)\nrf_predictions = rf.predict(X_test)\n\nprint(f\"Decision Tree - unique values: {len(np.unique(predictions))}\")  # 4\nprint(f\"Random Forest - unique values: {len(np.unique(rf_predictions))}\")  # ~47\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDecision Tree - unique values: 4\nRandom Forest - unique values: 14\n```\n:::\n:::\n\n\n**Why More Unique Values?**\n- Each of the 100 trees has its own 4 leaf values (different due to bootstrapping)\n- Averaging different combinations creates many more possible outputs\n- With 100 trees × 4 leaves = 400 individual predictions to combine\n- Result: Much smoother predictions (though still technically discrete)\n\n### Visualization: Tree vs Forest\n\n::: {#5ec1fb6d .cell execution_count=6}\n``` {.python .cell-code}\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Single tree\naxes[0].plot(X_test, predictions, 'r-', linewidth=2, drawstyle='steps-post')\naxes[0].scatter(X_train, y_train, s=100, alpha=0.6, edgecolors='black')\naxes[0].plot(X_test, y_true, 'g--', alpha=0.5, linewidth=2)\naxes[0].set_title('Single Decision Tree (4 unique values)', fontsize=12)\naxes[0].set_xlabel('x')\naxes[0].set_ylabel('y')\naxes[0].grid(alpha=0.3)\n\n# Random forest\naxes[1].plot(X_test, rf_predictions, 'b-', linewidth=2)\naxes[1].scatter(X_train, y_train, s=100, alpha=0.6, edgecolors='black')\naxes[1].plot(X_test, y_true, 'g--', alpha=0.5, linewidth=2)\naxes[1].set_title('Random Forest (47 unique values)', fontsize=12)\naxes[1].set_xlabel('x')\naxes[1].set_ylabel('y')\naxes[1].grid(alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](18A-decision-tree-regression_files/figure-html/cell-6-output-1.png){width=1334 height=470}\n:::\n:::\n\n\n**Observation**: The random forest curve is much smoother but still not perfectly continuous like the true x² function.\n\n### When Does This Matter?\n\nThe discretized nature is more problematic when:\n- ✗ You need smooth, continuous predictions (e.g., for derivatives or integration)\n- ✗ You're extrapolating far beyond training data\n- ✗ Small input changes shouldn't cause output jumps (some physical systems)\n- ✗ You have very few training samples (creating very few leaf nodes)\n\nIt's less problematic when:\n- ✓ You have lots of training data (enabling many leaf nodes)\n- ✓ You use random forests (averaging smooths the steps)\n- ✓ Prediction accuracy matters more than smoothness\n- ✓ You're interpolating within the training range\n\n### Comparison with Linear Regression\n\n::: {#7dcaac93 .cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.linear_model import LinearRegression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nlr_predictions = lr.predict(X_test)\n\nprint(f\"Linear Regression - unique values: {len(np.unique(lr_predictions))}\")\n# 100 (every input gets a unique output on the fitted line)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLinear Regression - unique values: 100\n```\n:::\n:::\n\n\nLinear regression produces a **truly continuous** function - infinitely many possible outputs along the fitted line. This is fundamentally different from tree-based methods.\n\n## Advantages and Disadvantages of Decision Trees\n\n### Advantages ✓\n\n1. **Easy to interpret**: You can literally draw the decision process\n2. **No feature scaling needed**: Decisions are based on thresholds, not magnitudes\n3. **Handles non-linearity**: No need to manually create polynomial features\n4. **Captures interactions**: Automatically considers feature combinations\n5. **Works with mixed data**: Can handle both numerical and categorical features\n\n### Disadvantages ✗\n\n1. **Overfitting**: Deep trees can memorize training data\n2. **Instability**: Small data changes can drastically alter the tree structure\n3. **Non-smooth predictions**: Predictions are step functions (piecewise constant)\n4. **Greedy algorithm**: Makes locally optimal splits, not globally optimal tree\n\n### Example 4: Overfitting in Action\n\nConsider this dataset with a true relationship: $y = x^2 + \\text{noise}$\n\n```\nTraining data (5 points):\nx: [1, 2, 3, 4, 5]\ny: [1.1, 4.2, 8.9, 16.1, 25.2]\n```\n\n**Shallow tree (max_depth=2)**:\n```\n         [x < 3.5?]\n           /      \\\n         Yes       No\n         /          \\\n    [Avg: 4.7]  [Avg: 20.7]\n```\nGeneralization: Good ✓\n\n**Deep tree (max_depth=5)**:\n```\nEach point gets its own leaf!\nx=1 → predict 1.1\nx=2 → predict 4.2\n...\n```\nGeneralization: Poor ✗ (won't predict well for x=1.5 or x=6)\n\n## Random Forest Regression: Wisdom of the Crowd\n\n### The Core Idea\n\n**Problem**: Single trees are unstable and prone to overfitting.  \n**Solution**: Build many trees and average their predictions.\n\nA **random forest** is an ensemble of decision trees, where each tree is:\n1. Trained on a **bootstrap sample** of the data (random sampling with replacement)\n2. At each split, considers only a **random subset of features**\n\n### Why Does This Work?\n\n**Intuition**: If you ask 100 people to estimate something, their average is often better than most individuals. Random forests apply this \"wisdom of the crowd\" principle.\n\n- **Bootstrap sampling** creates diversity: each tree sees slightly different data\n- **Random feature selection** reduces correlation between trees\n- **Averaging** reduces variance while maintaining low bias\n\n### Example 5: Building a Random Forest\n\nDataset: Predicting miles per gallon (MPG) from horsepower, weight, and year.\n\n```\nOriginal data (8 cars):\n(HP, Weight, Year, MPG)\n(100, 2500, 2015, 30)\n(150, 3000, 2015, 25)\n(200, 3500, 2016, 20)\n(120, 2600, 2016, 28)\n(180, 3200, 2017, 22)\n(110, 2700, 2017, 29)\n(160, 3100, 2018, 24)\n(140, 2800, 2018, 26)\n```\n\n**Tree 1**: Bootstrap sample (random with replacement)\n- Sample: rows [1, 1, 3, 4, 5, 7, 8, 8]\n- At root: randomly consider features [HP, Weight]\n- Best split: Weight < 2900 → ...\n\n**Tree 2**: Different bootstrap sample\n- Sample: rows [2, 2, 3, 4, 6, 6, 7, 8]\n- At root: randomly consider features [HP, Year]\n- Best split: HP < 140 → ...\n\n**Tree 3**: Another bootstrap sample\n- Sample: rows [1, 2, 4, 4, 5, 6, 7, 8]\n- At root: randomly consider features [Weight, Year]\n- Best split: Year < 2017 → ...\n\n... build 100 or 1000 such trees ...\n\n**Prediction**: For a car with (130, 2750, 2017):\n- Tree 1 predicts: 28.5\n- Tree 2 predicts: 27.0\n- Tree 3 predicts: 28.0\n- ... (97 more trees) ...\n- **Final prediction: average of all 100 trees** = 27.8\n\n### Example 6: Reducing Overfitting\n\nLet's revisit the $y = x^2 + \\text{noise}$ example:\n\n**Single deep tree**: Memorizes noise, poor generalization  \n**Random forest**: \n\n```\nTree 1 (bootstrap sample 1): slightly different predictions\nTree 2 (bootstrap sample 2): slightly different predictions\n...\nTree 100 (bootstrap sample 100): slightly different predictions\n\nAverage prediction: Smooths out the noise, captures the x² trend\n```\n\n**Result**: Random forests are much more robust to overfitting than individual trees.\n\n## Hyperparameters to Tune\n\n### For Decision Trees:\n\n1. **max_depth**: Maximum depth of the tree\n   - Too shallow: underfitting\n   - Too deep: overfitting\n   - Example: max_depth=5 often works well\n\n2. **min_samples_split**: Minimum samples required to split a node\n   - Higher values: simpler trees\n   - Example: min_samples_split=20\n\n3. **min_samples_leaf**: Minimum samples required in a leaf\n   - Higher values: smoother predictions\n   - Example: min_samples_leaf=10\n\n### For Random Forests:\n\n4. **n_estimators**: Number of trees in the forest\n   - More trees: better performance but slower\n   - Example: n_estimators=100 (common default)\n\n5. **max_features**: Number of features to consider for each split\n   - Default: sqrt(total features) for classification, total_features/3 for regression\n   - Lower values: more diversity between trees\n\n6. **bootstrap**: Whether to use bootstrap samples\n   - Default: True (strongly recommended)\n\n## Example 7: Python Code Walkthrough\n\n::: {#cd8acb50 .cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Generate synthetic data: y = sin(x) + noise\nnp.random.seed(42)\nX = np.sort(np.random.uniform(0, 10, 100)).reshape(-1, 1)\ny = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Single Decision Tree\ndt = DecisionTreeRegressor(max_depth=3, random_state=42)\ndt.fit(X_train, y_train)\ny_pred_dt = dt.predict(X_test)\n\n# Random Forest\nrf = RandomForestRegressor(\n    n_estimators=100, \n    max_depth=3, \n    random_state=42\n)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\n\n# Evaluate\nprint(f\"Decision Tree R²: {r2_score(y_test, y_pred_dt):.3f}\")\nprint(f\"Random Forest R²: {r2_score(y_test, y_pred_rf):.3f}\")\n\nprint(f\"Decision Tree RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_dt)):.3f}\")\nprint(f\"Random Forest RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_rf)):.3f}\")\n\n# Visualize predictions\nX_plot = np.linspace(0, 10, 500).reshape(-1, 1)\ny_dt = dt.predict(X_plot)\ny_rf = rf.predict(X_plot)\n\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.scatter(X_train, y_train, alpha=0.5, label='Train')\nplt.plot(X_plot, y_dt, 'r-', linewidth=2, label='Decision Tree')\nplt.plot(X_plot, np.sin(X_plot), 'g--', alpha=0.5, label='True function')\nplt.legend()\nplt.title('Decision Tree Regression')\n\nplt.subplot(1, 2, 2)\nplt.scatter(X_train, y_train, alpha=0.5, label='Train')\nplt.plot(X_plot, y_rf, 'b-', linewidth=2, label='Random Forest')\nplt.plot(X_plot, np.sin(X_plot), 'g--', alpha=0.5, label='True function')\nplt.legend()\nplt.title('Random Forest Regression')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDecision Tree R²: 0.883\nRandom Forest R²: 0.897\nDecision Tree RMSE: 0.205\nRandom Forest RMSE: 0.193\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](18A-decision-tree-regression_files/figure-html/cell-8-output-2.png){width=1142 height=374}\n:::\n:::\n\n\n**Expected Output**:\n- Decision Tree: Step-like predictions (piecewise constant)\n- Random Forest: Smoother predictions averaging many step functions\n\n## Example 8: Real-World Application - Housing Prices\n\n::: {#d9c5feca .cell execution_count=9}\n``` {.python .cell-code}\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\n\n# Load data\nhousing = fetch_california_housing()\nX, y = housing.data, housing.target\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Note: No need to scale features for tree-based methods!\n# (But we would need to for linear regression)\n\n# Train Random Forest\nrf = RandomForestRegressor(\n    n_estimators=100,\n    max_depth=10,\n    min_samples_split=10,\n    random_state=42,\n    n_jobs=-1  # Use all CPU cores\n)\n\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\n\nprint(f\"R² Score: {r2_score(y_test, y_pred):.3f}\")\nprint(f\"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.3f}\")\n\n# Feature importance\nimportances = rf.feature_importances_\nfeatures = housing.feature_names\n\nfor name, imp in sorted(zip(features, importances), \n                        key=lambda x: x[1], \n                        reverse=True):\n    print(f\"{name}: {imp:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nR² Score: 0.773\nRMSE: 0.545\nMedInc: 0.600\nAveOccup: 0.140\nLatitude: 0.076\nLongitude: 0.076\nHouseAge: 0.047\nAveRooms: 0.030\nAveBedrms: 0.016\nPopulation: 0.016\n```\n:::\n:::\n\n\n**Interpretation**: Feature importance shows which variables matter most for prediction. This is a huge advantage over linear regression where coefficient interpretation can be tricky.\n\n## When to Use Which Method?\n\n### Use Decision Trees when:\n- ✓ Interpretability is crucial\n- ✓ You have limited data\n- ✓ You need a quick baseline model\n- ✓ Features are mostly categorical\n\n### Use Random Forests when:\n- ✓ Predictive accuracy is the priority\n- ✓ You have enough data (100+ samples)\n- ✓ The relationship is complex/non-linear\n- ✓ You need feature importance rankings\n- ✓ You want robust predictions with less tuning\n\n### Use Linear Regression when:\n- ✓ The relationship is truly linear\n- ✓ You need to extrapolate beyond training data\n- ✓ You need smooth predictions\n- ✓ Coefficient interpretation is important\n\n## Key Takeaways\n\n1. **Decision trees** partition the feature space into rectangles and predict the average within each region\n2. **Splitting** is done greedily to minimize variance (RSS) in resulting groups\n3. **Single trees** are interpretable but prone to overfitting and instability\n4. **Random forests** build many diverse trees through bootstrapping and random feature selection\n5. **Averaging predictions** reduces variance while maintaining the trees' ability to capture non-linearity\n6. **No feature scaling needed** for tree-based methods\n7. **Feature importance** is automatically computed and very useful\n8. **Hyperparameter tuning** is important but random forests are fairly robust to default settings\n\n## Advanced Topics (Brief)\n\n### Out-of-Bag (OOB) Error\nSince each tree uses only ~63% of the data (due to bootstrap sampling), the remaining ~37% can be used for validation without a separate test set.\n\n### Extremely Randomized Trees (Extra Trees)\nInstead of finding the best split, random thresholds are tried for random features. Even faster and sometimes more robust.\n\n### Gradient Boosting\nInstead of averaging trees in parallel (bagging), build trees sequentially where each tree corrects errors of previous ones. Often achieves better performance (XGBoost, LightGBM, CatBoost).\n\n## Practice Problems\n\n1. **Conceptual**: Why can't a decision tree extrapolate beyond the range of training data? (Hint: think about what happens in the leaves)\n\n2. **Applied**: Load a dataset with a non-linear relationship and compare:\n   - Linear regression\n   - Polynomial regression (degree 2)\n   - Decision tree (vary max_depth)\n   - Random forest\n   \n   Plot predictions and compare R² scores.\n\n3. **Challenge**: The random forest gives you feature importances, but what if two features are highly correlated? How does this affect interpretation?\n\n## Summary\n\nDecision tree and random forest regression are powerful tools in the data scientist's toolkit. They handle non-linearity naturally, require minimal preprocessing, and provide excellent predictive performance. While single trees offer interpretability, random forests sacrifice some interpretability for superior accuracy and robustness. Understanding when and how to use these methods will significantly expand your regression modeling capabilities.\n\n---\n\n*Next steps*: Explore gradient boosting methods (XGBoost, LightGBM) which often outperform random forests, especially on structured/tabular data competitions.\n\n",
    "supporting": [
      "18A-decision-tree-regression_files"
    ],
    "filters": [],
    "includes": {}
  }
}