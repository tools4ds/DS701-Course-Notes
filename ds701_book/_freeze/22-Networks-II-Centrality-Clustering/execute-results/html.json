{
  "hash": "e4014035bd736524c527ced9d4c04394",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Network Centrality and Clustering\njupyter: python3\n---\n\n\n\n# Overview\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/22-Networks-II-Centrality-Clustering.ipynb)\n\nWe previously introduced the concept of a graph to describe networks. To further analyze networks we will discuss network centrality and clustering.\n\n## Motivation\n\nA common question in the analysis of networks is to understand the relative *importance* of the nodes in the network.\n\nFor example:\n\n* in a social network, who are the most influential individuals?\n* on the Web, which pages are more informative?\n* in road network, which intersections are most heavily used?\n\nThe key idea is that the __structure__ of the network should give us some information about the relative importance of the nodes in the network.\n\n## Centrality and Clustering\n\nTo analyze networks we want to be able to classify:\n\n* important __nodes__, and\n* important __groups of nodes__.\n\nDetermining which nodes are important nodes leads to the notion of __centrality__.\n\nDetermining which groups of nodes are important leads to the notion of __clustering__ and __partitioning__.\n\nIn order to classify important nodes and groups of nodes, we will use graph theory and linear algebra.\n\n# Centrality\n\nDo some nodes in the network have a special role? Are some nodes more *important* than others?\n\nThese are questions of __centrality__ (or __prestige__).  \n\n## Definition\n\n* __Centrality__ in graph theory and network analysis refers to measures that identify the most important vertices (nodes) within a graph. \n* These measures assign numbers or rankings to nodes based on their position and influence within the network. \n* Centrality can help determine which nodes are most influential, critical for connectivity, or central to the flow of information.\n\nWe will study three basic notions of centrality:\n\n1. __Closeness Centrality__:  A central node is close to all others.\n1. __Betweenness Centrality__: A central node is on many paths through the network.\n1. __Eigenvector Centrality__: A central node is connected to other central nodes (sometimes called **status** centrality).\n\nWe'll look at a very famous network analysis dataset: Zachary's karate club.\n\n## Karate Club Graph\n\nThe back story: from 1970 to 1972 the anthropologist Wayne Zachary studied the social relationships inside a university karate club.\n\nWhile he was studying the club, a factional division led to a splitting of the club in two.   \n\nThe club became split between those who rallied around the club president and those who rallied around the karate instructor.\n\n::: aside\nYou can read the story of the Karate club [here](https://www.jstor.org/stable/pdf/3629752.pdf).  This dataset has become so famous that it has spawned [its own academic traditions](https://networkkarate.tumblr.com).\n:::\n\n---\n\nHere's a view of the social network of the karate club. \n\n::: {#ad59fa2a .cell execution_count=3}\n``` {.python .cell-code}\nGk = nx.karate_club_graph()\nnp.random.seed(9)\nfig = plt.figure(figsize = (12, 6))\nax1 = fig.add_subplot(121)\nnx.draw_networkx(Gk, ax = ax1, pos = nx.circular_layout(Gk), \n                 with_labels = False, node_color='skyblue')\nplt.title('Circular Layout')\nplt.axis('off')\nax2 = fig.add_subplot(122)\nnx.draw_networkx(Gk, ax = ax2, pos = nx.spring_layout(Gk), \n                 with_labels = False, node_color='skyblue')\nplt.title('Spring Layout')\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-3-output-1.png){width=912 height=483}\n:::\n:::\n\n\n## Closeness Centrality\n\nThe closeness centrality of a node $i$ is an indicator of the proximity between node $i$ and all the other nodes in the graph.\n\nLet $G=(V, E)$ be a connected graph with $n$ nodes. \n\nLet $d(i,j)$ be the shortest path distance between node $i$ and node $j$ in $G$.   \n\nThe standard way of formulating closeness centrality is the reciprocal of the total distance to all other nodes\n\n$$\n\\text{closeness}(i) = \\frac{1}{\\sum_{j \\in V} d(i,j)}.\n$$\n\n## Interpretation\n\n::: {.incremental}\n* **Large Closeness Centrality**: \n    * Indicates that a node is, on average, close to all other nodes in the network. \n    * This means it can quickly interact with or reach other nodes. \n    * Nodes with high closeness centrality are often considered central or influential because they can efficiently spread information or resources throughout the network.\n\n* **Small Closeness Centrality**: \n    * Indicates that a node is, on average, farther away from all other nodes in the network. \n    * These nodes are less central and may take longer to interact with or reach other nodes. \n    * Nodes with low closeness centrality are often on the periphery of the network and are less influential in terms of spreading information or resources.\n:::\n---\n\n::: {#60244218 .cell execution_count=4}\n``` {.python .cell-code}\n# Assuming Gk is already defined as a graph\n# Calculate closeness centrality\ncent = list(nx.closeness_centrality(Gk).values())\n\n# Set random seed for reproducibility\nnp.random.seed(9)\n\n# Create figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n# Draw the first subplot with circular layout\npos1 = nx.circular_layout(Gk)\nnodes1 = nx.draw_networkx_nodes(Gk, pos=pos1, ax=ax1, node_color=cent, cmap=plt.cm.plasma)\nnx.draw_networkx_edges(Gk, pos=pos1, ax=ax1)\nax1.set_title('Closeness Centrality')\nax1.axis('off')\n\n# Draw the second subplot with spring layout\npos2 = nx.spring_layout(Gk)\nnodes2 = nx.draw_networkx_nodes(Gk, pos=pos2, ax=ax2, node_color=cent, cmap=plt.cm.plasma)\nnx.draw_networkx_edges(Gk, pos=pos2, ax=ax2)\nax2.set_title('Closeness Centrality')\nax2.axis('off')\n\n# Add colorbar to the figure\ncbar = fig.colorbar(nodes1, ax=[ax1, ax2], orientation='horizontal', fraction=0.05, pad=0.05)\ncbar.set_label('Closeness Centrality')\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-4-output-1.png){width=763 height=449 fig-align='center'}\n:::\n:::\n\n\nIn this graph, most nodes are close to most other nodes.   \n\nHowever we can see that some nodes are slightly more central than others.\n\n## Distribution of Closeness Centrality\n\n::: {#0027c584 .cell execution_count=5}\n``` {.python .cell-code}\nplt.figure(figsize = (6, 4))\nplt.hist(cent, bins=np.linspace(0, 1, 30))\nplt.xlabel('Closeness Centrality', size = 14)\nplt.ylabel('Number of Nodes', size = 14)\nplt.title('Distribution of Closeness Centrality', size = 16)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-5-output-1.png){width=516 height=386 fig-align='center'}\n:::\n:::\n\n\n::: {.callout-tip appearance=\"minimal\"}\nHigher closeness centrality means closer to all other nodes.\n:::\n\n## Betweenness Centrality\n\nAlternatively we might be interested in the question, *is the node on many paths?*\n\nBetweenness captures how important a node is to the communication process, or *how much* information passes through the node.\n\nFirst, let's consider the case in which there is only one shortest path between any pair of nodes.\n\nThen, the betweenness centrality of node $i$ is the __number of shortest paths that pass through $i$__.\n\nMathematically we have\n\n$$\n\\text{betweenness}(i) = \\sum_{i \\neq j \\neq k \\in V}\n\\begin{cases} \n1 &\\text{if path from }j\\text{ to }k\\text{ goes through }i, \\\\\n0 &\\text{otherwise}.\n\\end{cases}\n$$\n\nFor a graph with $n$ nodes, we can normalize this to a value between 0 and 1 by\ndividing by ${n \\choose 2} = n(n-1)/2$.\n\n## General Case: Dependency\n\nIn a general graph, there may be __multiple__ shortest paths between $j$ and $k$.  \n\nTo handle this, we define:\n\n* $\\sigma(i \\mid j,k)$ is the number of shortest paths between $j$ and $k$ that pass through $i$, and \n* $\\sigma(j,k)$ is the total number of shortest paths between $j$ and $k$.\n\nThen we define the _dependency_ of $i$ on the paths between $j$ and $k$ as the \n_ratio of those two quantities_:\n\n$$\n\\text{dependency}(i \\mid j,k) = \\frac{\\sigma(i \\mid j,k )}{\\sigma(j,k)}.\n$$\n\nYou can think of this as *the probability that a shortest path between $j$ and $k$ goes through $i$*.\n\n## General Case: Betweenness Centrality\n\nFinally, the betweenness centrality of node $i$ is the sum of the dependencies of $i$ on all pairs of nodes $j$ and $k$.\n    \n$$\n\\boxed{\\text{betweenness}(i) = \\sum_{i \\neq j \\neq k \\in V} \\text{dependency}(i \\mid j, k).}\n$$\n\n> Note that many nodes will have a betweenness centrality of zero -- no shortest paths go through them.\n\n---\n\n::: {#ac326bdc .cell execution_count=6}\n``` {.python .cell-code}\n# Create the graph\nGk = nx.karate_club_graph()\n\n# Calculate betweenness centrality\ncent = list(nx.betweenness_centrality(Gk).values())\n\n# Set random seed for reproducibility\nnp.random.seed(9)\n\n# Create figure and subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n\n# Draw the first subplot with circular layout\npos1 = nx.circular_layout(Gk)\nnodes1 = nx.draw_networkx_nodes(Gk, pos=pos1, ax=ax1, node_color=cent, cmap=plt.cm.plasma)\nnx.draw_networkx_edges(Gk, pos=pos1, ax=ax1)\nax1.set_title('Betweenness Centrality')\nax1.axis('off')\n\n# Draw the second subplot with spring layout\npos2 = nx.spring_layout(Gk)\nnodes2 = nx.draw_networkx_nodes(Gk, pos=pos2, ax=ax2, node_color=cent, cmap=plt.cm.plasma)\nnx.draw_networkx_edges(Gk, pos=pos2, ax=ax2)\nax2.set_title('Betweenness Centrality')\nax2.axis('off')\n\n# Add colorbar to the figure\ncbar = fig.colorbar(nodes1, ax=[ax1, ax2], orientation='horizontal', fraction=0.05, pad=0.05)\ncbar.set_label('Betweenness Centrality')\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-6-output-1.png){width=763 height=449 fig-align='center'}\n:::\n:::\n\n\nWe start to see with this metric the *importance* of two or three key members of the karate club.\n\n## Distribution of Betweenness\n\n::: {#594129db .cell execution_count=7}\n``` {.python .cell-code}\nplt.figure(figsize = (6, 4))\nplt.hist(cent, bins=np.linspace(0, 1, 30))\nplt.xlabel('Betweenness Centrality', size = 14)\nplt.ylabel('Number of Nodes', size = 14)\nplt.title('Distribution of Betweenness Centrality', size = 16)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-7-output-1.png){width=516 height=386 fig-align='center'}\n:::\n:::\n\n\nThe higher the betweenness centrality of a node, the more it is on many shortest paths.\n\n## Eigenvector Centrality\n\nEigenvector centrality is a measure used in network analysis to determine the influence of a node within a network.\n\nThe centrality score of a node is proportional to the sum of the centrality scores of its neighbors.\n\nThe main idea of eigenvector (or status) centrality is that __*high status* nodes are connected to *high status* nodes__.\n\nWe calculate this with some matrix algebra.\n\n\n\n## Adjacency Matrices\n\nFirst, represent the graphs by its __adjacency matrix__.\n\nGiven an $n$-node undirected graph $G = (V, E)$, the adjacency matrix $A$ is defined as\n\n$$ \nA_{ij} = \n\\begin{cases}\n1, & \\text{if $(i, j)\\in E$ }\\\\\n0, & \\text{otherwise}\n\\end{cases}.\n$$\n\n<br>\n\n<!-- 3x3 adjacency matrix in HTML table format with bold first column and a vertical line divider -->\n\n<table>\n  <tr>\n    <th>ID</th>\n    <th>0</th>\n    <th>1</th>\n    <th>2</th>\n  </tr>\n  <tr>\n    <td><b>0</b></td>\n    <td style=\"border-left:2px solid #222;\">0</td>\n    <td>1</td>\n    <td>0</td>\n  </tr>\n  <tr>\n    <td><b>1</b></td>\n    <td style=\"border-left:2px solid #222;\">1</td>\n    <td>0</td>\n    <td>1</td>\n  </tr>\n  <tr>\n    <td><b>2</b></td>\n    <td style=\"border-left:2px solid #222;\">0</td>\n    <td>1</td>\n    <td>0</td>\n  </tr>\n</table>\n\n\n\n\n## Karate Club Adjacency Matrix\n\n::: {.columns}\n::: {.column}\n\n<br>\n\nAn important way to think about adjacency matrices is that __column $j$ holds $j$'s neighbors__.\n\nThe adjacency matrix is _nonnegative_ and _symmetric_.\n\n:::\n::: {.column}\n\n::: {#dbdecb5d .cell execution_count=8}\n\n::: {.cell-output .cell-output-display execution_count=490}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-8-output-1.png){fig-align='center'}\n:::\n:::\n\n\n:::\n:::\n\n\n\n\n## Reminder: Properties of nonnegative symmetric matrices\n\nA __nonnegative symmetric matrix__ $A$ has the following properties:\n\n1. All eigenvalues of $A$ are real.\n2. All eigenvalues of $A$ are nonnegative.\n3. There is only __1 corresponding eigenvector with all positive entries__.\n\nThis is a consequence of the [Perron-Frobenius Theorem](https://nhigham.com/2021/07/13/what-is-the-perron-frobenius-theorem/).\n\n## Eigenvector Centrality\n\nRemember:\n\n* Eigenvector centrality is a measure used in network analysis to determine the influence of a node within a network. \n* The centrality score of a node is proportional to the sum of the centrality scores of its neighbors. \n* The main idea of eigenvector (or status) centrality is that __*high status* nodes are connected to *high status* nodes__. \n\n## Eigenvector Centrality Definition\n\nGiven a graph with $n$ nodes and let $A$ be the adjacency matrix of the graph.\n\nThe eigenvector centrality $x_i$ of node $i$ is defined as proportional to the sum of the centrality scores of its neighbors.\n\n$$\nx_i = \\frac{1}{\\lambda} \\sum_{j=1}^{n} A_{ij} x_j,\n$$\n\nwhere\n\n- $x_i$ is called the eigenvector centrality of node $i$.\n- $\\lambda$ is a constant (which we'll show is an eigenvalue). \n\nPut another way, the importance of node $i$ is proportional to the sum of the importance of the other nodes $j$ directly connected to $i$.\n\n## Matrix Form\n\nIn matrix form, this can be written as\n\n$$\n\\mathbf{Ax} = \\lambda \\mathbf{x},\n$$\n\nwhere $\\mathbf{x}$ is the eigenvector corresponding to the the eigenvalue $\\lambda$ of the adjacency matrix $\\mathbf{A}$. \n\nThe eigenvector $\\mathbf{x}$ gives the centrality scores for all nodes in the network.\n\n\n* In general, there may be multiple eigenvalues $\\lambda$ for which the above equation holds. \n* In practice, the largest eigenvalue is used because it captures the most significant mode of influence propagation in the network.\n\n## Geometric Intuition\n\n- **Eigenvector as a direction**:\n    - The eigenvector is a direction in the network space. Applying the adjacency matrix (representing connections between nodes) to this vector, *stretches* the vector along that direction. The amount of stretch is determined by the eigenvalue. \n \n- **High centrality, high stretch**:\n    - A node with a large eigenvector centrality corresponds to an eigenvector that experiences a large *stretch* when transformed by the adjacency matrix. This indicates that it is connected to many other nodes that are also considered high-status. \n \n- **No direction change**:\n    - The eigenvector doesn't change its direction when transformed, only its magnitude. This means that a node with a high eigenvector centrality remains relatively *central* even when considering its connections to other central nodes. \n \n## Key Points on Eigenvector Centrality\n\n- **Eigenvector**: The vector $\\mathbf{x}$ represents the centrality scores.\n- **Largest Eigenvalue**: The centrality scores are derived from the eigenvector corresponding to the largest eigenvalue of the adjacency matrix.\n- **Perron-Frobenius Theorem**: For a connected graph, the largest eigenvalue is positive. In addition, there is only 1 corresponding eigenvector with all positive entries.\n\nThe [Perron-Frobenius Theorem](https://nhigham.com/2021/07/13/what-is-the-perron-frobenius-theorem/) is an important theoretical tool when working with adjacency matrices, which are by definition nonnegative and positive. \n\n## Eigenvector Centrality: Example\n\n::: {#429a310f .cell execution_count=9}\n``` {.python .cell-code}\nGk = nx.karate_club_graph()\ncent = list(nx.eigenvector_centrality(Gk).values())\nnp.random.seed(9)\nfig = plt.figure(figsize = (10, 5))\nax1 = fig.add_subplot(121)\nnx.draw_networkx(Gk, ax = ax1, pos = nx.circular_layout(Gk), \n                 node_color = cent,\n                 cmap = plt.cm.plasma,\n                 with_labels = False)\nplt.title('Eigenvector Centrality')\nplt.axis('off')\nax2 = fig.add_subplot(122)\nnx.draw_networkx(Gk, ax = ax2, pos = nx.spring_layout(Gk), \n                 node_color = cent,\n                 cmap = plt.cm.plasma,\n                 with_labels = False)\nplt.title('Eigenvector Centrality')\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-9-output-1.png){width=763 height=409 fig-align='center'}\n:::\n:::\n\n\n## Distribution of Eigenvector Centrality\n\n::: {#495aba63 .cell execution_count=10}\n``` {.python .cell-code}\nplt.figure(figsize = (6, 4))\nplt.hist(cent, bins=np.linspace(0, 1, 30))\nplt.xlabel('Eigenvector Centrality', size = 14)\nplt.ylabel('Number of Nodes', size = 14)\nplt.title('Distribution of Eigenvector Centrality', size = 16)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-10-output-1.png){width=516 height=386 fig-align='center'}\n:::\n:::\n\n\n<!-- \n## Eigenvector Centrality: Football Dataset\n\nAs a more detailed example, we can ask whether this definition of *prestige* applies to NCAA Division 1A football teams, based on who they play against. Remember this data is from 2000.\n\nWe can observe that some of the best teams that year, e.g., Wisconsin and Southern California have a large eigenvalue centrality score. It also appears that two of the powerhouse conferences (at that time) the Big 12 and Big 10 have many schools with larger eigenvalue scores.\n\n::: {#e4941ce5 .cell execution_count=11}\n``` {.python .cell-code}\n# data from http://www-personal.umich.edu/~mejn/netdata/\n# Read the football network graph\nfootball = nx.readwrite.gml.read_gml('data/football.gml')\n\n# Calculate eigenvector centrality\nec = nx.eigenvector_centrality(football)\ncent = [ec[i] for i in football.nodes()]\n\n# Set random seed for reproducibility\nnp.random.seed(1)\n\n# Create figure\nfig, ax = plt.subplots(figsize=(14, 14))\n\n# Draw the network with eigenvector centrality coloring\npos = nx.spring_layout(football)\nnodes = nx.draw_networkx_nodes(football, pos=pos, node_color=cent, cmap=plt.cm.plasma, ax=ax)\nnx.draw_networkx_edges(football, pos=pos, ax=ax)\nnx.draw_networkx_labels(football, pos=pos, ax=ax)\n\n# Add colorbar to the figure\ncbar = fig.colorbar(nodes, ax=ax, orientation='horizontal', fraction=0.05, pad=0.05)\ncbar.set_label('Eigenvector Centrality')\n\n# Set title and remove axis\nplt.title('Eigenvector Centrality on Football Network')\nplt.axis('off')\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-11-output-1.png){width=1061 height=1115}\n:::\n:::\n\n\n-->\n\n## Centrality Comparison: Karate Example\n\nLet's compare the three versions of centrality we've looked at so far.\n\n::: {#b3f2208c .cell execution_count=12}\n``` {.python .cell-code}\nGk = nx.karate_club_graph()\nfn = [nx.closeness_centrality, nx.betweenness_centrality, nx.eigenvector_centrality]\ntitle = ['Closeness Centrality', 'Betweenness Centrality', 'Eigenvector Centrality']\n#\nfig, axs = plt.subplots(2, 3, figsize = (14, 8))\nfor i in range(3):\n    cent = list(fn[i](Gk).values())\n    np.random.seed(9)\n    nx.draw_networkx(Gk, ax = axs[0, i], \n                 pos = nx.spring_layout(Gk), \n                 node_color = cent,\n                 cmap = plt.cm.plasma,\n                 with_labels = False)\n    axs[0, i].set_title(title[i], size = 14)\n    axs[0, i].axis('off')\n    #\n    axs[1, i].hist(cent, bins=np.linspace(0, 1, 30))\n    axs[1, i].set_ylim([0, 27])\n    axs[1, i].set_xlabel(title[i], size = 14)\naxs[1, 0].set_ylabel('Number of Nodes', size = 14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-12-output-1.png){width=1111 height=679}\n:::\n:::\n\n\nIt appears that we have identified the teacher and club president as important nodes based on our centrality measures.\n\n# Clustering and Partitioning\n\n## Clustering and Partitioning\n\nWe now turn to the question of finding important __groups__ of nodes.\n\nWhy might we want to cluster graph nodes?\n\n* Assigning computations to processors in a parallel computer\n* Segmenting images (finding boundaries between objects)\n* Clustering words found together in documents, or documents with similar words\n* Divide and conquer algorithms\n* Circuit layout in VLSI\n* Community detection in social networks\n\n## Min $s$-$t$ cut\n\nWe'll start with a problem that is fundamental to many other problems in graph analysis. \n\nIt involves finding the smallest set of edges that, if removed, would disconnect a specified source node (s) from a target node (t) in a graph.\n\nThe goal of a min $s$-$t$ cut is to partition the graph into two disjoint subsets\nsuch that:\n\n* $s$ is in one subset and $t$ is in the other, and \n* the total weight (or number) of the edges that need to be removed to achieve this separation is minimized.\n\n## Practical Example: Soviet Railway Network\n\n::: {.columns}\n::: {.column}\n\nHere is a schematic diagram of the railway network of the Western Soviet Union and Eastern European countries in the Cold War era. \n\nThe idea was to determine a minimum cut, which would best halt the flow of materials in the railway network. \n:::\n::: {.column}\n\n<!-- Image credit: On the history of the transportation and maximum flow problems. -->\n<!--     Alexander Schrijver in Math Programming, 91: 3, 2002. -->\n    \n![](figs/L23-max-flow-soviet-rail-1955.png){fig-align=\"center\" width=\"100%\"}\n\n:::\n:::\n\n::: aside\nFor an interesting historical perspective on the min-cut problem and its relation to the Cold War, see\n [\"On the history of the transportation and maximum flow problems,\"](https://link.springer.com/article/10.1007%2Fs101070100259) by Alexander Schrijver, in Mathematical Programming 91.3 (2002): 437-445.\n:::\n\n## Cost of a Cut\n\nLet $G = (V, E)$ be a weighted graph.\n\nA weighted graph assigns a weight $w(e)$ to each edge $e\\in E$.\n\nAn $s$-$t$ cut $C$ of $G$ is a partition of $V$ into $(U, V-U)$ such that $s \\in U$ and $t \\in V-U$.\n\n:::: {.fragment}\n\n::: {.columns}\n::: {.column}\nThe __cost__ of a cut is the total weight of the edges that go between the two parts:\n\n$$\n\\text{Cost}(C) = \\sum_{e(u,v),\\, u\\in U,\\, v\\in V-U} w(e).\n$$\n\n:::\n::: {.column}\n![](figs/L23-min-cut.png){width=\"85%\" fig-align=\"center\"}\n:::\n:::\n> For unweighted graphs, we can set $w(e) = 1$ for all edges, so the cost becomes the number of edges that cross the cut.\n\n::::\n\n## Min $s$-$t$ Cut Problem\n\nLet $G = (V, E)$ be a weighted graph with edge weights $w(e)$ for each $e \\in E$.\n\nAn $s$-$t$ cut $C$ is a partition of $V$ into $(U, V-U)$ such that $s \\in U$ and $t \\in V-U$.\n\nThe **min $s$-$t$ cut problem** seeks to find the partition that minimizes this cost:\n\n$$\n\\boxed{\\min_{C} \\text{Cost}(C) = \\min_{U \\subseteq V: s \\in U, t \\notin U} \\sum_{e(u,v),\\, u\\in U,\\, v\\in V-U} w(e).}\n$$\n\n\n\nThis is a very famous problem that can be solved in time that is polynomial in the number of nodes $|V|$ and edges $|E|$. \n\nIncreasingly better solutions have been found over the past 60+ years.  \n\n---\n\nWhat can a min $s$-$t$ cut tell us about a graph?\n\nLet's look at the karate club, in which we've highlighted the president and the instructor (in red and blue, respectively).\n\n::: {#812043f9 .cell execution_count=13}\n``` {.python .cell-code}\nG=nx.karate_club_graph()\nnp.random.seed(9)\npos = nx.spring_layout(G)\ncut_edges = nx.minimum_edge_cut(G, s=0, t=33)\n#\nfig = plt.figure(figsize=(10, 5))\nnode_color = 34 * ['skyblue']\nnode_color[0] = 'tomato'\nnode_color[33] = 'dodgerblue'\nnx.draw_networkx(G, pos=pos, \n                 with_labels=True, node_size=1000,\n                 node_color = node_color,\n                 font_size=16)\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-13-output-1.png){width=763 height=389 fig-align='center'}\n:::\n:::\n\n\n---\n\nAs mentioned, when Wayne Zachary studied the club, a conflict arose between the instructor and the president (nodes 0 and 33, respectively).  \n\nZachary predicted the way the club would split based on an $s$-$t$ min cut between the president and the instructor.   \n\nIn fact, he __correctly__ predicted every single member's eventual association except for node 8.\n\n::: {#33b9054f .cell execution_count=14}\n``` {.python .cell-code}\nGcopy = G.copy()\nGcopy.remove_edges_from(cut_edges)\ncc = nx.connected_components(Gcopy)\nnode_set = {node: i for i, s in enumerate(cc) for node in s}\ncolors = ['dodgerblue', 'tomato']\nnode_colors = [colors[node_set[v]-1] for v in G.nodes()]\nfig = plt.figure(figsize=(12, 5))\nnx.draw_networkx(G, node_color=node_colors, pos=pos, \n                 with_labels='True', node_size=1000, font_size=16)\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-14-output-1.png){width=912 height=389 fig-align='center'}\n:::\n:::\n\n\n## Minimum Cuts\n\nIn partitioning a graph, we may not have any particular $s$ and $t$ in mind.  \n\nRather, we may want to simply find the *minimal* way to disconnect the graph.\n\nClearly, we can do this using an $s$-$t$ min cut, by simply trying all $s$ and $t$ pairs.\n\n![](figs/L23-min-cut.png){width=\"45%\" fig-align=\"center\"}\n    \n---\n\nLet's try this approach of finding the minimum $s-t$ cut over all possibilities in the karate club graph.\n\n::: {#4e531678 .cell execution_count=15}\n``` {.python .cell-code}\nGcopy = G.copy()\nGcopy.remove_edges_from(nx.minimum_edge_cut(G))\ncc = nx.connected_components(Gcopy)\nnode_set = {node: i for i, s in enumerate(cc) for node in s}\n#\ncolors = ['tomato', 'dodgerblue']\nnode_colors = [colors[node_set[v]] for v in G]\nfig = plt.figure(figsize=(12,5))\nnx.draw_networkx(G, node_color=node_colors, pos=pos, with_labels='True', \n                 node_size=1000, font_size=16)\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-15-output-1.png){width=912 height=389}\n:::\n:::\n\n\nThis is in fact the minimum cut. Node 11 only has one edge to the rest of the graph, so the min cut is 1.\n\n---\n\nAs this example shows, minimum cut is not, in general, a good approach for clustering or partitioning.\n\nTo get a more useful partition, we need to define a new goal. The new goal will be to find a __balanced cut__.\n\n## Balanced Cuts\n\nThe idea to avoid the problem above is to normalize the cut by the size of the __smaller__ of the two components.\nThe problem above would be avoided because the smaller of the two cuts is just a single node.\n\nThis leads us to define the __isoperimetric ratio__\n    \n$$\n\\alpha = \\frac{E(U, V\\setminus U)}{\\min(|U|, |V\\setminus U|)},\n$$\n\nand the __isoperimetric number of G__\n    \n$$\n\\alpha(G) = \\min_U \\frac{E(U, V\\setminus U)}{\\min(|U|, |V\\setminus U|)},\n$$\n\nwhere $E(U, V\\setminus U)$ is the number of edges between the two parts.\n\nThe idea is that finding $\\alpha(G)$ gives a _balanced cut_ -- one that maximizes the number of disconnected nodes per edge removed.\n\n---\n\nUnfortunately, this is a challenging problem that is not computable in polynomial time.\n\nHowever, we can make good approximations, which we'll look at now.\n\nTo do so, we'll introduce __spectral graph theory__.\n\n## Spectral Graph Theory\n\nSpectral graph theory is the use of linear algebra to study the properties of graphs.\n\nTo introduce spectral graph theory, we define some terms.\n\nLet $G$ be an undirected graph with $n$ nodes. We define the $n\\times n$ matrix $D$ as a diagonal matrix of node degrees, i.e., $D = \\text{diag}(d_1, d_2, d_3, \\dots)$ where $d_i$ is the [_degree_](https://tools4ds.github.io/DS701-Course-Notes/21-Networks-I.html#degree) of node $i$.\n\nAssuming $G$ has adjacency matrix $A$, we define the __Laplacian__ of $G$ as\n\n$$\nL = D - A.\n$$\n\nWhere the matrix has positive entries on the diagonal and negative, symmetric entries off the diagonal.\n\n::: aside\nIf you want to study this in more detail, some excellent references are\n\n* [Allow Me to Introduce\nSpectral and Isoperimetric Graph Partitioning](https://people.eecs.berkeley.edu/~jrs/papers/partnotes.pdf) by Jonathan Shewchuck, which has outstanding visualizations and physical intuition.\n* [Spectral and Algebraic Graph Theory](http://cs-www.cs.yale.edu/homes/spielman/sagt/sagt.pdf) by Daniel Spielman which provides proofs and much more detail.\n:::\n\n---\n\nBelow we show the Laplacian matrix $L$ for the karate club network as a heatmap.\n\n::: {#45c22fa8 .cell execution_count=16}\n``` {.python .cell-code}\nL = nx.laplacian_matrix(nx.karate_club_graph()).todense()\nplt.figure(figsize = (6, 6))\nsns.heatmap(L, cmap = plt.cm.tab20)\nplt.axis('equal')\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-16-output-1.png){width=447 height=463 fig-align='center'}\n:::\n:::\n\n\n## Laplacian Matrix Interpretation\n\nNow let us think about an $n$-component vector $\\mathbf{x} \\in \\mathbb{R}^n$ as an __assignment of values__ to nodes in the graph $G$.   \n\nFor example, $\\mathbf{x}$ could encode node *importance* or *strength* or even a more concrete notion like *temperature* or *altitude*.\n\n## Laplacian Quadratic Form\n\nHere is an amazing fact about the Laplacian of $G$.\n    \nThe quadratic form\n\n$$\n\\mathbf{x}^TL\\mathbf{x} = \\sum_{(i,j)\\in E} (x_i - x_j)^2,\n$$\n\ni.e., $\\mathbf{x}^TL\\mathbf{x}$ is the sum of squared differences of $\\mathbf{x}$ __over the edges in $G$.__\n\n::: {.content-visible when-profile=\"slides\"}\nA proof of this is given in the [web notes](https://tools4ds.github.io/DS701-Course-Notes/22-Networks-II-Centrality-Clustering.html#proof-that-quadratic-form-is-sum-of-squared-differences).\n:::\n\n::: {.content-visible when-profile=\"web\"}\n\n### Proof that Quadratic Form is Sum of Squared Differences\n\nTo see that \n\n$$ \\mathbf{x}^TL\\mathbf{x} = \\sum_{(i,j)\\in E} (x_i - x_j)^2, $$\n\nfirst consider $\\mathbf{x}^TL\\mathbf{x}$.  Writing out the quadratic form explicitly, we have that\n\n$$ \\mathbf{x}^TL\\mathbf{x} = \\sum_{i, j} L_{ij}x_i x_j. $$\n\nNow, taking into account the values in $L$, we see that in the sum we will have the term $d_i x_i^2$ for each $i$, and also 2 terms of $-x_ix_j$ whenever $(i,j)\\in E$.  \n\nTurning to \n\n$$\\sum_{(i,j)\\in E} (x_i - x_j)^2 = \\sum_{(i,j)\\in E} x_i^2 - 2x_ix_j + x_j^2, $$\n\nwe have the same set of terms in the sum. \n:::\n\n---\n\nNow, let's think about a vector $\\mathbf{x}$ that __minimizes__ the differences over the edges in the graph.\n\n::: {.fragment}\nRemember that the $x_i$ are the values at nodes $i$ in the graph, which we can think of as a function on the graph.\n:::\n\n::: {.fragment}\nWe want these functions to be *smooth* -- neighboring nodes don't differ too much in their $x$ values.\n:::\n\n::: {.fragment}\nTo enforce this smoothness we want to minimize the sum of the squared differences between neighboring nodes.\n\n$$\n\\min_{\\Vert \\mathbf{x}\\Vert = 1}\\sum_{(i,j)\\in E} (x_i - x_j)^2.\n$$\n\nWe constrain $\\mathbf{x}$ to have a nonzero norm (e.g. $\\Vert \\mathbf{x}\\Vert = 1$), otherwise $\\mathbf{x} = \\mathbf{0}$ would be a trivial solution.\n:::\n\n---\n\nWe can express this in terms of the graph Laplacian:\n\n$$\n\\min_{\\Vert \\mathbf{x}\\Vert = 1}\\sum_{(i,j)\\in E} (x_i - x_j)^2 = \\min_{\\Vert \\mathbf{x}\\Vert = 1} \\mathbf{x}^TL\\mathbf{x}.\n$$\n\nFrom linear algebra, we know that when\n\n$$\n\\lambda = \\min_{\\Vert \\mathbf{x}\\Vert = 1} \\mathbf{x}^TL\\mathbf{x},\n$$\n\nthen $\\lambda$ is the __smallest eigenvalue of $L$__ and $\\mathbf{x}$ is the corresponding eigenvector.\n\n::: {.aside}\nThis is the standard eigenvalue equation with the constraint that $\\mathbf{x}$ is a unit vector.\n:::\n\nWe are connecting functions on the graph $G$ with eigenvectors of the matrix $L$. This is quite remarkable.\n\n---\n\nWhat do we know about $L$?\n\n1. $L$ is __symmetric.__  Therefore the eigenvectors of $L$ are orthogonal and its eigenvalues are real.\n2. $L$ is __positive semidefinite__, e.g. $\\mathbf{x}^T L\\mathbf{x} \\geq 0$ for all $\\mathbf{x}$.\n   * Therefore the eigenvalues of $L$ are all positive or zero. \n\nWe can order the eigenvalues from largest to smallest $\\lambda_n  \\geq \\dots \\geq \\lambda_2 \\geq \\lambda_1 \\geq 0.$\n\nHow do we know that $L$ is positive semidefinite?\n\n* Consider $\\sum_{(i,j)\\in E} (x_i - x_j)^2.$  This is always a nonnegative quantity.\n* As a result, $\\mathbf{x}^T L\\mathbf{x} \\geq 0$ for all $\\mathbf{x}$.\n\n---\n\nAssume that $G$ is _connected_ (e.g. there is a path between any two nodes).\n\nThen $L$ has a single eigenvalue of value $\\lambda_1 = 0$.   The corresponding eigenvector is $\\mathbf{w}_1 = \\mathbf{1} = [1, 1, 1, \\dots]^T$.\n\nIt is easy to verify that\n\n$$\nL{\\mathbf 1}=0 \\cdot {\\mathbf 1} = {\\mathbf 0}.\n$$\n\nRecall that row $i$ of $L$ consists of $d_i$ on the diagonal, and $d_i$ -1s in other positions.\n\n---\n\nThe second-smallest eigenvalue of $L$, $\\lambda_2$, is called the __Fiedler value.__\n\nWe know that all of the other eigenvectors of $L$ are orthogonal to $\\mathbf 1$, because $L$ is symmetric.\n\nAs a result, a definition of the second smallest eigenvalue is: \n\n$$\n\\lambda_2 = \\min_{\\Vert \\mathbf{x}\\Vert = 1, \\;\\mathbf{x}\\perp {\\mathbf 1}} \\mathbf{x}^TL\\mathbf{x}.\n$$\n\nNote that $\\mathbf{x} \\perp {\\mathbf 1}$ means that the sum of the entries in\n$\\mathbf{x}$ is zero, which implies that $\\mathbf{x}$ is __mean-centered__.\n\n## Fiedler Vector\n\nThe corresponding eigenvector to $\\lambda_2$ is called the __Fiedler vector.__\n\nIt minimizes\n\n$$\n\\mathbf{w}_2 = \\arg \\min_{\\Vert \\mathbf{x}\\Vert=1,\\;\\mathbf{x}\\perp {\\mathbf 1}} \\sum_{(i,j)\\in E} (x_i - x_j)^2.\n$$\n\nIf we think of $x_i$ as a 1-D coordinate for node $i$ in the graph, then choosing $\\mathbf{x} = \\mathbf{w}_2$ (the eigenvector corresponding to $\\lambda_2$) puts each node in a position that minimizes the sum of the squared stretching of each edge.\n\n::: {.content-visible when-profile=\"web\"}\n\n---\n\nRecall from physics that the energy in a stretched spring is proportional to the square of its stretched length.  \n\nIf we use the entries in $\\mathbf{w}_2$ to position the nodes of the graph along a single dimension, then using the Fiedler vector $\\mathbf{w}_2$ for node coordinates is __exactly the spring layout__ of nodes that we discussed in the last lecture -- __except that it is in one dimension only.__\n\nThis is the basis for the __spectral layout__ that we showed in the last lecture.\n\nIn the spectral layout, we use $\\mathbf{w}_2$ for the first dimension, and $\\mathbf{w}_3$ for the second dimension.\n\n$\\mathbf{w}_3$ is the eigenvector corresponding to \n\n$$\n\\lambda_3 = \\min_{\\Vert \\mathbf{x}\\Vert = 1, \\;\\mathbf{x}\\perp \\{\\mathbf{1}, \\mathbf{w}_2\\}} \\mathbf{x}^TL\\mathbf{x}.\n$$\n\n---\n\nLet's look again at layouts for the football network.\n\n::: {#e6075e30 .cell execution_count=17}\n``` {.python .cell-code}\nplt.figure(figsize = (12, 6))\nax1 = plt.subplot(121)\nnx.draw_networkx(football, ax = ax1,\n                 node_size=35, \n                 edge_color='gray', \n                 pos = nx.spectral_layout(football),\n                 with_labels=False, alpha=.8, linewidths=2)\nplt.axis('off')\nplt.title('Title 1 Football -- Spectral Layout', size = 16)\nax2 = plt.subplot(122)\nnx.draw_networkx(football, ax = ax2,\n                 node_size=35, \n                 edge_color='gray', \n                 pos = nx.spring_layout(football, seed = 0),\n                 with_labels=False, alpha=.8, linewidths=2)\nplt.axis('off')\nplt.title('Title 1 Football -- Spring Layout', size = 16)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-17-output-1.png){width=912 height=487}\n:::\n:::\n\n\nWhat is the difference between the spectral layout and the spring layout?\n\nIn one dimension, they are the same, but in multiple dimensions, the spectral layout optimizes each dimension separately.\n\n:::\n\n## Spectral Partitioning\n\nThis leads to key ideas in node partitioning.\n\nThe basic idea is to partition nodes according to the Fiedler vector $\\mathbf{w}_2$.\n\nThis can be shown to have provably good performance for the __balanced cut__ problem.\n\n::: aside\nSee [Spectral and Algebraic Graph Theory](http://cs-www.cs.yale.edu/homes/spielman/sagt/sagt.pdf) by Daniel Spielman, Chapter 20, where it is proved that for every $U \\subset V$ with $|U| \\leq |V|/2$, $\\alpha(G) \\geq \\lambda_2 (1-s)$ where $s = |U|/|V|$.   In particular, $\\alpha(G) \\geq \\lambda_2/2.$\n:::\n\nThere are a number of options for how to split based on the Fiedler vector.\n\nIf $\\mathbf{w}_2$ is the Fiedler vector, then split nodes according to a value $s$:\n    \n* bisection: $s$ is the median value in $\\mathbf{w}_2$\n* sign: separate positive and negative vaues ($s = 0$)\n* gap: separate according to the largest gap in the values of $\\mathbf{w}_2$\n* ratio cut: $s$ is the value that maximizes $\\alpha$\n\n\n## Example: Bisection Method\n\nConsider a graph with 8 nodes where the Fiedler vector has values (sorted for convenience):\n\n$$\n\\mathbf{w}_2 = [-0.6, -0.4, -0.2, -0.1, 0.15, 0.3, 0.45, 0.5]\n$$\n\n**Bisection approach:** Use $s = \\text{median}(\\mathbf{w}_2) = \\frac{-0.1 + 0.15}{2} = 0.025$\n\n**Result:**\n\n- **Group 1** (values $< 0.025$): nodes 1, 2, 3, 4\n- **Group 2** (values $\\geq 0.025$): nodes 5, 6, 7, 8\n\nThis ensures balanced partition sizes (4 nodes each).\n\n## Example: Sign Method\n\nUsing the same Fiedler vector:\n\n$$\n\\mathbf{w}_2 = [-0.6, -0.4, -0.2, -0.1, 0.15, 0.3, 0.45, 0.5]\n$$\n\n**Sign approach:** Use $s = 0$ to separate positive from negative values\n\n**Result:**\n\n- **Group 1** (negative values): nodes 1, 2, 3, 4\n- **Group 2** (positive values): nodes 5, 6, 7, 8\n\nFor this example, the sign method gives the same partition as bisection, but this won't always be the case.\n\n## Example: Gap Method\n\nUsing the same Fiedler vector:\n\n$$\n\\mathbf{w}_2 = [-0.6, -0.4, -0.2, -0.1, 0.15, 0.3, 0.45, 0.5]\n$$\n\n**Gap approach:** Find the largest gap between consecutive sorted values\n\nGaps: $0.2, 0.2, 0.1, 0.25, 0.15, 0.15, 0.05$\n\nLargest gap is $0.25$ (between $-0.1$ and $0.15$), so $s = 0.025$\n\n**Result:**\n\n- **Group 1** (values $< 0.025$): nodes 1, 2, 3, 4\n- **Group 2** (values $\\geq 0.025$): nodes 5, 6, 7, 8\n\nThis identifies the most natural \"break\" in the data.\n\n## Example: Ratio Cut Method\n\nConsider a different scenario where we try several cut values:\n\nFor $s = -0.3$:\n\n- Group 1: nodes 1, 2 (size = 2)\n- Group 2: nodes 3, 4, 5, 6, 7, 8 (size = 6)\n- Cut edges: 5, so [isoperimetric ratio](#balanced-cuts) $\\alpha = \\frac{5}{\\min(2,6)} = 2.5$\n\nFor $s = 0.025$:\n\n- Group 1: nodes 1, 2, 3, 4 (size = 4)\n- Group 2: nodes 5, 6, 7, 8 (size = 4)\n- Cut edges: 8, so [isoperimetric ratio](#balanced-cuts)$\\alpha = \\frac{8}{\\min(4,4)} = 2.0$\n\n**Ratio cut approach:** Choose $s$ that maximizes $\\alpha$\n\nThe actual implementation searches over possible cut values to find the optimal partition quality.\n\n---\n\nHere is a spectral parititioning for the karate club graph.\n\n::: {#9fb66a94 .cell execution_count=18}\n``` {.python .cell-code}\nG = nx.karate_club_graph()\nf = nx.fiedler_vector(G)\ns = np.zeros(len(f), dtype='int')\ns[f > 0] = 1\n#\nfig = plt.figure(figsize=(12,5))\ncolors = ['tomato', 'dodgerblue']\nnp.random.seed(9)\npos = nx.spring_layout(G)\nnode_colors = [colors[s[v]] for v in G]\nnx.draw_networkx(G, pos=pos, node_color=node_colors, with_labels='True',\n        node_size=1000, font_size=16)\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-18-output-1.png){width=912 height=389 fig-align='center'}\n:::\n:::\n\n\nInterestingly, this is almost the same as the $s$-$t$ min cut  based on the president and instructor.\n\n## Spectral Clustering\n\nIn many cases we would like to move beyond graph partitioning, to allow for clustering nodes into, say, $k$ clusters.\n\nThe idea of spectral clustering takes the observations about the Fiedler vector and extends them to more than one dimension.\n\n::: {.content-visible when-profile=\"slides\"}\nSee the [notes](https://tools4ds.github.io/DS701-Course-Notes/22-Networks-II-Centrality-Clustering.html#spectral-clustering-example) for more on spectral clustering.\n:::\n\n::: {.content-visible when-profile=\"web\"}\n\nLet's look again at the spectral layout of the football dataset.  \n\n## Spectral Clustering: Example\n\nHere we've labelled the nodes according to their conference, which we will think of as ground-truth labels.\n\n::: {#104ff21e .cell execution_count=19}\n``` {.python .cell-code}\nimport matplotlib.patches as mpatches\nimport re\ncmap = plt.cm.tab20\n#\n# data from http://www-personal.umich.edu/~mejn/netdata/\nfootball = nx.readwrite.gml.read_gml('data/football.gml')\nconf_name = {}\nwith open('data/football.txt', 'r') as fp:\n    for line in fp:\n        m = re.match(r'\\s*(\\d+)\\s+=\\s*([\\w\\s-]+)\\s*\\n', line)\n        if m:\n            conf_name[int(m.group(1))] = m.group(2)\nconf = [d['value'] for i, d in football.nodes.data()]\n#\n#\nplt.figure(figsize = (8, 8))\nnx.draw_networkx(football,\n                 pos = nx.spectral_layout(football), \n                 node_color = conf,\n                 with_labels = False,\n                 cmap = cmap)\nplt.title('Conference Membership in Football Network')\npatches = [mpatches.Patch(color = cmap(i/11), label = conf_name[i]) for i in range(12)]\nplt.legend(handles = patches)\nplt.axis('off')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-19-output-1.png){width=614 height=631 fig-align='center'}\n:::\n:::\n\n\n---\n\nNow, the key idea is that using the spectral layout, we have placed nodes into a Euclidean space.\n\nThis means we could use a __standard clustering algorithm__ in that space.\n\n::: {#caa5a6cc .cell execution_count=20}\n``` {.python .cell-code}\nplt.figure(figsize = (12, 10))\nnx.draw_networkx(football,\n                 pos = nx.spectral_layout(football), \n                 node_color = conf,\n                 with_labels = False,\n                 edgelist = [],\n                 cmap = cmap)\nplt.title('Conference Membership in Football Network')\npatches = [mpatches.Patch(color = cmap(i/11), label = conf_name[i]) for i in range(12)]\nplt.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\nplt.legend(handles = patches)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-20-output-1.png){width=962 height=801}\n:::\n:::\n\n\n---\n\nThis plot shows that many clusters are well-separated in this space, but some still overlap.\n\nTo address this, we can use additional eigenvectors of the Laplacian, i.e., $\\mathbf{w}_4, \\mathbf{w}_5, \\dots$.\n\nThe idea of spectral clustering is: \n\n* use enough of the smallest eigenvectors of $L$ to sufficiently \"spread out\" the nodes,\n* cluster the nodes in the Euclidean space created by this embedding.\n\n---\n\nMore specifically, given a graph $G$:\n\n* Compute $L$, the Laplacian of $G$\n* Compute the smallest $d$ eigenvectors of $L$, __excluding__ the smallest eigenvector (the ones vector)\n* Let $U \\in \\mathbb{R}^{n\\times d}$ be the matrix containing the eigenvectors $\\mathbf{w}_2, \\mathbf{w}_3, \\dots, \\mathbf{w}_{d+1}$ as columns\n* Let the position of each node $i$ be the point in $\\mathbb{R}^d$ given by row $i$ of $U$\n* Cluster the points into $k$ clusters using $k$-means\n\n---\n\nLet's explore the results of spectral clustering using $d = 2$ dimensions.\n\n::: {#03210b18 .cell execution_count=21}\n``` {.python .cell-code}\n# Here is a complete example of spectral clustering\n#\n# The number of dimensions of spectral layout\nk = 2\n#\n# Obtain the graph\nfootball\n#\n# Compute the eigenvectors of its Laplacian\nL = nx.laplacian_matrix(football).todense()\nw, v = np.linalg.eig(L)\nv = np.array(v)\n# \n# scale each eigenvector by its eigenvalue\nX = v @ np.diag(w)\n#\n# consider the eigenvectors in increasing order of their eigenvalues\nw_order = np.argsort(w)\nX = X[:, w_order]\n#\n# run kmeans using k top eigenvectors as coordinates\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(init='k-means++', n_clusters=11 , n_init=10)\nkmeans.fit_predict(X[:, 1:(k+1)])\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_\nerror = kmeans.inertia_\n#\n# visualize the result\nplt.figure(figsize = (14, 7))\nax1 = plt.subplot(121)\nnx.draw_networkx(football,\n                 ax = ax1,\n                 pos = nx.spectral_layout(football), \n                 node_color = labels,\n                 with_labels = False,\n                 edgelist = [],\n                 cmap = cmap,\n                 node_size = 100)\nplt.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\nplt.title(f'Spectral Clustering, 11 Clusters, dimension = {k}')\nax2 = plt.subplot(122)\nnx.draw_networkx(football,\n                 ax = ax2,\n                 pos = nx.spectral_layout(football), \n                 node_color = conf,\n                 with_labels = False,\n                 edgelist = [],\n                 cmap = cmap,\n                 node_size = 100)\nplt.title('Conference Membership in Football Network')\npatches = [mpatches.Patch(color = cmap(i/11), label = conf_name[i]) for i in range(12)]\nplt.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\nplt.legend(handles = patches, loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-21-output-1.png){width=1275 height=579}\n:::\n:::\n\n\n---\n\nThis is pretty good, but we can see that in some cases the clustering is not able to separate clusters that overlap in the visualization.\n\nWhich makes sense, as for the case $d = 2$, we are running $k$-means on the points just as we see them in the visualization.\n\nLet's try $d = 3$.   Now there will be another dimension available to the clustering, which we can't see in the visualization.\n\n---\n\n::: {#c2155e09 .cell execution_count=22}\n``` {.python .cell-code}\n# Here is a complete example of spectral clustering\n#\n# The number of dimensions of spectral layout\nk = 3\n#\n#\n# Compute the eigenvectors of its Laplacian\nL = nx.laplacian_matrix(football).todense()\nw, v = np.linalg.eig(L)\nv = np.array(v)\n# \n# scale each eigenvector by its eigenvalue\nX = v @ np.diag(w)\n#\n# consider the eigenvectors in increasing order of their eigenvalues\nw_order = np.argsort(w)\nX = X[:, w_order]\n#\n# run kmeans using k top eigenvectors as coordinates\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(init='k-means++', n_clusters=11 , n_init=10)\nkmeans.fit_predict(X[:, 1:(k+1)])\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_\nerror = kmeans.inertia_\n#\n# visualize the result\nplt.figure(figsize = (14, 7))\nax1 = plt.subplot(121)\nnx.draw_networkx(football,\n                 ax = ax1,\n                 pos = nx.spectral_layout(football), \n                 node_color = labels,\n                 with_labels = False,\n                 edgelist = [],\n                 cmap = cmap,\n                 node_size = 100)\nplt.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\nplt.title(f'Spectral Clustering, 11 Clusters, dimension = {k}')\nax2 = plt.subplot(122)\nnx.draw_networkx(football,\n                 ax = ax2,\n                 pos = nx.spectral_layout(football), \n                 node_color = conf,\n                 with_labels = False,\n                 edgelist = [],\n                 cmap = cmap,\n                 node_size = 100)\nplt.title('Conference Membership in Football Network')\npatches = [mpatches.Patch(color = cmap(i/11), label = conf_name[i]) for i in range(12)]\nplt.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\nplt.legend(handles = patches, loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-22-output-1.png){width=1275 height=579}\n:::\n:::\n\n\n---\n\nWe can see visually that using 3 dimensions is giving us a better clustering than 2 dimensions.\n\nWhat happens as we increase the dimension further?\n\nTo evaluate this question we can use Adjusted Rand Index.\n\n## ARI Spectral Clustering\n\n::: {#4852723b .cell execution_count=23}\n``` {.python .cell-code}\nimport sklearn.metrics as metrics\n#\n# Compute the eigenvectors of its Laplacian\nL = nx.laplacian_matrix(football).todense()\nw, v = np.linalg.eig(L)\nv = np.array(v)\n# \n# scale each eigenvector by its eigenvalue\nX = v @ np.diag(w)\n#\n# consider the eigenvectors in increasing order of their eigenvalues\nw_order = np.argsort(w)\nX = X[:, w_order]\n#\nmax_dimension = 15\nri = np.zeros(max_dimension - 1)\nfor k in range(1, max_dimension):\n    # run kmeans using k top eigenvectors as coordinates\n    kmeans = KMeans(init='k-means++', n_clusters = 11, n_init = 10)\n    kmeans.fit_predict(X[:, 1:(k+1)])\n    ri[k - 1] = metrics.adjusted_rand_score(kmeans.labels_, conf)\n#\nplt.figure(figsize = (8, 6))\nplt.plot(range(1, max_dimension), ri, 'o-')\nplt.xlabel('Number of Dimensions (Eigenvectors)', size = 14)\nplt.title('Spectral Clustering of Football Network Compared to Known Labels', size = 16)\nplt.ylabel('Adjusted Rand Index', size = 14)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-revealjs/cell-23-output-1.png){width=736 height=533}\n:::\n:::\n\n\nBased on this plot, it appears that the football graph can be best described as approximately six-dimensional.\n\nWhen we embed it in six dimensions and cluster there we get an extremely high Adjusted Rand Index.\n\n::: {#c7acf45f .cell execution_count=24}\n``` {.python .cell-code}\nprint(f'Maximum ARI is {np.max(ri):0.3f}, using {1 + np.argmax(ri)} dimensions for spectral embedding.')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMaximum ARI is 0.868, using 6 dimensions for spectral embedding.\n```\n:::\n:::\n\n\n:::\n\n## Recap\n\nNetworks are present in many interesting applications.\n\nWe have covered the following topics \n\n- Graph representations\n- Cluster coefficients\n- Centrality measures\n- Clustering -- (see notes)\n\nIn addition we used the [NewtorkX](https://networkx.org) package to visualize our networks in the following formats\n\n- circular\n- spring\n- spectral\n\n",
    "supporting": [
      "22-Networks-II-Centrality-Clustering_files"
    ],
    "filters": [],
    "includes": {}
  }
}