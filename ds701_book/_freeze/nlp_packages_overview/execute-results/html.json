{
  "hash": "d50e801a5c6d4e63720b3fc01752626e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'NLP Packages Overview'\njupyter: python3\n---\n\nThis document provides an overview of three powerful Python packages for Natural Language Processing: NLTK, spaCy, and BERTopic.\n\n# NLTK (Natural Language Toolkit)\n\n## Overview\n\n**NLTK** is one of the oldest and most comprehensive Python libraries for NLP, originally created for teaching and research.\n\n**Key Features:**\n\n- Extensive collection of text processing tools\n- Access to over 50 corpora and lexical resources (WordNet, TreeBank)\n- Text classification, tokenization, stemming, tagging, parsing\n- Educational focus with extensive documentation\n\n**Best For:**\n\n- Learning NLP concepts\n- Prototyping and research\n- Working with linguistic data structures\n- Academic projects and teaching\n\n**Limitations:**\n\n- Slower than modern alternatives\n- Less suited for production environments\n- Requires more manual pipeline construction\n\n## NLTK Example: Basic Text Processing\n\n::: {#2cd65b34 .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\n# Download required data (run once)\n# nltk.download('punkt')\n# nltk.download('stopwords')\n# nltk.download('wordnet')\n# nltk.download('averaged_perceptron_tagger')\n\ntext = \"\"\"Natural language processing (NLP) is a fascinating field. \nIt enables computers to understand and process human language. \nNLTK provides excellent tools for learning NLP concepts.\"\"\"\n\n# Tokenization\nsentences = sent_tokenize(text)\nwords = word_tokenize(text)\n\nprint(\"Sentences:\", len(sentences))\nprint(\"Words:\", len(words))\nprint(\"\\nFirst sentence tokens:\", word_tokenize(sentences[0]))\n\n# Remove stopwords\nstop_words = set(stopwords.words('english'))\nfiltered_words = [w for w in words if w.lower() not in stop_words and w.isalpha()]\nprint(\"\\nFiltered words:\", filtered_words)\n\n# Stemming vs Lemmatization\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\nwords_to_process = ['running', 'runs', 'ran', 'easily', 'fairly']\nprint(\"\\n{:<15} {:<15} {:<15}\".format(\"Original\", \"Stemmed\", \"Lemmatized\"))\nprint(\"-\" * 45)\nfor word in words_to_process:\n    stemmed = stemmer.stem(word)\n    lemmatized = lemmatizer.lemmatize(word, pos='v')  # v = verb\n    print(\"{:<15} {:<15} {:<15}\".format(word, stemmed, lemmatized))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSentences: 3\nWords: 30\n\nFirst sentence tokens: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', '.']\n\nFiltered words: ['Natural', 'language', 'processing', 'NLP', 'fascinating', 'field', 'enables', 'computers', 'understand', 'process', 'human', 'language', 'NLTK', 'provides', 'excellent', 'tools', 'learning', 'NLP', 'concepts']\n\nOriginal        Stemmed         Lemmatized     \n---------------------------------------------\nrunning         run             run            \nruns            run             run            \nran             ran             run            \neasily          easili          easily         \nfairly          fairli          fairly         \n```\n:::\n:::\n\n\n## NLTK Example: Part-of-Speech Tagging\n\n::: {#76b6b5e6 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"true\"}\nfrom nltk import pos_tag\n\nsentence = \"The quick brown fox jumps over the lazy dog\"\ntokens = word_tokenize(sentence)\npos_tags = pos_tag(tokens)\n\nprint(\"Part-of-Speech Tags:\")\nfor word, tag in pos_tags:\n    print(f\"  {word:10} -> {tag}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPart-of-Speech Tags:\n  The        -> DT\n  quick      -> JJ\n  brown      -> NN\n  fox        -> NN\n  jumps      -> VBZ\n  over       -> IN\n  the        -> DT\n  lazy       -> JJ\n  dog        -> NN\n```\n:::\n:::\n\n\n## NLTK Example: Sentiment Analysis\n\n::: {#f7e46d08 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"true\"}\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\n# Download required data\n# nltk.download('vader_lexicon')\n\nsia = SentimentIntensityAnalyzer()\n\ntexts = [\n    \"I absolutely love this product! It's amazing!\",\n    \"This is terrible. I hate it.\",\n    \"It's okay, nothing special.\",\n    \"The weather is nice today.\"\n]\n\nprint(\"Sentiment Analysis Results:\")\nprint(\"-\" * 60)\nfor text in texts:\n    scores = sia.polarity_scores(text)\n    print(f\"Text: {text}\")\n    print(f\"  Negative: {scores['neg']:.3f}, Neutral: {scores['neu']:.3f}, Positive: {scores['pos']:.3f}\")\n    print(f\"  Compound Score: {scores['compound']:.3f}\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSentiment Analysis Results:\n------------------------------------------------------------\nText: I absolutely love this product! It's amazing!\n  Negative: 0.000, Neutral: 0.311, Positive: 0.689\n  Compound Score: 0.871\n\nText: This is terrible. I hate it.\n  Negative: 0.694, Neutral: 0.306, Positive: 0.000\n  Compound Score: -0.778\n\nText: It's okay, nothing special.\n  Negative: 0.367, Neutral: 0.325, Positive: 0.309\n  Compound Score: -0.092\n\nText: The weather is nice today.\n  Negative: 0.000, Neutral: 0.588, Positive: 0.412\n  Compound Score: 0.421\n\n```\n:::\n:::\n\n\n# spaCy\n\n## Overview\n\n**spaCy** is a modern, industrial-strength NLP library designed for production use.\n\n**Key Features:**\n\n- Fast and efficient (Cython-optimized)\n- Pre-trained statistical models for multiple languages\n- Built-in support for NER, POS tagging, dependency parsing\n- Easy integration with deep learning frameworks (PyTorch, TensorFlow)\n- Beautiful visualization tools (displaCy)\n\n**Best For:**\n\n- Production NLP pipelines\n- Real-time processing\n- Named Entity Recognition\n- Document similarity and classification\n- Information extraction at scale\n\n**Limitations:**\n\n- Less flexible than NLTK for research\n- Fewer resources for learning basic concepts\n- Model-dependent (needs pre-trained models)\n\n## spaCy Example: Basic Text Analysis\n\n::: {#15c5568a .cell execution_count=4}\n``` {.python .cell-code code-fold=\"true\"}\nimport spacy\n\n# Load English model (run: python -m spacy download en_core_web_sm)\nnlp = spacy.load(\"en_core_web_sm\")\n\ntext = \"\"\"Apple Inc. is planning to open a new store in San Francisco next month. \nThe CEO, Tim Cook, announced this during a press conference.\"\"\"\n\ndoc = nlp(text)\n\n# Tokenization and linguistic features\nprint(\"Tokens and their attributes:\")\nprint(\"{:<15} {:<10} {:<10} {:<10}\".format(\"Token\", \"Lemma\", \"POS\", \"Is Stop?\"))\nprint(\"-\" * 50)\nfor token in doc[:10]:  # First 10 tokens\n    print(\"{:<15} {:<10} {:<10} {:<10}\".format(\n        token.text, \n        token.lemma_, \n        token.pos_, \n        str(token.is_stop)\n    ))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTokens and their attributes:\nToken           Lemma      POS        Is Stop?  \n--------------------------------------------------\nApple           Apple      PROPN      False     \nInc.            Inc.       PROPN      False     \nis              be         AUX        True      \nplanning        plan       VERB       False     \nto              to         PART       True      \nopen            open       VERB       False     \na               a          DET        True      \nnew             new        ADJ        False     \nstore           store      NOUN       False     \nin              in         ADP        True      \n```\n:::\n:::\n\n\n## spaCy Example: Named Entity Recognition\n\n::: {#6f30dde2 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"true\"}\n# Named Entity Recognition\nprint(\"\\n\\nNamed Entities:\")\nprint(\"{:<20} {:<15} {:<30}\".format(\"Entity\", \"Type\", \"Explanation\"))\nprint(\"-\" * 70)\nfor ent in doc.ents:\n    print(\"{:<20} {:<15} {:<30}\".format(\n        ent.text, \n        ent.label_, \n        spacy.explain(ent.label_)\n    ))\n\n# Visualize entities\nfrom spacy import displacy\n\ndisplacy.render(doc, style=\"ent\", jupyter=True)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\nNamed Entities:\nEntity               Type            Explanation                   \n----------------------------------------------------------------------\nApple Inc.           ORG             Companies, agencies, institutions, etc.\nSan Francisco        GPE             Countries, cities, states     \nnext month           DATE            Absolute or relative dates or periods\nTim Cook             PERSON          People, including fictional   \n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Apple Inc.\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n is planning to open a new store in \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    San Francisco\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n \n<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    next month\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n</mark>\n. <br>The CEO, \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    Tim Cook\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n, announced this during a press conference.</div></span>\n```\n:::\n:::\n\n\n## spaCy Example: Dependency Parsing\n\n::: {#d7ff567a .cell execution_count=6}\n``` {.python .cell-code code-fold=\"true\"}\nsentence = nlp(\"The quick brown fox jumps over the lazy dog\")\n\nprint(\"\\nDependency Parse:\")\nprint(\"{:<10} {:<10} {:<10} {:<10}\".format(\"Token\", \"Dependency\", \"Head\", \"Children\"))\nprint(\"-\" * 50)\nfor token in sentence:\n    children = \", \".join([child.text for child in token.children])\n    print(\"{:<10} {:<10} {:<10} {:<10}\".format(\n        token.text,\n        token.dep_,\n        token.head.text,\n        children if children else \"-\"\n    ))\n\n# Visualize dependency tree\ndisplacy.render(\n    sentence,\n    style=\"dep\",\n    jupyter=True,\n    options={\n        \"compact\": False,\n        \"color\": \"blue\",\n        \"bg\": \"#fff\",\n        \"distance\": 120,\n        \"width\": 700,\n        \"height\": 300,\n        \"font\": \"10px Arial\"    # Reduce font size\n    }\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nDependency Parse:\nToken      Dependency Head       Children  \n--------------------------------------------------\nThe        det        fox        -         \nquick      amod       fox        -         \nbrown      amod       fox        -         \nfox        nsubj      jumps      The, quick, brown\njumps      ROOT       jumps      fox, over \nover       prep       jumps      dog       \nthe        det        dog        -         \nlazy       amod       dog        -         \ndog        pobj       over       the, lazy \n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"9f1cba80d77142088ea27c4e9c24bb1d-0\" class=\"displacy\" width=\"1130\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: blue; background: #fff; font-family: 10px Arial; direction: ltr\">\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">quick</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">brown</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">fox</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">jumps</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">over</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">ADP</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">the</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">DET</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">lazy</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">dog</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\">NOUN</tspan>\n</text>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-0\" stroke-width=\"2px\" d=\"M70,182.0 C70,2.0 410.0,2.0 410.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M70,184.0 L62,172.0 78,172.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-1\" stroke-width=\"2px\" d=\"M190,182.0 C190,62.0 405.0,62.0 405.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M190,184.0 L182,172.0 198,172.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-2\" stroke-width=\"2px\" d=\"M310,182.0 C310,122.0 400.0,122.0 400.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M310,184.0 L302,172.0 318,172.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-3\" stroke-width=\"2px\" d=\"M430,182.0 C430,122.0 520.0,122.0 520.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M430,184.0 L422,172.0 438,172.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-4\" stroke-width=\"2px\" d=\"M550,182.0 C550,122.0 640.0,122.0 640.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M640.0,184.0 L648.0,172.0 632.0,172.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-5\" stroke-width=\"2px\" d=\"M790,182.0 C790,62.0 1005.0,62.0 1005.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M790,184.0 L782,172.0 798,172.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-6\" stroke-width=\"2px\" d=\"M910,182.0 C910,122.0 1000.0,122.0 1000.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M910,184.0 L902,172.0 918,172.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-7\" stroke-width=\"2px\" d=\"M670,182.0 C670,2.0 1010.0,2.0 1010.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-9f1cba80d77142088ea27c4e9c24bb1d-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M1010.0,184.0 L1018.0,172.0 1002.0,172.0\" fill=\"currentColor\"/>\n</g>\n</svg></span>\n```\n:::\n:::\n\n\n## spaCy Example: Document Similarity\n\n::: {#84022ff5 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\n# Document similarity using word vectors\ndoc1 = nlp(\"I love programming in Python\")\ndoc2 = nlp(\"I enjoy coding with Python\")\ndoc3 = nlp(\"The weather is nice today\")\n\nprint(\"\\nDocument Similarity (using word vectors):\")\nprint(f\"doc1 <-> doc2: {doc1.similarity(doc2):.3f}\")\nprint(f\"doc1 <-> doc3: {doc1.similarity(doc3):.3f}\")\nprint(f\"doc2 <-> doc3: {doc2.similarity(doc3):.3f}\")\n\n# Word similarity\nword1 = nlp(\"king\")\nword2 = nlp(\"queen\")\nword3 = nlp(\"apple\")\n\nprint(\"\\nWord Similarity:\")\nprint(f\"king <-> queen: {word1.similarity(word2):.3f}\")\nprint(f\"king <-> apple: {word1.similarity(word3):.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nDocument Similarity (using word vectors):\ndoc1 <-> doc2: 0.839\ndoc1 <-> doc3: 0.271\ndoc2 <-> doc3: 0.322\n\nWord Similarity:\nking <-> queen: 0.422\nking <-> apple: 0.690\n```\n:::\n:::\n\n\n# BERTopic\n\n## Overview\n\n**BERTopic** is a modern topic modeling technique that leverages transformer-based embeddings.\n\n**Key Features:**\n\n- Uses BERT embeddings for semantic understanding\n- Automatically determines optimal number of topics\n- UMAP for dimensionality reduction\n- HDBSCAN for clustering\n- Class-based TF-IDF (c-TF-IDF) for topic representation\n- Interactive visualizations\n\n**Best For:**\n\n- Topic discovery in document collections\n- Short text analysis (tweets, reviews, articles)\n- Dynamic topic modeling over time\n- High-quality, interpretable topics\n- Modern alternative to LDA\n\n**Limitations:**\n\n- Computationally expensive (needs embeddings)\n- Requires more memory than classical methods\n- Slower than LDA for very large corpora\n- GPU recommended for large datasets\n\n## BERTopic Example: Basic Topic Modeling\n\n::: {#73767b23 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"true\"}\nfrom bertopic import BERTopic\nfrom sklearn.datasets import fetch_20newsgroups\n\n# Load sample data\ncategories = ['sci.space', 'rec.sport.baseball', 'talk.politics.guns']\nnewsgroups = fetch_20newsgroups(\n    subset='train', \n    categories=categories, \n    remove=('headers', 'footers', 'quotes')\n)\ndocs = newsgroups.data[:500]  # Use subset for speed\n\n# Create and fit BERTopic model\nprint(\"Training BERTopic model...\")\ntopic_model = BERTopic(verbose=False, language=\"english\", min_topic_size=10)\ntopics, probabilities = topic_model.fit_transform(docs)\n\nprint(f\"\\nDiscovered {len(set(topics)) - 1} topics (excluding outliers)\")\nprint(f\"Outlier documents (topic -1): {sum(1 for t in topics if t == -1)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining BERTopic model...\n\nDiscovered 1 topics (excluding outliers)\nOutlier documents (topic -1): 0\n```\n:::\n:::\n\n\n## BERTopic Example: Explore Topics\n\n::: {#93d61ad7 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\n# Get topic information\ntopic_info = topic_model.get_topic_info()\nprint(\"\\nTopic Information:\")\nprint(topic_info[['Topic', 'Count', 'Name']].head(10))\n\n# Show representative words for each topic\nprint(\"\\n\\nTop Words per Topic:\")\nprint(\"=\" * 80)\nfor topic_id in range(min(5, len(set(topics)) - 1)):  # Show first 5 topics\n    topic_words = topic_model.get_topic(topic_id)\n    if topic_words:\n        words = [word for word, score in topic_words[:8]]\n        print(f\"\\nTopic {topic_id}: {', '.join(words)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTopic Information:\n   Topic  Count             Name\n0      0    484  0_the_to_of_and\n1      1     16     1_anaheim___\n\n\nTop Words per Topic:\n================================================================================\n\nTopic 0: the, to, of, and, in, is, that, for\n```\n:::\n:::\n\n\n## BERTopic Example: Topic Visualization\n\n```python\n#| code-fold: false\n# Visualize topics\nfig = topic_model.visualize_topics()\nfig.show()\n\n# Visualize topic hierarchy\nfig_hierarchy = topic_model.visualize_hierarchy(top_n_topics=10)\nfig_hierarchy.show()\n\n# Visualize barchart for top topics\nfig_barchart = topic_model.visualize_barchart(top_n_topics=5, n_words=10)\nfig_barchart.show()\n```\n\n## BERTopic Example: Find Similar Documents\n\n::: {#7cf7fb04 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"true\"}\n# Find documents similar to a query\nsimilar_docs, similarity_scores = topic_model.find_topics(\n    \"space exploration and satellites\", \n    top_n=3\n)\n\nprint(\"\\nTopics similar to 'space exploration and satellites':\")\nfor topic_id, score in zip(similar_docs, similarity_scores):\n    print(f\"\\nTopic {topic_id} (similarity: {score:.3f}):\")\n    words = [word for word, _ in topic_model.get_topic(topic_id)[:5]]\n    print(f\"  Key words: {', '.join(words)}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTopics similar to 'space exploration and satellites':\n\nTopic 0 (similarity: 0.207):\n  Key words: the, to, of, and, in\n\nTopic 1 (similarity: 0.122):\n  Key words: anaheim, , , , \n```\n:::\n:::\n\n\n## BERTopic Example: Dynamic Topic Modeling\n\n::: {#91e2454b .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\n# Topic modeling over time (if you have timestamps)\nimport pandas as pd\nimport numpy as np\n\n# Create fake timestamps for demonstration\ntimestamps = pd.date_range('2020-01-01', periods=len(docs), freq='D')\n\n# Fit dynamic topic model\ntopics_over_time = topic_model.topics_over_time(\n    docs, \n    timestamps, \n    nr_bins=10\n)\n\nprint(\"\\nTopics Over Time:\")\nprint(topics_over_time.head(15))\n\n# Visualize topics over time\nfig_timeline = topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=5)\nfig_timeline.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTopics Over Time:\n    Topic                   Words  Frequency               Timestamp\n0       0    the, to, of, and, in         48 2019-12-31 12:01:26.400\n1       1         anaheim, , , ,           2 2019-12-31 12:01:26.400\n2       0    the, of, to, and, in         49 2020-02-19 21:36:00.000\n3       1         anaheim, , , ,           1 2020-02-19 21:36:00.000\n4       0    the, of, and, to, in         49 2020-04-09 19:12:00.000\n5       1         anaheim, , , ,           1 2020-04-09 19:12:00.000\n6       0    the, to, and, of, in         49 2020-05-29 16:48:00.000\n7       1         anaheim, , , ,           1 2020-05-29 16:48:00.000\n8       0  the, to, of, and, that         49 2020-07-18 14:24:00.000\n9       1         anaheim, , , ,           1 2020-07-18 14:24:00.000\n10      0    the, to, and, of, in         49 2020-09-06 12:00:00.000\n11      1         anaheim, , , ,           1 2020-09-06 12:00:00.000\n12      0    the, to, and, of, in         47 2020-10-26 09:36:00.000\n13      1         anaheim, , , ,           3 2020-10-26 09:36:00.000\n14      0    the, to, of, and, in         50 2020-12-15 07:12:00.000\n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        </script>\n        <script type=\"module\">import \"https://cdn.plot.ly/plotly-3.1.1.min\"</script>\n        \n```\n:::\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.1.1.min.js\" integrity=\"sha256-HUEFyfiTnZJxCxur99FjbKYTvKSzwDaD3/x5TqHpFu4=\" crossorigin=\"anonymous\"></script>                <div id=\"2f7c11b1-a66b-4e17-a729-d9007e86dcf2\" class=\"plotly-graph-div\" style=\"height:450px; width:1250px;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"2f7c11b1-a66b-4e17-a729-d9007e86dcf2\")) {                    Plotly.newPlot(                        \"2f7c11b1-a66b-4e17-a729-d9007e86dcf2\",                        [{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: the, to, of, and, in\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: the, of, to, and, in\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: the, of, and, to, in\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: the, to, and, of, in\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: the, to, of, and, that\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: the, to, and, of, in\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: the, to, and, of, in\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: the, to, of, and, in\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: the, to, of, and, in\",\"\\u003cb\\u003eTopic 0\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: the, to, and, of, in\"],\"marker\":{\"color\":\"#E69F00\"},\"mode\":\"lines\",\"name\":\"0_the_to_of_and\",\"x\":[\"2019-12-31T12:01:26.400000000\",\"2020-02-19T21:36:00.000000000\",\"2020-04-09T19:12:00.000000000\",\"2020-05-29T16:48:00.000000000\",\"2020-07-18T14:24:00.000000000\",\"2020-09-06T12:00:00.000000000\",\"2020-10-26T09:36:00.000000000\",\"2020-12-15T07:12:00.000000000\",\"2021-02-03T04:48:00.000000000\",\"2021-03-25T02:24:00.000000000\"],\"y\":{\"dtype\":\"i1\",\"bdata\":\"MDExMTExLzIvLw==\"},\"type\":\"scatter\"},{\"hoverinfo\":\"text\",\"hovertext\":[\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: anaheim, , , , \",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: anaheim, , , , \",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: anaheim, , , , \",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: anaheim, , , , \",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: anaheim, , , , \",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: anaheim, , , , \",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: anaheim, , , , \",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: anaheim, , , , \",\"\\u003cb\\u003eTopic 1\\u003c\\u002fb\\u003e\\u003cbr\\u003eWords: anaheim, , , , \"],\"marker\":{\"color\":\"#56B4E9\"},\"mode\":\"lines\",\"name\":\"1_anaheim___\",\"x\":[\"2019-12-31T12:01:26.400000000\",\"2020-02-19T21:36:00.000000000\",\"2020-04-09T19:12:00.000000000\",\"2020-05-29T16:48:00.000000000\",\"2020-07-18T14:24:00.000000000\",\"2020-09-06T12:00:00.000000000\",\"2020-10-26T09:36:00.000000000\",\"2021-02-03T04:48:00.000000000\",\"2021-03-25T02:24:00.000000000\"],\"y\":{\"dtype\":\"i1\",\"bdata\":\"AgEBAQEBAwMD\"},\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(36,36,36)\"},\"error_y\":{\"color\":\"rgb(36,36,36)\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"baxis\":{\"endlinecolor\":\"rgb(36,36,36)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(36,36,36)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"contour\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.6}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermapbox\"}],\"scattermap\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scattermap\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":1,\"tickcolor\":\"rgb(36,36,36)\",\"ticks\":\"outside\"}},\"colorscale\":{\"diverging\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"sequential\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"sequentialminus\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]]},\"colorway\":[\"#1F77B4\",\"#FF7F0E\",\"#2CA02C\",\"#D62728\",\"#9467BD\",\"#8C564B\",\"#E377C2\",\"#7F7F7F\",\"#BCBD22\",\"#17BECF\"],\"font\":{\"color\":\"rgb(36,36,36)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"margin\":{\"b\":0,\"l\":0,\"r\":0,\"t\":30},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"rgb(232,232,232)\",\"gridwidth\":2,\"linecolor\":\"rgb(36,36,36)\",\"showbackground\":true,\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"rgb(232,232,232)\",\"linecolor\":\"rgb(36,36,36)\",\"showgrid\":false,\"showline\":true,\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zeroline\":false,\"zerolinecolor\":\"rgb(36,36,36)\"}}},\"xaxis\":{\"showgrid\":true},\"yaxis\":{\"showgrid\":true,\"title\":{\"text\":\"Frequency\"}},\"title\":{\"font\":{\"size\":22,\"color\":\"Black\"},\"text\":\"\\u003cb\\u003eTopics over Time\\u003c\\u002fb\\u003e\",\"y\":0.95,\"x\":0.4,\"xanchor\":\"center\",\"yanchor\":\"top\"},\"hoverlabel\":{\"font\":{\"size\":16,\"family\":\"Rockwell\"},\"bgcolor\":\"white\"},\"width\":1250,\"height\":450,\"legend\":{\"title\":{\"text\":\"\\u003cb\\u003eGlobal Topic Representation\"}}},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('2f7c11b1-a66b-4e17-a729-d9007e86dcf2');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };            </script>        </div>\n```\n:::\n:::\n\n\n# Package Comparison\n\n## Quick Comparison Table\n\n| Feature | NLTK | spaCy | BERTopic |\n|---------|------|-------|----------|\n| **Primary Use** | Education, Research | Production NLP | Topic Modeling |\n| **Speed** | Slow | Fast | Moderate |\n| **Ease of Use** | Moderate | Easy | Easy |\n| **Pre-trained Models** | Limited | Excellent | Uses transformer embeddings |\n| **Customization** | High | Moderate | Moderate |\n| **Memory Usage** | Low | Low-Moderate | High |\n| **Best For** | Learning, Prototyping | NER, Pipelines, Real-time | Topic Discovery |\n| **Visualization** | Limited | Excellent (displaCy) | Excellent (interactive) |\n| **GPU Support** | No | Yes (for training) | Recommended |\n| **Community** | Large, Academic | Large, Industry | Growing |\n\n## When to Use Each Package\n\n**Use NLTK when:**\n\n- Learning NLP concepts\n- Need access to linguistic resources (WordNet, TreeBank)\n- Working on academic research\n- Prototyping ideas\n- Need maximum flexibility\n\n**Use spaCy when:**\n\n- Building production systems\n- Need fast, accurate NER\n- Processing large volumes of text\n- Want beautiful visualizations\n- Need dependency parsing\n- Building information extraction pipelines\n\n**Use BERTopic when:**\n\n- Discovering topics in document collections\n- Working with short texts (tweets, reviews)\n- Need interpretable, coherent topics\n- Want to avoid specifying number of topics\n- Have access to GPU resources\n- Analyzing topic evolution over time\n\n## Combining Packages\n\nThese packages can be used together effectively:\n\n::: {#e9865ef0 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\n# Example: Use NLTK for preprocessing, spaCy for NER, BERTopic for topics\n\nimport nltk\nimport spacy\nfrom bertopic import BERTopic\n\nnlp = spacy.load(\"en_core_web_sm\")\n\ndocuments = [\n    \"Apple Inc. announced new products in Cupertino yesterday.\",\n    \"Google is developing AI technology in Mountain View.\",\n    \"Microsoft released a new version of Windows in Seattle.\"\n]\n\n# Step 1: Use NLTK for basic preprocessing\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\n# Step 2: Use spaCy for NER and lemmatization\nprocessed_docs = []\nfor doc in documents:\n    spacy_doc = nlp(doc)\n    \n    # Extract entities\n    entities = [(ent.text, ent.label_) for ent in spacy_doc.ents]\n    print(f\"\\nDocument: {doc}\")\n    print(f\"Entities: {entities}\")\n    \n    # Lemmatize and remove stopwords\n    lemmatized = [token.lemma_ for token in spacy_doc \n                  if not token.is_stop and not token.is_punct]\n    processed_docs.append(\" \".join(lemmatized))\n\nprint(\"\\n\\nProcessed documents:\")\nfor i, doc in enumerate(processed_docs, 1):\n    print(f\"{i}. {doc}\")\n\n# Step 3: Use BERTopic for topic modeling (would need more documents in practice)\n# topic_model = BERTopic()\n# topics, probs = topic_model.fit_transform(processed_docs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nDocument: Apple Inc. announced new products in Cupertino yesterday.\nEntities: [('Apple Inc.', 'ORG'), ('Cupertino', 'GPE'), ('yesterday', 'DATE')]\n\nDocument: Google is developing AI technology in Mountain View.\nEntities: [('Google', 'ORG'), ('AI', 'ORG'), ('Mountain View', 'GPE')]\n\nDocument: Microsoft released a new version of Windows in Seattle.\nEntities: [('Microsoft', 'ORG'), ('Windows', 'NORP'), ('Seattle', 'GPE')]\n\n\nProcessed documents:\n1. Apple Inc. announce new product Cupertino yesterday\n2. Google develop AI technology Mountain View\n3. Microsoft release new version Windows Seattle\n```\n:::\n:::\n\n\n# Summary\n\n- **NLTK**: Comprehensive, educational, flexible but slower\n- **spaCy**: Fast, production-ready, excellent for NER and pipelines\n- **BERTopic**: Modern topic modeling with transformer embeddings\n\nChoose based on your specific needs: learning (NLTK), production (spaCy), or topic discovery (BERTopic). Often, combining these tools yields the best results!\n\n",
    "supporting": [
      "nlp_packages_overview_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}