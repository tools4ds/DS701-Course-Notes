{
  "hash": "9b33c3ae54057f314e4081c34415ec82",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Time Series Analysis'\njupyter: python3\nbibliography: references.bib\n---\n\n## Colab\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/26-TimeSeries.ipynb)\n\n\n# Introduction to Time Series\n\n## What is a Time Series?\n\n::: {#066793c1 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-2-output-1.png){width=816 height=302 fig-align='center'}\n:::\n:::\n\n\n**Time series** = data points indexed by time (hourly, daily, monthly...)\n\nTwo key goals:\n\n1. **Analysis**: What patterns exist? Trends? Seasonality? Anomalies?\n2. **Forecasting**: What comes next?\n\n## Time Series Are Everywhere\n\n::: {#723c706f .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-3-output-1.png){width=819 height=302 fig-align='center'}\n:::\n:::\n\n\n| Domain | Example Questions |\n|--------|-------------------|\n| **Finance** | Will this stock go up? What's my portfolio risk? |\n| **Healthcare** | Is this patient's vitals trending abnormally? |\n| **Climate** | How much warmer will next decade be? |\n| **Retail** | How much inventory do I need for Black Friday? |\n\n## The Four Components\n\nEvery time series can be decomposed into:\n\n| Component | What it captures | Example |\n|-----------|------------------|---------|\n| **Trend** | Long-term direction | \"Are EV sales growing?\" |\n| **Seasonality** | Fixed-period cycles | \"Ice cream sales spike in summer\" |\n| **Cyclic** | Variable-length oscillations | \"Business cycles\" |\n| **Noise** | Random variation | \"Day-to-day fluctuations\" |\n\n## Today's Roadmap\n\n1. **Visualize** â€” See patterns before modeling\n2. **Decompose** â€” Separate trend, seasonality, noise\n3. **Test** â€” Is it stationary? (Spoiler: it matters)\n4. **Model** â€” AR, MA, ARIMA, SARIMA\n5. **Forecast** â€” Predict the future (with uncertainty)\n\n::: {.content-visible when-profile=\"slides\"}\n## Time and Date in Python\n\n> See course notes for datetime/pandas manipulation details \n> (based on [Python for Data Analysis, 3rd Ed.](https://wesmckinney.com/book/time-series))\n:::\n\n::: {.content-visible when-profile=\"web\"}\n# Time and Date Manipulation\n\nMany time series data sets are indexed by date or time. The python `datetime`\nlibrary and the `pandas` library provide a powerful set of tools for manipulating\ntime series data.\n\nThe [Time Series](https://wesmckinney.com/book/time-series) chapter of the book\n[Python for Data Analysis, 3rd Ed.](https://wesmckinney.com/book/time-series)\nprovides a good overview of these tools. We'll share a few excerpts here.\n\n::: {#e6bce370 .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\n\nnow = datetime.now()\nprint(f\"Date and time when this cell was executed: {now}\")\nprint(f\"Year: {now.year}, month: {now.month}, day: {now.day}\")\n\ndelta = now - datetime(2024, 1, 1)\nprint(f\"Since beginning of 2024 till when this cell was run there were {delta.days} days and {delta.seconds} seconds.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDate and time when this cell was executed: 2025-12-01 16:41:28.256360\nYear: 2025, month: 12, day: 1\nSince beginning of 2024 till when this cell was run there were 700 days and 60088 seconds.\n```\n:::\n:::\n\n\nYou can also convert between strings and datetime.\n\n::: {#88e1cd96 .cell execution_count=5}\n``` {.python .cell-code}\n# string to datetime\ndate_string = \"2024-01-01\"\ndate_object = datetime.strptime(date_string, \"%Y-%m-%d\")\nprint(date_object)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2024-01-01 00:00:00\n```\n:::\n:::\n\n\nYou can also format datetime objects as strings.\n\n::: {#eded54b8 .cell execution_count=6}\n``` {.python .cell-code}\n# datetime to string\nnow_str = now.strftime(\"%Y-%m-%d\")\nprint(now_str)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n2025-12-01\n```\n:::\n:::\n\n\nSee Table 11.2 in the [book](https://wesmckinney.com/book/time-series) for a list of formatting codes.\n\nLet's explore some of the pandas time series tools.\n\nCreate a time series with a datetime index.\n\n::: {#5c7a815d .cell execution_count=7}\n``` {.python .cell-code}\nlonger_ts = pd.Series(np.random.standard_normal(1000),\n                      index=pd.date_range(\"2022-01-01\", periods=1000))\nprint(type(longer_ts))\nlonger_ts\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.series.Series'>\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=86}\n```\n2022-01-01    0.280913\n2022-01-02    0.698618\n2022-01-03    0.184406\n2022-01-04   -0.668553\n2022-01-05   -0.663901\n                ...   \n2024-09-22   -0.494914\n2024-09-23   -0.047742\n2024-09-24    0.798230\n2024-09-25   -0.165061\n2024-09-26   -0.838268\nFreq: D, Length: 1000, dtype: float64\n```\n:::\n:::\n\n\nWe can access just the samples from 2023 with simply:\n\n::: {#687a1d9f .cell execution_count=8}\n``` {.python .cell-code}\nlonger_ts[\"2023\"]\n```\n\n::: {.cell-output .cell-output-display execution_count=87}\n```\n2023-01-01   -0.110746\n2023-01-02    2.472920\n2023-01-03    1.638953\n2023-01-04   -0.599483\n2023-01-05    1.439102\n                ...   \n2023-12-27   -0.229206\n2023-12-28   -0.612435\n2023-12-29    1.310627\n2023-12-30   -1.157617\n2023-12-31    1.020339\nFreq: D, Length: 365, dtype: float64\n```\n:::\n:::\n\n\nOr the month of September 2023:\n\n::: {#a17a7c27 .cell execution_count=9}\n``` {.python .cell-code}\nlonger_ts[\"2023-09\"]\n```\n\n::: {.cell-output .cell-output-display execution_count=88}\n```\n2023-09-01    0.879811\n2023-09-02    2.202641\n2023-09-03    1.052441\n2023-09-04    1.140192\n2023-09-05    1.827791\n2023-09-06   -1.106154\n2023-09-07    0.598685\n2023-09-08    0.353720\n2023-09-09   -0.097680\n2023-09-10   -0.143724\n2023-09-11    1.432225\n2023-09-12    2.368028\n2023-09-13   -0.634542\n2023-09-14   -0.787193\n2023-09-15    0.552188\n2023-09-16   -1.152153\n2023-09-17   -1.407178\n2023-09-18   -0.520248\n2023-09-19    1.281355\n2023-09-20    0.013912\n2023-09-21   -0.098277\n2023-09-22   -1.563940\n2023-09-23   -1.380164\n2023-09-24   -1.818156\n2023-09-25    1.103920\n2023-09-26    0.580952\n2023-09-27   -1.651803\n2023-09-28   -0.583347\n2023-09-29    0.133064\n2023-09-30   -2.725405\nFreq: D, dtype: float64\n```\n:::\n:::\n\n\nOr slice by date range:\n\n::: {#c97f887c .cell execution_count=10}\n``` {.python .cell-code}\nlonger_ts[\"2023-03-01\":\"2023-03-10\"]\n```\n\n::: {.cell-output .cell-output-display execution_count=89}\n```\n2023-03-01   -0.576257\n2023-03-02   -0.720482\n2023-03-03    1.328086\n2023-03-04    0.276022\n2023-03-05    0.939506\n2023-03-06   -0.031456\n2023-03-07   -1.946310\n2023-03-08    0.472625\n2023-03-09   -0.387750\n2023-03-10   -0.451032\nFreq: D, dtype: float64\n```\n:::\n:::\n\n\nor:\n\n::: {#ef644953 .cell execution_count=11}\n``` {.python .cell-code}\nlonger_ts[\"2023-09-15\":]\n```\n\n::: {.cell-output .cell-output-display execution_count=90}\n```\n2023-09-15    0.552188\n2023-09-16   -1.152153\n2023-09-17   -1.407178\n2023-09-18   -0.520248\n2023-09-19    1.281355\n                ...   \n2024-09-22   -0.494914\n2024-09-23   -0.047742\n2024-09-24    0.798230\n2024-09-25   -0.165061\n2024-09-26   -0.838268\nFreq: D, Length: 378, dtype: float64\n```\n:::\n:::\n\n\nThere are many more time series tools available that let you do things like:\n\n- Shifting and setting frequencies of date ranges\n- Time zone handling\n- Time series resampling\n- Time series rolling and expanding windows\n\n## Moving Window Functions\n\nLet's dive into the moving window functions.\n\n::: {#936b15fb .cell execution_count=12}\n``` {.python .cell-code}\nimport pandas as pd\nimport yfinance as yf\nimport os\n\n# Define the file path for cached data (10-year history)\naapl_10yr_file_path = os.path.join('data', 'aapl_stock_data_10yr.csv')\n\n# Check if the file exists and load it, otherwise download\nimport warnings\n\naapl_data = None\nif os.path.exists(aapl_10yr_file_path):\n    # Load data from file - yfinance creates multi-level headers, skip rows 1,2\n    aapl_data = pd.read_csv(aapl_10yr_file_path, header=0, skiprows=[1, 2], index_col=0, parse_dates=True)\nelse:\n    # Download data from yfinance\n    try:\n        aapl_data = yf.download('AAPL', start='2012-01-01', end='2022-01-01', progress=False)\n        \n        # Check if download was successful (non-empty dataframe)\n        if aapl_data.empty:\n            warnings.warn(\"Downloaded data is empty. Please check the ticker symbol and date range.\")\n            aapl_data = None\n        else:\n            # Flatten columns if MultiIndex (yfinance >= 0.2.40)\n            if isinstance(aapl_data.columns, pd.MultiIndex):\n                aapl_data.columns = aapl_data.columns.get_level_values(0)\n            \n            # Save to file for future use\n            aapl_data.to_csv(aapl_10yr_file_path)\n        \n    except Exception as e:\n        warnings.warn(f\"Error downloading data from yfinance: {str(e)}\")\n        aapl_data = None\n\nif aapl_data is not None:\n    print(aapl_data.head())\n    aapl_close_px = aapl_data['Close']\nelse:\n    print(\"AAPL data not available - skipping this example\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\n1 Failed download:\n['AAPL']: YFRateLimitError('Too Many Requests. Rate limited. Try after a while.')\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nAAPL data not available - skipping this example\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/j_/hxxgy5dd7655k_416s6t9kgc0000gq/T/ipykernel_74531/2732215069.py:22: UserWarning:\n\nDownloaded data is empty. Please check the ticker symbol and date range.\n\n```\n:::\n:::\n\n\n::: {#2d999af7 .cell execution_count=13}\n``` {.python .cell-code}\n# Plot the closing prices\nimport matplotlib.pyplot as plt\n\nif aapl_data is not None:\n    ax = aapl_close_px.plot(label='AAPL')\n    aapl_close_px.rolling(window=250).mean().plot(label='250d MA', ax=ax)\n    ax.legend()\n```\n:::\n\n\n:::\n\n# Visualization\n\nAlways visualize first! Let's explore patterns in a classic dataset.\n\n## Air Passengers Dataset\n\n\n\nWe're going to use a dataset of air passengers per month from 1949 to 1960.\n\n::: {#8bb06362 .cell execution_count=15}\n``` {.python .cell-code}\npath = os.path.join('data', 'air_passengers_1949_1960.csv')\nair_passengers = pd.read_csv(path, index_col='Date', parse_dates=True)\nair_passengers.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=94}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Number of Passengers</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1949-01-01</th>\n      <td>112</td>\n    </tr>\n    <tr>\n      <th>1949-02-01</th>\n      <td>118</td>\n    </tr>\n    <tr>\n      <th>1949-03-01</th>\n      <td>132</td>\n    </tr>\n    <tr>\n      <th>1949-04-01</th>\n      <td>129</td>\n    </tr>\n    <tr>\n      <th>1949-05-01</th>\n      <td>121</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Time Series Plot\n\nLet's look at the time series plot.\n\n::: {#7a0863fc .cell execution_count=16}\n``` {.python .cell-code}\nts = air_passengers['Number of Passengers']\nts.plot(ylabel='Number of Passengers', title='Air Passengers 1949-1960', figsize=(10, 4))\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-16-output-1.png){width=816 height=376 fig-align='center'}\n:::\n:::\n\n\nClearly there are some trends and seasonality in the data.\n\nClearly: **trend** (going up) + **seasonality** (annual pattern) + **growing amplitude**.\n\n---\n\nLet's visualize that seasonal pattern more explicitly:\n\n::: {#09514b11 .cell execution_count=17}\n``` {.python .cell-code}\n# Seasonal plot of air_passengers\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Extract month and year from the index\nair_passengers['Month'] = air_passengers.index.month\nair_passengers['Year'] = air_passengers.index.year\n\n# Create a seasonal plot\n#plt.figure(figsize=(10, 4))\nsns.lineplot(data=air_passengers, x='Month', y='Number of Passengers', hue='Year', palette='tab10')\n# plt.title('Seasonal Plot of Air Passengers')\n# plt.ylabel('Number of Passengers')\n# plt.xlabel('Month')\n# plt.legend(title='Year', bbox_to_anchor=(1.05, 1), loc='upper left')\n# plt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-17-output-1.png){width=593 height=429 fig-align='center'}\n:::\n:::\n\n\nNotice: the seasonal amplitude is **growing** over time â€” important for model choice!\n\n---\n\nLet's look at the distribution across years and months:\n\n::: {#1e230b18 .cell execution_count=18}\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/j_/hxxgy5dd7655k_416s6t9kgc0000gq/T/ipykernel_74531/2554698556.py:9: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-18-output-2.png){width=1142 height=374 fig-align='center'}\n:::\n:::\n\n\n**Left**: Trend is obvious. **Right**: July-August are peak travel months.\n\n## Autocorrelation: The Key Diagnostic\n\n**Autocorrelation** = correlation between $y_t$ and $y_{t-k}$ (lagged by $k$ steps)\n\n$$\n\\rho_k = \\frac{\\sum_{t=k+1}^{T} (y_t - \\bar{y})(y_{t-k} - \\bar{y})}{\\sum_{t=1}^{T} (y_t - \\bar{y})^2}\n$$\n\n**Why it matters:**\n\n- High autocorrelation (for $k>0$) â†’ values depend on past â†’ predictable!\n- Peaks at regular lags â†’ seasonality\n- No autocorrelation â†’ white noise (unpredictable)\n\n## ACF of Air Passengers\n\n::: {#05bca141 .cell execution_count=19}\n``` {.python .cell-code}\nfrom statsmodels.graphics.tsaplots import plot_acf\n\nplot_acf(air_passengers['Number of Passengers'], lags=48)\nplt.title('Autocorrelation Plot of Air Passengers')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-19-output-1.png){width=590 height=431 fig-align='center'}\n:::\n:::\n\n\n**Reading the ACF**: Blue shading = 95% CI. Peak at lag 12 is significant â†’ yearly seasonality confirmed!\n\n# Time Series Decomposition\n\n## The Goal: Separate Signal from Noise\n\nWe want to break apart: **Observed = Trend + Seasonality + Residual**\n\nWhy?\n\n- **Understand** the underlying patterns\n- **Remove** seasonality for cleaner modeling  \n- **Detect** anomalies in the residuals\n- **Forecast** each component separately\n\n## Additive vs Multiplicative\n\n**How do the components combine?**\n\n| Model | Formula | When to use |\n|-------|---------|-------------|\n| **Additive** | $Y = T + S + \\epsilon$ | Seasonal amplitude is constant |\n| **Multiplicative** | $Y = T \\times S \\times \\epsilon$ | Seasonal amplitude grows with trend |\n\nLook back at air passengers: amplitude **grows** â†’ multiplicative is better!\n\n## Classical Decomposition: The Algorithm\n\n**Steps:**\n\n1. **Estimate trend** â€” moving average smooths out seasonality\n2. **Remove trend** â€” detrended = observed - trend  \n3. **Estimate seasonality** â€” average each month's detrended values\n4. **Residual** â€” what's left after removing trend & seasonality\n\n---\n\nLet's do it step by step:\n\n::: {#423bd8d2 .cell execution_count=20}\n``` {.python .cell-code}\nimport numpy as np\n\n# Step 1: Trend via moving average\ntrend = ts.rolling(window=12, center=True).mean()\n\n# Step 2: Detrend\ndetrended_ts = ts - trend\n\n# Step 3: Seasonal pattern (average by month)\nseasonal_ts = detrended_ts.groupby(detrended_ts.index.month).mean()\n\n# Step 4: Residual\nirregular_ts = detrended_ts - np.tile(seasonal_ts, len(detrended_ts) // len(seasonal_ts))\n\n# Plot all components\nfig, axes = plt.subplots(4, 1, figsize=(10, 8), sharex=False)\n\nts.plot(ax=axes[0], title='Original')\ntrend.plot(ax=axes[1], title='Trend (12-month MA)', color='orange')\ndetrended_ts.plot(ax=axes[2], title='Detrended', color='green')\nirregular_ts.plot(ax=axes[3], title='Residual', color='red')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-20-output-1.png){width=950 height=758 fig-align='center'}\n:::\n:::\n\n\n**Question**: Is the residual truly random? (Hint: look at the variance over time)\n\n\n## STL: A Better Decomposition\n\n**STL** = Seasonal-Trend decomposition using Loess [@cleveland1990stl]\n\nWhy STL over classical?\n\n- âœ… Handles **changing seasonality** (amplitude can vary)\n- âœ… Robust to **outliers**\n- âœ… Works with **any period** (not just 12 months)\n\n> See course notes for details on Loess (locally weighted regression)\n\n::: {.content-visible when-profile=\"web\"}\n\n## Locally Estimated Scatterplot Smoothing -- Loess[^gpt4o]\n\n[^gpt4o]: gpt-4o, personal communication, Nov 2024\n\n**Loess**, which stands for \"Locally Estimated Scatterplot Smoothing,\" is a non-parametric method used to estimate non-linear relationships in data. It is particularly useful for smoothing scatterplots and is a type of local regression.\n\n### Key Features of Loess:\n\n1. **Local Fitting**: Loess fits simple models to localized subsets of the data to build up a function that describes the deterministic part of the variation in the data, point by point.\n\n2. **Weighted Least Squares**: It uses weighted least squares to fit a polynomial surface to the data. The weights decrease with distance from the point of interest, giving more influence to points near the target point.\n\n3. **Flexibility**: Loess is flexible and can model complex relationships without assuming a specific global form for the data. It can adapt to various shapes and patterns in the data.\n\n4. **Smoothing Parameter**: The degree of smoothing is controlled by a parameter, often denoted as $\\alpha$ or the span. This parameter determines the proportion of data points used in each local fit. A smaller span results in a curve that follows the data more closely, while a larger span results in a smoother curve.\n\n5. **Polynomial Degree**: Loess can fit either linear or quadratic polynomials to the data. The choice of polynomial degree affects the smoothness and flexibility of the fit.\n\n### How Loess Works:\n\n- **Step 1**: For each point in the dataset, a neighborhood of points is selected based on the smoothing parameter.\n- **Step 2**: A weighted least squares regression is performed on the points in the neighborhood, with weights decreasing with distance from the target point.\n- **Step 3**: The fitted value at the target point is computed from the local regression model.\n- **Step 4**: This process is repeated for each point in the dataset, resulting in a smooth curve that captures the underlying trend.\n\n### Applications:\n\nLoess is widely used in exploratory data analysis to visualize trends and patterns in data. It is particularly useful when the relationship between variables is complex and not well-represented by a simple linear or polynomial model.\n\n### Example in Python:\n\nIn Python, the `statsmodels` library provides a function for performing Loess smoothing:\n\n::: {#89545c57 .cell execution_count=21}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom statsmodels.nonparametric.smoothers_lowess import lowess\n\n# Example data\nx = np.linspace(0, 10, 100)\ny = np.sin(x) + np.random.normal(0, 0.1, 100)\n\n# Apply Loess smoothing\nsmoothed = lowess(y, x, frac=0.2)\n\n# Plot\nplt.scatter(x, y, label='Data', alpha=0.5)\nplt.plot(smoothed[:, 0], smoothed[:, 1], color='red', label='Loess Smoothed')\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-21-output-1.png){width=582 height=411}\n:::\n:::\n\n\nIn this example, `frac` is the smoothing parameter that controls the amount of smoothing applied to the data.\n:::\n\n## In Practice: `statsmodels`\n\n::: {#7db082ea .cell execution_count=22}\n``` {.python .cell-code code-fold=\"false\"}\nfrom statsmodels.tsa.seasonal import seasonal_decompose, STL\n\n# Reload clean data\ndata = pd.read_csv(os.path.join('data', 'air_passengers_1949_1960.csv'), index_col='Date', parse_dates=True)\nts = data['Number of Passengers']\n```\n:::\n\n\n## Additive vs Multiplicative: See the Difference\n\n::: {#860d81c0 .cell execution_count=23}\n``` {.python .cell-code}\nfig, axes = plt.subplots(2, 4, figsize=(14, 6))\n\nfor i, model in enumerate(['additive', 'multiplicative']):\n    decomp = seasonal_decompose(ts, model=model)\n    decomp.observed.plot(ax=axes[i, 0], title='Observed' if i==0 else '')\n    decomp.trend.plot(ax=axes[i, 1], title='Trend' if i==0 else '')\n    decomp.seasonal.plot(ax=axes[i, 2], title='Seasonal' if i==0 else '')\n    decomp.resid.plot(ax=axes[i, 3], title='Residual' if i==0 else '')\n    axes[i, 0].set_ylabel(model.title())\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-23-output-1.png){width=1334 height=566 fig-align='center'}\n:::\n:::\n\n\n**Key insight**: Multiplicative residuals are more uniform â€” better fit!\n\n## STL in Action\n\n::: {#cb361d9e .cell execution_count=24}\n``` {.python .cell-code}\nfrom statsmodels.tsa.seasonal import STL\n\nstl = STL(ts, period=12, robust=True)\nresult = stl.fit()\n\nfig, axes = plt.subplots(4, 1, figsize=(10, 6), sharex=True)\nresult.observed.plot(ax=axes[0], title='STL Decomposition')\naxes[0].set_ylabel('Observed')\nresult.trend.plot(ax=axes[1])\naxes[1].set_ylabel('Trend')\nresult.seasonal.plot(ax=axes[2])\naxes[2].set_ylabel('Seasonal')\nresult.resid.plot(ax=axes[3])\naxes[3].set_ylabel('Residual')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-24-output-1.png){width=950 height=566 fig-align='center'}\n:::\n:::\n\n\n# Stationarity: Why It Matters\n\n## The Stationarity Problem\n\n**Stationary** = statistical properties (mean, variance) don't change over time\n\n**Why do we care?**\n\nMost forecasting models **assume stationarity**. If your data has:\n\n- ðŸ“ˆ Trend â†’ mean is changing\n- ðŸ“Š Growing variance â†’ variance is changing  \n- ðŸ”„ Seasonality â†’ periodic non-stationarity\n\n...the model will fail. We must **transform** the data first.\n\n## Making Data Stationary\n\n| Problem | Solution |\n|---------|----------|\n| Trend | **Differencing**: $y'_t = y_t - y_{t-1}$ |\n| Growing variance | **Log transform**: $y'_t = \\log(y_t)$ |\n| Seasonality | **Seasonal differencing**: $y'_t = y_t - y_{t-12}$ |\n\n## Testing for Stationarity\n\nTwo standard tests (both in `statsmodels`):\n\n| Test | Null Hypothesis | Reject means... |\n|------|-----------------|-----------------|\n| **ADF** | Has unit root (non-stationary) | Stationary âœ“ |\n| **KPSS** | Is stationary | Non-stationary âœ— |\n\nUse both â€” if they agree, you can trust the result!\n\n::: {.content-visible when-profile=\"web\"}\n\n## ADF Test Demonstration\n\nLet's apply the tests, starting with ADF:\n\n::: {#3d790ccb .cell execution_count=25}\n``` {.python .cell-code}\nimport pandas as pd\nfrom statsmodels.tsa.stattools import adfuller, kpss\n\nair_passengers = pd.read_csv(os.path.join('data', 'air_passengers_1949_1960.csv'))\n\n# Apply ADF and KPSS tests on the air passenger data\n\n# ADF Test\nresult_adf = adfuller(air_passengers['Number of Passengers'])\nprint('ADF Statistic:', result_adf[0])\nprint('p-value:', result_adf[1])\nprint('Critical Values:')\nfor key, value in result_adf[4].items():\n    print('\\t%s: %.3f' % (key, value))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nADF Statistic: 0.8153688792060467\np-value: 0.991880243437641\nCritical Values:\n\t1%: -3.482\n\t5%: -2.884\n\t10%: -2.579\n```\n:::\n:::\n\n\n### Interpretation:\n\n1. **ADF Statistic**: The ADF statistic is 0.815, which is greater than all the critical values at the 1%, 5%, and 10% significance levels.\n\n2. **p-value**: The p-value is 0.991, which is significantly higher than common significance levels (e.g., 0.01, 0.05, 0.10).\n\n3. **Critical Values**:\n   - 1%: -3.482\n   - 5%: -2.884\n   - 10%: -2.579\n\n### Conclusion:\n\n- **Fail to Reject the Null Hypothesis**: Since the ADF statistic (0.815) is greater than the critical values and the p-value (0.991) is much higher than typical significance levels, you fail to reject the null hypothesis. This suggests that the time series has a unit root and is non-stationary.\n\n- **Implication**: The time series data for the number of passengers is non-stationary, indicating that it may have a trend or other non-stationary components. To make the series stationary, you might consider differencing the data or applying other transformations, such as detrending or seasonal adjustment, before proceeding with further analysis or modeling.\n\n## KPSS Test Demonstration\n\nLet's apply the KPSS test.\n\n::: {#1ca2b65d .cell execution_count=26}\n``` {.python .cell-code}\n# KPSS Test\nresult_kpss = kpss(air_passengers['Number of Passengers'], regression='c')\nprint('\\nKPSS Statistic:', result_kpss[0])\nprint('p-value:', result_kpss[1])\nprint('Critical Values:')\nfor key, value in result_kpss[3].items():\n    print('\\t%s: %.3f' % (key, value))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nKPSS Statistic: 1.6513122354165206\np-value: 0.01\nCritical Values:\n\t10%: 0.347\n\t5%: 0.463\n\t2.5%: 0.574\n\t1%: 0.739\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/var/folders/j_/hxxgy5dd7655k_416s6t9kgc0000gq/T/ipykernel_74531/3460980279.py:2: InterpolationWarning:\n\nThe test statistic is outside of the range of p-values available in the\nlook-up table. The actual p-value is smaller than the p-value returned.\n\n\n```\n:::\n:::\n\n\nThe KPSS (Kwiatkowski-Phillips-Schmidt-Shin) test is another test used to assess the stationarity of a time series, but it has a different null hypothesis compared to the ADF test.\n\n### KPSS Test Interpretation:\n\n1. **Null Hypothesis (\\(H_0\\))**: The null hypothesis of the KPSS test is that the time series is stationary around a deterministic trend (i.e., it does not have a unit root).\n\n2. **Alternative Hypothesis (\\(H_1\\))**: The alternative hypothesis is that the time series is not stationary (i.e., it has a unit root).\n\n### Given Results:\n\n- **KPSS Statistic**: 1.651\n- **p-value**: 0.01\n- **Critical Values**:\n  - 10%: 0.347\n  - 5%: 0.463\n  - 2.5%: 0.574\n  - 1%: 0.739\n\n### Conclusion:\n\n- **Reject the Null Hypothesis**: The KPSS statistic (1.651) is greater than all the critical values at the 10%, 5%, 2.5%, and 1% significance levels. This, along with the low p-value (0.01), suggests that you reject the null hypothesis of stationarity.\n\n- **Implication**: The time series is likely non-stationary according to the KPSS test. This aligns with the ADF test results, which also indicated non-stationarity.\n\n### Overall Interpretation:\n\nBoth the ADF and KPSS tests suggest that the time series is non-stationary. This consistent result from both tests strengthens the conclusion that the series may need differencing or other transformations to achieve stationarity before further analysis or modeling.\n\n:::\n\n# Time Series Models\n\n## The ARIMA Family\n\nBuilding blocks for forecasting:\n\n| Model | Idea | Parameters |\n|-------|------|------------|\n| **AR(p)** | Past **values** predict future | $y_t = c + \\phi_1 y_{t-1} + ... + \\epsilon_t$ |\n| **MA(q)** | Past **errors** predict future | $y_t = \\mu + \\theta_1 \\epsilon_{t-1} + ... + \\epsilon_t$ |\n| **ARIMA(p,d,q)** | AR + MA + **differencing** | Combines both, d = differencing order |\n| **SARIMA** | ARIMA + **seasonality** | (p,d,q) Ã— (P,D,Q,s) |\n\n## ARIMA: The Workhorse\n\n$$\ny_t = c + \\underbrace{\\phi_1 y_{t-1} + ... + \\phi_p y_{t-p}}_{\\text{AR: past values}} + \\underbrace{\\theta_1 \\epsilon_{t-1} + ... + \\theta_q \\epsilon_{t-q}}_{\\text{MA: past errors}} + \\epsilon_t\n$$\n\n**ARIMA(p, d, q)**:\n\n- **p** = AR order (how many past values?)\n- **d** = differencing order (how many times to difference?)\n- **q** = MA order (how many past errors?)\n\n## SARIMA: Adding Seasonality\n\nFor data with seasonal patterns, add seasonal terms:\n\n**SARIMA(p,d,q) Ã— (P,D,Q,s)**\n\n- Lowercase (p,d,q) = short-term dynamics\n- Uppercase (P,D,Q) = seasonal dynamics  \n- **s** = seasonal period (12 for monthly, 4 for quarterly)\n\nExample: SARIMA(1,1,1)Ã—(1,1,1,12) for monthly data with yearly seasonality\n\n\n## SARIMA in Practice\n\nLet's forecast air passengers with SARIMA(1,1,1)Ã—(1,1,1,12):\n\n::: {#fb4c9856 .cell execution_count=27}\n``` {.python .cell-code}\nimport numpy as np\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n# Prepare data\npath = os.path.join('data', 'air_passengers_1949_1960.csv')\ndata = pd.read_csv(path)\ndata['Month'] = pd.date_range(start='1949-01', periods=len(data), freq='ME')\ndata.set_index('Month', inplace=True)\n\n# Log transform stabilizes growing variance\ndata['Log_Passengers'] = np.log(data['Number of Passengers'])\n```\n:::\n\n\n---\n\n::: {#e68b1cff .cell execution_count=28}\n``` {.python .cell-code code-fold=\"false\"}\n# Fit SARIMA model\nmodel = SARIMAX(data['Log_Passengers'], \n                order=(1, 1, 1),           # (p,d,q) \n                seasonal_order=(1, 1, 1, 12),  # (P,D,Q,s)\n                freq='ME')\nresults = model.fit()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning:\n\nNo frequency information was provided, so inferred frequency ME will be used.\n\n/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/lib/python3.10/site-packages/numpy/linalg/_linalg.py:3383: RuntimeWarning:\n\ndivide by zero encountered in matmul\n\n/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/lib/python3.10/site-packages/numpy/linalg/_linalg.py:3383: RuntimeWarning:\n\noverflow encountered in matmul\n\n/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/lib/python3.10/site-packages/numpy/linalg/_linalg.py:3383: RuntimeWarning:\n\ninvalid value encountered in matmul\n\n/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/lib/python3.10/site-packages/statsmodels/base/model.py:607: ConvergenceWarning:\n\nMaximum Likelihood optimization failed to converge. Check mle_retvals\n\n```\n:::\n:::\n\n\n## Model Diagnostics\n\n::: {#e40849a5 .cell execution_count=29}\n``` {.python .cell-code}\nresults.plot_diagnostics(figsize=(12, 6))\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-29-output-1.png){width=1142 height=566 fig-align='center'}\n:::\n:::\n\n\n**What to check:**\n\n- **Residuals**: Should look like white noise (no pattern)\n- **Histogram**: Should be roughly normal\n- **Q-Q plot**: Points should follow the diagonal\n- **ACF**: No significant spikes (we captured all structure)\n\n---\n\nFinally, let's make a forecast for 2 years out.\n\n::: {#bf31388c .cell execution_count=30}\n``` {.python .cell-code}\n# Forecasting\nforecast = results.get_forecast(steps=24)\nforecast_index = pd.date_range(data.index[-1] + pd.DateOffset(months=1), periods=24, freq='ME')\nforecast_values = np.exp(forecast.predicted_mean)  # Convert back from log\nconfidence_intervals = np.exp(forecast.conf_int())\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(data['Number of Passengers'], label='Observed')\nplt.plot(forecast_index, forecast_values, label='Forecast', color='red')\nplt.fill_between(forecast_index, confidence_intervals.iloc[:, 0], confidence_intervals.iloc[:, 1], color='pink', alpha=0.3)\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](26-TimeSeries_files/figure-html/cell-30-output-1.png){width=798 height=485}\n:::\n:::\n\n\nThe light pink area shows the 95% confidence interval for the forecast.\n\n**Question**: What observations do you have about the prediction?\n\n\n# Model Selection & Evaluation\n\n## How Do We Choose Parameters?\n\n**AIC & BIC**: Balance fit vs. complexity\n\n$$\\text{AIC} = T\\log\\left(\\frac{\\text{SSE}}{T}\\right) + 2k \\quad \\text{(lower is better)}$$\n\n| Criterion | Penalizes complexity... |\n|-----------|------------------------|\n| **AIC** | Moderately |\n| **BIC** | More heavily (prefers simpler models) |\n\nIn practice: try multiple (p,d,q) combinations, pick lowest AIC/BIC.\n\n## Time Series Cross-Validation\n\nâš ï¸ **Can't shuffle!** Must respect temporal order.\n\n**Rolling forecast origin:**\n\n```\nTrain: [----] Test: [.]\nTrain: [-----] Test: [.]\nTrain: [------] Test: [.]\n```\n\nAlways train on past, test on future â€” no data leakage!\n\n# Summary\n\n## Key Takeaways\n\n1. **Visualize first** â€” trends, seasonality, autocorrelation\n2. **Decompose** â€” separate signal from noise (STL > classical)\n3. **Check stationarity** â€” transform if needed (differencing, log)\n4. **Model** â€” ARIMA for non-seasonal, SARIMA for seasonal\n5. **Validate** â€” diagnostics + time-aware cross-validation\n\n## References\n\n- [Forecasting: Principles & Practice](https://otexts.com/fpp3/) â€” the definitive free textbook\n- [statsmodels documentation](https://www.statsmodels.org/stable/tsa.html)\n\n## Bibliography\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "26-TimeSeries_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}