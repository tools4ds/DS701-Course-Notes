---
title: 'Natural Language Processing'
jupyter: python3
bibliography: references.bib
---

[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/27-RNN.ipynb)

Natural language processing (NLP) is a subfield of artificial intelligence that allows computers to understand, process, and manipulate human language.

# Brief History of NLP

## NLP Evolution Timeline

::: {.columns}
::: {.column width="50%"}
**1950s-1970s: Rule-Based Era**

- Turing Test (1950), formal grammars, hand-crafted rules
- Systems like ELIZA (1966) - pattern matching chatbot
- Limitations: brittle, didn't scale, couldn't handle ambiguity

**1980s-2000s: Statistical Revolution**

- Shift from rules to learning from data
- N-grams, Hidden Markov Models, machine learning
- IBM alignment models for machine translation
:::
::: {.column width="50%"}
**2006-2017: Deep Learning**

- Neural networks applied to NLP
- Word2Vec (2013), RNNs, LSTMs for sequence modeling
- Attention mechanism (2014) - precursor to Transformers

**2017-Present: Transformer Era**

- Attention is All You Need (2017) - Transformer architecture
- BERT (2018), GPT-2/3 (2019/2020), ChatGPT (2022)
- LLMs with billions of parameters, human-level performance
:::
:::

# Lecture Outline

For the rest of this lecture we will cover:

- Text preprocessing and tokenization
- Numerical representations of words
  - Sparse representations (One-Hot, BoW, TF-IDF)
  - Word2Vec and static embeddings
  - Contextual embeddings
- Language models overview
  - N-gram models
  - Transformers (high-level)
- **Named Entity Recognition (NER)**
  - NER with NLTK
  - NER with spaCy
  - NER with BERT
- **Topic Modeling**
  - Latent Dirichlet Allocation (LDA)
  - BERTopic


# Numerical Representation of Words

## Numerical Representations of Words

Machine learning models for NLP are not able to process text in the form of characters and strings. Characters and strings must be converted to numbers in order to train our language models.

There are a number of ways to do this. These include

- sparse representations, like one-hot encodings and TF-IDF encodings
- word embeddings.

However, prior to creating a numerical representation of text, we need to **tokenize** the text.

## Tokenization

Tokenization is the process of splitting raw text into smaller pieces, called (drum-roll please), *tokens*. Tokens can be individual characters, words, or sentences.

Examples of character and word tokenization are shown for the following raw text

```Show me the money```

Character tokenization:

```['S', 'h', 'o', 'w', 'm', 'e', 't', 'h', 'e', 'm', 'o', 'n', 'e', 'y']```.

Word tokenization:

```['Show', 'me', 'the', 'money'] ```

---

This code block demonstrates both of these tokenization techniques.

```{python}
#| code-fold: false
# Character and word tokenization

sentence = "Show me the money"
word_tokens = sentence.split()
print(word_tokens)
character_tokens = [char for char in sentence if char != ' ']
print(character_tokens)
```

---

There are advantages and disadvantages to different tokenization methods. We showed two very simple strategies. 

However, there are other strategies, such as subword and sentence tokenization,
see for example [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte-pair_encoding),
and [SentencePiece](https://github.com/google/sentencepiece).

With tokenization, our goal is to not lose meaning with the tokens. With character based tokenization, especially for English (non-character based languages) we certainly lose meaning. 

Here is a demo of how to tokenize using the [transformers](https://huggingface.co/docs/transformers/en/index) package from Huggingface.

```{python}
#| code-fold: false
from transformers import AutoTokenizer, logging

logging.set_verbosity_warning()

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokens = tokenizer.tokenize(sentence)
print(tokens)

# Try a more advanced sentence
sentence2 = "Let's try to see if we can get this transformer to tokenize."
tokens2 = tokenizer.tokenize(sentence2)
print(tokens2)
```

## Tokens and Token IDs

Associated to each token is a unique token ID. The total number of unique tokens that a model can recognize and process is the *vocabulary size*. The *vocabulary* is the collection of all the unique tokens.

The tokens (and token ids) alone hold no (semantic) information. What is needed is a numerical representation that *encodes* this information. 

There are different ways to achieve this. One encoding technique that we already considered is one-hot encodings. Another more powerful encoding method, is the creation of word embeddings.

## Sparse Representations

Sparse representations encode text as high-dimensional vectors with mostly zeros.

**Common approaches:**

- **One-Hot Encoding**: Each word is a vector with a single 1 and rest 0s
  - Example: cat = [1,0,0], dog = [0,1,0], emu = [0,0,1]
- **Bag of Words (BoW)**: Count word frequencies, ignore order
- **TF-IDF**: Weight words by importance across documents

**Limitations:**

- High dimensionality (vocabulary size)
- No semantic meaning (cat and kitten are equally distant)
- Cannot capture word relationships

## Word Embeddings

Word embeddings represent words as dense vectors in high-dimensional spaces.

The individual values of the vector may be difficult to interpret, but the 
overall pattern is that _words with similar meanings are close to each other_, 
in the sense that their vectors have small angles with each other.

The similarity of two word embeddings is the cosine of the angle between the two
vectors. Recall that for two vectors $v_1, v_2\in\mathbb{R}^{n}$, the formula 
for the cosine of the angle between them is

$$ 
\cos{(\theta)} = \frac{v_1 \cdot v_2}{\Vert v_1 \Vert_2 \Vert v_2 \Vert_2}.
$$

Word embeddings can be static or contextual. A static embedding is when each word has a single embedding, e.g., Word2Vec. A contextual embedding (used by more complex language model embedding algorithms) allows the embedding for a word to change depending on its context in a sentence.

## Word2Vec: Static Embeddings

**Word2Vec** [@mikolov2013efficient] is a technique to learn static word embeddings from large text corpora.

**Key characteristics:**

- Each word has a **single fixed vector** representation
- Trained to predict context (CBOW) or predict word from context (Skip-gram)
- Captures semantic relationships: *king - man + woman ≈ queen*
- Fast to train and use
- Popular pre-trained models: Google News (300d), GloVe

**Advantages:**

- Efficient for large vocabularies
- Good for similarity tasks, clustering
- Pre-trained embeddings available

## Contextual vs Static Embeddings

| Feature | Static (Word2Vec, GloVe) | Contextual (BERT, GPT) |
|---------|--------------------------|------------------------|
| **Representation** | One vector per word | Different vectors per context |
| **Example** | "bank" always same vector | "bank" differs in "river bank" vs "bank account" |
| **Model Type** | Shallow neural network | Deep transformer model |
| **Training** | Fast, lightweight | Slow, resource-intensive |
| **Best For** | Similarity, clustering, simple tasks | Complex understanding, ambiguity resolution |
| **Dimensionality** | 100-300 dimensions | 768-1024+ dimensions |

**When to use Word2Vec:**

- Limited computational resources
- Task doesn't require deep context understanding
- Working with domain-specific corpora (train custom embeddings)

**When to use Contextual Embeddings:**

- Need to handle polysemy (words with multiple meanings)
- Complex NLP tasks (NER, question answering, translation)
- Have access to GPU resources

# Language Models


## Language Models

A language model is a statistical tool that predicts the probability of a sequence of words. It helps in understanding and generating human language by learning patterns and structures from large text corpora.

1. **N-gram Models**:
   - Predict the next word based on the previous  $n-1$ words.
   - Simple and effective for many tasks but limited by fixed context size.

1. **Neural Language Models**:
   - Use neural networks to capture more complex patterns.
   - Examples include *RNNs*, *LSTMs*, and **Transformers**.

See [RNNs and LSTMs](#27-RNN.qmd) for more details.

We'll discuss N-grams briefly followed by a deep dive on Transformers.

# N-gram Models


## N-gram models

- **Definition**: An n-gram model is a type of probabilistic language model used in natural language processing.
- **Purpose**: It predicts the next item in a sequence based on the previous \( n-1 \) items.
- **Types**:
  - **Unigram (n=1)**: Considers each word independently.
  - **Bigram (n=2)**: Considers pairs of consecutive words.
  - **Trigram (n=3)**: Considers triples of consecutive words.

## How N-gram Models Work

- **Example**: Let's consider a bigram model.
- **Training Data**: "I love machine learning. Machine learning is fun."
- **Bigrams**: 
  - "I love"
  - "love machine"
  - "machine learning"
  - "learning Machine"
  - "Machine learning"
  - "learning is"
  - "is fun"

- **Probability Calculation**:
  - P("learning" | "machine") = Count("machine learning") / Count("machine")


## Example of N-gram Model in Action

- **Sentence Completion**:
  - Given the sequence "machine learning", predict the next word.
  - Using the bigram model:
    - P("is" | "learning") = Count("learning is") / Count("learning")
    - P("fun" | "learning") = Count("learning fun") / Count("learning")

- **Prediction**:
  - If "learning is" appears more frequently than "learning fun" in the training data, the model predicts "is" as the next word.

# Transformers


## Transformers: High-Level Overview

Transformers [@vaswani2017attention] are the foundation of modern NLP systems.

**Key innovations:**

- Use **attention mechanism** to process entire sequences in parallel
- Revolutionized NLP and enabled LLMs (ChatGPT, BERT, GPT-4)
- Scalable across GPUs for massive models

## Transformer Architecture

![](drawio/Transformer_Enc_Dec.png){fig-align="center"}

**Components:**

- **Encoder**: Processes input text, creates rich representations
- **Decoder**: Generates output text using encoder representations
- **Attention**: Allows model to focus on relevant parts of input

## The Attention Mechanism

Attention allows models to understand which words are most relevant to each other.

**Example sentence:**

> "The elephant didn't cross the river because **it** was tired."

**Question:** What does "it" refer to?

- Attention helps the model determine that **"it" = "elephant"** (not "river")
- The model "attends" more strongly to "elephant" when processing "it"

## How Attention Works (Simplified)

For each word in a sentence:

1. **Calculate relevance scores** with all other words
2. **Apply softmax** to get attention weights (sum to 1)
3. **Compute weighted sum** of word representations
4. Result: each word's representation includes context from relevant words

**Multi-head attention**: Run multiple attention operations in parallel to capture different types of relationships

## Attention Visualized

:::: {.columns}
::: {.column width="60%"}
Darker connections show stronger attention.

When processing "it", the model attends most to "elephant".

This is how transformers handle long-range dependencies and ambiguity.
:::
::: {.column width="40%"}
![](drawio/VisualizedAttention.png)
:::
::::

# Transformer Architectures

## 3 Types of Transformer Models

1. **Encoder-Decoder** – Sequence-to-sequence tasks (e.g., machine translation)
   - Example: Original Transformer, T5

2. **Encoder-Only** – Understanding and classification tasks
   - Example: **BERT** (sentiment analysis, NER, classification)

3. **Decoder-Only** – Text generation and completion
   - Example: **GPT** (ChatGPT, text generation, AI assistants)

## BERT: Encoder Model

**BERT** (Bidirectional Encoder Representations from Transformers) [@devlin2019bert]

**Key features:**

- Reads text **bidirectionally** (considers context from both directions)
- Pre-trained on masked language modeling (predict hidden words)
- 340M parameters (BERT-Large), 110M (BERT-Base)
- Fine-tuned for specific tasks with one additional layer

**Common uses:** Text classification, NER, question answering, sentiment analysis

## GPT: Decoder Model

**GPT** (Generative Pre-trained Transformer) [@brown2020language]

**Key features:**

- Reads text **left-to-right** (autoregressive)
- Predicts next token: $P(t_1, t_2, \ldots, t_N) = P(t_1)\prod_{n=2}^{N} P(t_n | t_1, \ldots t_{n-1})$
- GPT-3: 175 billion parameters
- Excellent at text generation

**Common uses:** Text completion, creative writing, chatbots, code generation

## Transformer Applications

Transformers enable powerful NLP applications:

- **Machine Translation**: Translate between languages
- **Text Summarization**: Generate concise summaries
- **Question Answering**: Answer questions from context
- **Named Entity Recognition**: Identify entities in text
- **Sentiment Analysis**: Determine sentiment/emotion
- **Text Generation**: Create human-like text

# Named Entity Recognition (NER)

## What is Named Entity Recognition?

**Named Entity Recognition (NER)** is the task of identifying and classifying named entities in text into predefined categories.

**Common entity types:**

- **PERSON**: Names of people (e.g., "Barack Obama")
- **ORGANIZATION**: Companies, agencies (e.g., "Google", "FBI")
- **LOCATION**: Cities, countries, landmarks (e.g., "Paris", "Mount Everest")
- **DATE**: Dates and times (e.g., "January 1, 2024")
- **MONEY**: Monetary values (e.g., "$100")
- **GPE**: Geopolitical entities (countries, cities, states)

**Applications:** Information extraction, content classification, question answering, knowledge graphs

## NER with NLTK

NLTK (Natural Language Toolkit) provides basic NER capabilities using the `ne_chunk()` function.

```{python}
#| code-fold: false
import nltk
from nltk import word_tokenize, pos_tag, ne_chunk

# Download required NLTK data (run once)
# nltk.download('punkt')
# nltk.download('averaged_perceptron_tagger')
# nltk.download('maxent_ne_chunker')
# nltk.download('words')

text = "Apple Inc. was founded by Steve Jobs in Cupertino, California. In 2024, the company is worth over $3 trillion dollars."

# Tokenize, POS tag, then NER
tokens = word_tokenize(text)
pos_tags = pos_tag(tokens)
named_entities = ne_chunk(pos_tags)

print(named_entities)
```

## NLTK NER Results

NLTK uses a **rule-based and statistical approach** trained on Penn Treebank corpus.

**Advantages:**

- Easy to use, no GPU required
- Good for simple, well-formed text
- Lightweight

**Limitations:**

- Lower accuracy than modern methods
- Limited entity types
- Requires POS tagging first
- Poor with informal text or domain-specific entities

## NER with spaCy

spaCy is a modern, production-ready NLP library with excellent NER capabilities.

```{python}
#| code-fold: false
import spacy

# Load pre-trained model (run: python -m spacy download en_core_web_sm)
nlp = spacy.load("en_core_web_sm")

text = "Apple Inc. was founded by Steve Jobs in Cupertino, California. In 2024, the company is worth over $3 trillion dollars."

# Process text
doc = nlp(text)

# Extract named entities
print("Entities found:")
for ent in doc.ents:
    print(f"  {ent.text:20} -> {ent.label_:15} ({spacy.explain(ent.label_)})")
```

## spaCy NER Visualization

spaCy provides built-in visualization with `displacy`:

```{python}
#| code-fold: false
from spacy import displacy

text = "Apple Inc. was founded by Steve Jobs in Cupertino, California. In 2024, the company is worth over $3 trillion dollars."
doc = nlp(text)

# Visualize entities
displacy.render(doc, style="ent", jupyter=True)
```

**spaCy advantages:**

- Fast and accurate
- Multiple pre-trained models (small, medium, large)
- Easy visualization
- Production-ready

## NER with BERT

BERT-based NER models achieve state-of-the-art results using Hugging Face transformers.

```{python}
#| code-fold: false
from transformers import pipeline

# Load pre-trained BERT NER model
ner_pipeline = pipeline("ner", model="dslim/bert-base-NER", aggregation_strategy="simple")

text = "Apple Inc. was founded by Steve Jobs in Cupertino, California. In 2024, the company is worth over $3 trillion dollars."

# Perform NER
entities = ner_pipeline(text)

# Display results
print("BERT NER Results:")
for entity in entities:
    print(f"  {entity['word']:20} -> {entity['entity_group']:10} (score: {entity['score']:.3f})")
```

## BERT NER Details

**How BERT NER works:**

- Pre-trained BERT model fine-tuned on labeled NER datasets (e.g., CoNLL-2003)
- Predicts entity label for each token
- Aggregation combines sub-word tokens into complete entities

**Advantages:**

- State-of-the-art accuracy
- Handles context and ambiguity well
- Multiple pre-trained models available
- Can be fine-tuned on custom data

**Limitations:**

- Requires GPU for fast inference
- Larger model size
- More complex setup

## NER Tools Comparison

| Feature | NLTK | spaCy | BERT (Transformers) |
|---------|------|-------|---------------------|
| **Accuracy** | Moderate | High | Very High |
| **Speed** | Fast | Very Fast | Moderate (needs GPU) |
| **Ease of Use** | Moderate | Easy | Moderate |
| **GPU Required** | No | No | Recommended |
| **Model Size** | Small | Small-Medium | Large |
| **Customization** | Limited | Good | Excellent |
| **Best For** | Simple tasks, learning | Production, most use cases | Highest accuracy, custom domains |

**Recommendation:** Start with spaCy for most applications, use BERT when accuracy is critical.

# Topic Modeling

## What is Topic Modeling?

**Topic modeling** is an unsupervised learning technique that discovers abstract "topics" in a collection of documents.

**Key concepts:**

- **Topic**: A distribution over words (e.g., "sports" → {game, team, player, score, ...})
- **Document**: A mixture of topics
- **Goal**: Automatically discover topics and their distributions

**Applications:**

- Content recommendation
- Document organization
- Trend analysis
- Search and discovery
- Summarization

## Latent Dirichlet Allocation (LDA)

**LDA** [@blei2003latent] is the most popular classical topic modeling algorithm.

**How LDA works (simplified):**

1. Assume each document is a mixture of topics
2. Assume each topic is a mixture of words
3. Use statistical inference to discover these mixtures

**Key hyperparameters:**

- `n_topics`: Number of topics to discover
- `alpha`: Document-topic density (higher = more topics per document)
- `beta`: Topic-word density (higher = more words per topic)

## LDA with Scikit-Learn

```{python}
#| code-fold: false
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# Load sample data (subset for speed)
categories = ['sci.space', 'rec.sport.baseball', 'comp.graphics']
newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))
documents = newsgroups.data[:300]  # Use subset for demo

# Convert to document-term matrix
vectorizer = CountVectorizer(max_features=1000, stop_words='english', max_df=0.7)
doc_term_matrix = vectorizer.fit_transform(documents)

# Train LDA model
n_topics = 3
lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42, max_iter=20)
lda_model.fit(doc_term_matrix)

print(f"LDA model trained with {n_topics} topics")
```

## LDA Topics Visualization

```{python}
#| code-fold: false
import numpy as np

# Display top words for each topic
feature_names = vectorizer.get_feature_names_out()
n_top_words = 10

print("\nTop words per topic:")
for topic_idx, topic in enumerate(lda_model.components_):
    top_words_idx = topic.argsort()[-n_top_words:][::-1]
    top_words = [feature_names[i] for i in top_words_idx]
    print(f"\nTopic {topic_idx}: {', '.join(top_words)}")

# Show topic distribution for a sample document
doc_idx = 0
topic_dist = lda_model.transform(doc_term_matrix[doc_idx:doc_idx+1])[0]
print(f"\n\nSample document topic distribution:")
for i, prob in enumerate(topic_dist):
    print(f"  Topic {i}: {prob:.3f}")
```

## LDA Strengths and Limitations

**Strengths:**

- Interpretable topics (distributions over words)
- Well-established, proven method
- Fast training on moderate-sized corpora
- Works well on homogeneous documents

**Limitations:**

- Bag-of-words assumption (loses word order, context)
- Requires careful hyperparameter tuning
- Struggles with short documents
- Topics can be noisy or incoherent
- Doesn't leverage modern pre-trained embeddings

## BERTopic

**BERTopic** [@grootendorst2022bertopic] is a modern topic modeling technique using transformer embeddings.

**How BERTopic works:**

1. **Embed** documents using BERT (contextual embeddings)
2. **Reduce** dimensionality with UMAP
3. **Cluster** documents with HDBSCAN
4. **Extract** topic words using class-based TF-IDF (c-TF-IDF)

**Advantages over LDA:**

- Uses semantic embeddings (better captures meaning)
- No need to specify number of topics upfront
- Better with short documents
- More coherent topics

## BERTopic Implementation

```{python}
#| code-fold: false
from bertopic import BERTopic
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer

# Load data
categories = ['sci.space', 'rec.sport.baseball', 'comp.graphics']
newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))
documents = newsgroups.data[:300]

# Configure vectorizer to remove stop words
vectorizer_model = CountVectorizer(stop_words="english")

# Train BERTopic model with proper configuration
topic_model = BERTopic(vectorizer_model=vectorizer_model, verbose=False, language="english")
topics, probabilities = topic_model.fit_transform(documents)

print(f"BERTopic discovered {len(set(topics))} topics (including -1 for outliers)")
```

## BERTopic Results

```{python}
#| code-fold: false
# Show topic information
topic_info = topic_model.get_topic_info()
print("\nTopic Overview:")
print(topic_info.head(10))

# Show top words for each topic
print("\n\nTop words per topic:")
for topic_num in sorted(set(topics)):
    if topic_num == -1:  # Skip outlier topic
        continue
    words = topic_model.get_topic(topic_num)
    if words:
        top_words = ', '.join([word for word, score in words[:10]])
        print(f"\nTopic {topic_num}: {top_words}")
```

## BERTopic Visualization

```python
#| code-fold: false
# Visualize topics (interactive plot)
fig = topic_model.visualize_topics()
fig.show()

# Visualize topic hierarchy
fig_hierarchy = topic_model.visualize_hierarchy()
fig_hierarchy.show()
```

## LDA vs BERTopic

| Feature | LDA | BERTopic |
|---------|-----|----------|
| **Embeddings** | Bag-of-words (TF-IDF) | Contextual (BERT) |
| **Num Topics** | Must specify | Auto-detected |
| **Interpretability** | Good (probability distributions) | Excellent (semantic coherence) |
| **Speed** | Fast | Slower (embeddings + clustering) |
| **Short Documents** | Poor | Good |
| **Computational Cost** | Low | High (needs embeddings) |
| **Tunability** | Many hyperparameters | Fewer hyperparameters |
| **Best For** | Large corpora, speed matters | Quality topics, semantic understanding |

**Recommendation:** Use BERTopic for better quality topics when resources allow; use LDA for speed and large-scale applications.

# Summary

## NLP Tools and Applications Recap

**What we covered:**

1. **Text representation**: Sparse (TF-IDF) vs embeddings (Word2Vec vs BERT)
2. **Language models**: N-grams → Transformers
3. **Transformers**: Attention mechanism, BERT (encoder), GPT (decoder)
4. **Named Entity Recognition**: NLTK → spaCy → BERT NER
5. **Topic Modeling**: LDA → BERTopic

**Key takeaways:**

- Modern NLP relies heavily on transformers and contextual embeddings
- Choose tools based on accuracy needs, resources, and task complexity
- spaCy is excellent for production NER; BERT for highest accuracy
- BERTopic produces better topics than LDA but requires more resources

## References