{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Clustering In Practice\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "## Clustering in Practice\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/07-Clustering-II-in-practice.ipynb)\n",
        "\n",
        "\n",
        "...featuring $k$-means\n",
        "\n",
        "Today we'll do an extended example showing $k$-means clustering in practice and in the context of the python libraries\n",
        "__`scikit-learn.`__\n",
        "\n",
        "`scikit-learn` is the main python library for machine learning functions.\n",
        "\n",
        "Our goals are to learn:\n",
        "\n",
        "::: {.incremental}\n",
        "* How clustering is used in practice\n",
        "* Tools for evaluating the quality of a clustering\n",
        "* Tools for assigning meaning or labels to a cluster\n",
        "* Important visualizations\n",
        "* A little bit about feature extraction for text\n",
        ":::\n",
        "\n",
        "# Visualization\n",
        "\n",
        "## Training wheels: Synthetic data\n",
        "\n",
        "Generally, when learning about or developing a new unsupervised method, it's a \n",
        "good idea to try it out on a dataset in which you already know the \"right\" answer.\n",
        "\n",
        "One way to do that is to generate synthetic data that has some known properties.\n",
        " \n",
        "Among other things, `scikit-learn` contains tools for generating synthetic data\n",
        "for testing.\n",
        "\n",
        "We'll use [datasets.make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "import sklearn.datasets as sk_data\n",
        "X, y, gt_centers = sk_data.make_blobs(n_samples=100, centers=3, n_features=30,\n",
        "                          center_box=(-10.0, 10.0), random_state=0, return_centers=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check the shapes of the returned values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "print(\"X.shape: \", X.shape)\n",
        "print(\"y.shape: \", y.shape)\n",
        "print(\"gt_centers: \", gt_centers.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.notes}\n",
        "`datasets.makeblobs` takes as arguments:\n",
        "\n",
        "- `n_samples`: The number of samples to generate\n",
        "- `n_features`: The number of features, or in other words the dimensionality of\n",
        "each sample\n",
        "- `center_box`: The bounds of the cluster centers\n",
        "- `random_state`: The random seed for reproducibility\n",
        "- `return_centers`: A boolean flag, True to return the centers so that we have ground truth\n",
        ":::\n",
        "\n",
        "## Visualize the Data\n",
        "\n",
        "To get a sense of the raw data we can inspect it.\n",
        "\n",
        "For statistical visualization, a good library is [Seaborn](https://seaborn.pydata.org/).\n",
        "\n",
        "Let's plot the `X` data as a matrix\n",
        "[heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html), \n",
        "where every row is a data point and the columns are the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the figure size to make the plot smaller\n",
        "plt.figure(figsize=(7, 5))  # Adjust the width and height as needed\n",
        "sns.heatmap(X, xticklabels = False, yticklabels = False, linewidths = 0, cbar = True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"40%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the figure size to make the plot smaller\n",
        "plt.figure(figsize=(4, 3))  # Adjust the width and height as needed\n",
        "sns.heatmap(X, xticklabels = False, yticklabels = False, linewidths = 0, cbar = False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: {.column width=\"60%\"}\n",
        "Geometrically, these points live in a __30 dimensional__ space, so we cannot directly visualize their geometry.  \n",
        "\n",
        "This is a __big problem__ that you will run into time and again!\n",
        "\n",
        "We will discuss methods for visualizing high dimensional data later on.\n",
        "\n",
        "For now, we will use a method that can turn a set of pairwise distances into an approximate 2-D representation __in some cases.__\n",
        ":::\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "So let's compute the pairwise distances, _in 30 dimensions_, for visualization purposes.\n",
        "\n",
        "We can compute all pairwise distances in a single step using the `scikit-learn` [`metrics.euclidean_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html) function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "import sklearn.metrics as metrics\n",
        "euclidean_dists = metrics.euclidean_distances(X)\n",
        "# euclidean_dists"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "print(\"Matrix shape is: \", euclidean_dists.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at the upper left and lower right corners of the distances matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import sklearn.metrics as metrics\n",
        "import numpy as np\n",
        "\n",
        "euclidean_dists = metrics.euclidean_distances(X)\n",
        "\n",
        "# Function to print the matrix in the desired format\n",
        "def print_matrix(matrix):\n",
        "    # Upper left 3x3\n",
        "    upper_left = matrix[:3, :3]\n",
        "    # Lower right 3x3\n",
        "    lower_right = matrix[-3:, -3:]\n",
        "    \n",
        "    # Print the upper left 3x3\n",
        "    print(\"Upper left 3x3:\")\n",
        "    print(np.round(upper_left, 2))\n",
        "    \n",
        "    print(\"\\n...\")\n",
        "    \n",
        "    # Print the lower right 3x3\n",
        "    print(\"Lower right 3x3:\")\n",
        "    print(np.round(lower_right, 2))\n",
        "\n",
        "print_matrix(euclidean_dists)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that this produces a $100\\times100$ symmetric matrix where the diagonal\n",
        "is all zeros (distance from itself).\n",
        "\n",
        "---\n",
        "\n",
        "Let's look at a histogram of the distances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "# Compute the pairwise distances\n",
        "euclidean_dists = metrics.euclidean_distances(X)\n",
        "\n",
        "# Extract the lower triangular part of the matrix, excluding the diagonal\n",
        "lower_triangular_values = euclidean_dists[np.tril_indices_from(euclidean_dists, k=-1)]\n",
        "\n",
        "# Plot the histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(lower_triangular_values, bins=30, edgecolor='black')\n",
        "plt.title('Histogram of Lower Diagonal Values of Euclidean Distances')\n",
        "plt.xlabel('Distance')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.notes}\n",
        "Remember that these are the pairwise distances, _in 30 dimensions_. So at least\n",
        "with this dataset we see a clean separation presumably between inter-cluster\n",
        "distances and intra-cluster distances.\n",
        "\n",
        "How would the curse of dimensionality affect this? We discuss the curse of\n",
        "dimensionality later in the course.\n",
        ":::\n",
        "## Visualizing with MDS\n",
        "\n",
        "The idea behind [Multidimensional Scaling (MDS)](https://en.wikipedia.org/wiki/Multidimensional_scaling) is given a pairwise distance (or dissimilarity) matrix:\n",
        "\n",
        ":::: {.incremental}\n",
        "* Find a set of coordinates in 1, 2 or 3-D space that approximates those distances as well as possible.\n",
        "* The points that are close (or far) in high dimensional space should be close (or far) in the reduced dimension space.\n",
        "::::\n",
        "\n",
        "## MDS Continued\n",
        "\n",
        "Note that there are two forms of MDS:\n",
        "\n",
        "* Metric MDS, of which Classical MDS is a special case, and has a closed form solution\n",
        "  based on the eigenvectors of the centered distance matrix. \n",
        "    * $O(n^3)$ time complexity and $O(n^2)$ space complexity.\n",
        "* Non-Metric MDS, which tries to find a non-parametric monotonic relationship\n",
        "  between the dissimilarities and the target coordinates through an iterative approach. \n",
        "    * $O(n^2)$ time complexity.\n",
        "\n",
        "> Question If non-metric is faster, why not always use it?\n",
        "\n",
        "::: {.fragment}\n",
        "**Answer:** Metric MDS is more faithful to preserving the relative distances\n",
        "between poijnts, where as non-metric MDS preserves the order but not necessarily\n",
        "faithful to the distances.\n",
        "\n",
        "In general, MDS may not always work well if, for example the dissimilarities\n",
        "are not well modeled by a metric like Euclidean distance.\n",
        ":::\n",
        "\n",
        "## MDS Visualization Result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| label: code-mds\n",
        "import sklearn.manifold\n",
        "import matplotlib.pyplot as plt\n",
        "mds = sklearn.manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=0,\n",
        "                   dissimilarity = \"precomputed\", n_jobs = 1)\n",
        "fit = mds.fit(euclidean_dists)\n",
        "pos = fit.embedding_\n",
        "plt.scatter(pos[:, 0], pos[:, 1], s=8)\n",
        "plt.axis('square');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So we can see that, although our data lives in 30 dimensions, we can get a\n",
        "sense of how the points are clustered by approximately placing the points into\n",
        "two dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "Out of curiosity, we can visualize the pairwise distance matrix using a heatmap."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "#| fig-align: center\n",
        "sns.heatmap(euclidean_dists, xticklabels=False, yticklabels=False, linewidths=0, \n",
        "            square=True )\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Applying $k$-means\n",
        "\n",
        "## Applying  $k$-Means\n",
        "\n",
        "The Python package `scikit-learn` has a huge set of tools for unsupervised learning generally, and clustering specifically.  \n",
        "\n",
        "These are in __`sklearn.cluster.`__\n",
        "\n",
        "There are 3 functions in all the clustering classes, \n",
        "\n",
        "* **`fit()`**: builds the model from the training data.\n",
        "    * For $k$-means, this function find the centroids.\n",
        "* **`predict()`**: assigns labels to the data after building the models. \n",
        "    * For $k$-means this assigns the cluster number to a point.\n",
        "* **`fit_predict()`**: calls fit and predict in a single step.\n",
        "\n",
        "---\n",
        "\n",
        "Let's go back to the original 30-D synthetic dataset and apply $k$-means and\n",
        "show the cluster numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.cluster import KMeans\n",
        "kmeans = KMeans(init = 'k-means++', n_clusters = 3, n_init = 100, random_state=0)\n",
        "y_prime = kmeans.fit_predict(X)\n",
        "print(y_prime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For comparisons, here are the original cluster numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "Note that the $k$-means labels are different than the ground truth labels. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can remap the values according to\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "0 &\\rightarrow 0 \\\\\n",
        "1 &\\rightarrow 2 \\\\\n",
        "2 &\\rightarrow 1.\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Remap y_prime\n",
        "remap = {0: 0, 1: 2, 2: 1}\n",
        "y_prime_remapped = np.vectorize(remap.get)(y_prime)\n",
        "print(\"Remapped y_prime:\")\n",
        "print(y_prime_remapped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Calculate the number of mismatched entries\n",
        "mismatches = np.sum(y != y_prime_remapped)\n",
        "\n",
        "# Calculate the percentage of mismatched entries\n",
        "total_entries = len(y)\n",
        "percentage_mismatched = (mismatches / total_entries) * 100\n",
        "\n",
        "print(f\"Percentage of mismatched entries: {percentage_mismatched:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To reiterate, mismatches are 0.00%.\n",
        "\n",
        "---\n",
        "\n",
        "All the tools in `scikit-learn` are implemented as Python objects.\n",
        "\n",
        "Recall that the general sequence for using a tool from `scikit-learn` is:\n",
        "\n",
        "* create the object, probably with some hyperparameter settings or initialization,\n",
        "* run the method, generally by using the `fit()` function, and\n",
        "* examine the results, which are generally property variables of the object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "centroids = kmeans.cluster_centers_\n",
        "labels = kmeans.labels_\n",
        "inertia = kmeans.inertia_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we see the resulting cluster inertia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f'Clustering inertia: {inertia:0.1f}.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the Results of Clustering\n",
        "\n",
        "Let's visualize the results.  We'll do that by reordering the data items according to their cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "import numpy as np\n",
        "idx = np.argsort(labels)\n",
        "rX = X[idx, :]\n",
        "sns.heatmap(rX, xticklabels = False, yticklabels = False, linewidths = 0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can clearly see the feature similarities of the samples in the same cluster.\n",
        "\n",
        "---\n",
        "\n",
        "We can also sort the pairwise distance matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "rearranged_dists = euclidean_dists[idx,:][:,idx]\n",
        "sns.heatmap(rearranged_dists, xticklabels = False, yticklabels = False, linewidths = 0, square = True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here again, you can see the inter-cluster sample distances are small.\n",
        "\n",
        "\n",
        "# Cluster Evaluation\n",
        "\n",
        "## Cluster Evaluation\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "\n",
        "How do we know whether the clusters we get represent \"real\" structure in our data?\n",
        "\n",
        "Consider a dataset of 100 points _uniformly distributed_ in the unit square.\n",
        "\n",
        "> Question: Does that look uniformly distributed to you?\n",
        ":::\n",
        "::: {.column width=\"50%\"}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "import pandas as pd\n",
        "np.random.seed(42)\n",
        "unif_X = np.random.default_rng().uniform(0, 1, 500)\n",
        "unif_Y = np.random.default_rng().uniform(0, 1, 500)\n",
        "df = pd.DataFrame(np.column_stack([unif_X, unif_Y]), columns = ['X', 'Y'])\n",
        "df.plot('X', 'Y', kind = 'scatter', \n",
        "        colorbar = False, xlim = (0, 1), ylim = (0, 1), \n",
        "        figsize = (4, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "After running $k$-means on this data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "kmeans = KMeans(init = 'k-means++', n_clusters = 3, n_init = 500, random_state=0)\n",
        "df['label'] = kmeans.fit_predict(df[['X', 'Y']])\n",
        "df.plot('X', 'Y', kind = 'scatter', c = 'label', \n",
        "        colormap='viridis', colorbar = False, \n",
        "        xlim = (0, 1), ylim = (0, 1), \n",
        "        figsize = (4, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::: {.fragment}\n",
        "The point is: clustering algorithms output some \"clustering\" of the data.\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "The question is, does the clustering reflect __real__ structure?\n",
        "\n",
        ":::: {.fragment}\n",
        "Generally we encounter two problems:\n",
        "::::\n",
        "\n",
        "::: {.incremental}\n",
        "* Are there *real* clusters in the data?\n",
        "* If so, *how many* clusters are there?\n",
        ":::\n",
        "\n",
        ":::: {.fragment}\n",
        "There is often no definitive answer to either of these questions.\n",
        "\n",
        "You will often need to use your judgment in answering them.\n",
        "::::\n",
        "\n",
        "::: {.content-hidden}\n",
        "\n",
        "## Cluster Evaluations\n",
        "\n",
        "(Under Development)\n",
        "\n",
        "### With Ground Truth Data\n",
        "\n",
        "* How to evaluate clustering algorithms when you have ground truth data\n",
        "    * how do you align labels?\n",
        "    * different accuracy measures\n",
        "\n",
        "### Without Ground Truth Data\n",
        "\n",
        "* Can you make guesses to the underlying probability distributions to show that\n",
        "  it is non-uniform?\n",
        "\n",
        "## Clustering Metrics\n",
        "\n",
        "[sklearn clustering examples](https://scikit-learn.org/stable/auto_examples/cluster/index.html)\n",
        "\n",
        "- [sklearn performance evaluation guide](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)\n",
        "- [sklearn metrics](https://scikit-learn.org/stable/api/sklearn.metrics.html)\n",
        "    - [sklearn clustering metrics](https://scikit-learn.org/stable/api/sklearn.metrics.html#module-sklearn.metrics.cluster)\n",
        "        - [sklearn rand score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html)\n",
        "- [sklearn example: adjustment for chance in clustering performance evaluation](https://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html)\n",
        "\n",
        ":::\n",
        "\n",
        "## Clustering Metrics\n",
        "\n",
        "Broadly separated into two categories.\n",
        "\n",
        ":::: {.fragment}\n",
        "1. **Ground truth based metrics**\n",
        "\n",
        "We'll look at (Adjusted) Rand Index.\n",
        "\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "> Note: Even with ground truth labels, we can't apply accuracy measures like we will\n",
        "> see in supervised learning approaches since clustering algorithms don't try to\n",
        "> match exact labels, but rather group similar items together.\n",
        "\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "2. **Internal cluster metrics (no ground truth external labels)**\n",
        "\n",
        "We'll look at Silhouette Coefficient.\n",
        "::::\n",
        "\n",
        "\n",
        "## Rand Index (named after William M. Rand)\n",
        "\n",
        "* Compare a proposed clustering, $C$, with ground truth labels, $T$.\n",
        "* Look at the ratio, compared to total possible pairs, of pairs of points that are either\n",
        "    * in the same cluster in both $T$ and $C$, or\n",
        "    * in different clusters in both $T$ and $C$.\n",
        "\n",
        "> Question: What are pairs not in either category?\n",
        "\n",
        "## Rand Index Formula\n",
        "\n",
        "The formula for the Rand Index is:\n",
        "\n",
        "$$\n",
        "\\text{RI}(T,C) = \\frac{a+b}{n \\choose 2},\n",
        "$$\n",
        "\n",
        "- where $a$ is the number of pairs of points that are _in the same cluster_ in both $T$ and $C$, and\n",
        "- $b$ is the number of pairs of points that are _in different clusters_ in both $T$ and $C$.\n",
        "\n",
        "We normalize the sum by the combinatorial number of pairs of points, $n \\choose 2$.\n",
        "\n",
        "The __Rand Index__ is a value that falls in the range $[0, 1]$.\n",
        "\n",
        "## Aside: n choose 2\n",
        "\n",
        "* From combinatorics\n",
        "\n",
        "**\"n choose 2\"** (written as $n \\choose 2$) represents the number of ways to choose 2 items from a set of n items, where the order doesn't matter.\n",
        "\n",
        "#### Formula\n",
        "\n",
        "$$\n",
        "C(n,2) = n! / (2! × (n-2)!) = n(n-1)/2\n",
        "$$\n",
        "\n",
        "#### What it means\n",
        "\n",
        "- It counts the number of **pairs** you can form from n items\n",
        "- Order doesn't matter (so {A,B} is the same as {B,A})\n",
        "- No repetition (you can't choose the same item twice)\n",
        "\n",
        "#### Examples\n",
        "\n",
        "- **3 choose 2**: From items {A, B, C}, you can form pairs {A,B}, {A,C}, {B,C} = **3 pairs**\n",
        "- **4 choose 2**: From items {A, B, C, D}, you can form pairs {A,B}, {A,C}, {A,D}, {B,C}, {B,D}, {C,D} = **6 pairs**\n",
        "- **5 choose 2**: From items {A, B, C, D, E}, you can form **10 pairs**\n",
        "\n",
        "## Rand Index Example 1\n",
        "\n",
        "Let's proceed by way of example.\n",
        "\n",
        "We'll create another synthetic dataset of 3 clusters, so we have ground truth\n",
        "labels $T$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "X_rand, y_rand = sk_data.make_blobs(n_samples=[100, 300, 50], \n",
        "                                    centers = [[1.2, 2.4],[1.5, 3], [1.8, 3.5]], \n",
        "                                    n_features = 2,\n",
        "                                    center_box = (-10.0, 10.0), \n",
        "                                    cluster_std = [.2, .3, .2], \n",
        "                                    random_state = 0)\n",
        "df_rand_gt = pd.DataFrame(np.column_stack([X_rand[:, 0], X_rand[:, 1], y_rand]), columns = ['X', 'Y', 'label'])\n",
        "df_rand_clust = df_rand_gt.copy()\n",
        "kmeans = KMeans(init = 'k-means++', n_clusters = 3, n_init = 100, random_state=0)\n",
        "df_rand_clust['label'] = kmeans.fit_predict(df_rand_gt[['X', 'Y']])\n",
        "df_rand_clust['label'] = df_rand_clust['label'].replace({0: 1, 1: 2, 2: 0})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rand Index Example 1: Plotting the Results\n",
        "\n",
        "We then run $k$-means with 3 clusters to on the datasets to get labels $C$ and plot the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "figs, axs = plt.subplots(1, 2, figsize = (12, 5))\n",
        "df_rand_gt.plot('X', 'Y', \n",
        "                kind = 'scatter', \n",
        "                c = 'label', \n",
        "                colormap='viridis', \n",
        "                ax = axs[0],\n",
        "                colorbar = False)\n",
        "axs[0].set_title('Ground Truth (T)')\n",
        "axs[0].set_axis_off()\n",
        "\n",
        "df_rand_clust.plot('X', 'Y', \n",
        "                    kind = 'scatter', \n",
        "                    c = 'label', \n",
        "                    colormap='viridis', \n",
        "                    ax = axs[1],\n",
        "                    colorbar = False)\n",
        "axs[1].set_title('Clustering (C)')\n",
        "axs[1].set_axis_off()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"The Rand index is: \", metrics.rand_score(df_rand_gt[\"label\"], df_rand_clust[\"label\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adjusted Rand Index\n",
        "\n",
        "How do we know whether a particular Rand Index (RI) score is significant?\n",
        "\n",
        "We might compare it to the RI for a __random__ assignment of points to labels.\n",
        "\n",
        "This leads to the __Adjusted Rand Index.__\n",
        "\n",
        "\n",
        "## The Adjusted Rand Index\n",
        "\n",
        "To *calibrate* the Rand Index this way, we use the expected Rand Index of random labels, denoted $E[\\text{RI}]$.   \n",
        "\n",
        "The Expected Rand Index considers $C$ to be a clustering that has the same cluster sizes as $T$, but the labels are assigned at random.\n",
        "\n",
        ":::: {.fragment}\n",
        "Using that, we define the adjusted Rand index as a simple __rescaling__ of RI:\n",
        "\n",
        "$$\n",
        "\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{1 - E[\\text{RI}]}\n",
        "$$\n",
        "\n",
        "Where $1$ represents $\\max(\\text{RI})$, the maximum possible Rand Index.\n",
        "\n",
        "$E[\\text{RI}]$ can be computed using combinatorics (we'll omit the derivation).\n",
        "\n",
        "The below code block computes and prints the adjusted Rand index for the example on the previous slide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"The adjusted Rand index is: \", metrics.adjusted_rand_score(df_rand_gt[\"label\"], df_rand_clust[\"label\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::::\n",
        "\n",
        "## Rand Index Example 2\n",
        "\n",
        "Let's consider again our 3-cluster dataset with known labels `y`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(5, 4))  # Adjust the width and height as needed\n",
        "sns.heatmap(X, xticklabels = False, yticklabels = False, linewidths = 0)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rand Index Example 2: Evaluating the Results\n",
        "\n",
        "Here is the Rand Index and adjusted Rand Index, when using $k$-means to cluster this dataset for 1 to 10 clusters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "def ri_evaluate_clusters(X, max_clusters, ground_truth):\n",
        "    ri = np.zeros(max_clusters+1)\n",
        "    ri[0] = 0\n",
        "    ari = np.zeros(max_clusters+1)\n",
        "    ari[0] = 0\n",
        "    for k in range(1,max_clusters+1):\n",
        "        kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10)\n",
        "        kmeans.fit_predict(X)\n",
        "\n",
        "        ri[k] = metrics.rand_score(kmeans.labels_, ground_truth)\n",
        "        ari[k] = metrics.adjusted_rand_score(kmeans.labels_, ground_truth)\n",
        "    return ri, ari\n",
        "    \n",
        "ri, ari = ri_evaluate_clusters(X, 10, y)\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(range(1, len(ri)), ri[1:], 'ro-', range(1, len(ari)), ari[1:], 'b*-')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.title('$k$-means Clustering Compared to Known Labels')\n",
        "plt.ylabel('Index value')\n",
        "plt.legend(['RI', 'ARI'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RI vs ARI\n",
        "\n",
        "The ARI provides a more nuanced evaluation of clustering similarity compared to the RI by accounting for the chance grouping of elements. Here are the key differences:\n",
        "\n",
        ":::: {.columns}\n",
        "::: {.column width=\"50%\"}\n",
        "__Rand Index (RI)__\n",
        "\n",
        "- Measures Agreement: The RI measures the agreement between two clusterings by considering all pairs of elements and counting pairs that are either in the same or different clusters in both clusterings.\n",
        "- Range: The RI ranges from 0 to 1, where 1 indicates perfect agreement and 0 indicates no agreement.\n",
        ":::\n",
        "::: {.column width=\"50%\"}\n",
        "__Adjusted Rand Index (ARI)__\n",
        "\n",
        "- Adjusts for Chance: The ARI adjusts the RI by considering the expected similarity that might occur by random chance. This adjustment makes the ARI a more reliable measure, especially when dealing with random or unbalanced clusterings.\n",
        "- Range: The ARI can take values from -1 to 1. A value of 0 indicates that the clustering is no better than random, while a value of 1 indicates perfect agreement. Negative values indicate that the clustering is worse than random.\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Information Provided by ARI\n",
        "\n",
        "The ARI corrects for the fact that some level of agreement between clusterings can occur purely by chance. This makes it a more accurate measure of the true similarity between clusterings.\n",
        "\n",
        "The ARI’s range allows for a clearer interpretation of clustering performance. An ARI of 0 means the clustering is no better than random, which is more informative than an RI of 0.5, which might still be influenced by chance.\n",
        "\n",
        "The ARI allows for better comparison between different clustering results, as it normalizes the index to account for the chance agreement.\n",
        "\n",
        "## Deciding on the Number of Clusters\n",
        "\n",
        "The second question we face in evaluating a clustering is how many clusters are present.\n",
        "\n",
        "In practice, to use $k$-means or most other clustering methods, one must choose $k$, the number of clusters, via some process.\n",
        "\n",
        "## Inspecting Clustering Error\n",
        "\n",
        "The first thing you might do is to look at the $k$-means objective function  and see if it levels off after a certain point.\n",
        "\n",
        "Recall that the $k$-means objective can be considered the clustering \"error\".\n",
        "\n",
        "If the error stops going down, that would suggest that the clustering is not improving as the number of clusters is increased.\n",
        "\n",
        "Let's calculate the error for 1-11 clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "error = np.zeros(11)\n",
        "for k in range(1,11):\n",
        "    kmeans = KMeans(init='k-means++', n_clusters = k, n_init = 10)\n",
        "    kmeans.fit_predict(X)\n",
        "    error[k] = kmeans.inertia_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Inspecting Clustering Error\n",
        ":::\n",
        "\n",
        "For our synthetic data, here is the $k$-means objective, as a function of $k$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.plot(range(1, len(error)), error[1:], 'o-')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.title(r'$k$-means clustering performance of synthetic data')\n",
        "plt.ylabel('Error')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.callout-warning}\n",
        "This synthetic data is not at all typical. You will almost never see such a sharp change in the error function as we see here.\n",
        ":::\n",
        "\n",
        "---\n",
        "\n",
        "Let's create a function to evaluate clusters for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "def evaluate_clusters(X,max_clusters):\n",
        "    error = np.zeros(max_clusters+1)\n",
        "    error[0] = 0\n",
        "    for k in range(1,max_clusters+1):\n",
        "        kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10)\n",
        "        kmeans.fit_predict(X)\n",
        "        error[k] = kmeans.inertia_\n",
        "    return error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Silhouette Coefficient\n",
        "\n",
        "Usually, the ground truth labels are not known.\n",
        "\n",
        ":::: {.fragment}\n",
        "In that case, evaluation must be performed using the model itself.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "Recall our definition of clustering: \n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "> a grouping of data objects, such that the objects within a group are similar (or near) to one another and dissimilar\n",
        "> (or far) from the objects in other groups.\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "This suggests a metric that could evaluate a clustering: comparing the distances between points within a cluster, to the\n",
        "distances between points in different clusters.\n",
        "::::\n",
        "\n",
        "## Silhouette Coefficient\n",
        "\n",
        "The Silhouette Coefficient is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a\n",
        "model with \"better defined\" clusters. \n",
        "We'll use [`sklearn.metrics.silhouette_score`](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient)\n",
        "\n",
        "::: {.incremental}\n",
        "- Let $a$ be the mean distance between a data point and all other points in the same cluster.\n",
        "- Let $b$ be the mean distance between a data point and all other points in the next nearest cluster. \n",
        ":::\n",
        "\n",
        "\n",
        ":::: {.fragment}\n",
        "Then the **Silhouette Coefficient** for a point is\n",
        "\n",
        "$$ \n",
        "s = \\frac{b-a}{\\max(a, b)}.\n",
        "$$\n",
        "\n",
        "where $\\max(a, b)$ is the larger of the two mean distances.\n",
        "\n",
        "::::\n",
        "\n",
        "\n",
        " \n",
        "## Why This Makes Sense\n",
        "\n",
        "The **max(a, b)** normalization ensures the coefficient is always between -1 and 1:\n",
        "\n",
        "- **If a < b** (good clustering): Point is closer to its own cluster\n",
        "  - max(a, b) = b\n",
        "  - s = (b - a) / b = 1 - (a/b) → **positive (0 to 1)**\n",
        "\n",
        "- **If a > b** (poor clustering): Point is closer to other clusters  \n",
        "  - max(a, b) = a\n",
        "  - s = (b - a) / a = (b/a) - 1 → **negative (-1 to 0)**\n",
        "\n",
        "- **If a = b** (boundary): Point is equidistant\n",
        "  - s = 0\n",
        "\n",
        "This creates a scale where:\n",
        "\n",
        "- **+1** = perfectly clustered (a = 0, b > 0)\n",
        "- **0** = on the boundary (a = b)  \n",
        "- **-1** = completely misclustered (b = 0, a > 0)\n",
        "\n",
        "## Overall Silhouette Score\n",
        "\n",
        "\n",
        "The overall silhouette score is the average of the silhouette coefficient for each data point.\n",
        "\n",
        "$$\n",
        "(1/n) \\sum_{i=1}^n s_i\n",
        "$$\n",
        "\n",
        "\n",
        "## Silhouette Score for our synthetic data\n",
        "\n",
        "We can calculate the Silhouette Score for our synthetic data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sc = metrics.silhouette_score(X, labels, metric='euclidean')\n",
        "print('Silhouette Score:', sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-visible when-profile=\"slides\"}\n",
        "## Silhouette Coefficient\n",
        ":::\n",
        "\n",
        "We can also evaluate it for 2-10 clusters and plot the results:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def sc_evaluate_clusters(X, max_clusters, n_init, seed):\n",
        "    s = np.zeros(max_clusters+1)\n",
        "    s[0] = 0\n",
        "    s[1] = 0\n",
        "    for k in range(2, max_clusters+1):\n",
        "        kmeans = KMeans(init='k-means++', n_clusters = k, n_init = n_init, random_state = seed)\n",
        "        kmeans.fit_predict(X)\n",
        "        s[k] = metrics.silhouette_score(X, kmeans.labels_, metric = 'euclidean')\n",
        "    return s\n",
        "\n",
        "s = sc_evaluate_clusters(X, 10, 10, 1)\n",
        "plt.plot(range(2, len(s)), s[2:], 'o-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.title('$k$-means clustering performance on synthetic data')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::: {.fragment}\n",
        "Again, these results are more perfect than typical. \n",
        "\n",
        "But the general idea is to look for a local maximum in the Silhouette Coefficient as the potential number of clusters.\n",
        "::::\n",
        "\n",
        "# k-means++\n",
        "\n",
        "## k-means++ initialization\n",
        "Proposed in 2007 by David Arthur and Sergei Vassilvitskii.\n",
        "\n",
        "K-means++ improves the initialization of cluster centers to enhance the quality of the final clustering results. Here’s how it works.\n",
        "\n",
        "- Choose an initial centroid $c_1$ randomly.\n",
        "- Choose the next centroid $c_i$ with probability proportional to the squared distances to each data point.\n",
        "    - This ensures that new centroids are spread out across the data space.\n",
        "- Repeat this process until $k$ centroids have been selected.\n",
        "\n",
        "Once the initial centroids are chosen, the standard k-means algorithm is applied.\n",
        "\n",
        "The main advantage of k-means++ over randomly assigning points is that it reduces the likelihood of poor clustering results due to suboptimal initial centroids. This often leads to faster convergence and better overall clustering quality.\n",
        "\n",
        "## Visualizing K-means++ Initialization Probability\n",
        "\n",
        "Let's create a visualization that demonstrates how K-means++ selects the second centroid based on the squared distances from the first selected point:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "\n",
        "# Create a small uniformly distributed 2D dataset\n",
        "np.random.seed(42)\n",
        "n_points = 100\n",
        "X = np.random.uniform(-5, 5, (n_points, 2))\n",
        "\n",
        "# Select the first centroid randomly (simulate k-means++ first step)\n",
        "first_centroid = X[0]  # For demonstration, we'll use the first point\n",
        "\n",
        "# Calculate squared distances from all points to the first centroid\n",
        "distances_squared = np.sum((X - first_centroid)**2, axis=1)\n",
        "\n",
        "# Normalize to get probabilities (proportional to squared distances)\n",
        "probabilities = distances_squared / np.sum(distances_squared)\n",
        "\n",
        "# Create 3D plot\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot the 2D points\n",
        "ax.scatter(X[:, 0], X[:, 1], c='lightblue', s=30, alpha=0.6, label='Data points')\n",
        "\n",
        "# Highlight the first centroid\n",
        "ax.scatter(first_centroid[0], first_centroid[1], c='red', s=100, marker='*', \n",
        "           label='First centroid', edgecolors='darkred', linewidth=2)\n",
        "\n",
        "# Create 3D visualization where z-axis represents probability\n",
        "# We'll create a grid to show the probability surface\n",
        "x_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 50)\n",
        "y_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 50)\n",
        "X_grid, Y_grid = np.meshgrid(x_range, y_range)\n",
        "\n",
        "# Calculate probability for each grid point\n",
        "grid_distances_squared = (X_grid - first_centroid[0])**2 + (Y_grid - first_centroid[1])**2\n",
        "grid_probabilities = grid_distances_squared / np.sum(grid_distances_squared)\n",
        "\n",
        "# Plot the probability surface\n",
        "surf = ax.plot_surface(X_grid, Y_grid, grid_probabilities, \n",
        "                      alpha=0.3, cmap='viridis', \n",
        "                      linewidth=0, antialiased=True)\n",
        "\n",
        "# Add the actual data points with their probabilities as z-values\n",
        "ax.scatter(X[:, 0], X[:, 1], probabilities, c='darkblue', s=20, alpha=0.8)\n",
        "\n",
        "# Add colorbar\n",
        "fig.colorbar(surf, ax=ax, shrink=0.5, aspect=20, label='Selection Probability')\n",
        "\n",
        "# Set labels and title\n",
        "ax.set_xlabel('X coordinate')\n",
        "ax.set_ylabel('Y coordinate')\n",
        "ax.set_zlabel('Selection Probability')\n",
        "ax.set_title('K-means++ Initialization Probability\\n(Probability ∝ Squared Distance from First Centroid)')\n",
        "\n",
        "# Add legend\n",
        "ax.legend()\n",
        "\n",
        "# Set viewing angle for better visualization\n",
        "ax.view_init(elev=20, azim=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This 3D visualization shows:\n",
        "\n",
        "- **Light Blue points**: The uniformly distributed 2D dataset\n",
        "- **Red star**: The first randomly selected centroid\n",
        "- **Surface and Dark Blue points**: The probability of selecting each point as the second centroid\n",
        "- **Z-axis**: Represents the selection probability (proportional to squared distance from first centroid)\n",
        "\n",
        "As you can see, points farther from the first centroid have higher selection probability, which helps ensure that the second centroid is well-separated from the first one.\n",
        "\n",
        "# Real Data Clustering Example\n",
        "\n",
        "## Taking the Training Wheels Off: Real Data\n",
        "\n",
        "As a classic \"real world\" example, we'll use the \n",
        "[\"20 Newsgroup\" data](https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset)\n",
        "provided as example data in scikit-learn.\n",
        "\n",
        "We borrow code from this sklearn\n",
        "[example](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html).\n",
        "\n",
        "Let's load the data and count the number of documents and categories."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# just use the following categories\n",
        "categories = ['comp.os.ms-windows.misc', 'sci.space', 'rec.sport.baseball']\n",
        "\n",
        "news_data = fetch_20newsgroups(\n",
        "    remove = ('headers', 'footers', 'quotes'),\n",
        "    subset = 'train', \n",
        "    categories = categories,\n",
        "    shuffle = True,\n",
        "    random_state = 42)\n",
        "\n",
        "labels = news_data.target\n",
        "unique_labels, category_sizes = np.unique(labels, return_counts=True)\n",
        "true_k = unique_labels.shape[0]\n",
        "\n",
        "print(f\"{len(news_data.data)} documents - {true_k} categories\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Document\n",
        "\n",
        "Here is an example of one of the documents:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(news_data.data[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Extraction\n",
        "\n",
        "We've discussed a bit the challenges of feature engineering.   \n",
        "\n",
        "One of the most basic issues concerns how to encode categorical or text data in a form usable by algorithms that expect\n",
        "numeric input.\n",
        "\n",
        "The starting point is to note that one can encode a set using a binary vector with one component for each potential set\n",
        "member.  \n",
        "\n",
        "## Bag of Words\n",
        "\n",
        "The so-called _bag of words_ encoding for a document is to treat the document as a _multiset_ of words.\n",
        "\n",
        "> A _multiset_ is like a set but allows for multiple instances of each element.\n",
        "\n",
        "That is, we simply count how many times each word occurs. It is a \"bag\" because all the order of the words in the\n",
        "document is lost.\n",
        "\n",
        "Surprisingly, we can still tell a lot about the document even without knowing its word ordering.\n",
        "\n",
        "## Bag of Words Example\n",
        "\n",
        "Consider this short document (one paragraph):\n",
        "\n",
        "\"The quick brown fox jumps over the lazy dog. The dog sleeps soundly at noon.\"\n",
        "\n",
        "We ignore word order and count occurrences of each word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: false\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "doc = (\n",
        "    \"The quick brown fox jumps over the lazy dog. \"\n",
        "    \"The dog sleeps soundly at noon.\"\n",
        ")\n",
        "\n",
        "# simple tokenization: lowercase and keep alphabetic words\n",
        "tokens = re.findall(r\"[a-zA-Z']+\", doc.lower())\n",
        "counts = Counter(tokens)\n",
        "\n",
        "# show vocabulary and frequencies sorted by word\n",
        "sorted_counts = sorted(counts.items())\n",
        "sorted_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also view the words and counts as separate vectors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vocabulary = [w for w, c in sorted_counts]\n",
        "frequencies = [c for w, c in sorted_counts]\n",
        "print('words   =', vocabulary)\n",
        "print('counts  =', frequencies)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Term Frequencies\n",
        "\n",
        "Counting the number of times each word occurs in a document yields a vector of __term frequencies.__\n",
        "\n",
        "However, simply using the \"bag of words\" directly has a number of drawbacks. First of all, large documents will have\n",
        "more words than small documents.   \n",
        "\n",
        "Hence it often makes sense to normalize the frequency vectors.\n",
        "\n",
        "$\\ell_1$ or $\\ell_2$ normalization are common.\n",
        "\n",
        "## Bag of Words: Normalization\n",
        "\n",
        "\n",
        "We can scale raw counts so documents of different lengths are comparable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "import math\n",
        "\n",
        "try:\n",
        "    sorted_counts  # reuse if already defined above\n",
        "except NameError:\n",
        "    import re\n",
        "    from collections import Counter\n",
        "    doc = (\n",
        "        \"The quick brown fox jumps over the lazy dog. \"\n",
        "        \"The dog sleeps soundly at noon.\"\n",
        "    )\n",
        "    tokens = re.findall(r\"[a-zA-Z']+\", doc.lower())\n",
        "    from collections import Counter\n",
        "    counts = Counter(tokens)\n",
        "    sorted_counts = sorted(counts.items())\n",
        "\n",
        "vocabulary = [w for w, c in sorted_counts]\n",
        "counts_vec = [c for w, c in sorted_counts]\n",
        "\n",
        "# L1 normalization: divide by sum of counts\n",
        "total = sum(counts_vec)\n",
        "l1 = [c / total for c in counts_vec]\n",
        "\n",
        "# L2 normalization: divide by Euclidean norm\n",
        "norm2 = math.sqrt(sum(c*c for c in counts_vec))\n",
        "l2 = [c / norm2 for c in counts_vec]\n",
        "\n",
        "print('words =', vocabulary)\n",
        "print('\\nL1-normalized counts =', [f'{x:.2f}' for x in l1])\n",
        "print(f\"Sum of L1-normalized counts = {sum(l1):.2f}\")\n",
        "\n",
        "print('\\nL2-normalized counts =', [f'{x:.2f}' for x in l2])\n",
        "print(f\"Sum of L2-normalized counts = {sum(l2):.2f}\")\n",
        "\n",
        "norm2norm = math.sqrt(sum(c*c for c in l2))\n",
        "print(f\"The L2 norm of L2-normalized counts = {norm2norm:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tf-idf\n",
        "\n",
        "Next, as noted in __scikit-learn__:\n",
        "\n",
        "::: {.incremental}\n",
        "* In a large text corpus, some words will be very frequent (e.g. “the”, “a”, “is” in English) hence carrying very\n",
        "little meaningful information about the actual contents of the document. \n",
        "* If we were to feed the direct count data directly to a classifier those very frequent terms would overshadow the\n",
        "frequencies of rarer yet more interesting terms.\n",
        "* In order to re-weight the count features into floating point values suitable for usage by a classifier it is very\n",
        "common to use the **tf–idf transform.**\n",
        "* __Tf__ means __term-frequency__ while __tf–idf__ means __term-frequency times inverse document-frequency.__\n",
        "* This is originally a term weighting scheme developed for information retrieval (as a ranking function for search\n",
        "engines results), that has also found good use in document classification and clustering.\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "The idea is that rare words are more informative than common words.  \n",
        "\n",
        "(This has connections to information theory).\n",
        ":::\n",
        "\n",
        "## Tf-idf Definition\n",
        "\n",
        "Hence, the definition of tf-idf is as follows.\n",
        "\n",
        "First:\n",
        "\n",
        "$$\n",
        "\\text{tf}(t,d) = \\text{Number of times term }t \\text{ occurs in document } d~.\n",
        "$$\n",
        "\n",
        "Next, if $N$ is the total number of documents in the corpus $D$ then:\n",
        "\n",
        "$$\n",
        "\\text{idf}(t,D)=\\log{\\frac{N}{|\\{d\\in D : t\\in d \\}|}},\n",
        "$$\n",
        "\n",
        "where the denominator is the number of documents in which the term $t$ appears.\n",
        "\n",
        "And finally:\n",
        "\n",
        "$$\n",
        "\\text{tf-idf}(t,d)=\\text{tf}(t,d)\\times \\text{idf}(t,D).\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words = 'english', min_df = 4, max_df = 0.8)\n",
        "data = vectorizer.fit_transform(news_data.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF on Newsgroup Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| code-fold: true\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np\n",
        "\n",
        "# Reuse previously loaded 20 Newsgroups subset if available; otherwise load it\n",
        "try:\n",
        "    news_data\n",
        "    categories\n",
        "except NameError:\n",
        "    from sklearn.datasets import fetch_20newsgroups\n",
        "    categories = ['comp.os.ms-windows.misc', 'sci.space', 'rec.sport.baseball']\n",
        "    news_data = fetch_20newsgroups(\n",
        "        remove=('headers', 'footers', 'quotes'),\n",
        "        subset='train',\n",
        "        categories=categories,\n",
        "        shuffle=True,\n",
        "        random_state=42,\n",
        "    )\n",
        "\n",
        "# pick one example document from each category\n",
        "docs_by_cat = []\n",
        "labels = news_data.target\n",
        "for cat_index in range(len(categories)):\n",
        "    # first document whose label matches the category index\n",
        "    indices = np.where(labels == cat_index)[0]\n",
        "    if len(indices) > 0:\n",
        "        docs_by_cat.append(news_data.data[indices[0]])\n",
        "\n",
        "# compute TF-IDF on the selected three documents\n",
        "tfidf_vec = TfidfVectorizer(stop_words='english', min_df=1, max_df=1.0)\n",
        "X = tfidf_vec.fit_transform(docs_by_cat)\n",
        "feature_names = np.array(tfidf_vec.get_feature_names_out())\n",
        "\n",
        "def top_terms(row_vector, k=10):\n",
        "    row = row_vector.toarray().ravel()\n",
        "    if row.sum() == 0:\n",
        "        return []\n",
        "    top_idx = np.argsort(row)[-k:][::-1]\n",
        "    return list(zip(feature_names[top_idx], row[top_idx]))\n",
        "\n",
        "for i, doc in enumerate(docs_by_cat):\n",
        "    print(f\"\\nDocument {i+1} (category: {categories[i]}): Top TF-IDF terms\")\n",
        "    for term, score in top_terms(X[i], k=10):\n",
        "        print(f\"  {term:20s} {score:.4f}\")\n",
        "\n",
        "print(f\"The shape of the TF-IDF matrix is {X.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Getting to know the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(f'The datas type is {type(data)} and the shape is {data.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For what it's worth we can look at the data feature matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax1 = plt.subplots(1,1,figsize=(8,4))\n",
        "dum = sns.heatmap(data[1:100,1:200].todense(), xticklabels=False, yticklabels=False, \n",
        "            linewidths=0, cbar=True, ax=ax1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(news_data.target)\n",
        "print(news_data.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selecting the Number of Clusters -- Error\n",
        "\n",
        "Now let's look at the different cluster measures versus number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "error = evaluate_clusters(data, 10)\n",
        "plt.plot(range(1, len(error)), error[1:])\n",
        "plt.title('$k$-means Clustering Performance on Newsgroup Articles')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Error')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selecting the Number of Clusters -- RI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "ri, ari = ri_evaluate_clusters(data, 10, news_data.target)\n",
        "plt.plot(range(1, len(ari)), ari[1:], 'o-')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.title('$k$-means Clustering Compared to Known Labels\\nNewsgroup Articles')\n",
        "plt.ylabel('Adjusted Rand Index')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selecting the Number of Clusters -- Silhouette Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "s = sc_evaluate_clusters(data, 10, 100, 3)\n",
        "plt.plot(range(2, len(s)), s[2:], 'o-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.title('$k$-means clustering performance on Newsgroup Articles')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Looking into the clusters\n",
        "\n",
        "Run $k$-means with 3 clusters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "k = 3\n",
        "kmeans = KMeans(n_clusters = k, init = 'k-means++', max_iter = 100, n_init = 25, random_state = 3)\n",
        "kmeans.fit_predict(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Find the top 10 terms per cluster:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "asc_order_centroids = kmeans.cluster_centers_.argsort()#[:, ::-1]\n",
        "order_centroids = asc_order_centroids[:,::-1]\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "for i in range(k):\n",
        "    print(f'Cluster {i}:')\n",
        "    for ind in order_centroids[i, :10]:\n",
        "        print(f' {terms[ind]}')\n",
        "    print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pairwise Distances Matrix\n",
        "\n",
        "Let's calculate the pairwise distances matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "euclidean_dists = metrics.euclidean_distances(data)\n",
        "labels = kmeans.labels_\n",
        "idx = np.argsort(labels)\n",
        "clustered_dists = euclidean_dists[idx][:,idx]\n",
        "fig, ax1 = plt.subplots(1,1,figsize=(6,6))\n",
        "dum = sns.heatmap(clustered_dists, xticklabels=False, yticklabels=False, linewidths=0, square=True, cbar=True, ax=ax1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MDS Embedding\n",
        "\n",
        "Let's visualize with MDS.   \n",
        "\n",
        "Note that MDS is a slow algorithm and we can't do all 1700+ data points quickly, so we will take a random sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import random\n",
        "n_items = euclidean_dists.shape[0]\n",
        "random.seed(42)\n",
        "subset = random.sample(range(n_items), 500)\n",
        "\n",
        "fit = mds.fit(euclidean_dists[subset][:, subset])\n",
        "pos = fit.embedding_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have the labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MDS Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "colorblind_palette = ['#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2']\n",
        "cols = [colorblind_palette[l] for l in labels[subset]]\n",
        "plt.scatter(pos[:, 0], pos[:, 1], s = 12, c = cols)\n",
        "plt.title('MDS Embedding of Newsgroup Articles')\n",
        "\n",
        "unique_labels = np.unique(labels[subset])\n",
        "# Create legend handles and labels based on unique labels\n",
        "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=colorblind_palette[l], markersize=10) for l in unique_labels]\n",
        "legend_labels = [f'Cluster {l}' for l in unique_labels]\n",
        "# Add the legend to the plot\n",
        "plt.legend(handles, legend_labels, title='Clusters')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> Question? Why does it produce this toroid shape?\n",
        "\n",
        "::: {.fragment}\n",
        "**Possible Answer:** The data is high-dimensional and could be suffering from \n",
        "curse of dimensionality where most points are nearly the same distance from\n",
        "each other.\n",
        ":::\n",
        "\n",
        "\n",
        "## Recap and Next\n",
        "\n",
        "We've covered:\n",
        "\n",
        "* How clustering is used in practice\n",
        "* Tools for evaluating the quality of a clustering\n",
        "* Tools for assigning meaning or labels to a cluster\n",
        "* Important visualizations\n",
        "* A little bit about feature extraction for text\n",
        "\n",
        "Next time, we'll look at: \n",
        "\n",
        "* Hierarchical clustering\n",
        "* Gaussian mixture models"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}