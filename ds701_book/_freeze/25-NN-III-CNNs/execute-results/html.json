{
  "hash": "1fab83557e55bd49c326851253ec4e43",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'NN III -- Stochastic Gradient Descent, Batches and Convolutional Neural Networks'\njupyter: python3\n---\n\n\n## Recap\n\nSo far we covered\n* Gradients, gradient descent and back propagation\n* Fully connected neural networks (Multi-Layer Perceptron)\n* Training of MLPs using back propagation\n\nToday, we'll cover\n* _Stochastic_ gradient descent (SGD)\n* Convolutional Neural Networks (CNNs)\n* Training a CNN with SGD\n\n## Batches and Stochastic Gradient Descent\n\n\n* Compute the gradient (e.g. forward pass and backward pass) with only a _random subset_\nof the input data.\n\n> We call the subset a _batch_.\n\n* Work through the dataset by _randomly sampling without replacement_. This is the _stochastic_ part.\n\n* One pass through the data is called an _epoch_.\n\nFor squared error loss with $N$ input samples, the loss for (full-batch) gradient descent was\n\n$$\nL = \\sum_{i=0}^{N-1} \\ell_i = \\sum_{i=0}^{N-1} \\left( y - \\hat{y}  \\right)^2\n$$\n\nFor _Stochastic Gradient Descent_, we calculate the loss only on a _batch_ at as time.\nFor every time $t$, let's denote the batch as $\\mathcal{B}_t$\n\n$$\nL_t = \\sum_{i \\in \\mathcal{B}_t} \\ell_i = \\sum_{i \\in \\mathcal{B}_t} \\left( y - \\hat{y}  \\right)^2\n$$\n\nLet's look at an example.\n\n::: {#de3f9024 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Generate 12 evenly spaced x values between 1 and 4\nx = np.linspace(1, 4, 12)\n\n# Add normally distributed noise to the x values\nx += np.random.normal(0, 1.0, 12)\n\n# Calculate the corresponding y values for the line y = 2x\ny = 2 * x\n\n# Add normally distributed noise to the y values\ny += np.random.normal(0, 1.0, 12)\n\n# Shuffle the points and split them into 3 groups of 4\nindices = np.random.permutation(12)\ncolors = ['red', 'green', 'blue', 'purple']\nlabels = ['batch 1', 'batch 2', 'batch 3', 'batch 4']\n\n# Plot each group of points with a different color and label\nfor i in range(4):\n    plt.scatter(x[indices[i*3:(i+1)*3]], y[indices[i*3:(i+1)*3]], color=colors[i], label=labels[i])\n\n# Display the legend\nplt.legend()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-html/cell-3-output-1.png){width=558 height=411}\n:::\n:::\n\n\nSay we have a training data set of 12 points and we want to use a _batch size_ of 3.\n\nDivide the 12 points into batches of 3 by randomlly selecting points without replacement.\n\n::: {#f6b8169d .cell hide_input='true' slideshow='{\"slide_type\":\"subslide\"}' tags='[\"hide-input\"]' execution_count=3}\n``` {.python .cell-code}\n# Shuffle the points and split them into 3 groups of 4\nindices = np.random.permutation(12)\ncolors = ['red', 'green', 'blue', 'purple']\nlabels = ['batch 1', 'batch 2', 'batch 3', 'batch 4']\n\n# Plot each group of points with a different color and label\nfor i in range(4):\n    plt.scatter(x[indices[i*3:(i+1)*3]], y[indices[i*3:(i+1)*3]], color=colors[i], label=labels[i])\n\n# Display the legend\nplt.legend()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-html/cell-4-output-1.png){width=558 height=411}\n:::\n:::\n\n\nWe can resample again to create a different set of batches.\n\nOptionally, you can shuffle after every epoch.\n\n::: {#2c5fb99f .cell hide_input='true' slideshow='{\"slide_type\":\"subslide\"}' tags='[\"hide-input\"]' execution_count=4}\n``` {.python .cell-code}\ncolors = ['red', 'lightgray', 'lightgray', 'lightgray']\nlabels = ['batch 1', 'batch 2', 'batch 3', 'batch 4']\n\n# Plot each group of points with a different color and label\nfor i in range(4):\n    plt.scatter(x[indices[i*3:(i+1)*3]], y[indices[i*3:(i+1)*3]], color=colors[i], label=labels[i])\n\n# Display the legend\nplt.legend()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-html/cell-5-output-1.png){width=558 height=411}\n:::\n:::\n\n\nThen for every training iteration, you calculate the forward pass and backward pass loss with only the data from the batch.\n\nAbove, we use data from the 1st batch.\n\n::: {#06caa4d3 .cell hide_input='true' slideshow='{\"slide_type\":\"subslide\"}' tags='[\"hide-input\"]' execution_count=5}\n``` {.python .cell-code}\ncolors = ['lightgray', 'green', 'lightgray', 'lightgray']\nlabels = ['batch 1', 'batch 2', 'batch 3', 'batch 4']\n\n# Plot each group of points with a different color and label\nfor i in range(4):\n    plt.scatter(x[indices[i*3:(i+1)*3]], y[indices[i*3:(i+1)*3]], color=colors[i], label=labels[i])\n\n# Display the legend\nplt.legend()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-html/cell-6-output-1.png){width=558 height=411}\n:::\n:::\n\n\n::: {#a7e9aba5 .cell hide_input='true' slideshow='{\"slide_type\":\"subslide\"}' tags='[\"hide-input\"]' execution_count=6}\n``` {.python .cell-code}\ncolors = ['lightgray', 'lightgray', 'blue', 'lightgray']\nlabels = ['batch 1', 'batch 2', 'batch 3', 'batch 4']\n\n# Plot each group of points with a different color and label\nfor i in range(4):\n    plt.scatter(x[indices[i*3:(i+1)*3]], y[indices[i*3:(i+1)*3]], color=colors[i], label=labels[i])\n\n# Display the legend\nplt.legend()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-html/cell-7-output-1.png){width=558 height=411}\n:::\n:::\n\n\n::: {#29a19dfa .cell hide_input='true' slideshow='{\"slide_type\":\"subslide\"}' tags='[\"hide-input\"]' execution_count=7}\n``` {.python .cell-code}\ncolors = ['lightgray', 'lightgray', 'lightgray', 'purple']\nlabels = ['batch 1', 'batch 2', 'batch 3', 'batch 4']\n\n# Plot each group of points with a different color and label\nfor i in range(4):\n    plt.scatter(x[indices[i*3:(i+1)*3]], y[indices[i*3:(i+1)*3]], color=colors[i], label=labels[i])\n\n# Display the legend\nplt.legend()\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-html/cell-8-output-1.png){width=558 height=411}\n:::\n:::\n\n\n### Advantages of Stochastic Gradient Descent\n\nThere are two main advantages to _Stochastic Gradient Descent_.\n\n1. You don't read and compute on every input data sample for every training iteration, \n    * Speeds up iteration while still making optimization progress\n    * This works better with limited GPU memory and CPU cache. Not slowing down by thrashing limited memory.\n\n2. Improves training convergence by adding _noise_ to the weight updates.\n    * Can avoid getting stuck in a local minima.\n\nAn example\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n<center>    \n    \n<img src=\"figs/NN-figs/L25-GD-vs-SGD.png\" width=\"50%\"> \n    \n</center> \n\n\nThis a contour plot showing a loss surface for a model with only 2 parameters.\n\nFor full-batch gradient descent, starting points 1 and 3 still end up at the\nglobal minimum, but starting point 2 get stuck in a local minimum.\n\nFor stochastic gradient descent, starting point 1 still ends up at the global \nminimum, but now starting point 2 also avoids the local minimum and ends up at\nthe global minimum.\n\n## Load an Image Dataset in Batches in PyTorch\n\n::: {#7c6f6c5d .cell slideshow='{\"slide_type\":\"skip\"}' execution_count=8}\n``` {.python .cell-code}\n%matplotlib inline\n```\n:::\n\n\n::: {#14ebd573 .cell slideshow='{\"slide_type\":\"skip\"}' execution_count=9}\n``` {.python .cell-code}\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\n```\n:::\n\n\n### 1. Load and Scale MNIST\n\nLoad MNIST handwritten digit dataset with 60K training samples and 10K test samples.\n\n::: {#42f2ecef .cell execution_count=10}\n``` {.python .cell-code}\n# Define a transform to scale the pixel values from [0, 255] to [-1, 1]\ntransform = transforms.Compose([transforms.ToTensor(),\n                                transforms.Normalize((0.5,), (0.5,))])\n\nbatch_size = 64\n\n# Download and load the training data\ntrainset = torchvision.datasets.MNIST('./data/MNIST_data/', download=True,\n                                    train=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, \n                                          shuffle=True)\n\n# Download and load the test data\ntestset = torchvision.datasets.MNIST('./data/MNIST_data/', download=True,\n                                    train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, \n                                         shuffle=True)\n```\n:::\n\n\n`torchvision.dataset.MNIST` is a convenience class which inherits from\n`torch.utils.data.Dataset` (see [doc](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset))\nthat wraps a particular dataset and overwrites a `__getitem__()` method which retrieves a data sample given an \nindex or a key.\n\nIf we give the argument `train=True`, it returns the training set, while the \nargument `train=False` returns the test set.\n\n`torch.utils.data.DataLoader()` takes a dataset as in the previous line and\nreturns a python _iterable_ which lets you loop through the data.\n\nWe give `DataLoader` the _batch size_, and it will return a batch of data samples\non each iteration.\n\nBy passing `shuffle=True`, we are telling the data loader to shuffle the batches\nafter every epoch.\n\n::: {#301b6278 .cell slideshow='{\"slide_type\":\"subslide\"}' execution_count=11}\n``` {.python .cell-code}\nprint(f\"No. of training images: {len(trainset)}\")\nprint(f\"No. of test images: {len(testset)}\")\nprint(\"The dataset classes are:\")\nprint(trainset.classes)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNo. of training images: 60000\nNo. of test images: 10000\nThe dataset classes are:\n['0 - zero', '1 - one', '2 - two', '3 - three', '4 - four', '5 - five', '6 - six', '7 - seven', '8 - eight', '9 - nine']\n```\n:::\n:::\n\n\nWe can see the data loader, `trainloader` in action in the code below to\nget a batch and visualize it.\n\nEverytime we rerun the cell we will get a different batch.\n\n::: {#5ab99ee8 .cell execution_count=12}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# get some random training images\ndataiter = iter(trainloader)\nimages, labels = next(dataiter)\n\n# show images\nimshow(torchvision.utils.make_grid(images))\n```\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-html/cell-13-output-1.png){width=424 height=415}\n:::\n:::\n\n\nWe can display the training labels for the image as well.\n\n::: {#51e167f9 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=13}\n``` {.python .cell-code}\nfrom IPython.display import display, HTML\n\n# Assuming batch_size is 64 and images are displayed in an 8x8 grid\nlabels_grid = [trainset.classes[labels[j]] for j in range(64)]\nlabels_grid = np.array(labels_grid).reshape(8, 8)\n\ndf = pd.DataFrame(labels_grid)\n\n# Generate HTML representation of DataFrame with border\nhtml = df.to_html(border=1)\n\n# Display the DataFrame\ndisplay(HTML(html))\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9 - nine</td>\n      <td>7 - seven</td>\n      <td>1 - one</td>\n      <td>4 - four</td>\n      <td>7 - seven</td>\n      <td>9 - nine</td>\n      <td>2 - two</td>\n      <td>4 - four</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4 - four</td>\n      <td>5 - five</td>\n      <td>4 - four</td>\n      <td>0 - zero</td>\n      <td>9 - nine</td>\n      <td>6 - six</td>\n      <td>9 - nine</td>\n      <td>8 - eight</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1 - one</td>\n      <td>7 - seven</td>\n      <td>7 - seven</td>\n      <td>7 - seven</td>\n      <td>8 - eight</td>\n      <td>9 - nine</td>\n      <td>2 - two</td>\n      <td>9 - nine</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1 - one</td>\n      <td>6 - six</td>\n      <td>2 - two</td>\n      <td>7 - seven</td>\n      <td>2 - two</td>\n      <td>7 - seven</td>\n      <td>6 - six</td>\n      <td>1 - one</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0 - zero</td>\n      <td>3 - three</td>\n      <td>6 - six</td>\n      <td>6 - six</td>\n      <td>8 - eight</td>\n      <td>2 - two</td>\n      <td>6 - six</td>\n      <td>7 - seven</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5 - five</td>\n      <td>5 - five</td>\n      <td>2 - two</td>\n      <td>9 - nine</td>\n      <td>4 - four</td>\n      <td>7 - seven</td>\n      <td>1 - one</td>\n      <td>8 - eight</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>4 - four</td>\n      <td>9 - nine</td>\n      <td>0 - zero</td>\n      <td>8 - eight</td>\n      <td>4 - four</td>\n      <td>0 - zero</td>\n      <td>3 - three</td>\n      <td>4 - four</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>6 - six</td>\n      <td>6 - six</td>\n      <td>7 - seven</td>\n      <td>6 - six</td>\n      <td>5 - five</td>\n      <td>1 - one</td>\n      <td>2 - two</td>\n      <td>6 - six</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\n## Convolutional Network Applications\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n<center>    \n    \n<img src=\"figs/NN-figs/L25-img-class.svg\">\n    \n</center> \n\n* Multi-class classification problem ( >2 possible classes)\n* Convolutional network with classification output\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n<center>    \n    \n<img src=\"figs/NN-figs/L25-obj-det.png\">\n    \n</center> \n\n* Localize and classify objects in an image\n* Convolutional network with classification _and_ regression output\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n<center>    \n    \n<img src=\"figs/NN-figs/L25-img-seg.png\">\n    \n</center> \n\n* Classify each pixel in an image to 2 or more classes\n* Convolutional encoder-decoder network with a classification values for each pixel.\n\n## Convolutional Neural Networks\n\nProblems with fully-connected networks\n\n* Size\n    * 224x224 RGB image = 150,528 dimensions\n    * Hidden layers generally larger than inputs\n    * One hidden layer = 150,520x150,528 weights -- 22 billion\n* Nearby pixels statistically related\n    * But fully connected network doesn't exploit spatial correlation\n* Should be stable under transformations\n    * Donâ€™t want to re-learn appearance at different parts of image\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n#### Classification Invariant to Shift\n\n<center>    \n    \n<img src=\"figs/NN-figs/L25-shift-img-class.png\">\n    \n</center> \n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n#### Image Segmentation Invariant to Shift\n\n<center>    \n    \n<img src=\"figs/NN-figs/L25-shift-seg.png\">\n    \n</center> \n\nSolution: Convolutional Neural Networks\n\n* Parameters only look at local data regions\n\n* Shares parameters across image or signal\n\n### Example with 1-D Input Data\n\nIn _convolutional neural networks_, we define a set of weights that we move across\nthe input data.\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n<center>\n    \n<img src=\"figs/NN-figs/L25-conv04.png\">\n    \n</center> \n\nExample with 3 weights and input of length 6.\n\nFor figure (a), we calculate \n\n$$ z_2 = \\omega_1 x_1 + \\omega_2 x_2 + \\omega_3 x_3 $$\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n<center> \n    \n<img src=\"figs/NN-figs/L25-conv05.png\">\n    \n</center> \n\nTo calculate $z_2$, we shift the weights over 1 place (figure (b)) and then\nweight and sum the inputs. We can generalize the equation slightly.\n\n$$ z_i = \\omega_1 x_{i - 1} + \\omega_2 x_i + \\omega_3 x_{i+1} $$\n\nBut what do we do about $z_1$?\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n<center>\n    \n<img src=\"figs/NN-figs/L25-conv06.png\">\n    \n</center> \n\nWe can calculate $z_1$ by _padding_ our input data. In figure (c), we\nsimply add $0$, which means we can now calculate $z_1$.\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n<center>\n    \n<img src=\"figs/NN-figs/L25-conv07.png\">\n    \n</center> \n\nAlternatively, we can just reduce the size of the output, by only calculating where\nwe have _valid_ input data, as in figure (d).\n\nFor 1-D data, this reduces the output size by 1 at the beginning and end of the\ndata, so by 2 overall for length-3 filter.\n\n<!-- Image Credit \"https://udlbook.github.io/udlbook/\"-->\n\n<center>    \n    \n<img src=\"figs/NN-figs/L25-conv-fig10-3.png\">\n    \n</center> \n\nThere are a few design choices one can make with convolution layers, such as:\n\n1. _filter length_, e.g. size 3 in figures (a) and (b)\n2. _stride_, which is how much you shift to calculate the next output. Common values are\n    1. _stride 1_ as we saw in the previous examples and in figures (c) and (d)\n    2. _stride 2_, where you shift by 2 instead of 1, an effectively halve the size of the output as in figures (a) and (b)\n3. _dilation_, where you expand the filter as in figure (d)\n\n### 2D Convolution\n\nFor images and video frames we use a two-dimensional convolution\n(called `conv2d` in PyTorch) which is an extension of the 1-D\nconvolution as shown in the following illustration.\n\n<!-- Image Credit \"https://cs231n.github.io/convolutional-networks/\"-->\n<center>    \n    \n<img src=\"figs/NN-figs/L25-conv-2d.png\">\n    \n</center> \n\n[cs231n](https://cs231n.github.io/convolutional-networks/)\n\nTo see this figure animated, clone the class repo and click on the file `./conv-demo/index.html`.\n\n## Define a Convolutional Neural Network in PyTorch\n\nWe will do the following steps in order:\n\n1. We already loaded and scaled the MNIST training and test datasets using\n   ``torchvision``\n2. Define a Convolutional Neural Network\n3. Define a loss function\n4. Train the network on the training data\n5. Test the network on the test data\n\n::: {#4b058780 .cell slideshow='{\"slide_type\":\"subslide\"}' execution_count=14}\n``` {.python .cell-code}\n# network for MNIST\nimport torch\nfrom torch import nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = nn.functional.relu(x)\n        x = self.conv2(x)\n        x = nn.functional.relu(x)\n        x = nn.functional.max_pool2d(x, 2)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = nn.functional.relu(x)\n        x = self.fc2(x)\n        output = nn.functional.log_softmax(x, dim=1)\n        return output\n\nnet = Net()\n```\n:::\n\n\nWhere\n\n```python\nCLASS torch.nn.Conv2d(in_channels, out_channels, kernel_size, \n                      stride=1, padding_mode='valid', ...)\n```\n\n\n| Layer   | Kernel Size | Stride | Input Shape | Input Channels | Output Channels | Output Shape |\n| ------- | ----------- | ------ | ----------- | -------------- | --------------- | ------------ |\n| Conv2D/ReLU  | (3x3)       | 1      |  28x28      |    1           |    32           |  26x26       |\n| Conv2D/ReLU  | (3x3)       | 1      |  26x26      |    32          |    64           |  24x24       |\n| Max_pool2d | (2x2)    | 2      |  24x24      |    64          |    64           |  12x12       |\n| Flatten |             |        |  12x12      |    64          |    1            |  9216x1      |\n| FC/ReLU |             |        |  9216x1     |    1           |    1            |  128x1       |\n| FC Linear |           |        |  128x1      |    1           |    1            |  10x1        |\n| Soft Max |            |        |  10x1      |    1           |    1            |  10x1        |\n\n<!-- Image Credit \"https://alexlenail.me/NN-SVG/AlexNet.html\"-->\n<center>    \n    \n<img src=\"figs/NN-figs/L25-mnist-cnn2.svg\">\n    \n</center> \n\n[NN-SVG](https://alexlenail.me/NN-SVG/AlexNet.html)\n\n### 3. Define a Loss function and optimizer\nWe'll use a Classification Cross-Entropy loss and SGD with momentum.\n\n::: {#70c76d29 .cell execution_count=15}\n``` {.python .cell-code}\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n```\n:::\n\n\n#### Cross Entropy Loss\n\n* Popular loss function for multi-class classification that measures the _dissimilarity_ between the predicted class log probability $\\log(p_i)$ and the true class $y_i$.\n\n$$ - \\sum_i y_i \\log(p_i) $$\n\nSee for example [here](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) for more information.\n\n#### Momentum\n\nMomentum is a technique used in optimizing neural networks that helps accelerate gradients vectors in the right directions, leading to faster convergence. It is inspired by physical laws of motion where the optimizer uses 'momentum' to push over hilly terrain and valleys to find the global minimum.\n\nIn gradient descent, the weight update rule with momentum is given by:\n\n\n$$ \nm_{t+1} = \\beta m_t + (1 - \\beta) \\nabla J(w)\n$$\n\n$$\nw_{t+1} = w_t - \\alpha m_{t+1}\n$$\n\nwhere:\n\n* $m_t$ is the momentum (which drives the update at iteration $t$), \n* $\\beta \\in [0, 1)$, typically 0.9, controls the degree to which the gradient is smoothed over time, and \n* $\\alpha$ is the learning rate.\n\nSee _Understanding Deep Learning_, Section 6.3 to learn more.\n\n### 4. Train the network\n\n::: {#abec3b37 .cell execution_count=16}\n``` {.python .cell-code}\nprint(f\"[Epoch #, Iteration #] loss\")\n\n# loop over the dataset multiple times\n# change this value to 2\nfor epoch in range(1):  \n    \n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get the inputs; data is a list of [inputs, labels]\n        inputs, labels = data\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        outputs = net(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        # print statistics\n        running_loss += loss.item()\n        if i % 100 == 99:    # print every 2000 mini-batches\n            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n            running_loss = 0.0\n\nprint('Finished Training')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[Epoch #, Iteration #] loss\n[1,   100] loss: 0.112\n[1,   200] loss: 0.082\n[1,   300] loss: 0.032\n[1,   400] loss: 0.022\n[1,   500] loss: 0.020\n[1,   600] loss: 0.018\n[1,   700] loss: 0.016\n[1,   800] loss: 0.016\n[1,   900] loss: 0.015\nFinished Training\n```\n:::\n:::\n\n\nDisplay some of the images from the test set with the ground truth labels.\n\n::: {#e834e907 .cell execution_count=17}\n``` {.python .cell-code}\ndataiter = iter(testloader)\nimages, labels = next(dataiter)\n\n# print images\nimshow(torchvision.utils.make_grid(images))\n#print('GroundTruth: ', ' '.join(f'{testset.classes[labels[j]]:5s}' for j in range(4)))\n```\n\n::: {.cell-output .cell-output-display}\n![](25-NN-III-CNNs_files/figure-html/cell-18-output-1.png){width=424 height=415}\n:::\n:::\n\n\n::: {#83e2b01b .cell hide_input='true' tags='[\"hide-input\"]' execution_count=18}\n``` {.python .cell-code}\nfrom IPython.display import display, HTML\n\n# Assuming batch_size is 64 and images are displayed in an 8x8 grid\nlabels_grid = [testset.classes[labels[j]] for j in range(64)]\nlabels_grid = np.array(labels_grid).reshape(8, 8)\n\ndf = pd.DataFrame(labels_grid)\n\n# Generate HTML representation of DataFrame with border\nhtml = df.to_html(border=1)\n\n# Display the DataFrame\ndisplay(HTML(html))\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3 - three</td>\n      <td>4 - four</td>\n      <td>9 - nine</td>\n      <td>3 - three</td>\n      <td>9 - nine</td>\n      <td>4 - four</td>\n      <td>2 - two</td>\n      <td>5 - five</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5 - five</td>\n      <td>9 - nine</td>\n      <td>6 - six</td>\n      <td>2 - two</td>\n      <td>5 - five</td>\n      <td>3 - three</td>\n      <td>7 - seven</td>\n      <td>8 - eight</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3 - three</td>\n      <td>2 - two</td>\n      <td>0 - zero</td>\n      <td>7 - seven</td>\n      <td>5 - five</td>\n      <td>1 - one</td>\n      <td>5 - five</td>\n      <td>9 - nine</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3 - three</td>\n      <td>4 - four</td>\n      <td>0 - zero</td>\n      <td>9 - nine</td>\n      <td>9 - nine</td>\n      <td>4 - four</td>\n      <td>5 - five</td>\n      <td>2 - two</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8 - eight</td>\n      <td>6 - six</td>\n      <td>1 - one</td>\n      <td>0 - zero</td>\n      <td>1 - one</td>\n      <td>2 - two</td>\n      <td>9 - nine</td>\n      <td>0 - zero</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5 - five</td>\n      <td>6 - six</td>\n      <td>8 - eight</td>\n      <td>8 - eight</td>\n      <td>9 - nine</td>\n      <td>7 - seven</td>\n      <td>9 - nine</td>\n      <td>4 - four</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0 - zero</td>\n      <td>9 - nine</td>\n      <td>3 - three</td>\n      <td>9 - nine</td>\n      <td>8 - eight</td>\n      <td>6 - six</td>\n      <td>1 - one</td>\n      <td>3 - three</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2 - two</td>\n      <td>3 - three</td>\n      <td>0 - zero</td>\n      <td>1 - one</td>\n      <td>4 - four</td>\n      <td>5 - five</td>\n      <td>8 - eight</td>\n      <td>9 - nine</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nLet's run inference (forward pass) on the model to get numeric outputs.\n\n::: {#0991e810 .cell execution_count=19}\n``` {.python .cell-code}\noutputs = net(images)\n```\n:::\n\n\nGet the index of the element with highest value and print the label \nassociated with that index.\n\n::: {#ce5828a7 .cell execution_count=20}\n``` {.python .cell-code}\n_, predicted = torch.max(outputs, 1)\n\n#print('Predicted: ', ' '.join(f'{testset.classes[predicted[j]]:5s}'\n#                              for j in range(4)))\n```\n:::\n\n\n::: {#40ad8b64 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=21}\n``` {.python .cell-code}\n# Assuming batch_size is 64 and images are displayed in an 8x8 grid\nlabels_grid = [testset.classes[predicted[j]] for j in range(64)]\nlabels_grid = np.array(labels_grid).reshape(8, 8)\n\ndf = pd.DataFrame(labels_grid)\n\n# Generate HTML representation of DataFrame with border\nhtml = df.to_html(border=1)\n\n# Display the DataFrame\ndisplay(HTML(html))\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6 - six</td>\n      <td>4 - four</td>\n      <td>9 - nine</td>\n      <td>3 - three</td>\n      <td>9 - nine</td>\n      <td>4 - four</td>\n      <td>2 - two</td>\n      <td>5 - five</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3 - three</td>\n      <td>9 - nine</td>\n      <td>6 - six</td>\n      <td>2 - two</td>\n      <td>5 - five</td>\n      <td>3 - three</td>\n      <td>7 - seven</td>\n      <td>5 - five</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3 - three</td>\n      <td>2 - two</td>\n      <td>0 - zero</td>\n      <td>7 - seven</td>\n      <td>5 - five</td>\n      <td>1 - one</td>\n      <td>5 - five</td>\n      <td>9 - nine</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3 - three</td>\n      <td>4 - four</td>\n      <td>0 - zero</td>\n      <td>9 - nine</td>\n      <td>4 - four</td>\n      <td>4 - four</td>\n      <td>5 - five</td>\n      <td>2 - two</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8 - eight</td>\n      <td>6 - six</td>\n      <td>1 - one</td>\n      <td>0 - zero</td>\n      <td>1 - one</td>\n      <td>2 - two</td>\n      <td>9 - nine</td>\n      <td>0 - zero</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5 - five</td>\n      <td>6 - six</td>\n      <td>8 - eight</td>\n      <td>8 - eight</td>\n      <td>9 - nine</td>\n      <td>7 - seven</td>\n      <td>9 - nine</td>\n      <td>4 - four</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0 - zero</td>\n      <td>4 - four</td>\n      <td>3 - three</td>\n      <td>9 - nine</td>\n      <td>8 - eight</td>\n      <td>6 - six</td>\n      <td>1 - one</td>\n      <td>3 - three</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2 - two</td>\n      <td>3 - three</td>\n      <td>0 - zero</td>\n      <td>1 - one</td>\n      <td>4 - four</td>\n      <td>5 - five</td>\n      <td>7 - seven</td>\n      <td>9 - nine</td>\n    </tr>\n  </tbody>\n</table>\n```\n:::\n:::\n\n\nEvaluate over the entire test set.\n\n::: {#142d873a .cell execution_count=22}\n``` {.python .cell-code}\ncorrect = 0\ntotal = 0\n# since we're not training, we don't need to calculate the gradients for our outputs\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        # calculate outputs by running images through the network\n        outputs = net(images)\n        # the class with the highest energy is what we choose as prediction\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of the network on the 10000 test images: 92 %\n```\n:::\n:::\n\n\nEvaluate the performance per class.\n\n::: {#483d3312 .cell execution_count=23}\n``` {.python .cell-code}\n# prepare to count predictions for each class\ncorrect_pred = {classname: 0 for classname in testset.classes}\ntotal_pred = {classname: 0 for classname in testset.classes}\n\n# again no gradients needed\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predictions = torch.max(outputs, 1)\n        # collect the correct predictions for each class\n        for label, prediction in zip(labels, predictions):\n            if label == prediction:\n                correct_pred[testset.classes[label]] += 1\n            total_pred[testset.classes[label]] += 1\n\n\n# print accuracy for each class\nfor classname, correct_count in correct_pred.items():\n    accuracy = 100 * float(correct_count) / total_pred[classname]\n    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy for class: 0 - zero is 98.9 %\nAccuracy for class: 1 - one is 97.0 %\nAccuracy for class: 2 - two is 90.3 %\nAccuracy for class: 3 - three is 92.6 %\nAccuracy for class: 4 - four is 91.6 %\nAccuracy for class: 5 - five is 89.2 %\nAccuracy for class: 6 - six is 93.6 %\nAccuracy for class: 7 - seven is 89.2 %\nAccuracy for class: 8 - eight is 90.8 %\nAccuracy for class: 9 - nine is 91.0 %\n```\n:::\n:::\n\n\n### To Dig Deeper\n\nLook at common CNN network architectures. \n\nFor example in _Understanding Deep Learning_ section 10.5.\n\n",
    "supporting": [
      "25-NN-III-CNNs_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}