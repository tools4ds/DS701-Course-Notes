{
  "hash": "375a774aaa9fe0f42d823bd1bd04bb61",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Distances and Time Series\njupyter: python3\n---\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/05-Distances-Timeseries.ipynb)\n\nWe will start building some tools for making comparisons of data objects with particular attention to time series.\n\nWorking with data, we can encounter a wide variety of different data objects\n\n::: {.incremental}\n* records of users,\n* images,\n* videos,\n* text (webpages, books),\n* strings (DNA sequences), and\n* time series.\n:::\n\n::: {.fragment}\nHow can we compare them?\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Lecture Overview\n\nWe cover the following topics in today's lecture\n\n:::: {.incremental}\n- feature space and matrix representations of data,\n- metrics, norms, similarity, and dissimilarity,\n- bit vectors, sets, and time series.\n::::\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Time Series Data\nSome examples of time series data\n\n:::: {.incremental}\n- stock prices,\n- weather data,\n- electricity consumption,\n- website traffic,\n- retail sales, and\n- various economic indicators.\n::::\n:::\n\n## Feature space representation\n\nUsually a data object consists of a set of attributes.\n\nThese are also commonly called __features.__\n\n* (\"J. Smith\", 25, \\$ 200,000)\n* (\"M. Jones\", 47, \\$ 45,000)\n\nIf all $d$ dimensions are real-valued then we can visualize each data object as a point in a $d$-dimensional vector space.\n \n* `(25, USD 200000)` $\\rightarrow \\begin{bmatrix}25\\\\200000\\end{bmatrix}$.\n\nLikewise If all features are binary then we can think of each data object as a binary vector in vector space.\n\nThe space is called __feature space.__\n\n::: {.content-hidden when-profile=\"web\"}\n## One-hot encoding\n:::\nVector spaces are such a useful tool that we often use them even for non-numeric data.\n\nFor example, consider a categorical variable that can be only one of \"house\", \"tree\", or \"moon\". For such a variable, we can use a __one-hot__ encoding.  \n\n::: {.content-hidden when-profile=\"slides\"}\nWe would encode as follows:\n:::\n\n::: {.incremental}\n* `house`: $[1, 0, 0]$\n* `tree`:  $[0, 1, 0]$\n* `moon`:  $[0, 0, 1]$\n:::\n\n::: {.fragment}\nSo an encoding of `(25, USD 200000, 'house')` could be: \n$$\\begin{bmatrix}25\\\\200000\\\\1\\\\0\\\\0\\end{bmatrix}.$$\n::: \n\n## Why not alternative encoding?\n\nWhy would we not encode as follows:\n\n\n* `house`: $[1]$\n* `tree`:  $[2]$\n* `moon`:  $[3]$\n\n::: {.fragment}\nSo an encoding of `(25, USD 200000, 'house')` could be: \n$$\\begin{bmatrix}25\\\\200000\\\\1\\end{bmatrix}.$$\n::: \n\n::: {.fragment}\nThis is not a good idea because it implicitly assumes that the categories are ordered.\n\nOne hot encoding treats the categorical data as orthogonal from a vector space perspective.\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## ðŸŽ¯ Class Activity: Feature Encoding\n\n**Task**: Work in pairs to encode the following data into feature vectors.\n\n**Data to encode**:\n\n- Person A: (age=30, salary=$75,000, city=\"New York\", education=\"PhD\")\n- Person B: (age=25, salary=$50,000, city=\"Boston\", education=\"Masters\") \n- Person C: (age=35, salary=$90,000, city=\"New York\", education=\"Bachelors\")\n\n**Instructions**:\n\n1. Create one-hot encodings for categorical variables (city: NY, Boston, LA; education: Bachelors, Masters, PhD)\n2. Normalize the numerical features (age: divide by 100, salary: divide by 100,000)\n3. Write out the complete feature vectors for all three people\n4. Calculate the Euclidean distance between Person A and Person B\n\n**Discussion**: Why might we normalize numerical features? What does the distance tell us about similarity?\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Encodings\n:::\n\nWe will see many other encodings that take non-numeric data and encode them into vectors or matrices.\n\nFor example, there are vector or matrix encodings for\n\n::: {.incremental}\n* graphs,\n* images, and\n* text.\n:::\n\n## Matrix representation of data\n\nWe generally store data in a matrix form as\n\n$$ \n\\mbox{$m$ data objects}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\\overbrace{\\left[\\begin{array}{ccccc}\n\\begin{array}{c} x_{11} \\\\ \\vdots \\\\ x_{i1} \\\\ \\vdots \\\\ x_{m1} \\end{array}&\n\\begin{array}{c} \\cdots  \\\\ \\ddots \\\\ \\cdots  \\\\ \\ddots \\\\ \\cdots  \\end{array}&\n\\begin{array}{c} x_{1j} \\\\ \\vdots \\\\ x_{ij} \\\\ \\vdots \\\\ x_{mj} \\end{array}&\n\\begin{array}{c} \\cdots  \\\\ \\ddots \\\\ \\cdots  \\\\ \\ddots \\\\ \\cdots  \\end{array}&\n\\begin{array}{c} x_{1n} \\\\ \\vdots \\\\ x_{in} \\\\ \\vdots \\\\ x_{mn} \\end{array}\n\\end{array}\\right]}^{\\mbox{$n$ features}} \n$$\n\nThe number of rows is denoted by $m$ and the number of columns by $n$. The rows are instances or records of data and the columns are the features.\n\n## Metrics\n\nA metric is a function $d(x, y)$ that satisfies the following properties.\n\n:::: {.columns}\n::: {.column width=\"70%\"}\n::: {.incremental}\n* $d(x, x) = 0$\n* $d(x, y) > 0 \\hspace{1cm} \\forall x\\neq y$ (positivity)\n* $d(x, y) = d(y, x)$ (symmetry)\n* $d(x, y)\\leq d(x, z) + d(z, y)$ (triangle inequality)\n:::\n:::\n::: {.column width=\"30%\"}\n:::: {.fragment}\n![](figs/TriangleInequality.png){fig-align=\"center\"}\n::::\n:::\n::::\n\n::: {.fragment}\nWe can use a metric to determine how __similar__ or __dissimilar__ two objects are.\n\nA metric is a measure of the dissimilarity between two objects. The larger the\nmeasure, the more dissimilar the objects are.\n\nIf the objects are vectors, then the metric is also commonly called a __distance__.\n:::\n\n::: {.content-visible when-profile=\"web\"}\nSometimes we will use \"distance\" informally, i.e., to refer to a similarity or\ndissimilarity function even if we are not sure it is a metric.   \n\nWe'll try to say \"dissimilarity\" in those cases though.\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Distance Matrices\n:::\n\nThe distance matrix is defined as\n\n$$ \n\\mbox{$m$ data objects}\\left\\{\\begin{array}{c}\\;\\\\\\;\\\\\\;\\\\\\;\\\\\\;\\end{array}\\right.\\;\\;\n\\overbrace{\\left[\\begin{array}{ccccc}\n\\begin{array}{c} 0  \\\\  d(x_1, x_2) \\\\ d(x_1,x_3) \\\\ \\vdots \\\\ d(x_1,x_m)  \\end{array} &\n\\begin{array}{c} \\; \\\\  0      \\\\ d(x_2,x_3) \\\\ \\vdots \\\\ d(x_2,x_m)  \\end{array} &\n\\begin{array}{c} \\; \\\\ \\;      \\\\ 0      \\\\ \\vdots \\\\ \\cdots   \\end{array} &\n\\begin{array}{c} \\; \\\\ \\;      \\\\ \\;     \\\\ \\ddots \\\\ d(x_{m-1},x_m)   \\end{array}  &\n\\begin{array}{c} \\; \\\\ \\;      \\\\ \\;     \\\\ \\;     \\\\[6pt] 0 \\end{array} &\n\\end{array}\\right]}^{\\mbox{$m$ data objects}},\n$$\n\nwhere $x_i$ denotes the $i$-th column of the data matrix $X$.\n\n## Norms\n\nLet $\\mathbf{u}, \\mathbf{v}\\in\\mathbb{R}^{n}$ and $a\\in\\mathbb{R}$. The vector function $p(\\mathbf{v})$ is called a __norm__ if\n\n\n::: {.incremental}\n* $p(a\\mathbf{v}) = |a|p(\\mathbf{v})$,\n* $p(\\mathbf{u} + \\mathbf{v}) \\leq p(\\mathbf{u}) + p(\\mathbf{v})$,\n* $p(\\mathbf{v}) = 0$ if and only if $\\mathbf{v} = 0$.\n:::\n\n::: {.fragment}\n__Every norm defines a corresponding metric.__ \n\nIn particular If $p()$ is a norm, then $d(\\mathbf{x}, \\mathbf{y}) = p(\\mathbf{x}-\\mathbf{y})$ is a metric.\n:::\n\n::: {.fragment}\n**But not every metric can be derived from a norm!**\n\nFor example, the discrete metric is not a norm.\n$$\nd(x,y) =\n\\begin{cases}\n0 & x=y \\\\\n1 & x \\neq y\n\\end{cases}\n$$\n\n\n\n\n:::\n\n::: {.content-visible when-profile=\"web\"}\n### $\\ell_p$ norm\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## $\\ell_p$ norm\n:::\n\nA general class of norms are called __$\\ell_p$__ norms, where $p \\geq 1.$\n\n$$\n\\Vert \\mathbf{x} \\Vert_p = \\left(\\sum_{i=1}^d |x_i|^p\\right)^{\\frac{1}{p}}.\n$$ \n\nThe corresponding distance that an $\\ell_p$ norm defines is called the _Minkowski distance._\n\n$$\n\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_p = \\left(\\sum_{i=1}^d |x_i - y_i|^p\\right)^{\\frac{1}{p}}.\n$$\n\n::: {.fragment}\nThe choice of $p$ affects what gets emphasized in the distance calculation:\n\n::: {.incremental}\n* For small $p$ (e.g. $p=1$):\n  * More emphasis on many small differences\n  * Less sensitive to outliers/large differences\n* For large $p$ (e.g. $p\\rightarrow \\infty$):\n  * More emphasis on the largest difference\n  * Very sensitive to outliers\n:::\n:::\n\n\n::: {.content-visible when-profile=\"web\"}\n### $\\ell_2$ norm\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## $\\ell_2$ norm\n:::\n\nA special -- but very important -- case is the $\\ell_2$ norm.\n\n$$\n\\Vert \\mathbf{x} \\Vert_2 = \\sqrt{\\sum_{i=1}^d |x_i|^2}.\n$$\n\nWe've already mentioned it: it is the __Euclidean__ norm.\n\nThe distance defined by the $\\ell_2$ norm is the same as the Euclidean distance between two vectors $\\mathbf{x}, \\mathbf{y}$.\n\n$$ \n\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_2  = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}.\n$$\n\n::: {.content-visible when-profile=\"web\"}\n### $\\ell_1$ norm\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## $\\ell_1$ norm\n:::\n\nAnother important special case is the $\\ell_1$ norm.\n\n$$ \\Vert \\mathbf{x} \\Vert_1 = \\sum_{i=1}^d |x_i|.$$\n\nThis defines the __Manhattan__ distance, or (for binary vectors), the __Hamming__ distance:\n\n$$ \\Vert \\mathbf{x} - \\mathbf{y} \\Vert_1 = \\sum_{i=1} |x_i - y_i|.$$\n\n![](figs/L05-manhattan-distance.png){fig-align=\"center\" width=\"400px\"}\n\n::: {.content-visible when-profile=\"web\"}\n### $\\ell_\\infty$ norm\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## $\\ell_\\infty$ norm\n:::\n\nIf we take the limit of the $\\ell_p$ norm as $p$ gets large we get the $\\ell_\\infty$ norm.  \n\nWe have that\n\n$$\n\\Vert \\mathbf{x} \\Vert_{\\infty} = \\max_{i} \\vert x_{i} \\vert .\n$$\n\n::: {.fragment}\nWhat is the metric that this norm induces?\n:::\n\n::: {.fragment}\nAnswer: The $\\ell_\\infty$ norm induces the __Chebyshev__ distance which is also known as the __maximum__ distance.\n\n$$\n\\Vert \\mathbf{x} - \\mathbf{y} \\Vert_{\\infty} = \\max_{i} \\vert x_{i} - y_{i} \\vert .\n$$\n:::\n\n::: {.content-visible when-profile=\"web\"}\n## $\\ell_0$ norm\n\nAnother related idea is the $\\ell_0$ \"norm,\" which is not a norm, but is in a sense what we get from the $p$-norm for $p = 0$.\n\nNote that this is __not__ a norm, but it gets called that anyway.   \n\nThis \"norm\" simply counts the number of __nonzero__ elements in a vector.\n\nThis is called the vector's __sparsity.__\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Visualizing norms\n:::\n\nHere is the notion of a \"circle\" under each of three norms.\n\nThat is, for each norm, the set of vectors having norm 1, or distance 1 from the origin.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n![](figs/L5-Vector-Norms.png){fig-align=\"center\"}\n:::\n::: {.column width=\"50%\"}\n\n<br><br>\n$|x_1| + |x_2| = 1$\n\n<br><br>\n$\\sqrt{x_1^2 + x_2^2} = 1$\n\n<br><br>\n$\\max(|x_1|, |x_2|) = 1$\n:::\n::::\n\n\n[Source](https://commons.wikimedia.org/w/index.php?curid=678101)\n\n\n::: {.content-visible when-profile=\"slides\"}\n## What norm should I use?\n\nThe choice of norm depends on the characteristics of your data and the problem you're trying to solve.\n\n:::: {.columns}\n::: {.column width=\"33%\"}\n__$\\ell_1$ norm__\n\n- Use when your data is sparse.\n- Robust to outliers.\n:::\n::: {.column width=\"33%\"}\n__$\\ell_2$ norm__\n\n- Use when measuring distances in Euclidean space.\n- Smooth and differentiable.\n:::\n::: {.column width=\"33%\"}\n__$\\ell_\\infty$ norm__\n\n- Use when you need uniform bounds.\n- Limits maximum deviation.\n:::\n::::\n\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## ðŸŽ¯ Class Activity: Norm Comparison\n\n**Task**: Compare how different norms measure distance between the same two points.\n\n**Given**: Two 2D points A = (3, 4) and B = (1, 1)\n\n**Instructions**:\n\n1. Calculate the distance between A and B using:\n   - $\\ell_1$ norm (Manhattan distance)\n   - $\\ell_2$ norm (Euclidean distance) \n   - $\\ell_\\infty$ norm (Chebyshev distance)\n\n2. Now consider points C = (0, 0) and D = (1, 0)\n   - Calculate all three distances again\n   - Which norm gives the smallest distance? Largest?\n\n3. **Think-pair-share**: \n   - In what real-world scenarios would you prefer each norm?\n   - How does the choice of norm affect outlier sensitivity?\n\n**Bonus**: Sketch the \"unit circles\" for each norm in 2D space.\n:::\n\n## Similarity and Dissimilarity Functions\n\n:::: {.columns}\n::: {.column width=\"44%\"}\nSimilarity functions quantify how similar two objects are. The higher the similarity score, the more alike the objects.\n\n:::: {.fragment}\n__Examples__\n\n:::: {.incremental}\n- cosine similarity,\n- Jaccard similarity _(intersection over union)_.\n::::\n::::\n:::\n::: {.column width=\"55%\"}\nDissimilarity functions quantifies the difference between two objects. The higher the dissimilarity score, the more different the objects are.\n\n:::: {.fragment}\n__Examples__\n\n:::: {.incremental}\n- Manhattan distance,\n- Hamming distance.\n::::\n::::\n:::\n::::\n\n::: {.content-visible when-profile=\"web\"}\n### Similarity\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Similarity\n:::\n\nWe know that the inner product of two vectors can be used to compute the __cosine of the angle__ between them\n\n$$ \\cos(\\theta) = \\frac{\\mathbf{x}^T\\mathbf{y}}{\\Vert\\mathbf{x}\\Vert \\Vert\\mathbf{y}\\Vert} \\equiv \\cos(\\mathbf{x}, \\mathbf{y})  .$$\n\nThis value is \n\n* close to 1 when $\\mathbf{x} \\approx \\mathbf{y}$ ($\\theta \\approx 0$) and \n* close to 0 when $\\mathbf{x}$ and $\\mathbf{y}$ are orthogonal ($\\theta \\approx 90^\\circ$). \n\nWe can use this formula to define a __similarity__ function called the __cosine similarity__ $\\cos(\\mathbf{x}, \\mathbf{y})$.\n\n<br>\n\nAbuse of notation? ðŸ¤”\n\n::: {.content-visible when-profile=\"web\"}\n### Dissimilarity\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Dissimilarity\n:::\n\n:::: {.fragment}\nGiven a similarity function $s(\\mathbf{x}, \\mathbf{y})$, how could we convert it to a dissimilarity function $d(\\mathbf{x}, \\mathbf{y})$?\n::::\n\n:::: {.fragment}\nTwo straightforward ways of doing that are:\n\n$$d(\\mathbf{x},\\mathbf{y}) = 1\\,/\\,s(\\mathbf{x},\\mathbf{y})$$\n\nor \n\n$$d(\\mathbf{x},\\mathbf{y}) = k - s(\\mathbf{x},\\mathbf{y})$$\n\nfor some properly chosen $k$.\n::::\n\n:::: {.fragment}\nFor cosine similarity, one often uses:\n    \n$$ d(\\mathbf{x}, \\mathbf{y}) = 1 - \\cos(\\mathbf{x}, \\mathbf{y})$$\n::::\n\n::: {.content-visible when-profile=\"web\"}\nNote however that this is __not a metric!__\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## ðŸŽ¯ Class Activity: Similarity Calculations\n\n**Task**: Calculate different similarity measures between two documents represented as word frequency vectors.\n\n**Given**: Two documents with word counts:\n\n- Document A: [cat: 3, dog: 2, bird: 1, fish: 0]\n- Document B: [cat: 1, dog: 3, bird: 0, fish: 2]\n\n**Instructions**:\n\n1. Convert to normalized vectors (divide by total word count)\n2. Calculate cosine similarity between the documents\n3. Calculate the dissimilarity using: d = 1 - cosine_similarity\n4. **Quick discussion**: \n   - What does a cosine similarity of 0.5 mean?\n   - Why might cosine similarity be better than Euclidean distance for text documents?\n:::\n\n## Solution\n\n::: {#3d5befee .cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\n\ndef calculate_document_similarity():\n    \"\"\"\n    Calculate different similarity measures between two documents represented as word frequency vectors.\n    \"\"\"\n    print(\"Document Similarity Analysis\")\n    print(\"=\" * 40)\n    \n    # Given word frequency vectors\n    doc_a = np.array([3, 2, 1, 0])  # [cat, dog, bird, fish]\n    doc_b = np.array([1, 3, 0, 2])  # [cat, dog, bird, fish]\n    \n    print(\"Original word frequency vectors:\")\n    print(f\"Document A: {doc_a} (cat: 3, dog: 2, bird: 1, fish: 0)\")\n    print(f\"Document B: {doc_b} (cat: 1, dog: 3, bird: 0, fish: 2)\")\n    print()\n    \n    # Step 1: Convert to normalized vectors (divide by total word count)\n    doc_a_normalized = doc_a / np.sum(doc_a)\n    doc_b_normalized = doc_b / np.sum(doc_b)\n    \n    print(\"Step 1: Normalized vectors (divided by total word count)\")\n    print(f\"Document A normalized: {doc_a_normalized}\")\n    print(f\"Document B normalized: {doc_b_normalized}\")\n    print(f\"Sum of normalized A: {np.sum(doc_a_normalized):.6f}\")\n    print(f\"Sum of normalized B: {np.sum(doc_b_normalized):.6f}\")\n    print()\n    \n    # Step 2: Calculate cosine similarity\n    # Reshape for sklearn's cosine_similarity function\n    doc_a_reshaped = doc_a_normalized.reshape(1, -1)\n    doc_b_reshaped = doc_b_normalized.reshape(1, -1)\n    \n    cosine_sim = cosine_similarity(doc_a_reshaped, doc_b_reshaped)[0][0]\n    \n    print(\"Step 2: Cosine similarity calculation\")\n    print(f\"Cosine similarity: {cosine_sim:.6f}\")\n    print()\n    \n    # Manual calculation for verification\n    dot_product = np.dot(doc_a_normalized, doc_b_normalized)\n    norm_a = np.linalg.norm(doc_a_normalized)\n    norm_b = np.linalg.norm(doc_b_normalized)\n    cosine_sim_manual = dot_product / (norm_a * norm_b)\n    \n    print(\"Manual verification:\")\n    print(f\"Dot product: {dot_product:.6f}\")\n    print(f\"Norm of A: {norm_a:.6f}\")\n    print(f\"Norm of B: {norm_b:.6f}\")\n    print(f\"Cosine similarity (manual): {cosine_sim_manual:.6f}\")\n    print()\n    \n    # Step 3: Calculate dissimilarity\n    dissimilarity = 1 - cosine_sim\n    \n    print(\"Step 3: Dissimilarity calculation\")\n    print(f\"Dissimilarity (1 - cosine_similarity): {dissimilarity:.6f}\")\n    print()\n    \n    # Additional analysis\n    print(\"Additional Analysis:\")\n    print(\"-\" * 20)\n    \n    # Euclidean distance for comparison\n    euclidean_dist = np.linalg.norm(doc_a_normalized - doc_b_normalized)\n    print(f\"Euclidean distance: {euclidean_dist:.6f}\")\n    \n    # Manhattan distance\n    manhattan_dist = np.sum(np.abs(doc_a_normalized - doc_b_normalized))\n    print(f\"Manhattan distance: {manhattan_dist:.6f}\")\n    print()\n    \n    return {\n        'doc_a_normalized': doc_a_normalized,\n        'doc_b_normalized': doc_b_normalized,\n        'cosine_similarity': cosine_sim,\n        'dissimilarity': dissimilarity,\n        'euclidean_distance': euclidean_dist,\n        'manhattan_distance': manhattan_dist\n    }\n\nif __name__ == \"__main__\":\n    results = calculate_document_similarity()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDocument Similarity Analysis\n========================================\nOriginal word frequency vectors:\nDocument A: [3 2 1 0] (cat: 3, dog: 2, bird: 1, fish: 0)\nDocument B: [1 3 0 2] (cat: 1, dog: 3, bird: 0, fish: 2)\n\nStep 1: Normalized vectors (divided by total word count)\nDocument A normalized: [0.5        0.33333333 0.16666667 0.        ]\nDocument B normalized: [0.16666667 0.5        0.         0.33333333]\nSum of normalized A: 1.000000\nSum of normalized B: 1.000000\n\nStep 2: Cosine similarity calculation\nCosine similarity: 0.642857\n\nManual verification:\nDot product: 0.250000\nNorm of A: 0.623610\nNorm of B: 0.623610\nCosine similarity (manual): 0.642857\n\nStep 3: Dissimilarity calculation\nDissimilarity (1 - cosine_similarity): 0.357143\n\nAdditional Analysis:\n--------------------\nEuclidean distance: 0.527046\nManhattan distance: 1.000000\n\n```\n:::\n:::\n\n\n## Discussion points\n\n::: {.fragment}\n1. **What does a cosine similarity of 0.5 mean?**\n:::\n\n::: {.fragment}\nA cosine similarity of 0.5 means the documents are moderately similar.\nThe angle between their vectors is approximately 60 degrees.\nThis indicates some overlap in word usage patterns but not complete similarity.\n:::\n\n::: {.fragment}\n2. **Why might cosine similarity be better than Euclidean distance for text documents?**\n:::\n\n::: {.fragment}\n**Scale invariance:** Cosine similarity focuses on the direction of vectors,\nnot their magnitude, so document length doesn't affect the comparison.\n\n**Normalization:** It naturally handles documents of different lengths.\nWord frequency interpretation: It measures the relative proportion of words,\nwhich is more meaningful for text analysis than absolute differences.\n\n**Range:** Cosine similarity ranges from -1 to 1, with 1 being identical,\nmaking it more interpretable for similarity measures.\n:::\n\n\n\n## Bit vectors and Sets\n\nWhen working with bit vectors, the $\\ell_1$ metric is commonly used and is called the __Hamming__ distance.\n\n![](figs/L5-hamming-1.png){fig-align=\"center\" width=\"500px\"}\n\nThis has a natural interpretation: \"how well do the two vectors match?\"\n\nOr: \"What is the smallest number of bit flips that will convert one vector into the other?\"\n\n::: {.content-hidden when-profile=\"web\"}\n## Hamming distance\n:::\n\n![](figs/L5-hamming-2.png){fig-align=\"center\" width=\"700px\"}\n\n::: {.content-visible when-profile=\"web\"}\nIn other cases, the Hamming distance is not a very appropriate metric.\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Hamming distance with sets\n:::\n\nConsider the case in which the bit vector is being used to represent a _set_.\n\n**Definition**: A _set_ is an unordered collection of distinct elements.\n\nIn that case, Hamming distance measures the __size of the set difference.__\n\nFor example, consider two documents. We will use bit vectors to represent the sets of words in each document.\n\n:::: {.incremental}\n* Case 1: both documents are large, almost identical, but differ in 10 words.\n* Case 2: both documents are small, disjoint, have 5 words each.\n::::\n\n:::: {.fragment}\nWhat is the Hamming distance for each case?\n:::\n\n:::: {.fragment}\n$L_1(\\text{Case 1}) = 10$ and $L_1(\\text{Case 2}) = 5$. Is case 1 really more different than case 2? \n:::\n\n:::: {.fragment}\nWhat matters is not just the size of the set difference, but the size of the _normalized_ intersection.\n::::\n\n\n::: {.content-hidden when-profile=\"web\"}\n## Jaccard similarity\n:::\nThis leads to the _Jaccard_ similarity or _Intersection over Union (IoU)_:\n\n$$\nJ_{Sim}(\\mathbf{x}, \\mathbf{y}) = \\frac{|\\mathbf{x} \\cap \\mathbf{y}|}{|\\mathbf{x} \\cup \\mathbf{y}|}.\n$$\n\n:::: {.columns}\n::: {.column width=\"60%\"}\nThis takes on values from 0 to 1, so a natural dissimilarity metric is $1 - J_{Sim}().$\n\nIn fact, this is a __metric!__\n\n$$\nJ_{Dist}(\\mathbf{x}, \\mathbf{y}) = 1- \\frac{|\\mathbf{x} \\cap \\mathbf{y}|}{|\\mathbf{x} \\cup \\mathbf{y}|}.\n$$\n:::\n::: {.column width=\"40%\"}\n![](figs/L5-jaccard-1.png){fig-align=\"center\"}\n:::\n::::\n\n\n::: {.content-hidden when-profile=\"web\"}\n## Jaccard Similarity Example 1\n:::\n    \n::: {.content-visible when-profile=\"web\"}\nLet's revisit the previously introduces cases comparing documents.\n:::\n\nCase 1: Very large almost identical documents.\n\n![](figs/L5-jaccard-2.png){fig-align=\"center\" width=\"500px\"}\n\nHere $J_{Sim}(\\mathbf{x}, \\mathbf{y})$ is almost 1.\n\n::: {.content-hidden when-profile=\"web\"}\n## Jaccard Similarity Example 2\n:::\n\nCase 2: Very small disjoint documents.\n\n![](figs/L5-jaccard-3.png){fig-align=\"center\"}\n\nHere $J_{Sim}(\\mathbf{x}, \\mathbf{y})$ is 0.\n\n::: {.content-visible when-profile=\"slides\"}\n## ðŸŽ¯ Class Activity: Jaccard Similarity Practice\n\n**Task**: Calculate Jaccard similarity for different document scenarios.\n\n**Given**: Three documents represented as sets of unique words:\n\n- Document 1: {apple, banana, cherry, date}\n- Document 2: {apple, banana, cherry, elderberry}  \n- Document 3: {grape, kiwi, lemon, mango}\n\n**Instructions**:\n\n1. Calculate Jaccard similarity between:\n   - Document 1 and Document 2\n   - Document 1 and Document 3\n   - Document 2 and Document 3\n\n2. **Think-pair-share**:\n   - Which pair is most similar? Least similar?\n   - How does this compare to what you'd expect from Hamming distance?\n   - When would you prefer Jaccard similarity over Hamming distance?\n\n**Real-world application**: Consider how this applies to comparing user preferences, shopping baskets, or social media interests.\n:::\n\n## Time Series\n\nA time series is a sequence of real numbers, representing the measurements of a real variable at (possibly equal) time intervals.\n\nSome examples are\n\n::: {.incremental}\n* stock prices,\n* the volume of sales over time, and\n* daily temperature readings.\n:::\n\n::: {.fragment}\nA time series database is a large collection of time series.\n:::\n\n## Similarity of Time Series\n\nSuppose we wish to compare the following time series.\n\n::: {.incremental}\n* Stock price movements for companies over a time interval.\n* The motion data of two people walking.\n* Credit usage patterns for bank clients.\n:::\n\n:::: {.fragment}\nHow should we measure the \"similarity\" of these time series?\n::::\n\n::: {.fragment}\nThere are two problems to address.\n\n::: {.incremental}\n1. Defining a meaningful similarity (or distance) function.\n2. Finding an efficient algorithm to compute it.\n:::\n:::\n\n## Norm-based Similarity Measures\n\nWe could just view each sequence as a vector.\n\nThen we could use a $p$-norm, e.g., $\\ell_1, \\ell_2,$ or $\\ell_p$ to measure similarity.\n\n::: {.fragment}\n\n__Advantages__\n\n::: {.incremental}    \n1. Easy to compute - linear in the length of the time series (O(n)).\n2. It is a metric.\n:::\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Beware of Norm-based Similarity\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n:::: {.columns}\n::: {.column width=\"40%\"}\n__Disadvantage__\n\n1. May not be __meaningful!__\n\nWe may believe that $\\mathbf{ts1}$ and $\\mathbf{ts2}$ are the most \"similar\" pair of time series.\n:::\n::: {.column width=\"60%\"}\n![](figs/L5-ts-euclidean.png){fig-align=\"center\"}\n:::\n::::\n:::\n\n::: {.content-visible when-profile=\"web\"}\n__Disadvantage__\n1. May not be __meaningful!__\n\n![](figs/L5-ts-euclidean.png){fig-align=\"center\"}\n\nWe may believe that $\\mathbf{ts1}$ and $\\mathbf{ts2}$ are the most \"similar\" pair of time series.\n:::\n\n::: {.fragment}\nHowever, according to Euclidean distance: \n\n$$ \\Vert \\mathbf{ts1} - \\mathbf{ts2} \\Vert_2 = 26.9,$$\n\nwhile \n\n$$ \\Vert \\mathbf{ts1} - \\mathbf{ts3} \\Vert_2 = 23.2.$$\n:::\n\n::: {.content-visible when-profile=\"web\"}\n## Feature Engineering\n\nIn general, there may be different aspects of a time series that are important in different settings.\n\n::: {.fragment}\nThe first step therefore is to ask yourself \"what is important about time series in my application?\"\n:::\n\n::: {.fragment}\nThis is an example of __feature engineering.__\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Feature Engineering\n:::\n\nFeature engineering is the art of computing some derived measure from your data object that makes the important properties usable in a subsequent step.\n\n::: {.fragment}\nA reasonable approach is to\n\n::: {.incremental}    \n* extract the relevant features,\n* use a simple method (e.g., a norm) to define similarity over those features.\n:::\n:::\n\n::: {.fragment}\nIn the case above, one might think of using \n\n::: {.incremental}\n* Fourier coefficients (to capture periodicity),\n* histograms,\n* or something else!\n:::\n:::\n\n::: {.content-visible when-profile=\"web\"}\n## Dynamic Time Warping\n:::\n\n::: {.content-visible when-profile=\"slides\"}\n## Bump Hunting\n:::\n\nOne case that arises often is something like the following:  \"bump hunting\"\n\n![](figs/L5-DTW-1.png){fig-align=\"center\"} \n\nBoth time series have the same key characteristics: four bumps.\n\nBut a one-to-one match (ala Euclidean distance) will not detect the similarity.\n\n::: {.content-visible when-profile=\"web\"}\n(Be sure to think about why Euclidean distance will fail here.)\n:::\n\nA solution to this is called __dynamic time warping.__\n\n\n\n::: {.content-hidden when-profile=\"web\"}\n## Dynamic Time Warping\n:::\n\nThe basic idea is to allow stretching or compressing of signals along the time dimension.\n\n::: {.fragment}\n__Classic applications__\n\n::: {.incremental}\n* speech recognition\n* handwriting recognition\n:::\n:::\n\n::: {.fragment}\nSpecifically\n\n::: {.incremental}\n* Consider $X = x_1, x_2, \\dots, x_n$ and $Y = y_1, y_2, \\dots, y_m$.\n* We are allowed to modify each sequence by duplicating, deleting, or matching elements to form $X'$ and $Y'$.\n* We then search over the space of all possible alignments to find the one that minimizes the distance between $X'$ and $Y'$.\n:::\n:::\n\n::: {.content-hidden when-profile=\"web\"}\n## Visualizing DTW\n:::\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\nThere is a simple way to visualize this algorithm.\n\nConsider a matrix $M$ where $M_{ij} = |x_i - y_j|$.\n\n$M$ measures the amount of error we get if we match $x_i$ with $y_j$. \n\nSo we seek a __path, starting from lower left, through $M$ that minimizes the total error.__\n:::\n\n::: {.column width=\"50%\"}\n![](figs/L5-DTW-2.png){fig-align=\"center\" width=\"550px\"}\n:::\n::::\n\n## DTW restrictions\n\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\nThe basic restrictions on path are:\n    \nMonotonicity\n\n* The path should not go down or to the left.\n\nContinuity\n\n* No elements may be skipped in a sequence.\n\nRestrict amount of deviation from diagonal (Sakoe-Chiba constraint)\n\nThis can be solved via dynamic programming.\n\n:::\n\n::: {.column width=\"50%\"}\n![](figs/L5-DTW-2.png){fig-align=\"center\" width=\"550px\"}\n:::\n::::\n\n## Dynamic Time Warping Example\n\n::: {#e4be4cf3 .cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# sequences\nx = [1,1,2,2,3]\ny = [1,2,3,3,3]\nn, m = len(x), len(y)\n\n# build DTW table\ndtw = np.full((n+1,m+1), np.inf)\ndtw[0,0] = 0\nfor i in range(1,n+1):\n    for j in range(1,m+1):\n        cost = abs(x[i-1]-y[j-1])\n        dtw[i,j] = cost + min(dtw[i-1,j], dtw[i,j-1], dtw[i-1,j-1])\n\n# backtrack with tie-breaking: prefer diagonal\ni,j = n,m\npath = [(i-1,j-1)]\nwhile (i>0 and j>0):\n    candidates = [(i-1,j-1,\"â†–\"), (i-1,j,\"â†‘\"), (i,j-1,\"â†\")]\n    i,j,_ = min(candidates, key=lambda s: (dtw[s[0],s[1]], [\"â†–\",\"â†‘\",\"â†\"].index(s[2])))\n    if i>0 or j>0:\n        path.append((i-1,j-1))\npath.reverse()\n\n# Print the fully aligned sequences\nprint(\"Original sequences:\")\nprint(f\"X = {x}\")\nprint(f\"Y = {y}\")\nprint(f\"\\nDTW distance = {dtw[n,m]:.1f}\")\nprint(f\"\\nOptimal alignment path (0-indexed): {path}\")\nprint(\"\\nFully aligned sequences:\")\nprint(\"X_aligned: \", end=\"\")\nfor i, j in path:\n    print(f\"{x[i]:2d}\", end=\" \")\nprint()\nprint(\"Y_aligned: \", end=\"\")\nfor i, j in path:\n    print(f\"{y[j]:2d}\", end=\" \")\nprint()\nprint(\"\\nAlignment mapping:\")\nfor k, (i, j) in enumerate(path):\n    print(f\"Step {k}: X[{i}] = {x[i]} â†” Y[{j}] = {y[j]}\")\n\n# plot cost matrix + path\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.imshow(dtw[1:,1:], origin=\"lower\", cmap=\"Blues\", aspect=\"auto\")\npx, py = zip(*[(pi+1,pj+1) for pi,pj in path])\nplt.plot(np.array(py)-1, np.array(px)-1, 'r', linewidth=2)\nplt.colorbar(label=\"Cumulative Cost\")\nplt.title(\"DTW Cost Matrix with Optimal Path\")\nplt.xlabel(\"Y values\")\nplt.ylabel(\"X values\")\n\n# plot aligned sequences\nplt.subplot(1, 2, 2)\nx_aligned = [x[i] for i, j in path]\ny_aligned = [y[j] for i, j in path]\nplt.plot(range(len(x_aligned)), x_aligned, 'bo-', label='X aligned', linewidth=2)\nplt.plot(range(len(y_aligned)), y_aligned, 'ro-', label='Y aligned', linewidth=2)\nplt.xlabel('Alignment step')\nplt.ylabel('Value')\nplt.title('Aligned Sequences')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOriginal sequences:\nX = [1, 1, 2, 2, 3]\nY = [1, 2, 3, 3, 3]\n\nDTW distance = 0.0\n\nOptimal alignment path (0-indexed): [(0, 0), (1, 0), (2, 1), (3, 1), (4, 2), (4, 3), (4, 4)]\n\nFully aligned sequences:\nX_aligned:  1  1  2  2  3  3  3 \nY_aligned:  1  1  2  2  3  3  3 \n\nAlignment mapping:\nStep 0: X[0] = 1 â†” Y[0] = 1\nStep 1: X[1] = 1 â†” Y[0] = 1\nStep 2: X[2] = 2 â†” Y[1] = 2\nStep 3: X[3] = 2 â†” Y[1] = 2\nStep 4: X[4] = 3 â†” Y[2] = 3\nStep 5: X[4] = 3 â†” Y[3] = 3\nStep 6: X[4] = 3 â†” Y[4] = 3\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](05-Distances-Timeseries_files/figure-revealjs/cell-3-output-2.png){width=950 height=374}\n:::\n:::\n\n\n## To explore further\n\nSee \n\nT. Giorgino, \"Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package\", Journal of Statistical Software, 2003. [link](https://www.jstatsoft.org/article/view/v031i07)\n\nAnd python packages such as:\n\n* [dtw-python](https://dynamictimewarping.github.io/)\n* [dtaidistance](https://dtaidistance.readthedocs.io/en/latest/index.html)\n\n::: {.content-visible when-profile=\"slides\"}\n## ðŸŽ¯ Class Activity: DTW Concept Exploration\n\n**Task**: Explore why DTW is better than Euclidean distance for time series with temporal shifts.\n\n**Given**: Two simple time series:\n\n- Series A: [1, 2, 3, 2, 1] (peak at position 3)\n- Series B: [0, 1, 2, 3, 2, 1, 0] (peak at position 4)\n\n**Instructions**:\n\n1. **Calculate Euclidean distance** between the series (pad with zeros if needed)\n2. **Sketch the DTW alignment** by hand:\n   - Expand series A to better align with series B\n3. **Calculate the Euclidean distance of the aligned series**\n3. **Group discussion**:\n   - Why does Euclidean distance of the original series fail here?\n   - What real-world scenarios would benefit from DTW?\n\n::: {.fragment}\n**Scenarios**:\n\n- Speech recognition (different speaking speeds)\n- Gesture recognition (different execution speeds)\n- Stock price patterns (different market timing)\n:::\n:::\n\n## From Time series to Strings\n\nA closely related idea concerns strings.\n\nThe key point is that, like time series, strings are __sequences__.\n\nGiven two strings, one way to define a 'distance' between them is:\n\n* the minimum number of __edit operations__ that are needed to transform one string into the other.\n\nEdit operations are insertion, deletion, and substitution of single characters.\n\nThis is called __edit distance__ or __Levenshtein distance.__\n\n## Example\n\nFor example, given strings: ``s = VIVALASVEGAS`` and ``t = VIVADAVIS``\n\nwe would like to \n\n* compute the edit distance, and\n* obtain the optimal __alignment__.\n\n![](figs/viva-las-vegas.png){fig-align=\"center\" width=\"550px\"}\n\n\nA dynamic programming algorithm can also be used to find this distance, and it is __very similar to dynamic time-warping.__\n\nIn bioinformatics this algorithm is called __\"Smith-Waterman\" sequence alignment.__\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Recap\n\nWe covered the following topics\n\n- reviewed representations of data,\n- discussed metrics and norms,\n- discussed similarity and dissimilarity functions,\n- introduced time series, \n- feature engineering, and\n- dynamic time warping.\n:::\n\n",
    "supporting": [
      "05-Distances-Timeseries_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}