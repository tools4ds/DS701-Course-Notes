{
  "hash": "4faeb0936c3475d45d319f70f613ea9c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Neural Networks -- From Theory to Practice'\njupyter: python3\n---\n\n## Introduction\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/30-NN-consolidated.ipynb)\n\n::: {#39cfb8ea .cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom IPython.display import Image, HTML\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n```\n:::\n\n\nIn this lecture, we'll build an understanding of neural networks, starting from the foundations and moving to practical implementation using scikit-learn.\n\n\nWe'll cover:\n\n* How neural networks extend linear and logistic regression\n* The Multi-Layer Perceptron (MLP) architecture\n* Gradient descent and optimization\n* Practical implementation with scikit-learn's `MLPClassifier` and `MLPRegressor`\n\n\n## Effectiveness of Neural Networks\n\n\n![](figs/NN-figs/IntroModels.svg){width=\"50%\" fig-align=\"center\"}\n\n<!--\nFrom [Understanding Deep Learning, Simon J.D. Prince, MIT Press, 2023](http://udlbook.com)\n-->\n\n## Applications Across Domains\n\n![](figs/NN-figs/IntroModels2a.svg){width=\"50%\" fig-align=\"center\"}\n\n# From Regression to Neural Networks\n\n## Linear Regression Revisited\n\nRecall linear regression predicts a continuous output:\n\n$$\n\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p = \\mathbf{x}^T\\boldsymbol{\\beta}\n$$\n\nOr in matrix form for multiple samples:\n\n$$\n\\hat{\\mathbf{y}} = \\mathbf{X}\\boldsymbol{\\beta}\n$$\n\n<br>\n\n<details>\n<summary><b>Question:</b> What's the main limitation of linear regression?</summary>\n<b>Answer:</b> It can only model linear relationships between inputs and outputs!\n</details>\n\n## Logistic Regression \n\n* Adds Non-linearity\n\n* For binary classification, logistic regression applies a **sigmoid function**:\n\n$$\nP(y=1|\\mathbf{x}) = \\sigma(\\mathbf{x}^T\\boldsymbol{\\beta}) = \\frac{1}{1 + e^{-\\mathbf{x}^T\\boldsymbol{\\beta}}}\n$$\n\n## Logistic Regression, cont.\n\nThe sigmoid function introduces non-linearity:\n\n::: {#15284a71 .cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](30-NN-consolidated_files/figure-html/cell-3-output-1.png){width=514 height=376 fig-align='center'}\n:::\n:::\n\n\n## The Key Insight\n\n> A single neuron with a sigmoid activation is essentially logistic regression!\n\nNeural networks extend this by:\n\n::: {.incremental}\n1. **Multiple neurons** in parallel (learning different features)\n    - Universal Approximation Theorem guarantees that a network with a single hidden layer can approximate any continuous function to any desired accuracy.\n2. **Multiple layers** in sequence (learning hierarchical representations)\n    - Representational capacity is more efficiient\n3. **Various activation functions** (ReLU, tanh, etc.)\n    - Required to not collapse to a single linear transformation\n:::\n\n::: {.fragment}\nThis allows neural networks to learn complex, non-linear decision boundaries.\n:::\n\n# Artificial Neurons\n\n## The Artificial Neuron\n\nAn artificial neuron is loosely modeled on biological neurons:\n\n![](figs/NN-figs/neuron_model.jpeg){width=\"75%\" fig-align=\"center\"}\n\nFrom [cs231n](https://cs231n.github.io/neural-networks-1/)\n\n## Neuron Components\n\nA neuron performs the following operation:\n\n$$\n\\text{output} = f\\left(\\sum_{i=1}^n w_i x_i + b\\right)\n$$\n\nWhere:\n\n* $x_i$ are the **inputs**\n* $w_i$ are the **weights** (parameters to learn)\n* $b$ is the **bias** (another parameter)\n* $f$ is the **activation function** (introduces non-linearity)\n\n## Activation Functions\n\n**ReLU (Rectified Linear Unit)** - most popular today:\n$$\n\\text{ReLU}(x) = \\max(0, x)\n$$\n\n::: {#57411498 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![](30-NN-consolidated_files/figure-html/cell-4-output-1.png){width=427 height=302 fig-align='center'}\n:::\n:::\n\n\n<details>\n<summary><b>Why activation functions?</b></summary>\n<b>Answer:</b> Without non-linearity, multiple layers collapse to a single linear transformation!\n\n</details>\n\n# Multi-Layer Perceptron (MLP)\n\n## MLP Architecture\n\nA Multi-Layer Perceptron stacks multiple layers of neurons:\n\n![](figs/NN-figs/neural_net2.jpeg){width=\"55%\" fig-align=\"center\"}\n\nFrom [cs231n](https://cs231n.github.io/convolutional-networks/)\n\n* **Input layer**: Raw features\n* **Hidden layers**: Learn intermediate representations\n* **Output layer**: Final prediction\n\n## Matrix Formulation\n\n![FCN from UDL.](figs/NN-figs/L24-fcn-dag.png){width=\"75%\" fig-align=\"center\"}\n\n**Key property:** Every neuron in layer $i$ connects to every neuron in layer $i+1$.\n\nThis is also called a **Fully Connected Network (FCN)** or **Dense Network**.\n\n## MLP Mathematical Formulation\n\nFor a network with $K$ hidden layers:\n\n$$\n\\begin{aligned}\n\\mathbf{h}_1 &= f(\\boldsymbol{\\beta}_0 + \\boldsymbol{\\Omega}_0 \\mathbf{x}) \\\\\n\\mathbf{h}_2 &= f(\\boldsymbol{\\beta}_1 + \\boldsymbol{\\Omega}_1 \\mathbf{h}_1) \\\\\n&\\vdots \\\\\n\\mathbf{h}_K &= f(\\boldsymbol{\\beta}_{K-1} + \\boldsymbol{\\Omega}_{K-1} \\mathbf{h}_{K-1}) \\\\\n\\mathbf{\\hat{y}} &= \\boldsymbol{\\beta}_K + \\boldsymbol{\\Omega}_K \\mathbf{h}_K\n\\end{aligned}\n$$\n\nWhere:\n\n* $\\mathbf{h}_k$ = hidden layer activations\n* $\\boldsymbol{\\Omega}_k$ = weight matrices\n* $\\boldsymbol{\\beta}_k$ = bias vectors\n* $f$ = activation function (e.g., ReLU)\n\n# Training Neural Networks\n\n## The Loss Function\n\nTraining means finding weights that minimize a **loss function**:\n\n**For regression** (e.g., predicting house prices):\n$$\nL = \\frac{1}{N}\\sum_{i=1}^N (\\hat{y}_i - y_i)^2 \\quad \\text{(Mean Squared Error)}\n$$\n\n**For classification** (e.g., digit recognition):\n$$\nL = -\\frac{1}{N}\\sum_{i=1}^N \\sum_{c=1}^C y_{ic} \\log(\\hat{y}_{ic}) \\quad \\text{(Cross-Entropy)}\n$$\n\n**Goal:** Find parameters $\\theta = \\{\\boldsymbol{\\Omega}_k, \\boldsymbol{\\beta}_k\\}$ that minimize $L$.\n\n## Visualizing the Loss Surface\n\nThe loss function creates a surface over the parameter space:\n\n![](figs/L23-convex_cost_function.jpeg){width=\"55%\" fig-align=\"center\"}\n\n* Left: **Convex** loss surface (e.g., linear regression)\n* Right: **Non-convex** loss surface (e.g., neural networks)\n\nFor neural networks, we can't solve analytically—we need **gradient descent**!\n\n# Gradient Descent\n\n## The Gradient Descent Intuition\n\nImagine you're lost in foggy mountains and want to reach the valley:\n\n:::: {.columns}\n::: {.column width=\"30%\"}\n![](figs/L23-fog-in-the-mountains.jpeg){width=\"100%\"}\n:::\n::: {.column width=\"70%\"}\nWhat would you do?\n\n1. Look around 360 degrees\n2. Find the direction sloping **downward most steeply**\n3. Take a few steps in that direction\n4. Repeat until the ground is level\n\nThis is **gradient descent**!\n:::\n::::\n\n## The Gradient\n\nFor a function $L(\\mathbf{w})$ where $\\mathbf{w} = (w_1, \\ldots, w_n)$, the **gradient** is:\n\n$$\n\\nabla_\\mathbf{w} L(\\mathbf{w}) = \n\\begin{bmatrix}\n\\frac{\\partial L}{\\partial w_1}\\\\\n\\frac{\\partial L}{\\partial w_2}\\\\\n\\vdots \\\\\n\\frac{\\partial L}{\\partial w_n}\n\\end{bmatrix}\n$$\n\n* The gradient points in the direction of **steepest increase**\n* The negative gradient points toward **steepest decrease**\n\n## Gradient Descent Algorithm\n\nStart with random weights $\\mathbf{w}^{(0)}$, then iterate:\n\n$$\n\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\nabla_\\mathbf{w} L(\\mathbf{w}^{(t)})\n$$\n\nWhere:\n\n* $\\eta$ is the **learning rate** (step size)\n* $\\nabla_\\mathbf{w} L$ is the **gradient** of the loss\n\n**Stop when:**\n\n* Loss stops decreasing (convergence)\n* Maximum iterations reached\n\n## Learning Rate Matters\n\nThe learning rate $\\eta$ is crucial:\n\n**Too small:** Slow convergence\n\n**Too large:** May fail to converge or even diverge!\n\n::: {#f1230330 .cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](30-NN-consolidated_files/figure-html/cell-5-output-1.png){width=1142 height=374 fig-align='center'}\n:::\n:::\n\n\n# Stochastic Gradient Descent\n\n## Full Batch vs Stochastic GD\n\n**Full Batch Gradient Descent:** Compute gradient using ALL training samples:\n\n$$\n\\nabla_\\mathbf{w} L = \\frac{1}{N}\\sum_{i=1}^N \\nabla_\\mathbf{w} \\ell_i(\\mathbf{w})\n$$\n\n**Problems:**\n\n* Slow for large datasets (millions of samples!)\n* Memory intensive\n* Can get stuck in local minima\n\n## Stochastic Gradient Descent (SGD)\n\n**Stochastic Gradient Descent:** Historically meant using ONE random sample at a time:\n\n$$\n\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\nabla_\\mathbf{w} \\ell_i(\\mathbf{w}^{(t)})\n$$\n\n**Advantages:**\n\n* Much faster per iteration\n* Can escape local minima (due to noise)\n* Enables online learning\n\n**Disadvantage:**\n\n* _Extremely_ noisy gradient estimates\n* May not converge exactly to minimum\n\n## Mini-Batch Gradient Descent\n\n**Mini-Batch GD:** Best of both worlds—use a small batch of samples:\n\n$$\n\\nabla_\\mathbf{w} L \\approx \\frac{1}{B}\\sum_{i \\in \\text{batch}} \\nabla_\\mathbf{w} \\ell_i(\\mathbf{w})\n$$\n\nTypical batch sizes: 32, 64, 128, 256\n\n**Advantages:**\n\n* Balances speed and stability\n* Efficient GPU parallelization\n* Better gradient estimates than pure SGD\n\n**This is what most modern neural network training uses!**\n\n## Visualizing Batch Strategies\n\n::: {#2995d25e .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](30-NN-consolidated_files/figure-html/cell-6-output-1.png){width=951 height=470 fig-align='center'}\n:::\n:::\n\n\n(For illustration purposes only -- not a real training curve.)\n\n# Neural Networks in Scikit-Learn\n\n## MLPClassifier and MLPRegressor\n\nScikit-learn provides simple, high-level interfaces:\n\n* **`MLPClassifier`**: Multi-layer Perceptron classifier\n* **`MLPRegressor`**: Multi-layer Perceptron regressor\n\n**Key features:**\n\n* Multiple hidden layers with various activation functions\n* Multiple solvers: `'adam'`, `'sgd'`, `'lbfgs'`\n* Built-in regularization (L2 penalty)\n* Early stopping support\n* Easy integration with scikit-learn pipelines\n\n## Architecture Specification\n\nSpecify architecture as a tuple:\n\n```python\n# Single hidden layer with 100 neurons\nhidden_layer_sizes=(100,)\n\n# Two hidden layers: 100 and 50 neurons\nhidden_layer_sizes=(100, 50)\n\n# Three hidden layers\nhidden_layer_sizes=(128, 64, 32)\n```\n\nInput and output layers are automatically determined from your data!\n\n## Scikit-Learn vs PyTorch/TensorFlow\n\n<br>\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n### Use Scikit-Learn for:\n\n* Small to medium datasets (< 100K samples)\n* Standard feedforward architectures\n* Rapid prototyping needed\n* Integration with scikit-learn pipelines\n* CPU training is sufficient\n:::\n\n::: {.column width=\"50%\"}\n\n### Use PT/TF for:\n\n* Large datasets (> 100K samples)\n* Complex architectures (CNNs, RNNs)\n* GPU acceleration required\n* Production deployment\n* Research and experimentation\n:::\n::::\n\n# Classification Example: MNIST\n\n## Load the MNIST Dataset\n\nLet's classify handwritten digits (0-9):\n\n::: {#e8431473 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n# Load MNIST data\nprint(\"Loading MNIST dataset...\")\nX, y = fetch_openml('mnist_784', version=1, return_X_y=True, \n                    as_frame=False, parser='auto')\n\n# Convert labels to integers\ny = y.astype(int)\n\n# Use subset for faster demo\nX, _, y, _ = train_test_split(X, y, train_size=10000, \n                               stratify=y, random_state=42)\n\nprint(f\"Dataset shape: {X.shape}\")\nprint(f\"Number of classes: {len(np.unique(y))}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLoading MNIST dataset...\nDataset shape: (10000, 784)\nNumber of classes: 10\n```\n:::\n:::\n\n\n## Visualize the Data\n\n::: {#0a850ee8 .cell execution_count=7}\n``` {.python .cell-code code-fold=\"true\"}\nfig, axes = plt.subplots(2, 5, figsize=(12, 4))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(X[i].reshape(28, 28), cmap='gray')\n    ax.set_title(f'Label: {y[i]}')\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](30-NN-consolidated_files/figure-html/cell-8-output-1.png){width=1064 height=374 fig-align='center'}\n:::\n:::\n\n\nEach image is 28×28 pixels = 784 features\n\n## Prepare the Data\n\n::: {#0d25e5c8 .cell execution_count=8}\n``` {.python .cell-code code-fold=\"false\"}\n# Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Scale features to [0, 1]\nX_train_scaled = X_train / 255.0\nX_test_scaled = X_test / 255.0\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Test set: {X_test.shape[0]} samples\")\nprint(f\"Feature range: [{X_train_scaled.min():.2f}, {X_train_scaled.max():.2f}]\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining set: 8000 samples\nTest set: 2000 samples\nFeature range: [0.00, 1.00]\n```\n:::\n:::\n\n\n::: {.callout-important}\n**Always scale/normalize your features for neural networks!** This helps gradient descent converge faster.\n:::\n\n## Create the MLP\n\n::: {#0bf52bd1 .cell execution_count=9}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.neural_network import MLPClassifier\n\n# Create MLP with 2 hidden layers\nmlp = MLPClassifier(\n    hidden_layer_sizes=(128, 64),  # Architecture\n    activation='relu',              # Activation function\n    solver='adam',                  # Optimizer (uses mini-batches)\n    alpha=0.0001,                   # L2 regularization\n    batch_size=64,                  # Mini-batch size\n    learning_rate_init=0.001,       # Initial learning rate\n    max_iter=20,                    # Number of epochs\n    random_state=42,\n    verbose=True                    # Show progress\n)\n```\n:::\n\n\n## Train the model\n\n::: {#70f8b9f4 .cell execution_count=10}\n``` {.python .cell-code code-fold=\"false\"}\nprint(\"Training MLP...\")\nmlp.fit(X_train_scaled, y_train)\nprint(f\"Training completed in {mlp.n_iter_} iterations\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining MLP...\nIteration 1, loss = 0.71345206\nIteration 2, loss = 0.26439384\nIteration 3, loss = 0.18880844\nIteration 4, loss = 0.14070827\nIteration 5, loss = 0.10813206\nIteration 6, loss = 0.08241134\nIteration 7, loss = 0.06312883\nIteration 8, loss = 0.04742022\nIteration 9, loss = 0.03751731\nIteration 10, loss = 0.02680532\nIteration 11, loss = 0.02321482\nIteration 12, loss = 0.01516949\nIteration 13, loss = 0.01235069\nIteration 14, loss = 0.00823132\nIteration 15, loss = 0.00665425\nIteration 16, loss = 0.00515634\nIteration 17, loss = 0.00431783\nIteration 18, loss = 0.00336489\nIteration 19, loss = 0.00283319\nIteration 20, loss = 0.00253876\nTraining completed in 20 iterations\n```\n:::\n:::\n\n\n## Training Loss Curve\n\n::: {#086b0e6f .cell execution_count=11}\n``` {.python .cell-code code-fold=\"true\"}\nplt.figure(figsize=(10, 4))\nplt.plot(mlp.loss_curve_)\nplt.xlabel('Iteration (Epoch)')\nplt.ylabel('Loss')\nplt.title('Training Loss Curve')\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](30-NN-consolidated_files/figure-html/cell-12-output-1.png){width=812 height=376 fig-align='center'}\n:::\n:::\n\n\nThe loss decreases smoothly—our model is learning.\n\n## Evaluate Performance\n\n::: {#ad1cfb07 .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Make predictions\ny_pred = mlp.predict(X_test_scaled)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Test Accuracy: {accuracy:.4f}\")\n\n# Detailed report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTest Accuracy: 0.9555\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.99      0.97       191\n           1       0.98      1.00      0.99       226\n           2       0.94      0.96      0.95       215\n           3       0.95      0.93      0.94       202\n           4       0.98      0.94      0.96       206\n           5       0.96      0.94      0.95       168\n           6       0.98      0.97      0.98       196\n           7       0.93      0.96      0.95       185\n           8       0.96      0.95      0.95       206\n           9       0.91      0.91      0.91       205\n\n    accuracy                           0.96      2000\n   macro avg       0.96      0.96      0.96      2000\nweighted avg       0.96      0.96      0.96      2000\n\n```\n:::\n:::\n\n\n## Confusion Matrix\n\n::: {#cbcb59da .cell execution_count=13}\n``` {.python .cell-code code-fold=\"true\"}\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\nfig, ax = plt.subplots(figsize=(8, 6))\ncm = confusion_matrix(y_test, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=mlp.classes_)\ndisp.plot(ax=ax, cmap='Blues', values_format='d')\nplt.title('Confusion Matrix for MNIST Classification')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](30-NN-consolidated_files/figure-html/cell-14-output-1.png){width=586 height=523 fig-align='center'}\n:::\n:::\n\n\n## Visualize Predictions\n\n::: {#a896799f .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\n# Get prediction probabilities\ny_pred_proba = mlp.predict_proba(X_test_scaled)\n\nfig, axes = plt.subplots(2, 5, figsize=(12, 5))\nfor i, ax in enumerate(axes.flat):\n    ax.imshow(X_test[i].reshape(28, 28), cmap='gray')\n    pred_label = y_pred[i]\n    true_label = y_test[i]\n    confidence = y_pred_proba[i].max()\n    \n    color = 'green' if pred_label == true_label else 'red'\n    ax.set_title(f'True: {true_label}, Pred: {pred_label}\\nConf: {confidence:.2f}', \n                 color=color)\n    ax.axis('off')\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](30-NN-consolidated_files/figure-html/cell-15-output-1.png){width=1121 height=478 fig-align='center'}\n:::\n:::\n\n\n# Regression Example\n\n## California Housing Dataset\n\nLet's predict house prices using MLP regression:\n\n::: {#83981e82 .cell execution_count=15}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPRegressor\n\n# Load housing data\nhousing = fetch_california_housing()\nX_housing = housing.data\ny_housing = housing.target\n\nprint(f\"Dataset shape: {X_housing.shape}\")\nprint(f\"Features: {housing.feature_names}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset shape: (20640, 8)\nFeatures: ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n```\n:::\n:::\n\n\n**Target:** Median house value (in $100,000s)\n\n## Prepare Data and Define Model\n\n::: {#fcf6b790 .cell execution_count=16}\n``` {.python .cell-code code-fold=\"false\"}\n# Split and scale\nX_train_h, X_test_h, y_train_h, y_test_h = train_test_split(\n    X_housing, y_housing, test_size=0.2, random_state=42\n)\n\nscaler = StandardScaler()\nX_train_h_scaled = scaler.fit_transform(X_train_h)\nX_test_h_scaled = scaler.transform(X_test_h)\n\n# Train MLP Regressor with early stopping\nmlp_reg = MLPRegressor(\n    hidden_layer_sizes=(100, 50),\n    activation='relu',\n    solver='adam',\n    alpha=0.001,\n    batch_size=32,\n    max_iter=100,\n    early_stopping=True,\n    validation_fraction=0.1,\n    n_iter_no_change=10,\n    random_state=42,\n    verbose=False\n)\n```\n:::\n\n\n## Train the model\n\n::: {#15ef7818 .cell execution_count=17}\n``` {.python .cell-code code-fold=\"false\"}\nprint(\"Training MLP Regressor...\")\nmlp_reg.fit(X_train_h_scaled, y_train_h)\nprint(f\"Training stopped at iteration: {mlp_reg.n_iter_}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nTraining MLP Regressor...\nTraining stopped at iteration: 76\n```\n:::\n:::\n\n\n## Training Loss Curve\n\n::: {#9cc33b47 .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\nplt.figure(figsize=(10, 4))\nplt.plot(mlp_reg.loss_curve_)\nplt.xlabel('Iteration (Epoch)')\nplt.ylabel('Loss')\nplt.title('Training Loss Curve')\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](30-NN-consolidated_files/figure-html/cell-19-output-1.png){width=821 height=376 fig-align='center'}\n:::\n:::\n\n\n## Evaluate Regression Model\n\n::: {#62942eb0 .cell execution_count=19}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n\n# Make predictions\ny_pred_h = mlp_reg.predict(X_test_h_scaled)\n\n# Calculate metrics\nmse = mean_squared_error(y_test_h, y_pred_h)\nrmse = np.sqrt(mse)\nmae = mean_absolute_error(y_test_h, y_pred_h)\nr2 = r2_score(y_test_h, y_pred_h)\n\nprint(f\"Root Mean Squared Error: {rmse:.4f}\")\nprint(f\"Mean Absolute Error: {mae:.4f}\")\nprint(f\"R² Score: {r2:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRoot Mean Squared Error: 0.5138\nMean Absolute Error: 0.3460\nR² Score: 0.7986\n```\n:::\n:::\n\n\nAn $R^2$ of ~0.8 means our model explains 80% of the variance in house prices!\n\n## Predictions vs Actual\n\n::: {#c03ffa55 .cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\"}\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n# Scatter plot\naxes[0].scatter(y_test_h, y_pred_h, alpha=0.5)\naxes[0].plot([y_test_h.min(), y_test_h.max()], \n             [y_test_h.min(), y_test_h.max()], 'r--', lw=2)\naxes[0].set_xlabel('Actual Values')\naxes[0].set_ylabel('Predicted Values')\naxes[0].set_title('Predicted vs Actual House Prices')\naxes[0].grid(True)\n\n# Residual plot\nresiduals = y_test_h - y_pred_h\naxes[1].scatter(y_pred_h, residuals, alpha=0.5)\naxes[1].axhline(y=0, color='r', linestyle='--', lw=2)\naxes[1].set_xlabel('Predicted Values')\naxes[1].set_ylabel('Residuals')\naxes[1].set_title('Residual Plot')\naxes[1].grid(True)\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](30-NN-consolidated_files/figure-html/cell-21-output-1.png){width=950 height=470 fig-align='center'}\n:::\n:::\n\n\n# Hyperparameter Tuning\n\n## Grid Search for Optimal Architecture\n\nscikit-learn provides [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) for hyperparameter tuning:\n\nCreate a model without certain hyperparameters.\n\n::: {#9585d397 .cell execution_count=21}\n``` {.python .cell-code code-fold=\"false\"}\n# Create MLP\nmlp_grid = MLPClassifier(\n    max_iter=20,\n    random_state=42,\n    early_stopping=True,\n    validation_fraction=0.1,\n    n_iter_no_change=5,\n    verbose=False\n)\n\n# Use subset for faster demo\nX_grid = X_train_scaled[:1500]\ny_grid = y_train[:1500]\n```\n:::\n\n\n## Grid Search, cont.\n\nDefine the parameter grid and run the grid search.\n\n::: {#c6e73ebf .cell execution_count=22}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.model_selection import GridSearchCV\n\n# Define parameter grid\nparam_grid = {\n    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n    'activation': ['relu'],\n    'alpha': [0.0001, 0.001]\n}\n\nprint(\"Running Grid Search...\")\ngrid_search = GridSearchCV(mlp_grid, param_grid, cv=3, n_jobs=2, verbose=0)\ngrid_search.fit(X_grid, y_grid)\n\nprint(f\"\\nBest parameters: {grid_search.best_params_}\")\nprint(f\"Best CV score: {grid_search.best_score_:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning Grid Search...\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\nBest parameters: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50,)}\nBest CV score: 0.8740\n```\n:::\n:::\n\n\n## Grid Search Results\n\n::: {#fd584e23 .cell execution_count=23}\n``` {.python .cell-code code-fold=\"true\"}\nresults_df = pd.DataFrame(grid_search.cv_results_)\n\n\nn_configs = min(10, len(results_df))\ntop_results = results_df.nlargest(n_configs, 'mean_test_score')\n\n# plt.figure(figsize=(10, 5))\n# plt.barh(range(len(top_results)), top_results['mean_test_score'])\n# plt.yticks(range(len(top_results)), \n#            [f\"Config {i+1}\" for i in range(len(top_results))])\n# plt.xlabel('Mean CV Score')\n# plt.title('Top Hyperparameter Configurations')\n# plt.grid(True, axis='x')\n# plt.tight_layout()\n# plt.show()\n\nprint(\"\\nTop configurations:\")\nfor idx, row in top_results.iterrows():\n    print(f\"\\nConfiguration {idx + 1}:\")\n    for key, value in row['params'].items():\n        print(f\"  {key}: {value}\")\n\nprint(\"\\nMean CV Score:\")\nprint(top_results[['mean_test_score', 'std_test_score']].head())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nTop configurations:\n\nConfiguration 4:\n  activation: relu\n  alpha: 0.001\n  hidden_layer_sizes: (50,)\n\nConfiguration 1:\n  activation: relu\n  alpha: 0.0001\n  hidden_layer_sizes: (50,)\n\nConfiguration 2:\n  activation: relu\n  alpha: 0.0001\n  hidden_layer_sizes: (100,)\n\nConfiguration 5:\n  activation: relu\n  alpha: 0.001\n  hidden_layer_sizes: (100,)\n\nConfiguration 6:\n  activation: relu\n  alpha: 0.001\n  hidden_layer_sizes: (50, 50)\n\nConfiguration 3:\n  activation: relu\n  alpha: 0.0001\n  hidden_layer_sizes: (50, 50)\n\nMean CV Score:\n   mean_test_score  std_test_score\n3         0.874000        0.007483\n0         0.872000        0.008165\n1         0.870667        0.016357\n4         0.869333        0.015861\n5         0.869333        0.010625\n```\n:::\n:::\n\n\n# Best Practices\n\n## Data Preprocessing\n\n::: {.callout-tip}\n### Always preprocess your data!\n\n1. **Scale features:** Use `StandardScaler` or normalize to [0, 1]\n2. **Handle missing values:** Impute or remove\n3. **Encode categorical variables:** One-hot encoding\n4. **Use pipelines:** Ensures consistent preprocessing\n\n```python\nfrom sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('scaler', StandardScaler()),\n    ('mlp', MLPClassifier(hidden_layer_sizes=(100,)))\n])\n\npipeline.fit(X_train, y_train)\n```\n:::\n\n## Architecture Selection\n\n::: {.callout-tip}\n### Rules of thumb for architecture:\n\n1. **Start simple:** Try single hidden layer first\n2. **Layer sizes:** Between input and output dimensions\n3. **Depth vs width:** \n   - More layers → learn complex patterns\n   - But risk overfitting on small data\n4. **Typical architectures:**\n   - Small data: `(100,)` or `(50, 50)`\n   - Medium data: `(100, 50)` or `(128, 64, 32)`\n   - Large data: Consider PyTorch\n:::\n\n## Preventing Overfitting\n\nThree key techniques:\n\n**1. Regularization:** Add L2 penalty (`alpha` parameter)\n\n```python\nmlp = MLPClassifier(alpha=0.01)  # Stronger regularization\n```\n\n**2. Early Stopping:** Stop when validation performance plateaus\n\n```python\nmlp = MLPClassifier(early_stopping=True, \n                    validation_fraction=0.2,\n                    n_iter_no_change=10)\n```\n\n**3. Cross-Validation:** Get robust performance estimates\n\n```python\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(mlp, X_train, y_train, cv=5)\n```\n\n## Solver Selection\n\nDifferent solvers for different scenarios:\n\n| Solver | Best For | Notes |\n|--------|----------|-------|\n| `'adam'` | Most cases | Good default, adaptive learning rate |\n| `'sgd'` | Large datasets | Classic mini-batch SGD |\n| `'lbfgs'` | Small datasets | Faster for small data, more memory |\n\n::: {#26612960 .cell execution_count=24}\n``` {.python .cell-code code-fold=\"true\"}\n# Compare solvers\nsolvers = ['adam', 'sgd', 'lbfgs']\nresults = {}\n\nfor solver in solvers:\n    mlp = MLPClassifier(\n        hidden_layer_sizes=(50,),\n        solver=solver,\n        max_iter=50,\n        random_state=42,\n        verbose=False\n    )\n    mlp.fit(X_train_scaled[:2000], y_train[:2000])\n    score = mlp.score(X_test_scaled, y_test)\n    results[solver] = score\n    print(f\"{solver:10s}: {score:.4f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nadam      : 0.9000\nsgd       : 0.8330\nlbfgs     : 0.8800\n```\n:::\n:::\n\n\n## Common Issues\n\n::: {.callout-warning}\n### Convergence Warnings\n\nIf you see `ConvergenceWarning`:\n\n1. **Increase** `max_iter`\n2. **Decrease** `learning_rate_init`\n3. **Enable** `early_stopping=True`\n4. **Check** if data is properly scaled\n:::\n\n::: {.callout-warning}\n### Poor Performance\n\nIf accuracy is low:\n\n1. Is data scaled/normalized?\n2. Is architecture appropriate?\n3. Is learning rate too high/low?\n4. Do you need more iterations?\n5. Is regularization too strong?\n:::\n\n# Summary\n\n## Summary\n\n**Theory:**\n\n* Neural networks extend linear/logistic regression with multiple layers and non-linearity\n* MLPs learn hierarchical representations through hidden layers\n* Gradient descent optimizes the loss function\n* Mini-batch GD balances speed and stability\n\n**Practice:**\n\n* Scikit-learn's `MLPClassifier` and `MLPRegressor` for easy implementation\n* Always preprocess/scale your data\n* Use early stopping and regularization to prevent overfitting\n* Grid search helps find optimal hyperparameters\n\n## To Dig Deeper\n\nOther modules in the course notes:\n\n* [NN I -- Gradient Descent](./23-NN-I-Gradient-Descent.qmd)\n* [NN II -- Compute Graph and Backpropagation](./24-NN-II-Backprop.qmd)\n* [NN III -- SGD and CNNs](./25-NN-III-CNNs.qmd)\n* [NN IV -- NNs with Scikit-Learn](./29-NN-IV-Scikit-Learn.qmd)\n\nAdditional resources:\n\n* [Understanding Deep Learning, Simon J.D. Prince, MIT Press, 2023](http://udlbook.com)\n* DS542, Deep Learning for Data Science\n\n\n<!--\n## When to Use What\n\n::: {#bb3c35d5 .cell execution_count=25}\n\n::: {.cell-output .cell-output-display}\n![](30-NN-consolidated_files/figure-html/cell-26-output-1.png){width=1142 height=566 fig-align='center'}\n:::\n:::\n\n\n-->\n\n",
    "supporting": [
      "30-NN-consolidated_files"
    ],
    "filters": [],
    "includes": {}
  }
}