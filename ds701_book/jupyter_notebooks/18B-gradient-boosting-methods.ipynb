{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Gradient Boosting Methods\"\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "# Gradient Boosting Methods: XGBoost, LightGBM, and CatBoost\n",
        "\n",
        "<!--\n",
        "AI-GENERATED CODE\n",
        "Generated by: Cursor / Sonnet 4.5\n",
        "Prompt: \"As suggested at the end of @18A-decision-tree-regression.md , \"*Next steps*: Explore gradient boosting methods (XGBoost, LightGBM) which often outperform random forests, especially on structured/tabular data competitions.\"  Create a markdown file '18-B-gradient-boosting-methods.md' lecture.\"\n",
        "Date: October 29, 2025\n",
        "Purpose: Used to create the markdown file for the gradient boosting methods lecture.\n",
        "The code was reviewed and adapted.\n",
        "-->\n",
        "\n",
        "## Introduction\n",
        "\n",
        "You've learned that random forests build many independent trees in parallel and average their predictions. But what if, instead of building trees independently, each new tree could learn from the mistakes of previous trees? This is the key insight behind **gradient boosting** - arguably the most powerful machine learning technique for structured/tabular data.\n",
        "\n",
        "In Kaggle competitions and real-world applications, gradient boosting methods (especially XGBoost, LightGBM, and CatBoost) consistently win or place highly. Let's understand why.\n",
        "\n",
        "## The Core Idea: Learning from Mistakes\n",
        "\n",
        "### Random Forests vs Gradient Boosting\n",
        "\n",
        "**Random Forest**: \"Let's build 100 trees independently, each on different random samples, and average their predictions.\"\n",
        "\n",
        "**Gradient Boosting**: \"Let's build trees sequentially. Each new tree focuses on correcting the errors made by all previous trees.\"\n",
        "\n",
        "### Intuitive Example: Estimating House Prices\n",
        "\n",
        "Imagine you're estimating house prices with a team of experts:\n",
        "\n",
        "**Random Forest Approach:**\n",
        "- 100 experts independently look at the data\n",
        "- Each gives their estimate\n",
        "- You average all 100 estimates\n",
        "- Experts don't learn from each other\n",
        "\n",
        "**Gradient Boosting Approach:**\n",
        "- Expert 1 makes predictions (simple rules)\n",
        "  - Prediction: \"All houses cost $300K\"\n",
        "  - Error: Off by a lot!\n",
        "  \n",
        "- Expert 2 looks at Expert 1's mistakes\n",
        "  - \"Okay, Expert 1 overestimated small houses and underestimated large ones\"\n",
        "  - Expert 2 adds corrections: \"-$100K for houses < 1500 sqft, +$200K for houses > 2500 sqft\"\n",
        "  \n",
        "- Expert 3 looks at remaining mistakes\n",
        "  - \"Still underestimating houses in downtown\"\n",
        "  - Expert 3 adds: \"+$50K for downtown location\"\n",
        "  \n",
        "- Continue for 100 experts...\n",
        "- Final prediction: Expert 1 + Expert 2 + Expert 3 + ... + Expert 100\n",
        "\n",
        "Each new expert (tree) specializes in correcting the mistakes of all previous experts!\n",
        "\n",
        "## How Gradient Boosting Works: The Mathematics\n",
        "\n",
        "### The Algorithm (Simplified)\n",
        "\n",
        "1. **Initialize**: Start with a simple prediction (often the mean of training targets)\n",
        "   $$F_0(x) = \\bar{y}$$\n",
        "\n",
        "2. **For each tree m = 1 to M:**\n",
        "   \n",
        "   a. Calculate **residuals** (errors from current model):\n",
        "   $$r_i = y_i - F_{m-1}(x_i)$$\n",
        "   \n",
        "   b. Fit a new tree $h_m(x)$ to predict these residuals\n",
        "   \n",
        "   c. Add this tree to the ensemble (with a learning rate $\\eta$):\n",
        "   $$F_m(x) = F_{m-1}(x) + \\eta \\cdot h_m(x)$$\n",
        "\n",
        "3. **Final prediction**: $F_M(x)$ = sum of all trees\n",
        "\n",
        "**Key Insight**: Each tree is trained to predict the **residuals** (mistakes) of the previous model, not the original targets!\n",
        "\n",
        "### Example 1: Simple Gradient Boosting by Hand\n",
        "\n",
        "Let's predict house prices with just 3 trees:\n",
        "\n",
        "**Training Data:**\n",
        "| House | Features | True Price |\n",
        "|-------|----------|------------|\n",
        "| 1     | Small    | $200K      |\n",
        "| 2     | Medium   | $300K      |\n",
        "| 3     | Large    | $400K      |\n",
        "\n",
        "**Step 1: Initialize**\n",
        "- $F_0$ = mean price = $(200 + 300 + 400)/3 = 300K$\n",
        "- Predictions: [300K, 300K, 300K]\n",
        "- Residuals: [-100K, 0K, +100K]\n",
        "\n",
        "**Step 2: Tree 1 (predict residuals)**\n",
        "- Tree 1 learns: \"Small houses → -100K, Large houses → +100K\"\n",
        "- After Tree 1: $F_1$ = 300K + Tree1\n",
        "  - House 1: 300K + (-100K) = 200K ✓\n",
        "  - House 2: 300K + 0K = 300K ✓\n",
        "  - House 3: 300K + 100K = 400K ✓\n",
        "- New residuals: [0, 0, 0] - Perfect!\n",
        "\n",
        "In practice, trees are limited (max_depth, learning_rate), so we need many trees to gradually reduce errors.\n",
        "\n",
        "### Example 2: With Learning Rate\n",
        "\n",
        "Let's add a **learning rate** (η = 0.5) to prevent overfitting:\n",
        "\n",
        "**Step 1: Initialize**\n",
        "- $F_0$ = 300K\n",
        "- Residuals: [-100K, 0K, +100K]\n",
        "\n",
        "**Step 2: Tree 1**\n",
        "- Tree 1 predicts: [-100K, 0K, +100K]\n",
        "- But we only add **50%** of this (η = 0.5)\n",
        "- $F_1$ = 300K + 0.5 × Tree1\n",
        "  - House 1: 300K + 0.5×(-100K) = 250K\n",
        "  - House 2: 300K + 0.5×(0K) = 300K\n",
        "  - House 3: 300K + 0.5×(100K) = 350K\n",
        "- New residuals: [-50K, 0K, +50K]\n",
        "\n",
        "**Step 3: Tree 2**\n",
        "- Tree 2 predicts these new residuals: [-50K, 0K, +50K]\n",
        "- $F_2$ = 250K, 300K, 350K + 0.5 × Tree2\n",
        "  - House 1: 250K + 0.5×(-50K) = 225K\n",
        "  - House 2: 300K + 0.5×(0K) = 300K\n",
        "  - House 3: 350K + 0.5×(50K) = 375K\n",
        "- New residuals: [-25K, 0K, +25K]\n",
        "\n",
        "**Step 4: Tree 3...**\n",
        "- Continue, each time cutting the error in half\n",
        "\n",
        "**Why a learning rate?**\n",
        "- Slower learning (smaller steps) often generalizes better\n",
        "- More trees needed, but each tree is \"weaker\"\n",
        "- Prevents overfitting to training data\n",
        "\n",
        "## Example 3: Visualizing Sequential Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Simple non-linear relationship\n",
        "np.random.seed(42)\n",
        "X = np.linspace(0, 10, 50).reshape(-1, 1)\n",
        "y = np.sin(X).ravel() + np.random.normal(0, 0.2, X.shape[0])\n",
        "\n",
        "# Train with just 3 trees to see progression\n",
        "gb = GradientBoostingRegressor(\n",
        "    n_estimators=3,\n",
        "    max_depth=2,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Visualize each stage\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "X_plot = np.linspace(0, 10, 200).reshape(-1, 1)\n",
        "\n",
        "# Initial prediction (mean)\n",
        "axes[0].scatter(X, y, alpha=0.5)\n",
        "axes[0].axhline(y.mean(), color='r', linewidth=2)\n",
        "axes[0].set_title('Step 0: Initialize with mean')\n",
        "axes[0].set_ylim(-1.5, 1.5)\n",
        "\n",
        "# After each tree\n",
        "for i in range(1, 4):\n",
        "    gb_temp = GradientBoostingRegressor(\n",
        "        n_estimators=i,\n",
        "        max_depth=2,\n",
        "        learning_rate=1.0,\n",
        "        random_state=42\n",
        "    )\n",
        "    gb_temp.fit(X, y)\n",
        "    y_pred = gb_temp.predict(X_plot)\n",
        "    \n",
        "    axes[i].scatter(X, y, alpha=0.5)\n",
        "    axes[i].plot(X_plot, y_pred, 'r-', linewidth=2)\n",
        "    axes[i].plot(X_plot, np.sin(X_plot), 'g--', alpha=0.5)\n",
        "    axes[i].set_title(f'After Tree {i}')\n",
        "    axes[i].set_ylim(-1.5, 1.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**What you'll observe:**\n",
        "- Step 0: Flat line (mean)\n",
        "- After Tree 1: Rough approximation of the pattern\n",
        "- After Tree 2: Better fit, capturing more details\n",
        "- After Tree 3: Even closer to the true function\n",
        "\n",
        "Each tree adds a \"correction layer\" to improve the fit!\n",
        "\n",
        "## The Three Kings: XGBoost, LightGBM, and CatBoost\n",
        "\n",
        "All three implement gradient boosting but with different optimizations and features.\n",
        "\n",
        "### XGBoost (eXtreme Gradient Boosting)\n",
        "\n",
        "**History**: Released 2014, dominated Kaggle competitions for years\n",
        "\n",
        "**Key Features:**\n",
        "- **Regularization**: Built-in L1/L2 penalties prevent overfitting\n",
        "- **Handling missing values**: Learns best direction for missing data\n",
        "- **Parallel processing**: Fast training through parallelized tree construction\n",
        "- **Tree pruning**: Grows deep then prunes back (not greedy)\n",
        "- **Cache awareness**: Optimized memory access patterns\n",
        "\n",
        "**When to use:**\n",
        "- ✓ You have structured/tabular data\n",
        "- ✓ You need high accuracy\n",
        "- ✓ You want automatic handling of missing values\n",
        "- ✓ Your dataset is medium-sized (10K-1M rows)\n",
        "\n",
        "### LightGBM (Light Gradient Boosting Machine)\n",
        "\n",
        "**History**: Released by Microsoft in 2016\n",
        "\n",
        "**Key Innovation**: **Leaf-wise** (best-first) tree growth instead of level-wise\n",
        "\n",
        "**Traditional (level-wise):**\n",
        "```\n",
        "        Root\n",
        "       /    \\\n",
        "      A      B     <- Grow entire level\n",
        "     / \\    / \\\n",
        "    C   D  E   F   <- Then next level\n",
        "```\n",
        "\n",
        "**LightGBM (leaf-wise):**\n",
        "```\n",
        "        Root\n",
        "       /    \\\n",
        "      A      B\n",
        "     / \\\n",
        "    C   D            <- Split the leaf with highest gain\n",
        "       / \\\n",
        "      E   F          <- Then next best leaf\n",
        "```\n",
        "\n",
        "**Key Features:**\n",
        "- **Faster training**: Especially on large datasets\n",
        "- **Lower memory usage**: Through histogram-based binning\n",
        "- **Better accuracy**: On deep trees\n",
        "- **Handles categorical features**: Native support without encoding\n",
        "\n",
        "**When to use:**\n",
        "- ✓ You have **large datasets** (>100K rows)\n",
        "- ✓ Training speed matters\n",
        "- ✓ You have categorical features\n",
        "- ✓ You need lower memory footprint\n",
        "\n",
        "### CatBoost (Categorical Boosting)\n",
        "\n",
        "**History**: Released by Yandex in 2017\n",
        "\n",
        "**Key Innovation**: Superior handling of **categorical variables**\n",
        "\n",
        "**Key Features:**\n",
        "- **Ordered boosting**: Reduces overfitting through clever target encoding\n",
        "- **Categorical features**: No need for one-hot or label encoding\n",
        "- **Symmetric trees**: Faster prediction, better regularization\n",
        "- **Minimal tuning**: Works well with default parameters\n",
        "- **GPU acceleration**: Excellent GPU support\n",
        "\n",
        "**When to use:**\n",
        "- ✓ You have many **categorical features**\n",
        "- ✓ You want minimal hyperparameter tuning\n",
        "- ✓ You need fast predictions (deployment)\n",
        "- ✓ You want to avoid target leakage from categorical encoding\n",
        "\n",
        "## Example 4: Comparing All Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostRegressor\n",
        "import time\n",
        "\n",
        "# Load California housing dataset\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Dictionary to store results\n",
        "results = {}\n",
        "\n",
        "# 1. Random Forest (baseline)\n",
        "print(\"Training Random Forest...\")\n",
        "start = time.time()\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "rf_time = time.time() - start\n",
        "rf_pred = rf.predict(X_test)\n",
        "results['Random Forest'] = {\n",
        "    'R2': r2_score(y_test, rf_pred),\n",
        "    'RMSE': np.sqrt(mean_squared_error(y_test, rf_pred)),\n",
        "    'Time': rf_time\n",
        "}\n",
        "\n",
        "# 2. Sklearn Gradient Boosting\n",
        "print(\"Training Sklearn GradientBoosting...\")\n",
        "start = time.time()\n",
        "gb = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "gb.fit(X_train, y_train)\n",
        "gb_time = time.time() - start\n",
        "gb_pred = gb.predict(X_test)\n",
        "results['GradientBoosting'] = {\n",
        "    'R2': r2_score(y_test, gb_pred),\n",
        "    'RMSE': np.sqrt(mean_squared_error(y_test, gb_pred)),\n",
        "    'Time': gb_time\n",
        "}\n",
        "\n",
        "# 3. XGBoost\n",
        "print(\"Training XGBoost...\")\n",
        "start = time.time()\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "xgb_model.fit(X_train, y_train)\n",
        "xgb_time = time.time() - start\n",
        "xgb_pred = xgb_model.predict(X_test)\n",
        "results['XGBoost'] = {\n",
        "    'R2': r2_score(y_test, xgb_pred),\n",
        "    'RMSE': np.sqrt(mean_squared_error(y_test, xgb_pred)),\n",
        "    'Time': xgb_time\n",
        "}\n",
        "\n",
        "# 4. LightGBM\n",
        "print(\"Training LightGBM...\")\n",
        "start = time.time()\n",
        "lgb_model = lgb.LGBMRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42,\n",
        "    verbose=-1\n",
        ")\n",
        "lgb_model.fit(X_train, y_train)\n",
        "lgb_time = time.time() - start\n",
        "lgb_pred = lgb_model.predict(X_test)\n",
        "results['LightGBM'] = {\n",
        "    'R2': r2_score(y_test, lgb_pred),\n",
        "    'RMSE': np.sqrt(mean_squared_error(y_test, lgb_pred)),\n",
        "    'Time': lgb_time\n",
        "}\n",
        "\n",
        "# 5. CatBoost\n",
        "print(\"Training CatBoost...\")\n",
        "start = time.time()\n",
        "cat_model = CatBoostRegressor(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=3,\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "cat_model.fit(X_train, y_train)\n",
        "cat_time = time.time() - start\n",
        "cat_pred = cat_model.predict(X_test)\n",
        "results['CatBoost'] = {\n",
        "    'R2': r2_score(y_test, cat_pred),\n",
        "    'RMSE': np.sqrt(mean_squared_error(y_test, cat_pred)),\n",
        "    'Time': cat_time\n",
        "}\n",
        "\n",
        "# Display results\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df = results_df.sort_values('R2', ascending=False)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "print(results_df.to_string())\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Typical output:\n",
        "#                     R2      RMSE    Time\n",
        "# XGBoost          0.847     0.476   0.123\n",
        "# LightGBM         0.846     0.477   0.089\n",
        "# CatBoost         0.845     0.479   0.234\n",
        "# GradientBoosting 0.842     0.483   0.543\n",
        "# Random Forest    0.812     0.527   0.178"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Typical Observations:**\n",
        "- **Accuracy**: Gradient boosting methods beat random forest\n",
        "- **Speed**: LightGBM is fastest, sklearn GB is slowest\n",
        "- **Consistency**: All three (XGB, LGB, CB) perform similarly well\n",
        "\n",
        "## Key Hyperparameters\n",
        "\n",
        "### Common to All Boosting Methods:\n",
        "\n",
        "1. **n_estimators / iterations**: Number of trees\n",
        "   - More trees → better fit, but risk overfitting and slower\n",
        "   - Start: 100, increase to 500-1000 with proper early stopping\n",
        "   - Example: `n_estimators=500`\n",
        "\n",
        "2. **learning_rate / eta**: How much each tree contributes\n",
        "   - Lower → better generalization, need more trees\n",
        "   - Higher → faster training, risk overfitting\n",
        "   - Typical: 0.01 to 0.3\n",
        "   - Rule: learning_rate × n_estimators ≈ constant\n",
        "   - Example: `learning_rate=0.05`\n",
        "\n",
        "3. **max_depth / depth**: Maximum depth of each tree\n",
        "   - Deeper → can capture complex patterns, risk overfitting\n",
        "   - Shallower → faster, better regularization\n",
        "   - Typical: 3-8\n",
        "   - Example: `max_depth=5`\n",
        "\n",
        "4. **subsample**: Fraction of samples used for each tree\n",
        "   - < 1.0 → adds randomness, prevents overfitting\n",
        "   - Typical: 0.8\n",
        "   - Example: `subsample=0.8`\n",
        "\n",
        "5. **colsample_bytree**: Fraction of features per tree\n",
        "   - < 1.0 → adds randomness, like random forest\n",
        "   - Typical: 0.8\n",
        "   - Example: `colsample_bytree=0.8`\n",
        "\n",
        "### XGBoost Specific:\n",
        "\n",
        "6. **reg_alpha**: L1 regularization\n",
        "   - Higher → more regularization\n",
        "   - Example: `reg_alpha=0.1`\n",
        "\n",
        "7. **reg_lambda**: L2 regularization\n",
        "   - Higher → more regularization\n",
        "   - Example: `reg_lambda=1.0`\n",
        "\n",
        "### LightGBM Specific:\n",
        "\n",
        "8. **num_leaves**: Number of leaves (instead of max_depth)\n",
        "   - Controls model complexity\n",
        "   - Rule: `num_leaves < 2^max_depth`\n",
        "   - Example: `num_leaves=31`\n",
        "\n",
        "9. **min_child_samples**: Minimum samples in a leaf\n",
        "   - Prevents overfitting\n",
        "   - Example: `min_child_samples=20`\n",
        "\n",
        "### CatBoost Specific:\n",
        "\n",
        "10. **cat_features**: List of categorical feature indices\n",
        "    - Automatic handling\n",
        "    - Example: `cat_features=[0, 3, 5]`\n",
        "\n",
        "## Example 5: Hyperparameter Tuning with Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 300, 500],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'subsample': [0.8, 1.0],\n",
        "    'colsample_bytree': [0.8, 1.0]\n",
        "}\n",
        "\n",
        "# XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(random_state=42)\n",
        "\n",
        "# Grid search with cross-validation\n",
        "grid_search = GridSearchCV(\n",
        "    xgb_model,\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV score: {np.sqrt(-grid_search.best_score_):.4f}\")\n",
        "\n",
        "# Test performance\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.4f}\")\n",
        "print(f\"Test R²: {r2_score(y_test, y_pred):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 6: Early Stopping (Crucial!)\n",
        "\n",
        "Prevent overfitting by stopping when validation performance stops improving:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "\n",
        "# Split into train and validation\n",
        "X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# XGBoost with early stopping\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=1000,  # Set high\n",
        "    learning_rate=0.05,\n",
        "    max_depth=5,\n",
        "    random_state=42,\n",
        "    early_stopping_rounds=50\n",
        ")\n",
        "\n",
        "# Fit with early stopping\n",
        "xgb_model.fit(\n",
        "    X_train_sub, y_train_sub,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    verbose=10\n",
        ")\n",
        "\n",
        "print(f\"Best iteration: {xgb_model.best_iteration}\")\n",
        "print(f\"Best score: {xgb_model.best_score}\")\n",
        "\n",
        "# LightGBM version\n",
        "import lightgbm as lgb\n",
        "\n",
        "lgb_model = lgb.LGBMRegressor(\n",
        "    n_estimators=1000,\n",
        "    learning_rate=0.05,\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "lgb_model.fit(\n",
        "    X_train_sub, y_train_sub,\n",
        "    eval_set=[(X_val, y_val)],\n",
        "    callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
        ")\n",
        "\n",
        "# CatBoost version\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "\n",
        "cat_model = CatBoostRegressor(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.05,\n",
        "    depth=5,\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "cat_model.fit(\n",
        "    X_train_sub, y_train_sub,\n",
        "    eval_set=(X_val, y_val),\n",
        "    early_stopping_rounds=50\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Best Practice**: Always use early stopping with a validation set!\n",
        "\n",
        "## Example 7: Feature Importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train XGBoost\n",
        "xgb_model = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance\n",
        "importance = xgb_model.feature_importances_\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "# Create DataFrame for easy sorting\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importance\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('XGBoost Feature Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(importance_df)\n",
        "\n",
        "# Multiple importance types in XGBoost\n",
        "xgb.plot_importance(xgb_model, importance_type='gain')  # Average gain\n",
        "xgb.plot_importance(xgb_model, importance_type='weight')  # Number of splits\n",
        "xgb.plot_importance(xgb_model, importance_type='cover')  # Coverage\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 8: Handling Categorical Features with CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from catboost import CatBoostRegressor\n",
        "import pandas as pd\n",
        "\n",
        "# Create sample data with categorical features\n",
        "data = pd.DataFrame({\n",
        "    'size': [1200, 1500, 1800, 2000, 2200, 2500],\n",
        "    'neighborhood': ['A', 'B', 'A', 'C', 'B', 'C'],\n",
        "    'style': ['Modern', 'Victorian', 'Modern', 'Ranch', 'Victorian', 'Ranch'],\n",
        "    'price': [200, 250, 280, 320, 300, 380]\n",
        "})\n",
        "\n",
        "X = data[['size', 'neighborhood', 'style']]\n",
        "y = data['price']\n",
        "\n",
        "# Identify categorical columns\n",
        "cat_features = ['neighborhood', 'style']  # or indices [1, 2]\n",
        "\n",
        "# CatBoost handles categorical features natively!\n",
        "model = CatBoostRegressor(\n",
        "    iterations=100,\n",
        "    learning_rate=0.1,\n",
        "    depth=3,\n",
        "    cat_features=cat_features,\n",
        "    random_state=42,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "# Make predictions (can handle categorical inputs directly)\n",
        "new_house = pd.DataFrame({\n",
        "    'size': [1600],\n",
        "    'neighborhood': ['A'],\n",
        "    'style': ['Modern']\n",
        "})\n",
        "prediction = model.predict(new_house)\n",
        "print(f\"Predicted price: ${prediction[0]:.0f}K\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why this matters**: No need for label encoding or one-hot encoding, which can:\n",
        "- Create sparse high-dimensional data\n",
        "- Introduce order where none exists\n",
        "- Cause target leakage\n",
        "\n",
        "## Comparison Table: Which Method to Choose?\n",
        "\n",
        "| Criterion | Random Forest | XGBoost | LightGBM | CatBoost |\n",
        "|-----------|--------------|---------|----------|----------|\n",
        "| **Accuracy** | Good | Excellent | Excellent | Excellent |\n",
        "| **Training Speed** | Medium | Medium | **Fast** | Slow |\n",
        "| **Prediction Speed** | Slow | Medium | Medium | **Fast** |\n",
        "| **Memory Usage** | High | Medium | **Low** | Medium |\n",
        "| **Large Datasets (>1M)** | ✗ | ✓ | **✓✓** | ✓ |\n",
        "| **Categorical Features** | ✗ | △ | ✓ | **✓✓** |\n",
        "| **Handles Missing Data** | ✓ | **✓✓** | ✓ | ✓ |\n",
        "| **Overfitting Risk** | Low | Medium | **High** | Low |\n",
        "| **Hyperparameter Tuning** | Easy | Medium | **Hard** | Easy |\n",
        "| **GPU Support** | ✗ | ✓ | ✓ | **✓✓** |\n",
        "| **Interpretability** | Medium | Medium | Medium | Medium |\n",
        "\n",
        "### Decision Guide:\n",
        "\n",
        "**Use XGBoost if:**\n",
        "- You want excellent out-of-box performance\n",
        "- You have missing data\n",
        "- You need a balance of speed and accuracy\n",
        "- You're used to it (most tutorials use XGBoost)\n",
        "\n",
        "**Use LightGBM if:**\n",
        "- You have **large datasets** (>100K rows)\n",
        "- Training speed is critical\n",
        "- You need low memory footprint\n",
        "- You have some categorical features\n",
        "\n",
        "**Use CatBoost if:**\n",
        "- You have **many categorical features**\n",
        "- You want minimal hyperparameter tuning\n",
        "- Prediction speed (deployment) matters\n",
        "- You want to avoid target leakage\n",
        "\n",
        "**Use Random Forest if:**\n",
        "- You need an easy baseline\n",
        "- Interpretability is important\n",
        "- You have small data (<10K rows)\n",
        "- You want parallel training with zero dependencies between models\n",
        "\n",
        "## Common Pitfalls and Best Practices\n",
        "\n",
        "### Pitfall 1: Not Using Early Stopping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ❌ BAD: Fixed number of trees, might overfit\n",
        "model = xgb.XGBRegressor(n_estimators=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ✅ GOOD: Early stopping prevents overfitting\n",
        "model = xgb.XGBRegressor(n_estimators=1000, early_stopping_rounds=50)\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_val, y_val)]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pitfall 2: High Learning Rate with Few Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ❌ BAD: Fast learning, few trees → underfitting\n",
        "model = xgb.XGBRegressor(n_estimators=10, learning_rate=0.3)\n",
        "\n",
        "# ✅ GOOD: Slow learning, many trees → better generalization\n",
        "model = xgb.XGBRegressor(n_estimators=500, learning_rate=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pitfall 3: Ignoring Feature Scaling\n",
        "\n",
        "**Good news**: Gradient boosting doesn't require feature scaling!\n",
        "\n",
        "```python\n",
        "# No need to scale for tree-based methods\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ❌ Unnecessary for XGBoost/LightGBM/CatBoost\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# ✅ Use raw features\n",
        "model.fit(X, y)  # Works great!\n",
        "```\n",
        "\n",
        "### Pitfall 4: Not Encoding Categorical Features (except CatBoost)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Data with categorical feature\n",
        "df = pd.DataFrame({\n",
        "    'size': [1200, 1500, 1800],\n",
        "    'neighborhood': ['A', 'B', 'C'],\n",
        "    'price': [200, 250, 300]\n",
        "})\n",
        "\n",
        "# ❌ BAD: XGBoost/LightGBM can't handle strings directly\n",
        "model = xgb.XGBRegressor()\n",
        "# model.fit(df[['size', 'neighborhood']], df['price'])  # ERROR!\n",
        "\n",
        "# ✅ GOOD: Encode first (for XGBoost/LightGBM)\n",
        "df['neighborhood_encoded'] = df['neighborhood'].astype('category').cat.codes\n",
        "model.fit(df[['size', 'neighborhood_encoded']], df['price'])\n",
        "\n",
        "# ✅ BEST: Use CatBoost\n",
        "model = CatBoostRegressor(cat_features=['neighborhood'])\n",
        "model.fit(df[['size', 'neighborhood']], df['price'])  # Handles strings!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation Guide\n",
        "\n",
        "```bash\n",
        "# XGBoost\n",
        "pip install xgboost\n",
        "\n",
        "# LightGBM\n",
        "pip install lightgbm\n",
        "\n",
        "# CatBoost\n",
        "pip install catboost\n",
        "\n",
        "# All at once\n",
        "pip install xgboost lightgbm catboost\n",
        "```\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "1. **Gradient boosting builds trees sequentially**, each correcting errors of previous trees\n",
        "2. **Learning rate controls** how much each tree contributes (lower = better generalization)\n",
        "3. **XGBoost, LightGBM, and CatBoost** are the three leading implementations\n",
        "4. **LightGBM** is fastest on large datasets\n",
        "5. **CatBoost** excels with categorical features\n",
        "6. **XGBoost** is the most popular and well-documented\n",
        "7. **Always use early stopping** with a validation set\n",
        "8. **Gradient boosting typically outperforms random forests** on structured data\n",
        "9. **No feature scaling needed** for tree-based methods\n",
        "10. **Trade-off**: More complex to tune than random forests, but worth it for accuracy\n",
        "\n",
        "## Practice Problems\n",
        "\n",
        "1. **Conceptual**: Explain why gradient boosting can overfit more easily than random forests, even though both use decision trees.\n",
        "\n",
        "2. **Applied**: Load the Boston housing dataset and compare:\n",
        "   - Random Forest\n",
        "   - XGBoost\n",
        "   - LightGBM\n",
        "   - CatBoost\n",
        "   \n",
        "   Use cross-validation and report mean RMSE and training time for each.\n",
        "\n",
        "3. **Hyperparameter Tuning**: \n",
        "   - Start with `n_estimators=1000, learning_rate=0.01`\n",
        "   - Then try `n_estimators=100, learning_rate=0.1`\n",
        "   - Compare results. Which works better? Why?\n",
        "\n",
        "4. **Challenge**: Create a dataset with a categorical feature that has 100 unique values. Compare:\n",
        "   - XGBoost with one-hot encoding\n",
        "   - XGBoost with label encoding\n",
        "   - CatBoost with native categorical handling\n",
        "   \n",
        "   Which performs best? Why?\n",
        "\n",
        "## Summary\n",
        "\n",
        "Gradient boosting methods represent the state-of-the-art for structured/tabular data. While they require more careful tuning than random forests, the performance gains are substantial. In practice:\n",
        "\n",
        "- **Start with XGBoost** for most problems (great documentation, wide adoption)\n",
        "- **Switch to LightGBM** if you have large datasets or need speed\n",
        "- **Use CatBoost** if you have many categorical features or want minimal tuning\n",
        "\n",
        "Understanding these methods is essential for any data scientist working with structured data, and they're often the first choice for Kaggle competitions and production ML systems.\n",
        "\n",
        "---\n",
        "\n",
        "*Next steps*: Explore ensemble methods that combine different model types, and dive deeper into hyperparameter optimization techniques like Bayesian optimization.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/tgardos/Source/courses/ds701/DS701-Course-Notes/.venv/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}