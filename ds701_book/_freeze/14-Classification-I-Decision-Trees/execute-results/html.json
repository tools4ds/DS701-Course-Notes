{
  "hash": "6752b1f3d4837c493670c4e02abd5119",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Decision Trees and Random Forests\"\njupyter: python3\nbibliography: references.bib\nnocite: |\n  @Hastie2009\n---\n\n## Outline\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/14-Classification-I-Decision-Trees.ipynb)\n\n- Build a decision tree manually\n- Look at single and collective impurity measures\n- Selecting splitting attributes and test conditions\n- Scikit-learn implementation\n- Model training and evaluation\n- Bias and Variance\n- Random forests\n\n## Introduction\n\n\n\nWe'll now start looking into how to build models to predict an outcome variable from labeled data.\n\n**Classification** problems:\n\n- predict a category\n- e.g., spam/not spam, fraud/not fraud, default/not default, malignant/benign, etc.\n\n**Regression** problems:\n\n- predict a numeric value\n- e.g., price of a house, salary of a person, etc.\n\n\n## Loan Default Example\n\nWe'll use an example from [@Tan2018].\n\n![](figs/L14-terrier-savings-logo.png){height=\"200px\"}\n\nYou are a loan officer at **Terrier Savings and Loan**. \n\nYou have a dataset on loans that you have made in the past.\n\nYou want to build a model to predict whether a loan will default.\n\n## Loans Data Set\n\n::: {#a08b5044 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Home Owner</th>\n      <th>Marital Status</th>\n      <th>Annual Income</th>\n      <th>Defaulted Borrower</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Yes</td>\n      <td>Single</td>\n      <td>125000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>No</td>\n      <td>Married</td>\n      <td>100000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>No</td>\n      <td>Single</td>\n      <td>70000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Yes</td>\n      <td>Married</td>\n      <td>120000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>No</td>\n      <td>Divorced</td>\n      <td>95000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>No</td>\n      <td>Married</td>\n      <td>60000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Yes</td>\n      <td>Divorced</td>\n      <td>220000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>No</td>\n      <td>Single</td>\n      <td>85000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>No</td>\n      <td>Married</td>\n      <td>75000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>No</td>\n      <td>Single</td>\n      <td>90000</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Loans Data Set Summary\n\nHere's the summary info of the data set.\n\n::: {#fd44d538 .cell execution_count=4}\n``` {.python .cell-code}\nloans.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 10 entries, 1 to 10\nData columns (total 4 columns):\n #   Column              Non-Null Count  Dtype \n---  ------              --------------  ----- \n 0   Home Owner          10 non-null     object\n 1   Marital Status      10 non-null     object\n 2   Annual Income       10 non-null     int64 \n 3   Defaulted Borrower  10 non-null     object\ndtypes: int64(1), object(3)\nmemory usage: 400.0+ bytes\n```\n:::\n:::\n\n\n## Convert to Categorical Data Types\n\nSince some of the fields are categorical, let's convert them to categorical data types.\n\n::: {#79191222 .cell execution_count=5}\n``` {.python .cell-code code-fold=\"false\"}\nloans['Home Owner'] = loans['Home Owner'].astype('category')\nloans['Marital Status'] = loans['Marital Status'].astype('category')\nloans['Defaulted Borrower'] = loans['Defaulted Borrower'].astype('category')\nloans.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 10 entries, 1 to 10\nData columns (total 4 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   Home Owner          10 non-null     category\n 1   Marital Status      10 non-null     category\n 2   Annual Income       10 non-null     int64   \n 3   Defaulted Borrower  10 non-null     category\ndtypes: category(3), int64(1)\nmemory usage: 570.0 bytes\n```\n:::\n:::\n\n\n## Simple Model\n\nLooking at the table, let's just start with the simplest model possible and just\npredict that no one will default.\n\nSo the output of our model is just to always predict \"No\".\n\n::: {#ee26edaf .cell execution_count=6}\n\n::: {.cell-output .cell-output-display execution_count=5}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-6-output-1.svg){fig-align='center'}\n:::\n:::\n\n\n<!-- TODO: Look at this reference to consider pydotplus for size control -->\n\n\nWe see a 30% error rate since 3 out of 10 loans defaulted.\n\n---\n\nLet's split the data based on the \"Home Owner\" field. (`values = [# No, # Yes]`).\n\n::: {#33a030c8 .cell execution_count=7}\n\n::: {.cell-output .cell-output-display execution_count=6}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-7-output-1.svg){fig-align='center'}\n:::\n:::\n\n\n::: {.fragment}\nWe see that the left node (`Home Owner == Yes`) has a 0% error rate since all the samples are `Defaulted == No`. We don't split this node since all the samples are of the same class. We call this node a **leaf node** and we'll color it orange.\n:::\n\n::: {.fragment}\nThe right node (`Home Owner == No`) has a 43% error rate since 3 out of 7 loans defaulted. \n\nLet's split this node into two nodes based on the **Marital Status** field.\n:::\n---\n\nLet's split on the \"Marital Status\" field.\n\nWe see that the 3 defaulted loans are all for single or divorced people. Since the node is\nall one class, we don't split this node and we call it a **leaf node**.\n\n::: {#f1a6efb3 .cell execution_count=8}\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-8-output-1.svg){fig-align='center'}\n:::\n:::\n\n\n--- \n\nWe can list the subsets for the two criteria to calculate the error rate.\n\n::: {#32509e28 .cell .fig-cap-location-top execution_count=9}\n``` {.python .cell-code}\nloans[(loans['Home Owner'] == \"No\") & (loans['Marital Status'].isin(['Single', 'Divorced']))]\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Home Owner</th>\n      <th>Marital Status</th>\n      <th>Annual Income</th>\n      <th>Defaulted Borrower</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>No</td>\n      <td>Single</td>\n      <td>70000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>No</td>\n      <td>Divorced</td>\n      <td>95000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>No</td>\n      <td>Single</td>\n      <td>85000</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>No</td>\n      <td>Single</td>\n      <td>90000</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n\nTable: Home Owner == No and Marital Status == Single or Divorced --> Defaulted == Yes\n:::\n:::\n\n\n::: {.fragment}\nError rate for predicting `Defaulted == Yes` is 25%.\n:::\n\n---\n\nand...\n\n::: {#6b80dd25 .cell .fig-cap-location-top execution_count=10}\n``` {.python .cell-code}\nloans[(loans['Home Owner'] == \"No\") & (loans['Marital Status'] == \"Married\")]\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Home Owner</th>\n      <th>Marital Status</th>\n      <th>Annual Income</th>\n      <th>Defaulted Borrower</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>No</td>\n      <td>Married</td>\n      <td>100000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>No</td>\n      <td>Married</td>\n      <td>60000</td>\n      <td>No</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>No</td>\n      <td>Married</td>\n      <td>75000</td>\n      <td>No</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n\nTable: Home Owner == No and Marital Status == Married --> Defaulted == No\n:::\n:::\n\n\n::: {.fragment}\nError rate for predicting `Defaulted == No` is 0%.\n:::\n\n---\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n* Let's try to split on the \"Annual Income\" field. \n\n* We see that the person with income of 70K doesn't default, so we split the node into two nodes based on the \"Income\" field. \n\n* We arbitrarily pick a threshold of $75K.\n:::\n::: {.column width=\"50%\"}\n\n::: {#d8b74298 .cell execution_count=11}\n\n::: {.cell-output .cell-output-display execution_count=10}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-11-output-1.svg){fig-align='center'}\n:::\n:::\n\n\n:::\n::::\n\n## Evaluating the Model\n\n::: {.incremental}\n* We've dispositioned every data point by walking down the tree to a leaf node.\n\n* How do we know if this tree is good? \n\n    * We arbitrarily picked the order of the fields to split on.\n\n* Is there a way to systematically pick the order of the fields to split on? \n\n    * This is called the **splitting criterion**.\n\n* There's also the question of when to stop splitting, or the **stopping criterion**. \n\n* So far, we stopped splitting when we reached a node of pure class but there are \nreasons to stop splitting even without pure classes, which we'll see later.\n:::\n\n## Specifying the Test Condition\n\nBefore we continue, we should take a moment to consider how we specify a test condition of a node.\n\nHow we specify a test condition depends on the attribute type which can be:\n\n* Binary (Boolean)\n* Nominal (Categorical, e.g., cat, dog, bird)\n* Ordinal (e.g., Small, Medium, Large)\n* Continuous (e.g., 1.5, 2.1, 3.7)\n\nAnd depends on the number of ways to split:\n\n* __multi-way__\n* __binary__\n\n---\n\nFor a __Nominal (Categorical)__ attribute:\n\n* In a __Multi-way split__ we can use as many partitions as there are distinct values of the attribute:\n\n::: {#b25edd4f .cell execution_count=12}\n\n::: {.cell-output .cell-output-display execution_count=11}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-12-output-1.svg){fig-align='center'}\n:::\n:::\n\n\n---\n\nFor a __Nominal (Categorical)__ attribute:\n\n* In a __Binary split__ we divide the values into two groups.  \n\n* In this case, we need to find an optimal partitioning of values into groups, which we discuss shortly.\n\n::: {layout-ncol=3}\n\n::: {#4308bc1c .cell execution_count=13}\n\n::: {.cell-output .cell-output-display execution_count=12}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-13-output-1.svg){}\n:::\n:::\n\n\n::: {#89acb270 .cell execution_count=14}\n\n::: {.cell-output .cell-output-display execution_count=13}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-14-output-1.svg){}\n:::\n:::\n\n\n::: {#3882b9d5 .cell execution_count=15}\n\n::: {.cell-output .cell-output-display execution_count=14}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-15-output-1.svg){}\n:::\n:::\n\n\n:::\n\n---\n\nFor an __Ordinal__ attribute, we can use a multi-way split with as many partitions\nas there are distinct values.\n\n::: {#0cbf392e .cell execution_count=16}\n\n::: {.cell-output .cell-output-display execution_count=15}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-16-output-1.svg){fig-align='center'}\n:::\n:::\n\n\n---\n\nOr we can use a binary split as long we preserve the ordering of the values.\n\n::: {layout-ncol=2}\n\n::: {#a7edca12 .cell execution_count=17}\n\n::: {.cell-output .cell-output-display execution_count=16}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-17-output-1.svg){}\n:::\n:::\n\n\n::: {#877b2cfc .cell execution_count=18}\n\n::: {.cell-output .cell-output-display execution_count=17}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-18-output-1.svg){}\n:::\n:::\n\n\n:::\n\n::: {.callout-warning}\nBe careful not to violate the ordering of values such as {Small, Large} and {Medium, X-Large}.\n:::\n\n---\n\nA __Continuous__ attribute can be handled two ways:\n\n::: {layout-ncol=2}\n\n::: {#dd654c37 .cell execution_count=19}\n\n::: {.cell-output .cell-output-display execution_count=18}\n![It can be thresholded to form a binary split.](14-Classification-I-Decision-Trees_files/figure-html/cell-19-output-1.svg){}\n:::\n:::\n\n\n::: {#40d1d1ed .cell execution_count=20}\n\n::: {.cell-output .cell-output-display execution_count=19}\n![Or it can be split into contiguous ranges to form an ordinal categorical attribute.](14-Classification-I-Decision-Trees_files/figure-html/cell-20-output-1.svg){}\n:::\n:::\n\n\n:::\n\n::: {.fragment}\n\nNote that finding good partitions for $k$ nominal attributes can be expensive, $\\mathcal{O}(2^k)$, \npossibly involving combinatorial searching of groupings.  \n\nHowever for ordinal or continuous attributes, sweeping through a range of $n$\nthreshold values can be more efficient if $n \\approx k$. $\\mathcal{O}(n)$ for a sorted list.\n:::\n\n## Selecting Attribute and Test Condition\n\n::: {.incremental}\n* Ideally, we want to pick attributes and test conditions that maximize the homogeneity of the splits.\n\n* We can use an **impurity index** to measure the homogeneity in a node.\n\n* We'll look at ways of measuring impurity of a node and then collective impurity of its child nodes.\n:::\n\n## Impurity Measures\n\nThe following are three impurity indices:\n\n$$\n\\begin{aligned}\n\\textnormal{Gini index} &= 1 - \\sum_{i=0}^{c-1}  p_i(t)^2 \\\\\n\\textnormal{Entropy} &= -\\sum_{i=0}^{c-1}  p_i(t) \\log_2 p_i(t) \\\\\n\\textnormal{Classification error} &= 1 - \\max_i p_i(t)\n\\end{aligned}\n$$\n\nwhere $p_i(t)$ is the **relative frequency** of training instances of class $i$ at a node $t$ and $c$ is the number of classes.\n\n::: {.callout-note}\nBy convention, we set $0 \\log_2 0 = 0$ in entropy calculations.\n:::\n\n\n## Impurity Measures\n\nThe following are three impurity indices:\n\n$$\n\\begin{aligned}\n\\textnormal{Gini index} &= 1 - \\sum_{i=0}^{c-1}  p_i(t)^2 \\\\\n\\textnormal{Entropy} &= -\\sum_{i=0}^{c-1}  p_i(t) \\log_2 p_i(t) \\\\\n\\textnormal{Classification error} &= 1 - \\max_i p_i(t)\n\\end{aligned}\n$$\n\n\nAll three impurity indices equal 0 when all the records at a node belong to the same class.\n\nAll three impurity indices reach their maximum value when the classes are evenly distributed among the child nodes.\n\n---\n\nWe can plot the three impurity indices to get a sense of how they behave for **binary classification** problems.\n\n::: {#5741d456 .cell execution_count=21}\n\n::: {.cell-output .cell-output-display}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-21-output-1.png){width=672 height=449 fig-align='center'}\n:::\n:::\n\n\nThey all maintain the same ordering for every relative frequency, i.e., Entropy > Gini > Misclassification error.\n\n\n## Impurity Example 1\n\n\n\n| Node $N_1$ | Count | p |\n| --- | --- | --- |\n| Class=0 | 0 | $0/6 = 0$ |\n| Class=1 | 6 | $6/6 = 1$ |\n\n<br>\n\n$$\n\\begin{aligned}\n\\textnormal{Gini} &= 1 - \\left(\\frac{0}{6}\\right)^2 - \\left(\\frac{6}{6}\\right)^2 = 0 \\\\\n\\textnormal{Entropy} &= -\\left(\\frac{0}{6} \\log_2 \\frac{0}{6} + \\frac{6}{6} \\log_2 \\frac{6}{6}\\right) = 0 \\\\\n\\textnormal{Error} &= 1 - \\max\\left[\\frac{0}{6}, \\frac{6}{6}\\right] = 0\n\\end{aligned}\n$$\n\n\n\n## Impurity Example 2\n\n\n| Node $N_2$ | Count | p |\n| --- | --- | --- |\n| Class=0 | 1 | $1/6 = 0.167$ |\n| Class=1 | 5 | $5/6 = 0.833$ |\n\n<br>\n\n$$\n\\begin{aligned}\n\\textnormal{Gini} &= 1 - \\left(\\frac{1}{6}\\right)^2 - \\left(\\frac{5}{6}\\right)^2 = 0.278 \\\\\n\\textnormal{Entropy} &= -\\left(\\frac{1}{6} \\log_2 \\frac{1}{6} + \\frac{5}{6} \\log_2 \\frac{5}{6}\\right) = 0.650 \\\\\n\\textnormal{Error} &= 1 - \\max\\left[\\frac{1}{6}, \\frac{5}{6}\\right] = 0.167\n\\end{aligned}\n$$\n\n\n\n## Impurity Example 3\n\n\n| Node $N_3$ | Count |\n| --- | --- |\n| Class=0 | 3 |\n| Class=1 | 3 |\n\n<br>\n\n$$\n\\begin{aligned}\n\\textnormal{Gini} &= 1 - \\left(\\frac{3}{6}\\right)^2 - \\left(\\frac{3}{6}\\right)^2 = 0.5 \\\\\n\\textnormal{Entropy} &= -\\left(\\frac{3}{6} \\log_2 \\frac{3}{6} + \\frac{3}{6} \\log_2 \\frac{3}{6}\\right) = 1 \\\\\n\\textnormal{Error} &= 1 - \\max\\left[\\frac{3}{6}, \\frac{3}{6}\\right] = 0.5\n\\end{aligned}\n$$\n\n## Impurity Class Exercise\n\nCalculate Gini impurity for the following node:\n\n<br>\n\n| Node $N_4$ | Count | p |\n| --- | --- | --- |\n| Class=0 | 3 |  |\n| Class=1 | 6 |  |\n| Class=2 | 1 |  |\n\n<br>\n\n$$\n\\textnormal{Gini} = \n$$\n\n<!--\n$$\n\\begin{aligned}\n\\textnormal{Gini} &= 1 - \\left(\\frac{3}{10}\\right)^2 - \\left(\\frac{6}{10}\\right)^2 - \\left(\\frac{1}{10}\\right)^2 \\\\\n&= 1 - \\left(\\frac{9}{100}\\right) - \\left(\\frac{36}{10}\\right) - \\left(\\frac{1}{100}\\right) \\\\\n&= 1 - 0.09 - 0.36 - 0.01 \\\\\n&= 0.54\n\\end{aligned}\n$$\n-->\n\n\n## Collective Impurity of Child Nodes\n\nWe can compute the collective impurity of child nodes by taking a weighted sum of the impurities of the child nodes.\n\n$$\nI(\\textnormal{children}) = \\sum_{j=1}^{k} \\frac{N(v_j)}{N}\\; I(v_j)\n$$\n\nHere we split $N$ training instances into $k$ child nodes, $v_j$ for $j=1, \\ldots, k$.\n\n$N(v_j)$ is the number of training instances at child node $v_j$ and $I(v_j)$ is the impurity at child node $v_j$.\n\n## Impurity Example\n\nLet's compute collective impurity on our loans dataset to see which feature to split on.\n\n::: {layout-ncol=3}\n\n::: {#b52b666d .cell execution_count=22}\n\n::: {.cell-output .cell-output-display execution_count=21}\n![(a) Collective Entropy: 0.690](14-Classification-I-Decision-Trees_files/figure-html/cell-22-output-1.svg){}\n:::\n:::\n\n\n::: {#54e0fb3f .cell execution_count=23}\n\n::: {.cell-output .cell-output-display execution_count=22}\n![(b) Collective Entropy: 0.686](14-Classification-I-Decision-Trees_files/figure-html/cell-23-output-1.svg){}\n:::\n:::\n\n\n::: {#0dd48dfe .cell execution_count=24}\n\n::: {.cell-output .cell-output-display execution_count=23}\n![(c) Collective Entropy index: 0.00](14-Classification-I-Decision-Trees_files/figure-html/cell-24-output-1.svg){}\n:::\n:::\n\n\n:::\n\n::: {.callout-tip}\nTry calculating the collective Entropy for (a) and (b) and see if you get the same values.\n:::\n\n::: {.callout-important}\nThe collective entropy for (c) is 0. Why would we not want to use this node?\n:::\n\n---\n\nThere are two ways to overcome this problem. \n\n1. One way is to _generate only binary decision trees_, thus avoiding the difficulty of handling attributes with varying\n   number of partitions. This strategy is employed by decision tree classifiers such as **CART**. \n2. Another way is to modify the splitting criterion to take into account the number of partitions produced by the\n   attribute. For example, in the **C4.5** decision tree algorithm, a measure known as **gain ratio** is used to compensate\n   for attributes that produce a large number of child nodes.\n\n::: aside\nCART stands for Classification And Regression Tree.\n:::\n\n## Gain Ratio\n\nSee [@Tan2018, Chap. 3, p. 127]:\n\n* Having a low impurity value alone is insufficient to find a good attribute test condition for a node. \n* Having more child nodes can make a decision tree more complex and consequently more susceptible to overfitting. \n\nHence, the number of children produced by the splitting attribute should also be taken into consideration while deciding the best attribute test condition. \n\n## Gain Ratio Formula\n\n$$\n\\text{Gain ratio} = \\frac{\\Delta_{\\text{info}}}{\\text{Split Info}} = \\frac{\\text{Entropy(Parent)} - \\sum_{i=1}^{k} \\frac{N(v_i)}{N} \\text{Entropy}(v_i)}{- \\sum_{i=1}^{k} \\frac{N(v_i)}{N} \\log_2 \\frac{N(v_i)}{N}}\n$$\n\nwhere $N(v_i)$ is the number of instances assigned to node $v_i$ and $k$ is the total number of splits. \n\nThe split information measures the entropy of splitting a node into its child nodes and evaluates if the split results\nin a larger number of equally-sized child nodes or not.\n\n## Gain Ratio Example\n\nLet's compare the Gain Ratio for **Home Owner** and **Marital Status** attributes using the loans dataset.\n\nRecall from the earlier example that the parent node has 3 Yes and 7 No defaulters (10 total instances).\n\n**Parent node entropy:**\n\n$$\n\\text{Entropy(Parent)} = -\\frac{3}{10} \\log_2 \\frac{3}{10} - \\frac{7}{10} \\log_2 \\frac{7}{10} = 0.881\n$$\n\n## Ex: Gain Ratio for Home Owner Attribute\n\nThe Home Owner attribute splits the data into 2 child nodes:\n\n- Yes: 0 Yes, 3 No (3 instances)\n- No: 3 Yes, 4 No (7 instances)\n\nFrom the earlier calculation, the Information Gain is:\n\n$$\n\\Delta_{\\text{info}} = 0.881 - \\left(\\frac{3}{10} \\times 0 + \\frac{7}{10} \\times 0.985\\right) = 0.192\n$$\n\nNow we calculate the Split Information:\n\n$$\n\\text{Split Info} = -\\frac{3}{10} \\log_2 \\frac{3}{10} - \\frac{7}{10} \\log_2 \\frac{7}{10} = 0.881\n$$\n\nTherefore, the Gain Ratio is:\n\n$$\n\\text{Gain Ratio(Home Owner)} = \\frac{0.192}{0.881} = 0.218\n$$\n\n## Ex: Gain Ratio for Marital Status Attribute\n\nThe Marital Status attribute splits the data into 3 child nodes:\n\n- Single: 2 Yes, 3 No (5 instances)\n- Married: 0 Yes, 3 No (3 instances)\n- Divorced: 1 Yes, 1 No (2 instances)\n\nFrom the earlier calculation, the Information Gain is:\n\n$$\n\\Delta_{\\text{info}} = 0.881 - \\left(\\frac{5}{10} \\times 0.971 + \\frac{3}{10} \\times 0 + \\frac{2}{10} \\times 1\\right) = 0.195\n$$\n\nNow we calculate the Split Information:\n\n$$\n\\text{Split Info} = -\\frac{5}{10} \\log_2 \\frac{5}{10} - \\frac{3}{10} \\log_2 \\frac{3}{10} - \\frac{2}{10} \\log_2 \\frac{2}{10} = 1.486\n$$\n\nTherefore, the Gain Ratio is:\n\n$$\n\\text{Gain Ratio(Marital Status)} = \\frac{0.195}{1.486} = 0.131\n$$\n\n## Example: Comparison of Gain Ratios\n\n| Attribute | Information Gain | Split Info | Gain Ratio |\n|-----------|-----------------|------------|------------|\n| Home Owner | 0.192 | 0.881 | **0.218** |\n| Marital Status | 0.195 | 1.486 | 0.131 |\n\nAlthough Marital Status has a slightly higher Information Gain, it has a **lower Gain Ratio** because it splits the data into 3 child nodes rather than 2. \n\nThe Gain Ratio penalizes attributes that create more splits, making **Home Owner** the preferred choice.\n\n## Identifying the Best Attribute Test Condition\n\n![](figs/L14-splitting-criteria-gini.png){width=\"70%\" fig-align=\"center\"}\n\nHere's an example of how to identify the best attribute test condition using the Collective Impurity of\nthe Gini Impurity index.\n\n::: {#e738df98 .cell execution_count=25}\n``` {.python .cell-code}\ndef gini_impurity(yes_count, no_count):\n    \"\"\"Calculate Gini impurity for a node\"\"\"\n    total = yes_count + no_count\n    if total == 0:\n        return 0\n    \n    p_yes = yes_count / total\n    p_no = no_count / total\n    \n    gini = 1 - (p_yes**2 + p_no**2)\n    return gini\n\ndef weighted_gini(n1_yes, n1_no, n2_yes, n2_no):\n    \"\"\"Calculate weighted average Gini impurity after split\"\"\"\n    n1_total = n1_yes + n1_no\n    n2_total = n2_yes + n2_no\n    total = n1_total + n2_total\n    \n    gini_n1 = gini_impurity(n1_yes, n1_no)\n    gini_n2 = gini_impurity(n2_yes, n2_no)\n    \n    weighted_gini = (n1_total/total) * gini_n1 + (n2_total/total) * gini_n2\n    return weighted_gini\n\n# Parent node\nparent_yes = 3\nparent_no = 7\nparent_gini = gini_impurity(parent_yes, parent_no)\n\nprint(\"=\" * 60)\nprint(\"PARENT NODE\")\nprint(\"=\" * 60)\nprint(f\"Yes: {parent_yes}, No: {parent_no}\")\nprint(f\"Gini Impurity: {parent_gini:.3f}\")\nprint()\n\n# Define all four splits\nsplits = {\n    \"Home Owner\": {\n        \"N1\": {\"yes\": 0, \"no\": 3},  # Yes (owns home)\n        \"N2\": {\"yes\": 3, \"no\": 4}   # No (doesn't own home)\n    },\n    \"Marital Status (Split 1: Single vs Married+Divorced)\": {\n        \"N1\": {\"yes\": 2, \"no\": 3},  # Single\n        \"N2\": {\"yes\": 1, \"no\": 4}   # Married, Divorced\n    },\n    \"Marital Status (Split 2: Single+Married vs Divorced)\": {\n        \"N1\": {\"yes\": 2, \"no\": 6},  # Single, Married\n        \"N2\": {\"yes\": 1, \"no\": 1}   # Divorced\n    },\n    \"Marital Status (Split 3: Single+Divorced vs Married)\": {\n        \"N1\": {\"yes\": 3, \"no\": 4},  # Single, Divorced\n        \"N2\": {\"yes\": 0, \"no\": 3}   # Married\n    }\n}\n\n# Calculate for each split\nresults = []\nfor split_name, nodes in splits.items():\n    n1_yes = nodes[\"N1\"][\"yes\"]\n    n1_no = nodes[\"N1\"][\"no\"]\n    n2_yes = nodes[\"N2\"][\"yes\"]\n    n2_no = nodes[\"N2\"][\"no\"]\n    \n    gini_n1 = gini_impurity(n1_yes, n1_no)\n    gini_n2 = gini_impurity(n2_yes, n2_no)\n    weighted_gini_value = weighted_gini(n1_yes, n1_no, n2_yes, n2_no)\n    gini_gain = parent_gini - weighted_gini_value\n    \n    results.append({\n        'name': split_name,\n        'weighted_gini': weighted_gini_value,\n        'gini_gain': gini_gain,\n        'gini_n1': gini_n1,\n        'gini_n2': gini_n2,\n        'n1_yes': n1_yes,\n        'n1_no': n1_no,\n        'n2_yes': n2_yes,\n        'n2_no': n2_no\n    })\n    \n    print(\"=\" * 60)\n    print(f\"SPLIT: {split_name}\")\n    print(\"=\" * 60)\n    print(f\"Node N1: Yes={n1_yes}, No={n1_no}, Total={n1_yes+n1_no}\")\n    print(f\"  Gini(N1) = {gini_n1:.3f}\")\n    print(f\"Node N2: Yes={n2_yes}, No={n2_no}, Total={n2_yes+n2_no}\")\n    print(f\"  Gini(N2) = {gini_n2:.3f}\")\n    print(f\"\\nWeighted Gini (after split): {weighted_gini_value:.3f}\")\n    print(f\"Gini Gain: {gini_gain:.3f}\")\n    print()\n\n# Summary: Find best split\nprint(\"=\" * 60)\nprint(\"SUMMARY - BEST SPLIT\")\nprint(\"=\" * 60)\nresults_sorted = sorted(results, key=lambda x: x['gini_gain'], reverse=True)\nfor i, result in enumerate(results_sorted, 1):\n    print(f\"{i}. {result['name']}\")\n    print(f\"   Weighted Gini: {result['weighted_gini']:.3f}, Gini Gain: {result['gini_gain']:.3f}\")\nprint()\nprint(f\"Best split: {results_sorted[0]['name']}\")\nprint(f\"Maximum Gini Gain: {results_sorted[0]['gini_gain']:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n============================================================\nPARENT NODE\n============================================================\nYes: 3, No: 7\nGini Impurity: 0.420\n\n============================================================\nSPLIT: Home Owner\n============================================================\nNode N1: Yes=0, No=3, Total=3\n  Gini(N1) = 0.000\nNode N2: Yes=3, No=4, Total=7\n  Gini(N2) = 0.490\n\nWeighted Gini (after split): 0.343\nGini Gain: 0.077\n\n============================================================\nSPLIT: Marital Status (Split 1: Single vs Married+Divorced)\n============================================================\nNode N1: Yes=2, No=3, Total=5\n  Gini(N1) = 0.480\nNode N2: Yes=1, No=4, Total=5\n  Gini(N2) = 0.320\n\nWeighted Gini (after split): 0.400\nGini Gain: 0.020\n\n============================================================\nSPLIT: Marital Status (Split 2: Single+Married vs Divorced)\n============================================================\nNode N1: Yes=2, No=6, Total=8\n  Gini(N1) = 0.375\nNode N2: Yes=1, No=1, Total=2\n  Gini(N2) = 0.500\n\nWeighted Gini (after split): 0.400\nGini Gain: 0.020\n\n============================================================\nSPLIT: Marital Status (Split 3: Single+Divorced vs Married)\n============================================================\nNode N1: Yes=3, No=4, Total=7\n  Gini(N1) = 0.490\nNode N2: Yes=0, No=3, Total=3\n  Gini(N2) = 0.000\n\nWeighted Gini (after split): 0.343\nGini Gain: 0.077\n\n============================================================\nSUMMARY - BEST SPLIT\n============================================================\n1. Home Owner\n   Weighted Gini: 0.343, Gini Gain: 0.077\n2. Marital Status (Split 3: Single+Divorced vs Married)\n   Weighted Gini: 0.343, Gini Gain: 0.077\n3. Marital Status (Split 1: Single vs Married+Divorced)\n   Weighted Gini: 0.400, Gini Gain: 0.020\n4. Marital Status (Split 2: Single+Married vs Divorced)\n   Weighted Gini: 0.400, Gini Gain: 0.020\n\nBest split: Home Owner\nMaximum Gini Gain: 0.077\n```\n:::\n:::\n\n\n## Splitting Continuous Attributes\n\nFor quantitative attributes like _Annual Income_, we need to find some threshold $\\tau$ that\nminimizes the impurity index.\n\nThe following table illustrates the process.\n\n![](figs/L14-splitting-continuous-attribs.png)\n\n\n**Procedure:**\n\n1. Sort all the training instances by _Annual Income_ in increasing order.\n2. Pick thresholds half way between consecutive values.\n3. Compute the Gini impurity index for each threshold.\n4. Select the threshold that minimizes the Gini impurity index.\n\n::: {#d4773da3 .cell execution_count=26}\n``` {.python .cell-code}\ndef gini_impurity(yes_count, no_count):\n    \"\"\"Calculate Gini impurity for a node\"\"\"\n    total = yes_count + no_count\n    if total == 0:\n        return 0\n    \n    p_yes = yes_count / total\n    p_no = no_count / total\n    \n    gini = 1 - (p_yes**2 + p_no**2)\n    return gini\n\ndef weighted_gini_split(left_yes, left_no, right_yes, right_no):\n    \"\"\"Calculate weighted Gini impurity for a binary split\"\"\"\n    left_total = left_yes + left_no\n    right_total = right_yes + right_no\n    total = left_total + right_total\n    \n    if total == 0:\n        return 0\n    \n    gini_left = gini_impurity(left_yes, left_no)\n    gini_right = gini_impurity(right_yes, right_no)\n    \n    weighted_gini = (left_total/total) * gini_left + (right_total/total) * gini_right\n    return weighted_gini\n\n# Data from the table\nsorted_values = [60, 70, 75, 85, 90, 95, 100, 120, 125, 220]\nclasses = ['No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No']\n\n# Calculate split positions (midpoints between consecutive values)\nsplit_positions = []\nfor i in range(len(sorted_values) - 1):\n    split_pos = (sorted_values[i] + sorted_values[i+1]) / 2\n    split_positions.append(split_pos)\n\n# Parent node statistics\nparent_yes = classes.count('Yes')\nparent_no = classes.count('No')\nparent_gini = gini_impurity(parent_yes, parent_no)\n\nprint(\"=\" * 80)\nprint(\"CONTINUOUS ATTRIBUTE: Annual Income\")\nprint(\"=\" * 80)\nprint(f\"Parent Node: Yes={parent_yes}, No={parent_no}\")\nprint(f\"Parent Gini Impurity: {parent_gini:.3f}\")\nprint()\n\nprint(\"=\" * 80)\nprint(\"EVALUATING ALL POSSIBLE SPLITS\")\nprint(\"=\" * 80)\n\nbest_split = None\nbest_gini = float('inf')\nbest_gain = -float('inf')\n\nresults = []\n\nfor i, split_pos in enumerate(split_positions):\n    # Count classes on left (<= split) and right (> split)\n    left_yes = 0\n    left_no = 0\n    right_yes = 0\n    right_no = 0\n    \n    for j, value in enumerate(sorted_values):\n        if value <= split_pos:\n            if classes[j] == 'Yes':\n                left_yes += 1\n            else:\n                left_no += 1\n        else:\n            if classes[j] == 'Yes':\n                right_yes += 1\n            else:\n                right_no += 1\n    \n    # Calculate weighted Gini\n    weighted_gini = weighted_gini_split(left_yes, left_no, right_yes, right_no)\n    gini_gain = parent_gini - weighted_gini\n    \n    results.append({\n        'split_position': split_pos,\n        'left_yes': left_yes,\n        'left_no': left_no,\n        'right_yes': right_yes,\n        'right_no': right_no,\n        'weighted_gini': weighted_gini,\n        'gini_gain': gini_gain\n    })\n    \n    print(f\"Split at {split_pos:6.1f}:\")\n    print(f\"  Left  (<=): Yes={left_yes}, No={left_no}, Total={left_yes+left_no}\")\n    print(f\"  Right ( >): Yes={right_yes}, No={right_no}, Total={right_yes+right_no}\")\n    print(f\"  Weighted Gini: {weighted_gini:.3f}\")\n    print(f\"  Gini Gain: {gini_gain:.3f}\")\n    \n    if weighted_gini < best_gini:\n        best_gini = weighted_gini\n        best_split = split_pos\n        best_gain = gini_gain\n    \n    print()\n\nprint(\"=\" * 80)\nprint(\"BEST SPLIT\")\nprint(\"=\" * 80)\nprint(f\"Best Split Position: {best_split}\")\nprint(f\"Minimum Weighted Gini: {best_gini:.3f}\")\nprint(f\"Maximum Gini Gain: {best_gain:.3f}\")\n\n# Show the split details\nbest_result = [r for r in results if r['split_position'] == best_split][0]\nprint(f\"\\nSplit Details (Annual Income <= {best_split}):\")\nprint(f\"  Left  (<=): Yes={best_result['left_yes']}, No={best_result['left_no']}\")\nprint(f\"  Right ( >): Yes={best_result['right_yes']}, No={best_result['right_no']}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n================================================================================\nCONTINUOUS ATTRIBUTE: Annual Income\n================================================================================\nParent Node: Yes=3, No=7\nParent Gini Impurity: 0.420\n\n================================================================================\nEVALUATING ALL POSSIBLE SPLITS\n================================================================================\nSplit at   65.0:\n  Left  (<=): Yes=0, No=1, Total=1\n  Right ( >): Yes=3, No=6, Total=9\n  Weighted Gini: 0.400\n  Gini Gain: 0.020\n\nSplit at   72.5:\n  Left  (<=): Yes=0, No=2, Total=2\n  Right ( >): Yes=3, No=5, Total=8\n  Weighted Gini: 0.375\n  Gini Gain: 0.045\n\nSplit at   80.0:\n  Left  (<=): Yes=0, No=3, Total=3\n  Right ( >): Yes=3, No=4, Total=7\n  Weighted Gini: 0.343\n  Gini Gain: 0.077\n\nSplit at   87.5:\n  Left  (<=): Yes=1, No=3, Total=4\n  Right ( >): Yes=2, No=4, Total=6\n  Weighted Gini: 0.417\n  Gini Gain: 0.003\n\nSplit at   92.5:\n  Left  (<=): Yes=2, No=3, Total=5\n  Right ( >): Yes=1, No=4, Total=5\n  Weighted Gini: 0.400\n  Gini Gain: 0.020\n\nSplit at   97.5:\n  Left  (<=): Yes=3, No=3, Total=6\n  Right ( >): Yes=0, No=4, Total=4\n  Weighted Gini: 0.300\n  Gini Gain: 0.120\n\nSplit at  110.0:\n  Left  (<=): Yes=3, No=4, Total=7\n  Right ( >): Yes=0, No=3, Total=3\n  Weighted Gini: 0.343\n  Gini Gain: 0.077\n\nSplit at  122.5:\n  Left  (<=): Yes=3, No=5, Total=8\n  Right ( >): Yes=0, No=2, Total=2\n  Weighted Gini: 0.375\n  Gini Gain: 0.045\n\nSplit at  172.5:\n  Left  (<=): Yes=3, No=6, Total=9\n  Right ( >): Yes=0, No=1, Total=1\n  Weighted Gini: 0.400\n  Gini Gain: 0.020\n\n================================================================================\nBEST SPLIT\n================================================================================\nBest Split Position: 97.5\nMinimum Weighted Gini: 0.300\nMaximum Gini Gain: 0.120\n\nSplit Details (Annual Income <= 97.5):\n  Left  (<=): Yes=3, No=3\n  Right ( >): Yes=0, No=4\n```\n:::\n:::\n\n\n## Run Decision Tree on Loans Data Set\n\nLet's run the Scikit-learn Decision Tree, `sklearn.tree`, on the loans data set.\n\n`sklearn.tree` requires all fields to be numeric.\n\nSo first we have to convert the categorical fields to category index numeric fields.\n\n::: {#94d464c8 .cell execution_count=27}\n``` {.python .cell-code code-fold=\"false\"}\nloans['Defaulted Borrower'] = loans['Defaulted Borrower'].cat.codes\nloans['Home Owner'] = loans['Home Owner'].cat.codes\nloans['Marital Status'] = loans['Marital Status'].cat.codes\nloans.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Home Owner</th>\n      <th>Marital Status</th>\n      <th>Annual Income</th>\n      <th>Defaulted Borrower</th>\n    </tr>\n    <tr>\n      <th>ID</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2</td>\n      <td>125000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>100000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>2</td>\n      <td>70000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>120000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>95000</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\nThen the independent variables are all the fields except the \"Defaulted Borrower\" field, which we'll assign to `X`.\n\nThe dependent variable is the \"Defaulted Borrower\" field, which we'll assign to `y`.\n\n::: {#0f26b61b .cell execution_count=28}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn import tree\n\nX = loans.drop('Defaulted Borrower', axis=1)\ny = loans['Defaulted Borrower']\n```\n:::\n\n\n<br>\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n`X`:\n\n::: {#e18c735c .cell execution_count=29}\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 10 entries, 1 to 10\nData columns (total 3 columns):\n #   Column          Non-Null Count  Dtype\n---  ------          --------------  -----\n 0   Home Owner      10 non-null     int8 \n 1   Marital Status  10 non-null     int8 \n 2   Annual Income   10 non-null     int64\ndtypes: int64(1), int8(2)\nmemory usage: 180.0 bytes\n```\n:::\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n`y`:\n\n::: {#2a8a75d0 .cell execution_count=30}\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.series.Series'>\nIndex: 10 entries, 1 to 10\nSeries name: Defaulted Borrower\nNon-Null Count  Dtype\n--------------  -----\n10 non-null     int8 \ndtypes: int8(1)\nmemory usage: 90.0 bytes\n```\n:::\n:::\n\n\n:::\n::::\n\n\n---\n\nLet's fit a decision tree to the data.\n\n::: {#4fdcce75 .cell execution_count=31}\n``` {.python .cell-code code-fold=\"false\"}\nclf = tree.DecisionTreeClassifier(criterion='gini', random_state=42)\nclf = clf.fit(X, y)\n```\n:::\n\n\nLet's plot the tree.\n\n::: {#8312efdf .cell execution_count=32}\n\n::: {.cell-output .cell-output-display}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-32-output-1.png){width=540 height=389}\n:::\n:::\n\n\nInterestingly, the tree was built using only the Income field.\n\nThat's arguably an advantage of Decision Trees: they automatically perform feature selection.\n\n\n## Ensemble Methods\n\n(See @Tan2018, Chapter 4)\n\nMotivated around the idea that combining several noisy classifiers can result in a better prediction\nunder certain conditions.\n\n* The base classifiers are independent\n* The base classifiers are noisy (high variance)\n* The base classifiers are low (ideally zero) bias\n\n## Bias and Variance\n\n__Bias__\n\n* Definition: Error due to overly simplistic models.\n* High bias: Model underfits the data.\n    * Example: Shallow decision trees.\n* Low bias: Model accurately captures the underlying patterns in the data.\n    * Example: Deep decision trees.\n\n__Variance__\n\n* Definition: Error due to overly complex models.\n* High Variance: Model overfits the data.\n    * Example: Deep decision trees.\n* Low variance: Model predictions are stable and consistent across different training datasets.\n\n---\n\n![](figs/bias_variance_tradeoff.png){fig-align=\"center\" width=\"60%\"}\n\n## Bias Variance Trade-Off\n\nGoal: Find a balance to minimize total error.\n\nBias-Variance Trade-off: Low bias and low variance are ideal but challenging to achieve simultaneously.\n\n![[Source](https://serokell.io/blog/bias-variance-tradeoff)](figs/bias_variance_tradeoff2.png){fig-align=\"center\" width=\"60%\"}\n\n## Random Forests\n\nRandom forests are an ensemble of decision trees that:\n\n* Construct a set of base classifiers from random sub-samples of the training data.\n* Train each base classifier to completion.\n* Take a majority vote of the base classifiers to form the final prediction.\n\n## Titanic Example\n\nWe'll use the [Titanic data set](https://www.kaggle.com/competitions/titanic) and\nexcerpts of this [Kaggle tutorial](https://www.kaggle.com/code/jhoward/how-random-forests-really-work/)\nto illustrate the concepts of overfitting and random forests.\n\n::: {#75a8eb7d .cell execution_count=33}\n``` {.python .cell-code}\nimport pandas as pd\n\nimport os\nimport urllib.request\n\n# Check if the directory exists, if not, create it\nif not os.path.exists('data/titanic'):\n    os.makedirs('data/titanic')\n\nif not os.path.exists('data/titanic/train.csv'):\n    url = 'https://raw.githubusercontent.com/tools4ds/DS701-Course-Notes/refs/heads/main/ds701_book/data/titanic/train.csv'\n    urllib.request.urlretrieve(url, 'data/titanic/train.csv')\n\ndf_train = pd.read_csv('data/titanic/train.csv', index_col='PassengerId')\n\nif not os.path.exists('data/titanic/test.csv'):\n    url = 'https://raw.githubusercontent.com/tools4ds/DS701-Course-Notes/refs/heads/main/ds701_book/data/titanic/test.csv'\n    urllib.request.urlretrieve(url, 'data/titanic/test.csv')\n\ndf_test = pd.read_csv('data/titanic/test.csv', index_col='PassengerId')\n```\n:::\n\n\n---\n\nLet's look at the training data.\n\n::: {#ebedc480 .cell execution_count=34}\n``` {.python .cell-code}\ndf_train.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 891 entries, 1 to 891\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Survived  891 non-null    int64  \n 1   Pclass    891 non-null    int64  \n 2   Name      891 non-null    object \n 3   Sex       891 non-null    object \n 4   Age       714 non-null    float64\n 5   SibSp     891 non-null    int64  \n 6   Parch     891 non-null    int64  \n 7   Ticket    891 non-null    object \n 8   Fare      891 non-null    float64\n 9   Cabin     204 non-null    object \n 10  Embarked  889 non-null    object \ndtypes: float64(2), int64(4), object(5)\nmemory usage: 83.5+ KB\n```\n:::\n:::\n\n\nWe can see that there are 891 entries with 11 fields. 'Age', 'Cabin', and 'Embarked' have missing values.\n\n---\n\n::: {#8f9ba293 .cell execution_count=35}\n``` {.python .cell-code}\ndf_train.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n    <tr>\n      <th>PassengerId</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n---\n\nLet's look at the test data.\n\n::: {#0f2de16c .cell execution_count=36}\n``` {.python .cell-code}\ndf_test.info()\ndf_test.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 418 entries, 892 to 1309\nData columns (total 10 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    418 non-null    int64  \n 1   Name      418 non-null    object \n 2   Sex       418 non-null    object \n 3   Age       332 non-null    float64\n 4   SibSp     418 non-null    int64  \n 5   Parch     418 non-null    int64  \n 6   Ticket    418 non-null    object \n 7   Fare      417 non-null    float64\n 8   Cabin     91 non-null     object \n 9   Embarked  418 non-null    object \ndtypes: float64(2), int64(3), object(5)\nmemory usage: 35.9+ KB\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=35}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n    <tr>\n      <th>PassengerId</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>892</th>\n      <td>3</td>\n      <td>Kelly, Mr. James</td>\n      <td>male</td>\n      <td>34.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>330911</td>\n      <td>7.8292</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>893</th>\n      <td>3</td>\n      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n      <td>female</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>363272</td>\n      <td>7.0000</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>894</th>\n      <td>2</td>\n      <td>Myles, Mr. Thomas Francis</td>\n      <td>male</td>\n      <td>62.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>240276</td>\n      <td>9.6875</td>\n      <td>NaN</td>\n      <td>Q</td>\n    </tr>\n    <tr>\n      <th>895</th>\n      <td>3</td>\n      <td>Wirz, Mr. Albert</td>\n      <td>male</td>\n      <td>27.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>315154</td>\n      <td>8.6625</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>896</th>\n      <td>3</td>\n      <td>Hirvonen, Mrs. Alexander (Helga E Lindqvist)</td>\n      <td>female</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3101298</td>\n      <td>12.2875</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThere are 418 entries in the test set with same fields except for 'Survived', which is what we need to predict.\n\n---\n\nWe'll do some data cleaning and preparation.\n\n::: {#206eef53 .cell execution_count=37}\n``` {.python .cell-code}\nimport numpy as np\n\ndef proc_data(df):\n    df['Fare'] = df.Fare.fillna(0)\n    df.fillna(modes, inplace=True) # Fill missing values with the mode\n    df['LogFare'] = np.log1p(df['Fare'])  # Create a new column for the log of the fare + 1\n    df['Embarked'] = pd.Categorical(df.Embarked)  # Convert to categorical\n    df['Sex'] = pd.Categorical(df.Sex)  # Convert to categorical\n\nmodes = df_train.mode().iloc[0] # Get the mode for each column\n\nproc_data(df_train)\nproc_data(df_test)\n```\n:::\n\n\nLook at the dataframes again.\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n\n::: {#4d1eddf3 .cell execution_count=38}\n``` {.python .cell-code}\ndf_train.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 891 entries, 1 to 891\nData columns (total 12 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   Survived  891 non-null    int64   \n 1   Pclass    891 non-null    int64   \n 2   Name      891 non-null    object  \n 3   Sex       891 non-null    category\n 4   Age       891 non-null    float64 \n 5   SibSp     891 non-null    int64   \n 6   Parch     891 non-null    int64   \n 7   Ticket    891 non-null    object  \n 8   Fare      891 non-null    float64 \n 9   Cabin     891 non-null    object  \n 10  Embarked  891 non-null    category\n 11  LogFare   891 non-null    float64 \ndtypes: category(2), float64(3), int64(4), object(3)\nmemory usage: 78.6+ KB\n```\n:::\n:::\n\n\n:::\n::: {.column width=\"50%\"}\n\n::: {#6816f974 .cell execution_count=39}\n``` {.python .cell-code}\ndf_test.info()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<class 'pandas.core.frame.DataFrame'>\nIndex: 418 entries, 892 to 1309\nData columns (total 11 columns):\n #   Column    Non-Null Count  Dtype   \n---  ------    --------------  -----   \n 0   Pclass    418 non-null    int64   \n 1   Name      418 non-null    object  \n 2   Sex       418 non-null    category\n 3   Age       418 non-null    float64 \n 4   SibSp     418 non-null    int64   \n 5   Parch     418 non-null    int64   \n 6   Ticket    418 non-null    object  \n 7   Fare      418 non-null    float64 \n 8   Cabin     418 non-null    object  \n 9   Embarked  418 non-null    category\n 10  LogFare   418 non-null    float64 \ndtypes: category(2), float64(3), int64(3), object(3)\nmemory usage: 33.7+ KB\n```\n:::\n:::\n\n\n:::\n::::\n\n---\n\nWe'll create lists of features by type.\n\n::: {#82c41b4e .cell execution_count=40}\n``` {.python .cell-code code-fold=\"false\"}\ncats=[\"Sex\",\"Embarked\"]  # Categorical\nconts=['Age', 'SibSp', 'Parch', 'LogFare',\"Pclass\"]  # Continuous\ndep=\"Survived\"  # Dependent variable\n```\n:::\n\n\n---\n\nLet's explore some fields starting with survival rate by gender.\n\n::: {#d5d44571 .cell execution_count=41}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.barplot(data=df_train, y='Survived', x=\"Sex\", ax=axs[0], hue=\"Sex\", palette=[\"#3374a1\",\"#e1812d\"]).set(title=\"Survival rate\")\nsns.countplot(data=df_train, x=\"Sex\", ax=axs[1], hue=\"Sex\", palette=[\"#3374a1\",\"#e1812d\"]).set(title=\"Histogram\");\n```\n\n::: {.cell-output .cell-output-display}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-41-output-1.png){width=886 height=449}\n:::\n:::\n\n\nIndeed, \"women and children first\" was enforced on the Titanic.\n\n---\n\nSince we don't have labels for the test data, we'll split the training data into training and validation.\n\n::: {#af92e03c .cell execution_count=42}\n``` {.python .cell-code code-fold=\"false\"}\nfrom numpy import random\nfrom sklearn.model_selection import train_test_split\n\nrandom.seed(42)\ntrn_df,val_df = train_test_split(df_train, test_size=0.25)\n\n# Replace categorical fields with numeric codes\ntrn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes)\nval_df[cats] = val_df[cats].apply(lambda x: x.cat.codes)\n```\n:::\n\n\n---\n\nLet's split the independent (input) variables from the dependent (output) variable.\n\n::: {#1fd7ff53 .cell execution_count=43}\n``` {.python .cell-code code-fold=\"false\"}\ndef xs_y(df):\n    xs = df[cats+conts].copy()\n    return xs,df[dep] if dep in df else None\n\ntrn_xs,trn_y = xs_y(trn_df)\nval_xs,val_y = xs_y(val_df)\n```\n:::\n\n\n---\n\nHere's the predictions for our extremely simple model, where `female` is coded as `0`:\n\n::: {#d8fd24b6 .cell execution_count=44}\n``` {.python .cell-code code-fold=\"false\"}\npreds = val_xs.Sex==0\n```\n:::\n\n\nWe'll use mean absolute error to measure how good this model is:\n\n::: {#21b24ea8 .cell execution_count=45}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.metrics import mean_absolute_error\nmean_absolute_error(val_y, preds)\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\n0.21524663677130046\n```\n:::\n:::\n\n\n---\n\nAlternatively, we could try splitting on a continuous column. We have to use a somewhat different chart to see how this might work -- here's an example of how we could look at `LogFare`:\n\n::: {#523d4534 .cell execution_count=46}\n``` {.python .cell-code}\ndf_fare = trn_df[trn_df.LogFare>0]\nfig,axs = plt.subplots(1,2, figsize=(11,5))\nsns.boxenplot(data=df_fare, x=dep, y=\"LogFare\", ax=axs[0], hue=dep, palette=[\"#3374a1\",\"#e1812d\"])\nsns.kdeplot(data=df_fare, x=\"LogFare\", ax=axs[1]);\n```\n\n::: {.cell-output .cell-output-display}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-46-output-1.png){width=874 height=429}\n:::\n:::\n\n\nThe [boxenplot](https://seaborn.pydata.org/generated/seaborn.boxenplot.html) above shows quantiles of `LogFare` for each group of `Survived==0` and `Survived==1`. \n\nIt shows that the average `LogFare` for passengers that didn't survive is around `2.5`, and for those that did it's around `3.2`. \n\nSo it seems that people that paid more for their tickets were more likely to get put on a lifeboat.\n\n---\n\nLet's create a simple model based on this observation:\n\n::: {#78e46ed6 .cell execution_count=47}\n``` {.python .cell-code code-fold=\"false\"}\npreds = val_xs.LogFare>2.7\n```\n:::\n\n\n...and test it out:\n\n::: {#5a96fcc2 .cell execution_count=48}\n``` {.python .cell-code code-fold=\"false\"}\nmean_absolute_error(val_y, preds)\n```\n\n::: {.cell-output .cell-output-display execution_count=47}\n```\n0.336322869955157\n```\n:::\n:::\n\n\nThis is quite a bit less accurate than our model that used `Sex` as the single binary split.\n\n## Full Decision Tree\n\nOk. Let's build a decision tree model using all the features.\n\n::: {#602f429d .cell execution_count=49}\n``` {.python .cell-code code-fold=\"false\"}\nclf = tree.DecisionTreeClassifier(criterion='gini', random_state=42)\nclf = clf.fit(trn_xs, trn_y)\n```\n:::\n\n\nLet's draw the tree.\n\n::: {#a2f2d1bc .cell execution_count=50}\n``` {.python .cell-code code-fold=\"false\"}\nannotations = tree.plot_tree(clf, \n               filled=True, \n               rounded=True,\n               feature_names=trn_xs.columns,\n               class_names=['No', 'Yes'])\n```\n\n::: {.cell-output .cell-output-display}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-50-output-1.png){width=541 height=389}\n:::\n:::\n\n\n## Full Tree -- Evaluation Error\n\nLet's see how it does on the validation set.\n\n::: {#250f4443 .cell execution_count=51}\n``` {.python .cell-code code-fold=\"false\"}\npreds = clf.predict(val_xs)\nmean_absolute_error(val_y, preds)\n```\n\n::: {.cell-output .cell-output-display execution_count=50}\n```\n0.2645739910313901\n```\n:::\n:::\n\n\n:::: {.fragment}\nThat is quite a bit worse than splitting on `Sex` alone!!\n::::\n\n## Stopping Criteria -- Minimum Samples Split\n\nLet's train the decision tree again but with stopping criteria based on the number of samples in a node.\n\n::: {#87fe364b .cell execution_count=52}\n``` {.python .cell-code code-fold=\"false\"}\nclf = tree.DecisionTreeClassifier(criterion='gini', random_state=42, min_samples_split=20)\nclf = clf.fit(trn_xs, trn_y)\n```\n:::\n\n\nLet's draw the tree.\n\n::: {#913d9a2b .cell execution_count=53}\n``` {.python .cell-code code-fold=\"false\"}\nannotations = tree.plot_tree(clf, \n               filled=True, \n               rounded=True,\n               feature_names=trn_xs.columns,\n               class_names=['No', 'Yes'])\n```\n\n::: {.cell-output .cell-output-display}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-53-output-1.png){width=540 height=389}\n:::\n:::\n\n\n## Min Samples Split -- Evaluation Error\n\nLet's see how it does on the validation set.\n\n::: {#e40853a1 .cell execution_count=54}\n``` {.python .cell-code code-fold=\"false\"}\npreds = clf.predict(val_xs)\nmean_absolute_error(val_y, preds)\n```\n\n::: {.cell-output .cell-output-display execution_count=53}\n```\n0.18834080717488788\n```\n:::\n:::\n\n\n## Decision Tree -- Maximum Depth\n\nLet's train the decision tree again but with a maximum depth of 3.\n\n::: {#af7973b2 .cell execution_count=55}\n``` {.python .cell-code code-fold=\"false\"}\nclf = tree.DecisionTreeClassifier(criterion='gini', random_state=42, max_depth=3)\nclf = clf.fit(trn_xs, trn_y)\n```\n:::\n\n\nLet's draw the tree.\n\n::: {#44517088 .cell execution_count=56}\n``` {.python .cell-code code-fold=\"false\"}\nannotations = tree.plot_tree(clf, \n               filled=True, \n               rounded=True,\n               feature_names=trn_xs.columns,\n               class_names=['No', 'Yes'])\n```\n\n::: {.cell-output .cell-output-display}\n![](14-Classification-I-Decision-Trees_files/figure-html/cell-56-output-1.png){width=540 height=389}\n:::\n:::\n\n\n## Maximum Depth -- Evaluation Error\n\nLet's see how it does on the validation set.\n\n::: {#714fee04 .cell execution_count=57}\n``` {.python .cell-code code-fold=\"false\"}\npreds = clf.predict(val_xs)\nmean_absolute_error(val_y, preds)\n```\n\n::: {.cell-output .cell-output-display execution_count=56}\n```\n0.19730941704035873\n```\n:::\n:::\n\n\n## Random Forest\n\nLet's try a random forest.\n\n::: {#3b658535 .cell execution_count=58}\n``` {.python .cell-code code-fold=\"false\"}\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf = clf.fit(trn_xs, trn_y)\n```\n:::\n\n\nLet's see how it does on the validation set.\n\n::: {#05cf6944 .cell execution_count=59}\n``` {.python .cell-code code-fold=\"false\"}\npreds = clf.predict(val_xs)\nmean_absolute_error(val_y, preds)\n```\n\n::: {.cell-output .cell-output-display execution_count=58}\n```\n0.21076233183856502\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n\n## Recap\n\n* Decision Trees\n* Impurity Measures\n* Avoiding Overfitting\n* Random Forests\n\n:::\n\n## References\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "14-Classification-I-Decision-Trees_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}