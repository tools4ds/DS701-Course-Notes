{
  "hash": "4854243aa82388bc3ecc359ae06da5ad",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Natural Language Processing'\njupyter: python3\nbibliography: references.bib\n---\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/27-RNN.ipynb)\n\nNatural language processing (NLP) is a subfield of artificial intelligence that allows computers to understand, process, and manipulate human language.\n\n# History of NLP\n\n- The Dawn of NLP (1950-1970s)\n- The Statistical Revolution (1980s-2000s)\n- The Deep Learning Era (2000s-present)\n\n## The Dawn of NLP\n\nNLP has its origin ins the 1950s. Two important figures in the development of computational NLP.\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n- **Alan Turing**. British mathematician, logician, and computer scientist. Considered the father of modern computer science.\n  - *Computing Machinery and Intelligence*. Turing proposed a criterion for determining a machine's intelligence. \n  - This criterion is know as the **Turing Test**. \n  - This test involves the interpretation and generation of natural language by a computer.\n:::\n::: {.column width=\"40%\"}\n![](figs/AlanTuring.png){fig-align=\"center\"}\n:::\n::::\n\n---\n\n## Turing Test\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n- An interrogator communicates with a computer and a human. \n- The computer and human attempt to convince the interrogator that they are human. \n- If the interrogator cannot determine who is human, the computer wins the test. \n:::\n::: {.column width=50\"%\"}\n![](figs/TuringTestDiagram.png)\n:::\n::::\n\n:::: {.fragment}\nDo you think LLMs are able to pass the Turing Test?\n::::\n\n---\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n- **Noam Chomsky**. Is an American professor and public intellectual known for his work in linguistics, political activism and social criticism. He worked at MIT (retiring in 2002).\n  - His book, *Syntactic Structures*, revolutionized the scientific study of language.\n  - He proposed a mathematical theory of language that introduces a generative model which enumerates the sentences in a language.\n  - This work helped lay the foundation for modern NLP techniques.\n:::\n::: {.column width=\"50%\"}\n![](figs/NoamChomsky.png){fig-align=\"center\"}\n:::\n::::\n\n## Rule-Based Systems\n\nThe NLP research of the 1950's focused on rules-based systems. Linguists would craft large sets of rules to capture the grammatical syntax and vocabulary of specific languages.\n\nThough important at the time, these systems were quite limited. They weren't able to capture the nuances and many exceptions in languages. They were poor at capturing slang.\n\nThe creation and maintenance of such rule based systems for every language was not scalable.\n\nThese systems focused predominantly on syntax and vocabulary. They were not able to capture the deeper meaning and context of the texts they were analyzing.\n\n:::: {.fragment}\nSee for example [WordNet](https://wordnet.princeton.edu/), ([Wikpedia:WordNet](https://en.wikipedia.org/wiki/WordNet)).\n::::\n\n\n## The Statistical Revolution\n\nDue to the limitations or rule-based systems coupled with a steady increase in computational power and large collections of data, this period of time saw the advent of machine learning algorithms applied to language processing.\n\nIn contrast to rule-based systems, these statistical models learned patterns from data. This allowed them to better handle the variations and complexities of natural language. IBM research developed a set of important machine translation models, called the [IBM alignment models](http://www2.statmt.org/survey/Topic/IBMModels).  \n\nThe concepts of Recurrent Neural Networks were introduced in 1986 in the paper, \"Learning Representations by Back-Propagating Errors,\" by [@rumelhart1986learning].\n\nIn addition, the use of n-gram models became more formalized and widely adopted. An n-gram language model predicts the probability of a word based on the previous n-1 words in a sequence, making it a fundamental tool in natural language processing for tasks like text prediction and speech recognition.\n\n## The Deep Learning Era\n\nThe 2000s ushered in the era of deep learning. This is when we saw the application of neural networks to NLP.\n\nHere are some of the notable developments.\n\n- **@bengio2000neural**. Used feed-forward neural networks as a language model. Significantly outperformed n-gram models.\n- **@sutskever2014sequence** Used LSTMs with an encoder-decoder architecture which produced (at the time) state-of-the-art results for machine translation.\n- **@bahdanau2014neural** Proposed the concept of attention in RNNs. Attention is a mechanism that allows models to focus on specific parts of the input sequence when generating each part of the output sequence, improving the handling of long-range dependencies and context.\n- **@vaswani2017attention** Introduce the Transformer architecture. This allows for parallelization of sequential data with attention. Transformers become the basis for LLMs.\n\n# Lecture Outline\n\nFor the rest of this lecture we will cover:\n\n- Numerical representations of words\n- Language models\n  - N-gram models\n  - Transformers\n- Transformer Architectures\n\n\n# Numerical Representation of Words\n\n## Numerical Representations of Words\n\nMachine learning models for NLP are not able to process text in the form of characters and strings. Characters and strings must be converted to numbers in order to train our language models.\n\nThere are a number of ways to do this. These include\n\n- sparse representations, like one-hot encodings and TF-IDF encodings\n- word embeddings.\n\nHowever, prior to creating a numerical representation of text, we need to **tokenize** the text.\n\n## Tokenization\n\nTokenization is the process of splitting raw text into smaller pieces, called (drum-roll please), *tokens*. Tokens can be individual characters, words, or sentences.\n\nExamples of character and word tokenization are shown for the following raw text\n\n```Show me the money```\n\nCharacter tokenization:\n\n```['S', 'h', 'o', 'w', 'm', 'e', 't', 'h', 'e', 'm', 'o', 'n', 'e', 'y']```.\n\nWord tokenization:\n\n```['Show', 'me', 'the', 'money'] ```\n\n---\n\nThis code block demonstrates both of these tokenization techniques.\n\n::: {#fdc1c414 .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\n# Character and word tokenization\n\nsentence = \"Show me the money\"\nword_tokens = sentence.split()\nprint(word_tokens)\ncharacter_tokens = [char for char in sentence if char != ' ']\nprint(character_tokens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Show', 'me', 'the', 'money']\n['S', 'h', 'o', 'w', 'm', 'e', 't', 'h', 'e', 'm', 'o', 'n', 'e', 'y']\n```\n:::\n:::\n\n\n---\n\nThere are advantages and disadvantages to different tokenization methods. We showed two very simple strategies. \n\nHowever, there are other strategies, such as subword and sentence tokenization,\nsee for example [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte-pair_encoding),\nand [SentencePiece](https://github.com/google/sentencepiece).\n\nWith tokenization, our goal is to not lose meaning with the tokens. With character based tokenization, especially for English (non-character based languages) we certainly lose meaning. \n\nHere is a demo of how to tokenize using the [transformers](https://huggingface.co/docs/transformers/en/index) package from Huggingface.\n\n::: {#d21fedc5 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\nfrom transformers import AutoTokenizer, logging\n\nlogging.set_verbosity_warning()\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\ntokens = tokenizer.tokenize(sentence)\nprint(tokens)\n\n# Try a more advanced sentence\nsentence2 = \"Let's try to see if we can get this transformer to tokenize.\"\ntokens2 = tokenizer.tokenize(sentence2)\nprint(tokens2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n['Show', 'me', 'the', 'money']\n['Let', \"'\", 's', 'try', 'to', 'see', 'if', 'we', 'can', 'get', 'this', 'transform', '##er', 'to', 'token', '##ize', '.']\n```\n:::\n:::\n\n\n## Tokens and Token IDs\n\nAssociated to each token is a unique token ID. The total number of unique tokens that a model can recognize and process is the *vocabulary size*. The *vocabulary* is the collection of all the unique tokens.\n\nThe tokens (and token ids) alone hold no (semantic) information. What is needed is a numerical representation that *encodes* this information. \n\nThere are different ways to achieve this. One encoding technique that we already considered is one-hot encodings. Another more powerful encoding method, is the creation of word embeddings.\n\n## Sparse Representations\n\nWe have previously considered the following sparse representations of textual data.\n\n### One-Hot Encoding\n- Each word is represented as a vector of zeros and a single one.\n- Simple but inefficient for large vocabularies.\n\n**Example**\n\nGiven the words cat, dog, and emu here are sample one-hot encodings\n\n$$\n\\begin{align*}\n\\text{cat} &= [1, 0, 0]^{T}, \\\\\n\\text{dog} &= [0, 1, 0]^{T}, \\\\\n\\text{emu} &= [0, 0, 1]^{T}. \\\\\n\\end{align*}\n$$\n\n---\n\n### Bag of Words (BoW)\n- Represents text as a collection of word counts.\n- Ignores grammar and word order.\n\n**Example**\n\nSuppose we have the following sentences\n\n1. The cat sat on the mat.\n1. The dog sat on the log.\n1. The emu sat on the mat.\n\n| Sentence                              | the | cat | sat | on | mat | dog | log | emu |\n|---------------------------------------|-----|-----|-----|----|-----|-----|-----|----------|\n| \"The cat sat on the mat.\"             |  2  |  1  |  1  | 1  |  1  |  0  |  0  |    0     |\n| \"The dog sat on the log.\"             |  2  |  0  |  1  | 1  |  0  |  1  |  1  |    0     |\n| \"The emu sat on the mat.\"        |  2  |  0  |  1  | 1  |  1  |  0  |  0  |    1     |\n\n---\n\n### TF-IDF (Term Frequency-Inverse Document Frequency)\n\n- Adjusts word frequency by its importance across documents.\n- Highlights unique words in a document.\n- See [Course Notes: Feature Extraction](https://tools4ds.github.io/DS701-Course-Notes/07-Clustering-II-in-practice.html#feature-extraction) \n  for more details.\n\n**Example**\n\nThe TF-IDF representations corresponding to the previous sentences.\n\n|       | cat       | dog       | log       | mat       | on        | emu  | sat       | the       |\n|-------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\n| Sentence 1 | 0.4698    | 0.0000    | 0.0000    | 0.4698    | 0.3546    | 0.0000    | 0.3546    | 0.7093    |\n| Sentence 2 | 0.0000    | 0.4698    | 0.4698    | 0.0000    | 0.3546    | 0.0000    | 0.3546    | 0.7093    |\n| Sentence 3 | 0.0000    | 0.0000    | 0.0000    | 0.4698    | 0.3546    | 0.4698    | 0.3546    | 0.7093    |\n\n## Word Embeddings\n\nWord embeddings represent words as dense vectors in high-dimensional spaces.\n\nThe individual values of the vector may be difficult to interpret, but the \noverall pattern is that _words with similar meanings are close to each other_, \nin the sense that their vectors have small angles with each other.\n\nThe similarity of two word embeddings is the cosine of the angle between the two\nvectors. Recall that for two vectors $v_1, v_2\\in\\mathbb{R}^{n}$, the formula \nfor the cosine of the angle between them is\n\n$$ \n\\cos{(\\theta)} = \\frac{v_1 \\cdot v_2}{\\Vert v_1 \\Vert_2 \\Vert v_2 \\Vert_2}.\n$$\n\nWord embeddings can be static or contextual. A static embedding is when each word has a single embedding, e.g., Word2Vec. A contextual embedding (used by more complex language model embedding algorithms) allows the embedding for a word to change depending on its context in a sentence.\n\n# Language Models\n\n\n## Language Models\n\nA language model is a statistical tool that predicts the probability of a sequence of words. It helps in understanding and generating human language by learning patterns and structures from large text corpora.\n\n1. **N-gram Models**:\n   - Predict the next word based on the previous  $n-1$ words.\n   - Simple and effective for many tasks but limited by fixed context size.\n\n1. **Neural Language Models**:\n   - Use neural networks to capture more complex patterns.\n   - Examples include *RNNs*, *LSTMs*, and **Transformers**.\n\nWe previously covered [RNNs and LSTMs](#27-RNN.qmd).\n\nWe'll discuss N-grams briefly followed by a deep dive on Transformers.\n\n# N-gram Models\n\n\n## N-gram models\n\n- **Definition**: An n-gram model is a type of probabilistic language model used in natural language processing.\n- **Purpose**: It predicts the next item in a sequence based on the previous \\( n-1 \\) items.\n- **Types**:\n  - **Unigram (n=1)**: Considers each word independently.\n  - **Bigram (n=2)**: Considers pairs of consecutive words.\n  - **Trigram (n=3)**: Considers triples of consecutive words.\n\n## How N-gram Models Work\n\n- **Example**: Let's consider a bigram model.\n- **Training Data**: \"I love machine learning. Machine learning is fun.\"\n- **Bigrams**: \n  - \"I love\"\n  - \"love machine\"\n  - \"machine learning\"\n  - \"learning Machine\"\n  - \"Machine learning\"\n  - \"learning is\"\n  - \"is fun\"\n\n- **Probability Calculation**:\n  - P(\"learning\" | \"machine\") = Count(\"machine learning\") / Count(\"machine\")\n\n\n## Example of N-gram Model in Action\n\n- **Sentence Completion**:\n  - Given the sequence \"machine learning\", predict the next word.\n  - Using the bigram model:\n    - P(\"is\" | \"learning\") = Count(\"learning is\") / Count(\"learning\")\n    - P(\"fun\" | \"learning\") = Count(\"learning fun\") / Count(\"learning\")\n\n- **Prediction**:\n  - If \"learning is\" appears more frequently than \"learning fun\" in the training data, the model predicts \"is\" as the next word.\n\n# Transformers\n\n\n## Transformers\n\nTransformers are a deep learning model for processing sequential (text) data @vaswani2017attention.\n\n- Rely on a mechanism called *Attention*.\n- Revolutionized the field of natural language processing (NLP) and artificial intelligence (AI).\n- The model is easy to scale across GPUs.\n- The building blocks for large language models (LLMs) such as:\n  - ChatGPT (OpenAI),\n  - LLama (Meta),\n  - BERT (Google),\n  - Megatron (NVIDIA).\n\nTo introduce the Transformer architecture we will consider machine translation. This is an application of sequence-to-sequence modeling. \n\n## Transformer Architecture\n\n![](drawio/Transformer.png){fig-align=\"center\"}\n\n---\n\n![](drawio/Transformer_Enc_Dec.png){fig-align=\"center\"}\n\n---\n\n![](drawio/TransformerOriginal.png){fig-align=\"center\"}\n\n## Encoder-Decoder Blocks\n\n### Encoder\n- **Function**: Processes input data (e.g., text) and converts it into a set of continuous representations.\n\n### Decoder\n- **Function**: Generates output sequences (e.g., translated text) from the encoded representations.\n\nThe encoder and decoder work together to transform input sequences into meaningful output sequences.\n\n## Encoder Input \n\nThe input to the encoder are word embeddings.\n\n:::: {.columns}\n::: {.column width=\"70%\"}\n\n- Words are embedded into a n-dimensional space, To establish the order of the words, we use positional encodings. .\n- The length $n$ of the vector is the *embedding dimension*.\n- Let $m$ denote the number of words in a sentence.\n\n:::\n::: {.column width=\"30%\"}\n![](drawio/InputVectors.png)\n:::\n::::\n\n## Positional Encodings\n\nTo establish the order of the words, we use positional encodings. \n\n:::: {.columns}\n::: {.column width=\"70%\"}\n- This is achieved by adding a vector $\\mathbf{t}^{(i)}$ to the word embedding $\\mathbf{x}^{(i)}$.\n- The positionally encoded word embeddings are the inputs to the transformer.\n:::\n::: {.column width=\"30%\"}\n![](drawio/PositionalEncodings.png)\n:::\n::::\n\n## Generating Position Vectors\n\nThe authors of @vaswani2017attention proposed the following function for positional encodings\n$$\n\\mathbf{t}_{j}^{(i)} = \n\\begin{cases} \n\\sin{\\left(\\frac{i}{10000^{2j/n}}\\right)} & \\text{for~$j$~even}, \\\\ \n\\cos{\\left(\\frac{i}{10000^{2j/n}}\\right)} & \\text{for~$j$~odd}. \\\\\n\\end{cases}\n$$\n\n---\n\nHere is sample code to generate an illustration of the positional encodings for\nembedding dimension $n=64$ and $m=10$ tokens.\n\n::: {#685ad061 .cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef get_angles(pos, i, d_model):\n  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n  return pos * angle_rates\n\ndef positional_encoding(position, d_model):\n  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                          np.arange(d_model)[np.newaxis, :],\n                          d_model)\n  \n  # apply sin to even indices in the array; 2i\n  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n  \n  # apply cos to odd indices in the array; 2i+1\n  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    \n  pos_encoding = angle_rads[np.newaxis, ...]\n    \n  return pos_encoding\n\n\ntokens = 10\ndimensions = 64\n\npos_encoding = positional_encoding(tokens, dimensions)\n\nplt.figure(figsize=(7, 5))\nplt.pcolormesh(pos_encoding[0], cmap='viridis')\nplt.xlabel('Embedding Dimensions', fontsize=16)\nplt.xlim((0, dimensions))\nplt.ylim((tokens, 0))\nplt.ylabel('Token Position', fontsize=16)\nplt.colorbar()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](28-NLP-save_files/figure-html/cell-4-output-1.png){width=583 height=442 fig-align='center'}\n:::\n:::\n\n\n## From Inputs to the Encoder\n\nThe positionally encoded vectors are transposed and stacked to form an input matrix. This input matrix is fed into the bottom of the encoder block.\n\n:::: {.columns}\n::: {.column width=\"70%\"}\nThe number of rows $m$ is the number of words in the sentence an the number of columns $n$ is the embedding dimension.\n$$\nX =\n\\begin{bmatrix}\n& & {\\mathbf{x}^{(1)}}^{T} &  & \\\\\n& & {\\mathbf{x}^{(2)}}^{T} & &\\\\\n& & \\vdots & & \\\\\n& & {\\mathbf{x}^{(m)}}^{T} & & \\\\\n\\end{bmatrix}\n\\in\\mathbb{R}^{m\\times n}.\n$$\n:::\n::: {.column width=\"30%\"}\n![](drawio/Encoder.png)\n:::\n::::\n\n# Attention\n\n\n## Attention\n\nAttention in language models is a mechanism that allows the model to focus on relevant parts of the input sequence by assigning different weights to different words, enabling it to capture long-range dependencies and context more effectively.\n\nAttention is needed in order to understand sentences such as, \n\n```\nThe elephant didn't cross the river because it was tired.\n```\n\nAttention allows a language model to understand correctly that *it* refers to the elephant and not the river.\n\n## Queries, Keys, and Values\n\nThe building blocks of the attention mechanism are query $\\mathbf{q}$, key $\\mathbf{k}$, and value $\\mathbf{v}$ vectors.\n\nThe query and key vectors $\\mathbf{k}, \\mathbf{q}\\in\\mathbb{R}^{d}$ and the value vector $\\mathbf{v}\\in\\mathbb{R}^{n}$.\n\nAt a high level\n\n- query vectors determine which parts of the input to focus on, \n- key vectors represent the input features, and \n- value vectors contain the actual data to be attended to.\n\n---\n\n:::: {.columns}\n::: {.column width=\"60%\"}\nEach $\\mathbf{q}^{(i)}$, $\\mathbf{k}^{(i)}$, and $\\mathbf{v}^{(i)}$ is associated with $\\mathbf{x}^{(i)}$. To compute the query, key, and value vectors, compute\n$$\n\\begin{align*}\n\\mathbf{q}^{(i)} &= {\\mathbf{x}^{(i)}}^{T}W^{Q}, \\\\\n\\mathbf{k}^{(i)} &= {\\mathbf{x}^{(i)}}^{T}W^{K}, \\\\\n\\mathbf{v}^{(i)} &= {\\mathbf{x}^{(i)}}^{T}W^{V}, \\\\\n\\end{align*}\n$$\n\nwhere $W^{Q}, W^{K}\\in\\mathbb{R}^{n\\times d}$ and $W^{V}\\in\\mathbb{R}^{n\\times n}$ are trainable matrices of weights.\n:::\n::: {.column width=\"40%\"}\n![](drawio/QueriesKeysValues.png)\n:::\n::::\n\n---\n\n:::: {.columns}\n::: {.column width=\"60%\"}\nThis operation can be vectorized to compute\n$$\n\\begin{align*}\nQ &= XW^{Q}, \\\\\nK &= XW^{K}, \\\\\nV &= XW^{V}. \\\\\n\\end{align*}\n$$\n:::\n::: {.column width=\"40%\"}\n![](drawio/VectorizedQKV.png)\n:::\n::::\n\n\n\n## Computing Attention\n\nGiven a query, key, and value vector, we compute attention in the following sequence of operations\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n1. Compute $\\mathbf{q}^{(i)}\\cdot \\mathbf{k}^{(j)} = s_{i,j}$ for each $j$.\n1. Compute $s_{i,j}=s_{i,j}/\\sqrt{d}$.\n1. Compute $\\tilde{s}_{i,:} = \\operatorname{softmax}(s_{i,:})$.\n1. Compute $\\mathbf{\\tilde{v}}^{(j)}=\\tilde{s}_{i, j} \\mathbf{v}^{(j)}$ for each $j$.\n1. Compute $\\mathbf{z}^{(i)} = \\sum_{j} \\mathbf{\\tilde{v}}^{(j)}$.\n:::\n::: {.column width=\"40%\"}\n![](drawio/ComputingAttention.png)\n:::\n::::\n\n::: aside\nLet $\\mathbf{s}\\in\\mathbb{R}^{n}$, recall $softmax(\\mathbf{s})_i = \\frac{e^{s_i}}{\\sum e^{s_{j}}}$.\n:::\n\n## Vectorized Attention\n\nAttention can be easily vectorized. The procedure is then\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n1. Compute $QK^T = S$.\n1. Compute $S=\\frac{1}{\\sqrt{d}}S$.\n1. Compute the softmax across rows $\\tilde{S} = \\operatorname{softmax}(S)$.\n1. Compute $Z = \\tilde{S}V$.\n:::\n::: {.column width=\"40%\"}\n![](drawio/ComputingVectorizedAttention.png)\n:::\n::::\n\n## Attention Visualized\n\n:::: {.columns}\n::: {.column width=\"60%\"}\nRecall the original sentence to be translated, \"The elephant did not cross the river because it was tired.\"\n\nA hypothetical visualization would demonstrate the links of the original sentence to the word *it*.\n\nThe darker the color, the more this word **attends** to the word *it*.\n\n:::\n::: {.column width=\"40%\"}\n![](drawio/VisualizedAttention.png)\n:::\n::::\n\n## Attention Summary\n\n1. Compute an attention score matrix $S_{ij} = \\mathbf{q}^{(i)}\\cdot \\mathbf{k}^{(j)}/\\sqrt{d}$.\n1. The softmax function is applied to each row of the matrix $S$.\n1. For a given row, the values in the columns of this matrix are the weights of the linear combination of the values vectors $\\mathbf{v}^{(i)}$. \n1. These weights tell us how much (or how little) each value vector contributes in the output $\\mathbf{z}^{(i)}$.\n1. When there is only one set of $W^{Q}, W^{K},$ and $W^{V}$ matrices this process is called self-attention.\n\n# Layer Normalization and Feed-Forward Neural Network\n\n## Layer Normalization\n\n:::: {.columns}\n::: {.column width=\"60%\"}\n- Training with features on different scales takes longer and is the potential cause of exploding gradients.\n- Layer normalization ensures all values along the embedding dimension have the same distribution.\n- Layer normalization is calculated by a modified Z-score equation.\n:::\n::: {.column width=\"40%\"}\n![](drawio/LayerNormalization.png)\n:::\n::::\n\n\n::: aside\nProposed in @ba2016layernormalization.\n:::\n\n---\n\n- Sum $\\mathbf{x}+\\mathbf{z}= \\mathbf{u}$.\n- Compute mean $\\mu$ and the variance $\\sigma^{2}$ of $\\mathbf{u}$.\n- Compute\n$$\n\\bar{\\mathbf{u}} = \\frac{\\mathbf{u}-\\mu}{\\sqrt{\\sigma^{2}+\\varepsilon}} \\odot \\boldsymbol{\\gamma} + \\boldsymbol{\\beta}.\n$$\n- $\\varepsilon$ is a small number.\n- $\\boldsymbol{\\gamma}, \\boldsymbol{\\beta}$ are trainable parameter vectors of length $n$.\n- The notation $\\odot$ indicates entry-wise multiplication.\n- Layer normalization can be vectorized to produce $\\bar{U}$.\n\n## Feed-Forward Neural Network (FFNN)\n\n- The vectorized output of the layer normalization layer is $\\bar{U}$.\n- The tensor $\\bar{U}$ is the input to the neural network.\n- There is 1 hidden layer.\n- $FFNN(\\bar{U}) = W_2\\operatorname{max}(0, W_1\\bar{U} + b_1) + b_2$.\n- The nonlinearity introduced by activation function of this layer allows the model to further differentiate the attention of each word.\n\n## Encoder Output\n\n- A 2nd layer normalization is applied to the output of the FFNN.\n- There is a residual connection to the output of the 1st layer normalization layer.\n- The output from the 2nd layer normalization is the output of the encoder.\n- At this stage, attention has been incorporated into the output.\n- The output is then sent to the decoder blocks.\n\n## Decoder Blocks\n\nThe architecture of a decoder block (in an encoder-decoder transformer) is nearly identical to than of an encoder block. \n\n:::: {.columns}\n::: {.column width=\"60%\"}\nThe decoder blocks consists of\n\n- self-attention layer\n- layer normalization\n- feed-forward neural network\n- layer normalization\n\n:::\n::: {.column width=\"40%\"}\n![](drawio/Decoder.png)\n:::\n::::\n\nThe major differences in the decoder\n\n1. An encoder-decoder attention layer. This layer attends to the output of the encoder with *cross attention* (e.g. keys and values come from the encoder and queries come from the decoder).\n1. The self-attention layer only attends to earlier positions (not future) in the output sequence.\n\n## Decoder Output\n\nThe final output of the decoder blocks is a linear layer followed by a softmax layer.\n\n:::: {.columns}\n::: {.column width=\"60%\"}\nThe linear layer outputs a vector that is fed into a softmax layer.\n\nThe softmax layer outputs the probability of a potential word being generated. \n\nThe word with the maximum probability is chosen and output by the model.\n:::\n::: {.column width=\"40%\"}\n![](drawio/DecoderOutput.png)\n:::\n::::\n\n# Transformer Architectures\n\n## 3 Types of Transformer Models\n\n1. **Encoder-Decoder** – used in sequence-to-sequence tasks, where one text string is converted to another (e.g., machine translation)\n\n1. **Encoder** – transforms text embeddings into representations that support variety of tasks (e.g., sentiment analysis, classification)\n Model Example: BERT\n\n1. **Decoder** – predicts the next token to continue the input text (e.g., ChatGPT, AI assistants)\n Model Example: GPT4, GPT4\n\n## Encoder Model Example: BERT (2019)\n\n### Bidirectional Encoder Representations from Transformers\n\n- Hyperparameters\n  - 30,000 token vocabulary\n  - 1024-dimensional word embeddings\n  - 24x transformer layers\n  - 16 heads in self-attention mechanism\n  - 4096 hidden units in middle of MLP\n- ~340 million parameters\n- Pre-trained in a self-supervised manner\n- Can be adapted to task with one additional layer and fine-tuned\n\n::: aside\nProposed in @devlin2019bert.\n:::\n\n## Encoder Pre-Training\n\n![](figs/EncoderPretraining.png){fig-align=\"center\"}\n\n- A small percentage of input embedding replaced with a generic <mask> token\n- Predict missing token from output embeddings\n- Added linear layer and softmax to generate probabilities over vocabulary\n- Trained on BooksCorpus (800M words) and English Wikipedia (2.5B words)\n\n\n## Encoder Fine-Tuning\n\n[](figs/EncoderFinetuning.png){fig-align=\"center\"}\n\n- Extra layer(s) appended to convert output vectors to desired output format.\n- 3rd Example: Text span prediction -- predict start and end location of answer to a question in passage of Wikipedia, see this [link](https://rajpurkar.github.io/SQuAD-explorer/).\n\n\n## Decoder Model Example: GPT3 (2020)\n\n### Generative Pre-trained Transformer\n\n- One purpose: generate the next token in a sequence.\n- This is an autoregressive model.\n- Factors the probability of a sentence of tokens $t_1, t_2, \\ldots t_N$ as\n$$\nP(t_1, t_2, \\ldots, t_N) = P(t_1)\\prod_{n=2}^{N}\nP(t_n | t_1, t_2, \\ldots t_{n-1}).\n$$\n\n::: aside\nProposed in @brown2020language.\n:::\n\n## Decoder: Masked Self-Attention\n\n![](figs/DecoderMaskedAttention.png)\n\n- During training we want to maximize the log probability of the input text under the autoregressive model.\n- We want to make sure the model doesn’t “cheat” during training by looking ahead at the next token.\n- Therefore, we mask the self attention weights corresponding to current and right context to negative infinity.\n\n\n## Decoder: Text Generation\n\n![](figs/DecoderTextGen.png)\n\n- Prompt with token string “<start> It takes great”\n- Generate next token for the sequence by \n  - picking most likely token\n  - sample from the probability distribution\n  - beam search – select the most likely sentence rather than picking in a greedy fashion\n\n<!-- \n# Applications\n\nWe will now build our own code to translate English sentences to Dutch sentences.\n\n## Overview\n\nWe will use the following Python packages to achieve this\n\n- Pytorch\n- Datasets\n- Transformers\n\nYou will need to execute this code on a computer with a GPU.\n\n## Data\n\n## AutoModelForSeq2SeqLM\n\n## Training\n\n## Evaluation\n\n## Inference\n\n# Review\n\n## NLP Recap\n\nIn this sequence of lectures we covered \n\n- historical roots of natural language processing\n- the Transformer architecture\n- example language translation -->\n\n# Course Recap, Evaluations, and References\n\n\n## DS701 Recap \n\nIn this course we introduced you to the Tools for Data Science. \n\nWe covered:\n\n- Important Python packages like Pandas, Scikit-Learn, Torch, statsmodel, and others.\n- Mathematical foundations of data science, including linear algebra, probability and statistics, and optimization\n- Unsupervised learning\n  - Clustering\n  - Dimensionality reduction\n- Supervised learning\n  - Classification\n  - Regression\n- Neural Networks and NLP\n  - CNNs, RNNs, Transformers\n- Graphs\n- Recommender systems\n\n\n## Course Evaluations\n\nPlease be sure to fill out the course evaluations. They can be found at the following links:\n\n:::: {.columns}\n::: {.column width=\"50%\"}\n### Section A1\n\nFollow this [link](https://go.blueja.io/tggejlP2zUuNRiyt28XZjQ) to submit a course evaluation for lecture section A1.\n\nFollow this [link](https://go.blueja.io/qOvwpG0d60CCqxXt0g-Pgg) to submit a course evaluation for discussion section A2.\n\n:::\n::: {.column width=\"50%\"}\n### Section C1\n\nFollow this [link](https://go.blueja.io/LLmzBnmznEuK22KOgKjfnA) to submit a course evaluation for Section C1.\n\n:::\n::::\n\n## References\n\n",
    "supporting": [
      "28-NLP-save_files"
    ],
    "filters": [],
    "includes": {}
  }
}