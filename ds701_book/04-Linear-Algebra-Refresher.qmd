---
title: Linear Algebra Refresher
jupyter: python3
---

[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/04-Linear-Algebra-Refresher.ipynb)

Linear alegbra is the branch of mathematics involving vectors and matrices. In particular, how vectors are transformed.  Knowledge of linear algebra is essential in data science. 

## Linear algebra

Linear algebra allows us to understand the operations and transformations used to manipulate and extract information from data.

:::: {.frag}
Examples

:::: {.incremental}
- Deep neural networks use matrix-vector and matrix-matrix multiplication.
- Natural language processing use the dot product to determine word similarity.
- Least squares uses matrix inverses and matrix factorizations to compute models for predicting continuous values.
- PCA (dimenstionality reduction) uses the matrix factorization called the Singluar Value Decomposition (SVD).
- Graphs are described by adjacency matrices. Eigenvectors and eigenvalues of this matrix provide information about the graph structure.
::::
::::

::: {.content-visible when-profile="slides"}
## Lecture overview
:::
:::: {.fragment}
Linear algebra is used to implement data science algorithms efficiently and accurately.
::::

:::: {.fragment}
You will not have to program linear algebra algorithms. You will use appropriate Python packages.
::::

::: {.content-hidden when-profile="slides"}
This lecture is a review of some aspects of linear algebra that are important for data science. Given the prerequisites for this course, I assume that you previously learned this material. 
:::

:::: {.fragment}
The goal of this lecture is to refresh the following topics:

:::: {.incremental}
- vectors,
- matrices,
- operations with vectors and matrices,
- eigenvectors and eigenvalues,
- linear systems and least squares,
- matrix factorizations.
::::
::::

::: {.content-hidden when-profile="web"}
## References
:::

Below is a list of very useful resources for learning about linear algebra:

- __Linear Algebra and Its Applications (6th edition)__, David C. Lay, Judi J. McDonald, and Steven R. Lay, Pearson, 2021,
- __Introduction to Linear Algebra (6th edition)__, Gilbert Strang, Wellesley-Cambridge Press, 2023,
    - Gilbert Strang's [lecture videos](https://youtube.com/playlist?list=PL221E2BBF13BECF6C&si=ImY77CfkyNVJvPtt)
- __Linear Algebra and Learning from Data__, Gilbert Strang, Wellesley-Cambridge Press, 2019,
- __Numerical Linear Algebra__, Lloyn N. Trefethen and David Bau, SIAM, 1997.


## Vectors

A vector of length $n$, $\mathbf{x}\in\mathbb{R}^{n}$, is a 1-dimensional (1-D) array of real numbers

$$
\mathbf{x} =
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}.
$$


When discussing vectors we will only consider column vectors. A row vector can always be obtained from a column vector via transposition

$$
\mathbf{x}^{T} = [x_1, x_2, \ldots, x_n].
$$

::: {.content-hidden when-profile="web"}
## Vectors continued 
:::

::: {.content-hidden when-profile="slides"}
Vectors in $\mathbb{R}^{2}$ can be visualized as points in a 2-D plane, whereas vectors in $\mathbb{R}^{3}$ can be visualized as points in a 3-D space.
:::

Let 

$$\mathbf{x}=\begin{bmatrix} 2 \\ 2 \end{bmatrix},~
\mathbf{y} = \begin{bmatrix} 3 \\ -1 \end{bmatrix},~
\mathbf{z} = \begin{bmatrix} -2 \\ -1 \end{bmatrix}.
$$

::: {.content-hidden when-profile="slides"}
These vectors are illustrated in @fig-vector-viz.
:::

```{python}
#| label: fig-vector-viz
#| fig-cap: "Illustration of vectors"
import matplotlib.pyplot as plt
import numpy as np
fig = plt.figure()
ax = plt.gca()
x = np.array([2, 2])
y = np.array([3, -1])
z = np.array([-2, -1])
V = np.array([x, y, z])
origin = np.array([[0, 0, 0], [0, 0, 0]])
plt.quiver(*origin, V[:, 0], V[:, 1], 
           color=['r', 'b', 'g'], 
           angles='xy', 
           scale_units='xy', 
           scale=1)
ax.set_xlim([-6, 6])
ax.set_ylim([-2, 4])
ax.text(3.3, -1.1, '$(3,-1)$', size=16)
ax.text(2.3, 1.9, '$(2,2)$', size=16)
ax.text(-3.7, -1.3, '$(-2,-1)$', size=16)
ax.grid()
plt.show()
```

::: {.content-hidden when-profile="web"}
## Vectors continued
:::

Recall the following definitions.

 1. Scalar multiplication: Let $c\in\mathbb{R}$, $\mathbf{x}\in\mathbb{R}^{n}$, then $$c\mathbf{x} = \begin{bmatrix} cx_1 \\ cx_2 \\ \vdots \\ cx_n \end{bmatrix}.$$
 1. Vector addition: Let $\mathbf{u},\mathbf{v}\in\mathbb{R}^{n}$ then
$$ \mathbf{u} + \mathbf{v} = \begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \vdots \\ u_n + v_n \end{bmatrix}.$$
1. Dot product: Let $\mathbf{u},\mathbf{v}\in\mathbb{R}^{n}$ then the dot product is defined as
$$ \mathbf{u}\cdot\mathbf{v} = \sum_{i=0}^n u_i v_i.$$
1. Vector 2-norm: The 2-norm of a vector $\mathbf{v}\in\mathbb{R}^{n}$ is defined as
    $$\Vert \mathbf{v}\Vert_2 = \sqrt{\mathbf{v}\cdot\mathbf{v}} = \sqrt{\sum_{i=1}^n v_i^2}.$$ 
    This norm is referred to as the $\ell_2$ norm. In these notes, the notation $\Vert \mathbf{v} \Vert$, indicates the 2-norm.
1. A unit vector $\mathbf{v}$ is a vector such that $\Vert \mathbf{v} \Vert_2 = 1$. All vectors of the form $\frac{\mathbf{v}}{\Vert \mathbf{v} \Vert_2 }$ are unit vectors.
1. Distance: Let $\mathbf{u},\mathbf{v}\in\mathbb{R}^{n}$, the distance between $\mathbf{u}$ and $\mathbf{v}$ is
$$ \Vert \mathbf{u} - \mathbf{v} \Vert_2.$$
1. Orthogonality: Two vectors $\mathbf{u},\mathbf{v}\in\mathbb{R}^{n}$ are orthogonal if and only if $\mathbf{u}\cdot\mathbf{v}=0$. 
1. Angle between vectors:  Let $\mathbf{u},\mathbf{v}\in\mathbb{R}^{n}$, the angle between these vectors is $$\cos{\theta} = \frac{\mathbf{u}\cdot\mathbf{v}}{\Vert \mathbf{u}\Vert_2 \Vert\mathbf{v}\Vert_2}.$$
1. A set of $n$ vectors $\mathbf{v}_{1}, \ldots, \mathbf{v}_{n}\in\mathbb{R}^n$ is linearly dependent if there exists scalars $a_1,\ldots, a_n$ not all zero such that 
    $$
    \sum_{i=1}^{n} a_i \mathbf{v}_i = 0.
    $$

    The vectors $\mathbf{v}_{1}, \ldots, \mathbf{v}_{n}$ are linearly independent if they are not linearly dependent, i.e., the equation
    $$
    a_1 \mathbf{v}_1 + \cdots + a_n \mathbf{v}_n = 0,
    $$
    is only satisfied if $a_i=0$ for $i=1, \ldots,n$.
1. Given a set of vectors $V = \{\mathbf{v}_{1}, \ldots, \mathbf{v}_{n}\}$, where $\mathbf{v}_i\in\mathbb{R}^n$, the span(V) is the set of all linear combinations of vectors in $V$.

::: {.content-hidden when-profile="slides"}
### Visualizing vector operations

For helpful visualizations of the above defintions, we'll restrict ourselves to $\mathbb{R}^{2}.$
:::

::: {.content-visible when-profile="slides"}
# Visualizing vector operations
:::

::: {.content-hidden when-profile="slides"}
#### Scalar multiplication
:::

::: {.content-visible when-profile="slides"}
## Scalar multiplication
:::

Multiplication by a scalar $c\in\mathbb{R}$. For $c>1$ the vector is lengthened. For $0<c<1$ the vector shrinks. If we negate $c$ the direction of the vector is flipped 180 degrees. This is shown in @fig-vector-scaling. In this figure we show the vector $\mathbf{x} = [2, 2]$ multiplied by the scalar value $c=2$.

```{python}
#| label: fig-vector-scaling
#| fig-cap: "Scalar multiplication of a vector"
import matplotlib.pyplot as plt
import numpy as np
fig = plt.figure()
ax = plt.gca()
x = np.array([2, 2])
y = np.array([4, 4])
V = np.array([x, y])
origin = np.array([[0, 0], [0, 0]])
plt.quiver(*origin, V[:, 0], V[:, 1], 
           color=['r', 'b'], 
           angles='xy', 
           scale_units='xy', 
           scale=1,
           alpha= 0.5)
ax.set_xlim([-5, 5])
ax.set_ylim([-1, 5])
ax.text(2.3, 1.9, '$x$', size=16)
ax.text(4.3, 3.9, '$cx$', size=16)
ax.grid()
plt.show()
```
::: {.content-hidden when-profile="slides"}
#### Vector addition
:::

::: {.content-visible when-profile="slides"}
## Vector addition
:::

We plot the sum of $\mathbf{u} = [1, 2]$ and $\mathbf{v} = [4, 1]$ in @fig-vector-addition. The sum $\mathbf{u} + \mathbf{v} = [5, 3]$ is obtained by placing the tip of one vector to the tail of the other vector. The sum is the vector that connects the unconnected tip to the unconnected tail. 

```{python}
#| label: fig-vector-addition
#| fig-cap: "Vector addition"
import matplotlib.pyplot as plt
import numpy as np
fig = plt.figure()
ax = plt.gca()
u = np.array([1, 2])
v = np.array([4, 1])
w = np.array([5, 3])
V = np.array([u, v, w])
origin = np.array([[0, 0, 0], [0, 0, 0]])
plt.quiver(*origin, V[:, 0], V[:, 1], 
           color=['b', 'b', 'r'], 
           angles='xy', 
           scale_units='xy', 
           scale=1)
ax.set_xlim([-1, 6])
ax.set_ylim([-1, 4])
ax.text(1.3, 1.9, '$u$', size=16)
ax.text(4.3, 1.2, '$v$', size=16)
ax.text(5.3, 2.9, '$u+v$', size=16)
plt.plot([1, 5], [2, 3], 'g--')
plt.plot([4, 5], [1, 3], 'g--')
ax.grid()
plt.show()
```

::: {.content-hidden when-profile="slides"}
#### Dot product
:::

::: {.content-visible when-profile="slides"}
## Dot product
:::

The dot product of two vectors $\mathbf{u}, \mathbf{v}$ can be used to project $\mathbf{u}$ onto $\mathbf{v}$. This is illustrated in @fig-dot-product.

```{python}
#| label: fig-dot-product
#| fig-cap: "Dot product"
import matplotlib.pyplot as plt
import numpy as np
fig = plt.figure()
ax = plt.gca()
u = np.array([1, 2])
v = np.array([4, 1])
# w = np.dot(u, v) * v / np.sqrt(np.dot(v, v))
w = np.dot(u, v) * v / np.dot(v, v)
V = np.array([u, v, w])
origin = np.array([[0, 0, 0], [0, 0, 0]])
plt.quiver(*origin, V[:, 0], V[:, 1], 
           color=['b', 'b', 'r'], 
           angles='xy', 
           scale_units='xy', 
           scale=1)
ax.set_xlim([-1, 6])
ax.set_ylim([-1, 4])
ax.text(1.3, 1.9, '$u$', size=16)
ax.text(4.3, 1.2, '$v$', size=16)
ax.text(0.4, -0.3, r'$\frac{u\cdot v}{\Vert v \Vert}$', size=16)
plt.plot([u[0], w[0]], [u[1], w[1]], 'g--')
# plt.plot([4, 5], [1, 3], 'g--')
ax.grid()
plt.show()
```

Observe that a right angle forms between the vectors $\mathbf{u}$ and $\mathbf{v}$ when $\mathbf{u}\cdot \mathbf{v} = 0$. 

```{python}
#| echo: false
print(f"u = {u}")
print(f"v = {v}")
print(f"u.v = {np.dot(u, v)}")
print(f"v.v = {np.dot(v, v)}")
print(f"sqrt(v.v) = {np.sqrt(np.dot(v, v))}")
print(f"v.v/sqrt(v.v) = {np.dot(v, v) / np.sqrt(np.dot(v, v))}")
print(f"norm(v) = {np.linalg.norm(v)}")
print(f"u.v / sqrt(v.v) = {np.dot(u, v) / np.sqrt(np.dot(v, v))}")

print(f"u.v * v / (v.v) = {w}")
```

## Matrices

A matrix $A\in\mathbb{R}^{m\times n}$ is a 2-D array of numbers

$$
A = 
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} &a_{m2} & \cdots & a_{mn} \\
\end{bmatrix},
$$

with $m$ rows and $n$ columns. The element at row $i$ and column $j$ is denoted $a_{ij}$. If $m=n$ we call it a square matrix.

::: {.content-hidden when-profile="web"}
## Matrices continued
:::

Similar to vectors, we can multiply matrices by scalar values and add matrices of the same dimension, i.e.,

1. Let $c\in\mathbb{R}$ and $A\in\mathbb{R}^{m\times n}$, then
$$
cA =
\begin{bmatrix}
ca_{11} & ca_{12} & \cdots & ca_{1n} \\
ca_{21} & ca_{22} & \cdots & ca_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
ca_{m1} & ca_{m2} & \cdots & ca_{mn} \\
\end{bmatrix}.
$$
1. Let $A, B\in\mathbb{R}^{m\times n}$, then 
$$
A + B =
\begin{bmatrix}
a_{11} + b_{11} & a_{12} + b_{12} & \cdots & a_{1n} + b_{1n} \\
a_{21} + b_{21} & a_{22} + b_{22} & \cdots & a_{2n} + b_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} + b_{m1} & a_{m2} + b_{m2} & \cdots & a_{mn} + b_{mn} \\
\end{bmatrix}
$$

The transpose $A^{T}$ is defined as

$$
A^{T} = 
\begin{bmatrix}
a_{11} & a_{21} & \cdots & a_{m1} \\
a_{12} & a_{22} & \cdots & a_{m2} \\
\vdots & \vdots & \ddots & \vdots \\
a_{1n} &a_{2n} & \cdots & a_{nm} \\
\end{bmatrix}.
$$

The transpose turns columns of the matrix into rows (equivalently rows into columns). A square matrix is called symmetric if $A=A^{T}$.

## Matrix multiplication

We discuss the following two important matrix multiplication operations

:::: {.incremental}
- matrix-vector multiplication,
- matrix-matrix multiplication.
::::

::: {.content-hidden when-profile="slides"}
### Matrix-vector multiplication
:::

::: {.content-visible when-profile="slides"}
## Matrix-vector multiplication
:::

Let $A\in\mathbb{R}^{m\times n}$ and $\mathbf{x}\in\mathbb{R}^{n}$, then $A\mathbf{x}\in\mathbb{R}^{m}$ is defined as 

$$
A\mathbf{x} = 
\begin{bmatrix}
x_1a_{11} + x_2 a_{12} + \cdots + x_na_{1n} \\
x_1a_{21} + x_2 a_{22} + \cdots + x_na_{2n} \\
\vdots \\
x_1a_{m1} + x_2 a_{m2} + \cdots + x_na_{mn} \\
\end{bmatrix}.
$$

::: {.content-visible when-profile="slides"}
## Matrix-vector multiplication continued
:::

Equivalently, this means that $A\mathbf{x}$ is a linear combination of the columns of $A$, i.e.,

$$
A\mathbf{x} = 
x_1 \begin{bmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1}  \end{bmatrix} 
+ 
x_2  \begin{bmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{m2}  \end{bmatrix}
+
\cdots
+
x_n \begin{bmatrix} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn}  \end{bmatrix}.
$$

Observe that the matrix $A$ is a linear transformation that maps vectors in $\mathbb{R}^{n}$ to $\mathbb{R}^{m}$.

::: {.content-hidden when-profile="slides"}
### Matrix-matrix multiplication
:::

::: {.content-visible when-profile="slides"}
## Matrix-matrix multiplication
:::

Let $A\in\mathbb{R}^{m\times n}$ and $B\in\mathbb{R}^{n\times p}$, then the elements of $C=AB\in\mathbb{R}^{m\times p}$ are

$$
c_{ij} = \sum_{k=1}^{n} a_{ik}b_{kj},
$$

for $i=1,\ldots, m$ and $j=1, \ldots, p$.

## Vector spaces

An essential concept in linear algebra is the notion of a **vector space**.

A vector space is a set $V$ such that for any 2 elements in the set, say $\mathbf{u},\mathbf{v}\in V$, and any scalars, $c$ and $d$, then $c\mathbf{u} + d\mathbf{v}\in V$. 

In addition, a vector space must satisfy the following properties

:::: {.incremental}
1. $\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}$ (associativity).
1. $\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$ (commutativity).
1. There exists $\mathbf{0}\in V$ such that $\mathbf{v} + \mathbf{0} = \mathbf{v}$ for all $\mathbf{v}\in V$ (identity element).
1. For every $\mathbf{v}\in V$, there exists $-\mathbf{v}\in V$ such that $\mathbf{v} + (-\mathbf{v}) = \mathbf{0}$ (inverse).
1. $c(d\mathbf{v}) = (cd)\mathbf{v}$
1. $1\mathbf{v} = \mathbf{v}$
1. $c(\mathbf{u} + \mathbf{v}) = c\mathbf{u} + c\mathbf{v}$
1. $(c + d)\mathbf{v} = c\mathbf{v} + d\mathbf{v}$
::::

::: {.content-hidden when-profile="web"}
## Vector spaces continued
:::

Some examples of vector spaces are:

:::: {.incremental}
- The set of n-dimensional vectors with real numbers, i.e., $\mathbb{R}^n$.
- Given a matrix $A\in\mathbb{R}^{m\times n}$, the column space $col(A)$, which is the span of all columns in the matrix $A$ is a vector space. The similarly defined row space is also a vector space.
- Given a matrix $A\in\mathbb{R}^{n\times n}$, the set of all solutions to the equation $A\mathbf{x} = \mathbf{0}$ is a vector space. This space is called the null space of matrix $A$.
- The set of all $m\times n$ matrices with real numbers is also a vector space.
- The set of all polynomials of degreen $n$ is a vector space.
::::

::: {.content-visible when-profile="web"}
## Important matrices

We introduce notation for some commonly used and important matrices.
:::

::: {.content-visible when-profile="slides"}
## Identity matrix
:::

The $n \times n$ identity matrix is

$$
I = 
\begin{bmatrix}
1 & 0 & \cdots & 0 \\
0 & 1 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & 1 \\
\end{bmatrix}.
$$

For every $A\in\mathbb{R}^{n\times n}$, then $AI = IA$. 

::: {.content-visible when-profile="slides"}
## Matrix inverse
:::

The inverse $A^{-1}\in\mathbb{R}^{n\times n}$ is defined as the matrix for which 
$$AA^{-1} = A^{-1}A = I,$$ 

When $A^{-1}$ exists the matrix is said to be invertible. 

Note that $(AB)^{-1} = B^{-1}A^{-1}$ for invertible $B\in\mathbb{R}^{n\times n}$.

::: {.content-visible when-profile="slides"}
## Diagonal matrices
:::
A diagonal matrix $D\in\mathbb{R}^{n\times n}$ has entries $d_{ij}=0$ if $i\neq j$, i.e.,

$$
D =
\begin{bmatrix}
d_{11} & 0 & \cdots & 0 \\
0 & d_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & d_{nn} \\
\end{bmatrix}.
$$

::: {.content-visible when-profile="slides"}
## Orthogonal matrices
:::
A square matrix $Q\in\mathbb{R}^{n}$ is orthogonal if 

$$QQ^{T}=Q^{T}Q=I.$$ 

In particular, the inverse of an orthogonal matrix is it's transpose.

::: {.content-visible when-profile="slides"}
## Lower triangular matrices
:::
A lower triangular matrix $L\in\mathbb{R}^{n\times n}$ is a matrix where all the entries above the main diagonal are zero

$$
L =
\begin{bmatrix}
l_{11} & 0 & \cdots & 0 \\
l_{12} & l_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
l_{1n} & l_{n2} & \cdots & l_{nn} \\
\end{bmatrix}.
$$

::: {.content-visible when-profile="slides"}
## Upper triangular matrices
:::
An upper triangular matrix $U\in\mathbb{R}^{n\times n}$ is a matrix where all the entries below the main diagonal are zero

$$
U = 
\begin{bmatrix}
u_{11} & u_{12} & \cdots & u_{1n} \\
0 & u_{22} & \cdots & u_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & u_{nn} \\
\end{bmatrix}.
$$

::: {.content-visible when-profile="web"}
The inverse of a lower triangular matrix is itself a lower triangular matrix. This is also true for upper triangular matrices, i.e., the inverse is also upper triangular.
:::

## Eigenvalues and eigenvectors

An eigenvector of an $n\times n$ matrix $A$ is a nonzero vector $\mathbf{x}$ such that 

$$A\mathbf{x} = \lambda\mathbf{x}$$ 

for some scalar $\lambda.$ 

The scalar $\lambda$ is called an eigenvalue.

An $n \times n$ matrix has at most $n$ distinct eigenvectors and at most $n$ distinct eigenvalues.

::: {.content-visible when-profile="slides"}
# Matrix decompositions
:::

::: {.content-visible when-profile="web"}
## Matrix decompositions

We introduce here important matrix decompositions. These are useful in solving linear equations. Furthermore they play an important role in various data science applications.
:::

::: {.content-visible when-profile="web"}
### LU factorization
:::

::: {.content-visible when-profile="slides"}
## LU factorization
:::

An LU decomposition of a square matrix $A\in\mathbb{R}^{n\times n}$ is a factorization of $A$ into a product of matrices

$$ 
A = LU,
$$

where $L$ is a lower triangular square matrix and $U$ is an triangular square matrix. For example, when $n=3$, we have

$$ 
\begin{bmatrix}
a_{11} & a_{12} & a_{13} \\
a_{21} & a_{22} & a_{23} \\
a_{31} & a_{32} & a_{33} \\
\end{bmatrix}
=
\begin{bmatrix}
l_{11} & 0 & 0 \\
l_{21} & l_{22} & 0\\
l_{31} & l_{32} & a_{33} \\
\end{bmatrix}
\begin{bmatrix}
u_{11} & u_{12} & u_{13} \\
0 & u_{22} & u_{23} \\
0 & 0 & u_{33} \\
\end{bmatrix}
$$

::: {.content-visible when-profile="web"}
### QR decomposition
:::

::: {.content-visible when-profile="slides"}
## QR factorization
:::

A QR decomposition of a square matrix $A\in\mathbb{R}^{n\times n}$ is a factorization of $A$ into a product of matrices

$$
A=QR,
$$
where $Q$ is an orthogonal square matrix and $R$ is an upper-triangular square matrix.

::: {.content-visible when-profile="web"}
### Eigendecomposition
:::

::: {.content-visible when-profile="slides"}
## Eigendecomposition
:::

Let $A\in\mathbb{R}^{n}$ have n linearly independent eigenvectors $\mathbf{x}_i$ for $i=1,\ldots, n$, then $A$ can be factorized as

$$
A = X\Lambda X^{-1},
$$
where $X$ is a matrix of the eigenvectors, and 

$$
\Lambda =
\begin{bmatrix}
\lambda_{1} & 0 & \cdots & 0 \\
0 & \lambda_{2} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \lambda_{n}  \\
\end{bmatrix},
$$
is a diagonal matrix of the eigenvalues. 

In this case, the matrix $A$ is said to be diagonalizable.

::: {.content-visible when-profile="slides"}
## Spectral decomposition
:::
A special case occurs when $A$ is symmetric. Recall that a matrix is symmetric when $A = A^T.$

In this case, it can be shown that the eigenvectors of $A$ are all mutually orthogonal. Consequently, $X^{-1} = X^{T}$ and we can decompose $A$ as:

$$A = XDX^T.$$

This amazing fact is known as the spectral theorem and this decomposition of $A$ is its spectral decomposition. The eigenvalues of a matrix are also called its spectrum.


::: {.content-visible when-profile="web"}
### Singular value decomposition
:::

::: {.content-visible when-profile="slides"}
## Singular value decomposition
:::


Let $A\in\mathbb{R}^{m\times n}$ with $m>n$, then $A$ admits a decomposition

$$
A = U\Sigma V^{T}.
$$
The matrices $U\in\mathbb{R}^{m\times m}$ and $V\in\mathbb{R}^{n\times n}$ are orthogonal. The columns of $U$ are the left singular vectors and the columns of $V$ are the right singular vectors.

The matrix $\Sigma\in\mathbb{R}^{m\times n}$ is a diagonal matrix of the form

$$
\Sigma = 
\begin{bmatrix}
\sigma_{11} & 0 & \cdots & 0 \\
0 & \sigma_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & \sigma_{mn} \\
\end{bmatrix}.
$$
The values $\sigma_{ij}$ are the singular values of the matrix $A$. Amazingly, it can be proven that every matrix $A\in\mathbb{R}^{m\times n}$ has a singular value decomposition.

## Linear systems of equations

A system of $m$ linear equations in $n$ unknowns can be written as

$$
\begin{align*}
a_{11} x_{1} + a_{12} x_{2} + \cdots + a_{1n} x_{n} &= b_1, \\
a_{21} x_{1} + a_{22} x_{2} + \cdots + a_{2n} x_{n} &= b_2, \\
\vdots  \qquad \qquad \quad \\
a_{m1} x_{1} + a_{m2} x_{2} + \cdots + a_{mn} x_{n} &= b_m.\\
\end{align*}
$$

Observe that this is simply the matrix vector equation

$$
A\mathbf{x}=\mathbf{b}.
$$

::: {.content-visible when-profile="slides"}
## Linear systems of equations continued
:::

A linear system of equations may have:

:::: {.incremental}
- infinitely many solutions,
- a unique solution,
- no solutions.
::::

:::: {.fragment}
When $m > n$, the system is said to be overdetermined and _in general_ has no  solutions. When $m < n$ the system is underdetermined and _in general_ has infinitely many solutions. For the case when $m=n$ and the matrix has $n$ linearly dependent columns, the solution is always unique.

For an invertible square matrix $A\mathbf{x}=\mathbf{b}$, the solution is always $\mathbf{x}=A^{-1}\mathbf{b}$. 
::::

::: {.content-visible when-profile="slides"}
## Linear systems of equations continued
:::
We can use matrix factorizations to help us solve a linear system of equation. We demonstrate how to do this with the LU decomposition. Observe that 
$$A\mathbf{x} = LU\mathbf{x} = \mathbf{b}.$$ 

Then 

$$
\mathbf{x} = U^{-1}L^{-1}\mathbf{b}.
$$ 

The process of inverting $L$ and $U$ is called backward and forward substitution.


## Least squares

In data science it is often the case that we have to solve the linear system 

$$ A \mathbf{x} = \mathbf{b}.$$ 

This problem may have no solution -- perhaps due to noise or measurement error.

In such a case, we look for a vector $\mathbf{x}$ such that $A\mathbf{x}$ is a good approximation to $\mathbf{b}.$

The quality of the approximation can be measured using the distance from $A\mathbf{x}$ to $\mathbf{b},$ i.e.,

$$\Vert A\mathbf{x} - \mathbf{b}\Vert_2.$$

::: {.content-visible when-profile="slides"}
## Least squares continued
:::
The general least-squares problem is given $A\in\mathbb{R}^{m\times n}$ and and $\mathbf{b}\in\mathbb{R}^{m}$, find a vector $\hat{\mathbf{x}}\in\mathbb{R}^{n}$ such that $\Vert A\mathbf{x}-\mathbf{b}\Vert_2$ is minimized, i.e. 

$$\hat{\mathbf{x}} = \arg\min_\mathbf{x} \Vert A\mathbf{x} - \mathbf{b}\Vert.$$

This emphasizes the fact that the least squares problem is a minimization problem.

::: {.content-visible when-profile="web"}
Minimizations problems are an example of a broad class of problems called _optimization_ problems. In optimization problems we attempt to find an optimal solution that minimizes (or maximizes) a set particular set of equations (and possibly constraints). 
:::

::: {.content-visible when-profile="slides"}
## Least squares continued
:::
We can connect the above minimization of the distance between vectors to the minimization of the sum of squared errors. Let $\mathbf{y} = A\mathbf{x}$ and observe that

$$\Vert A\mathbf{x}-\mathbf{b}\Vert_2^2 = \Vert \mathbf{y}-\mathbf{b}\Vert_2^2 =  \sum_i (y_i-b_i)^2.$$

::: {.content-visible when-profile="web"}
The above expression is the sum of squared errors. In statistics, the $y_i$ are the estimated values and the $b_i$ are the measured values. This is the most common measure of error used in statistics and is a key principle. 
:::

Minimizing the length of $A\mathbf{x} - \mathbf{b}$ is equivalent to minimizing the sum of the squared errors. 

::: {.content-visible when-profile="slides"}
## Least squares continued
:::
We can find $\hat{\mathbf{x}}$ using either 

:::: {.incremental}
* geometric arguments based on projections of the vector $\mathbf{b}$,
* by calculus (taking the derivative of the right-hand-side expression above and setting it equal to zero).
::::

:::: {.fragment}
Either way, we obtain the result that $\hat{\mathbf{x}}$ is the solution of:
    
$$A^TA\mathbf{x} = A^T\mathbf{b}.$$

This system of equations is called the normal equations.
::::

::: {.content-visible when-profile="slides"}
## Least squares continued
:::
We can prove that these equations always have at least one solution. 

When $A^TA$ is invertible, the system is said to be overdetermined. This means that there is a unique solution

$$\hat{\mathbf{x}} = (A^TA)^{-1}A^T\mathbf{b}.$$

::: {.content-visible when-profile="slides"}
## Least squares beware
:::
Be aware that computing the solution using $(A^TA)^{-1}A^T$ can be numerically unstable. A more stable method is to use the QR decomposition of $A$, i.e., $\hat{\mathbf{x}} = R^{-1}Q^T\mathbf{b}$. 

The NumPy function `np.linalg.lstsq()` solves the least squares problem in a stable way. 

Consider the following example where the solution to the least squares problem is `x=np.array([1, 1])`.

```{python}
#| code-fold: false
import numpy as np
A = np.array([[-1.42382504, -1.4238264 ],
              [ 1.26372846,  1.26372911], 
              [-0.87066174, -0.87066138]])
b = A @ np.array([1, 1])

# Pseudoinverse
x1 = np.linalg.inv(A.T @ A) @ A.T @ b
print(x1)
# QR
[Q, R] = np.linalg.qr(A)
x2 = np.linalg.solve(R, Q.T @ b)
print(x2)
# np.linalg.lstsq
x3, _, _, _ = np.linalg.lstsq(A, b, rcond=None)
print(x3) 
print(np.linalg.norm(x1-np.array([1, 1])))
print(np.linalg.norm(x2-np.array([1, 1])))
print(np.linalg.norm(x3-np.array([1, 1])))
```



