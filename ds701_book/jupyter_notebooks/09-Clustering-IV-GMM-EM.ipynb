{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Gaussian Mixture Models\n",
        "jupyter: python3\n",
        "fig-align: center\n",
        "---\n",
        "\n",
        "[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/09-Clustering-IV-GMM-EM.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](figs/L09-MultivariateNormal.png){fig-align=\"center\"}\n",
        "\n",
        "## From Hard to Soft Clustering\n",
        "\n",
        "__So far,__ we have seen how to cluster objects using $k$-means: \n",
        "\n",
        "1. Start with an initial set of cluster centers,\n",
        "1. Assign each object to its closest cluster center \n",
        "1. Recompute the centers of the new clusters\n",
        "1. Repeat 2 $\\rightarrow$ 3 until convergence\n",
        "\n",
        "In $k$-means clustering every object is assigned to a **single** cluster. \n",
        "\n",
        "This is called __hard__ assignment.\n",
        "\n",
        "However, there may be cases where we either __cannot__ use hard assignments or we do not __want__ to do it.\n",
        "\n",
        "In particular, we might have reason to believe that the best description of the data is a set of __overlapping__ clusters.\n",
        "\n",
        "# Overlapping Clusters\n",
        "\n",
        "## Rich or Poor Example\n",
        "\n",
        "Let's consider as an example a society that consists of just __two__ kinds of individuals: poor or rich.\n",
        "\n",
        "How might we model society as a mixture of poor and rich, when viewed in terms of the single feature age?\n",
        "\n",
        "## Rich or Poor Sampling\n",
        "\n",
        "Let's sample 20,000 rich individuals and 20,000 poor individuals. From this sample we get have the following histograms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "# original inspiration for this example from\n",
        "# https://www.cs.cmu.edu/~./awm/tutorials/gmm14.pdf\n",
        "from scipy.stats import multivariate_normal\n",
        "np.random.seed(4)\n",
        "df = pd.DataFrame(multivariate_normal.rvs(mean = np.array([37, 45]), cov = np.diag([196, 121]), size = 20000),\n",
        "                   columns = ['poor', 'rich'])\n",
        "df.hist(bins = range(80), sharex = True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We find that ages of the poor set have mean $\\mu=37$ with standard deviation $\\sigma=14$, while the ages of the rich set have mean $\\mu=45$ with standard deviation $\\sigma=11$.\n",
        "\n",
        "## Rich or Poor by Age"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "from scipy.stats import norm\n",
        "x = np.linspace(norm.ppf(0.001, loc = 37, scale = 14), norm.ppf(0.999, loc = 37, scale = 14), 100)\n",
        "plt.plot(x, norm.pdf(x, loc = 37, scale = 14),'b-', lw = 5, alpha = 0.6, label = 'poor')\n",
        "x = np.linspace(norm.ppf(0.001, loc = 45, scale = 11), norm.ppf(0.999, loc = 45, scale = 11), 100)\n",
        "plt.plot(x, norm.pdf(x, loc = 45, scale = 11),'g-', lw = 5, alpha = 0.6, label = 'rich')\n",
        "plt.xlim([15, 70])\n",
        "plt.xlabel('Age', size=14)\n",
        "plt.legend(loc = 'best')\n",
        "plt.title('Age Distributions')\n",
        "plt.ylabel(r'$p(x)$', size=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Soft Clustering\n",
        "\n",
        "Viewed along the age dimension we observe that there are two overlapping clusters.\n",
        "\n",
        "Furthermore, given some particular individual at a given age, say 25, we cannot say for sure which cluster they belong to.  \n",
        "\n",
        "Rather, we will use _probability_ to quantify our uncertainty about the cluster that any single individual belongs to.\n",
        "\n",
        "Thus, we could say that a given individual (\"John Smith\", age 25) belongs to the _rich_ cluster with some probability and the _poor_ cluster with a different probability.\n",
        "\n",
        "Naturally we expect the probabilities for John Smith to sum up to 1.\n",
        "\n",
        "This is called __soft assignment,__ and a clustering using this principle is called __soft clustering.__\n",
        "\n",
        "## Conditional Probability\n",
        "\n",
        "More formally, we say that an object can belong to each particular cluster with some probability, such that the sum of the probabilities adds up to 1 for each object. \n",
        "\n",
        "For example, assuming that we have two clusters $C_1$ and $C_2$, we can have that an object $x_1$ belongs to $C_1$ with probability $0.3$ and to $C_2$ with probability $0.7$.\n",
        "\n",
        "Note that the distribution over $C_1$ and $C_2$ only refers to object $x_1$.\n",
        "\n",
        "Thus, it is a __conditional__ probability:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "P(C_1 \\,|\\, x_1) &= 0.3, \\\\\n",
        "P(C_2 \\,|\\, x_1) &= 0.7.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "And to return to our previous example\n",
        "\n",
        "$$\n",
        "P(\\text{rich}\\,|\\,\\text{age 25}) + P(\\text{poor}\\,|\\,\\text{age 25}) = 1.\n",
        "$$\n",
        "\n",
        "## Lecture Overview\n",
        "\n",
        "Our goal with Gaussian mixture models (GMMs) is to show how to compute the probabilities that a data point belongs to a particular cluster.\n",
        "\n",
        "The main ideas behind GMMs are\n",
        "\n",
        "- Assume each cluster is normally distributed\n",
        "- Use the Expectation Maximization (ME) algorithm to iteratively determine the parameters of our the Gaussian clusters\n",
        "- Determine probabilities on whether a data point belongs to a particular cluster\n",
        "\n",
        "We will see that GMMs are better suited for clustering non-spherical distributions of points.\n",
        "\n",
        "To help us in understanding GMMs we will first review maximum likelihood estimation.\n",
        "\n",
        "# Maximum Likelihood Estimation (MLE)\n",
        "\n",
        "## Motivation\n",
        "\n",
        "Probability distributions are specified by their parameters.\n",
        "\n",
        "The Gaussian (Normal) distribution is determined by the parameters $\\mu$ and $\\sigma^{2}$, i.e.,\n",
        "\n",
        "$$\n",
        "f(x\\vert\\mu, \\sigma^{2}) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu^2)}{2\\sigma^2}}.\n",
        "$$\n",
        "\n",
        "**Parameter estimation** is the process of determining the parameters of a distribution based on a sample of observed data.\n",
        "\n",
        "**Maximum likelihood estimation** is a method estimate the parameters of an assumed probability distribution given a sample of observed data.\n",
        "\n",
        "## Likelihood Function\n",
        "\n",
        "The likelihood function $L(\\boldsymbol{\\theta}, x)$ represents the probability of observing the given data $x$ as a function of the parameters $\\boldsymbol{\\theta}$ of the distribution.\n",
        "\n",
        "The primary purpose of the likelihood function is to estimate the parameters that make the observed data $x$ most probable.\n",
        "\n",
        "The likelihood function for a set of samples $x_{n}~\\text{for}~n=1, \\ldots, N$ drawn from a Gaussian distribution is\n",
        "\n",
        "$$\n",
        "L(\\mu, \\sigma^{2}, x_1, \\ldots, x_n) = \\prod_{n=1}^{N}(x_n\\vert \\mu, \\sigma^{2}) = \\prod_{n=1}^{N}\\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x_n-\\mu^2)}{2\\sigma^2}}.\n",
        "$$\n",
        "\n",
        "--- \n",
        "\n",
        "For a particular set of parameters $\\mu, \\sigma^{2}$\n",
        "\n",
        "- **large** values of $L(\\mu, \\sigma^{2}, x_1, \\ldots, x_n)$ indicate the observed data is very probable (high likelihood)\n",
        "- **small** values of $L(\\mu, \\sigma^{2}, x_1, \\ldots, x_n)$ indicate the observed data is very improbable (low likelihood)\n",
        "\n",
        ":::: {.fragment}\n",
        "The parameters that maximize the likelihood function are called the **maximum likelihood estimates**.\n",
        "::::\n",
        "\n",
        "## Log-likelihood\n",
        "\n",
        "A common manipulation to obtain a more useful form of the likelihood function is to take its natural logarithm, i.e.,\n",
        "\n",
        "$$\n",
        "\\log{\\left(L(\\mu, \\sigma^{2}, x_1, \\ldots, x_n)\\right)} = -\\frac{N}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^{2}\\sum_{n=1}^{N}(x_n -\\mu)^{2}}.\n",
        "$$\n",
        "\n",
        "Using the log-likelihood we will be able to derive formulas for the maximum likelihood estimates. \n",
        "\n",
        "## Maximizing the log-likelihood\n",
        "\n",
        ":::: {.fragment}\n",
        "How do we maximize (optimize) a function of parameters?\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "To find the optimal parameters of a function, we compute partial derivatives of the function and set them equal to zero. The solution to these equations gives us a local optimal value for the parameters\n",
        "\n",
        "For the case of the Gaussian we compute\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla_{\\mu} L(\\mu, \\sigma^{2}, x_1, \\ldots, x_n) &= 0 \\\\\n",
        "\\nabla_{\\sigma} L(\\mu, \\sigma^{2}, x_1, \\ldots, x_n) &= 0 \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "::::\n",
        "\n",
        "## Gaussian MLEs\n",
        "\n",
        "The maximum likelihood estimates for a Gaussian distribution are given by\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\bar{\\mu} &= \\frac{1}{N}\\sum_{n=1}^{N} x_{n}, \\\\\n",
        "\\bar{\\sigma}^2 &= \\frac{1}{N}\\sum_{n=1}^{N}(x_{n} - \\mu)^{2}.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "## Summary of MLE\n",
        "\n",
        "Given samples $x_{1}, \\ldots, x_n$ drawn from a Gaussian distribution, we computed the maximum likelihood estimates by maximizing the log likelihood function\n",
        "\n",
        "$$\n",
        "\\log{\\left(L(\\mu, \\sigma^{2}, x_1, \\ldots, x_n)\\right)} = -\\frac{N}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^{2}\\sum_{n=1}^{N}(x_i -\\mu)^{2}}.\n",
        "$$\n",
        "\n",
        "This gives us the maximum likelihood estimates\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\bar{\\mu} &= \\frac{1}{N}\\sum_{n=1}^{N} x_{i}, \\\\\n",
        "\\bar{\\sigma}^2 &= \\frac{1}{N}\\sum_{n=1}^{N}(x_{i} - \\mu)^{2}.\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "We use this information to help our understanding of Gaussian Mixture models.\n",
        "\n",
        "# Gaussian Mixture Models\n",
        "\n",
        "## Univariate Gaussians\n",
        "\n",
        "In GMMs we assume that each of our clusters follows a Gaussian (normal) distribution with their own parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the parameters for the 4 Gaussians\n",
        "params = [\n",
        "    {\"mean\": 0, \"variance\": 1, \"color\": \"#377eb8\"},  # Blue\n",
        "    {\"mean\": 2, \"variance\": 0.5, \"color\": \"#4daf4a\"},  # Green\n",
        "    {\"mean\": -2, \"variance\": 1.5, \"color\": \"#e41a1c\"},  # Red\n",
        "    {\"mean\": 1, \"variance\": 2, \"color\": \"#984ea3\"}  # Purple\n",
        "]\n",
        "\n",
        "# Generate x values\n",
        "x = np.linspace(-10, 10, 1000)\n",
        "\n",
        "# Plot each Gaussian\n",
        "for param in params:\n",
        "    mean = param[\"mean\"]\n",
        "    variance = param[\"variance\"]\n",
        "    color = param[\"color\"]\n",
        "    \n",
        "    # Calculate the Gaussian\n",
        "    y = (1 / np.sqrt(2 * np.pi * variance)) * np.exp(- (x - mean)**2 / (2 * variance))\n",
        "    \n",
        "    # Plot the Gaussian\n",
        "    plt.plot(x, y, color=color, label=f\"$\\\\mu$: {mean}, $\\\\sigma^2$: {variance}\")\n",
        "\n",
        "# Add legend\n",
        "plt.legend()\n",
        "\n",
        "# Add title and labels\n",
        "plt.title(\"1D Gaussians with Different Means and Variances\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"Probability Density\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.notes}\n",
        "This is a similar situation to our previous example where we considered labeling a person as poor or rich based on the single feature age.\n",
        "\n",
        "This could be a hypothetical situation with $K=4$ normally distributed clusters.\n",
        ":::\n",
        "\n",
        "## Multivariate Gaussians\n",
        "\n",
        "Given data points $\\mathbf{x}\\in\\mathbb{R}^{d}$, i.e., $d$-dimensional points, then we use the multivariate Gaussian to describe the clusters. The formula for the multivariate Gaussian is\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}\\vert \\bar{\\mu}, \\Sigma) = \\frac{1}{(2\\pi)^{d/2}\\vert \\Sigma \\vert^{1/2}}e^{-\\frac{1}{2}(\\mathbf{x}-\\boldsymbol{\\mu})^T\\Sigma^{-1}(\\mathbf{x}-\\boldsymbol{\\mu})},\n",
        "$$\n",
        "\n",
        "where $\\Sigma\\in\\mathbb{R}^{d\\times d}$ is the covariance matrix, $\\vert \\Sigma\\vert$, denote the determinant of $\\Sigma$, and $\\boldsymbol{\\mu}$ is the $d$-dimensional vector of expected values.\n",
        "\n",
        "::: {.content-hidden when-profile=\"web\"}\n",
        "Recall also that the shape of a multivariate Gaussian – the direction of its axes and the width along each axis – is determined by the covariance matrix .\n",
        "\n",
        "The covariance matrix is the multidimensional analog of the variance. It determines the extent to which vector components are correlated.\n",
        ":::\n",
        "\n",
        "## Notation\n",
        "\n",
        "The notation $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$ is used to denote a univariate random variable that is normally distributed with expected value $\\mu$ and variance $\\sigma^{2}.$\n",
        "\n",
        "The notation $\\mathbf{x} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\Sigma)$ is used to denote a multivariate random variable that is normally distributed with mean $\\bar{\\boldsymbol{\\mu}}$ and covariance matrix $\\Sigma$.\n",
        "\n",
        "## Multivariate Example: Cars\n",
        "\n",
        "To illustrate a particular model, let us consider the properties of cars produced in the US, Europe, and Asia.\n",
        "\n",
        "For example, let's say we are looking to classify cars based on their model year and miles per gallon (mpg).  \n",
        "\n",
        "<!-- image credit: https://www.cs.cmu.edu/~./awm/tutorials/gmm14.pdf -->\n",
        "![](figs/L09-multivariate-example.png){width=\"90%}\n",
        "\n",
        "It seems that the data can be described (roughly) as a mixture of __three__ __multivariate__ Gaussian distributions.\n",
        "\n",
        "## Intuition\n",
        "\n",
        "Given $K$ clusters, the goal with a GMM is to determine the probability for whether a point $\\mathbf{x}\\in\\mathbb{R}^{d}$ belongs to a particular cluster. \n",
        "\n",
        "We assume that each cluster is distributed normally, according to some (unknown) parameters $\\boldsymbol{\\mu}_i, \\Sigma_i$. We also assume that the probability that a point belongs to a particular cluster is given by $w_i$.\n",
        "\n",
        "A Gaussian mixture model is defined by these parameters $w_i, \\boldsymbol{\\mu}_i, \\Sigma_i$ for $i=1, \\ldots, K$. \n",
        "\n",
        "We can use these parameters to compute the probability that a point $\\mathbf{x}$ belongs to a particular cluster $C_k$.\n",
        "\n",
        "Similar to MLE, we must compute the parameters $w_i, \\boldsymbol{\\mu}_i, \\Sigma_i$.\n",
        "\n",
        "## Learning the Parameters of a GMM\n",
        "\n",
        "Given some data how do we estimate the \n",
        "\n",
        "* cluster probability $w_i$,\n",
        "* cluster mean $\\boldsymbol{\\mu}_i$, and \n",
        "* cluster covariance  $\\Sigma_i$\n",
        "\n",
        "for each $i=1, \\ldots, K$?\n",
        "\n",
        ":::: {.fragment}\n",
        "We will formulate a likelihood function for these parameters $\\boldsymbol{\\theta}_i = (w_i, \\boldsymbol{\\mu}_i, \\Sigma_i)$ and then optimize this function.\n",
        "::::\n",
        "\n",
        "## Latent Variables\n",
        "\n",
        "We assume that each data point $x$ is produced by a **latent variable** $z$.\n",
        "\n",
        "This variable is called latent because it is never actually observed. It is used to help indicate cluster membership and is helpful for the derivation of the likelihood function.\n",
        "\n",
        "We can view this as a one-hot-encoding that identifies a probability of membership to one of the $K$ clusters.\n",
        "\n",
        "We impose a distribution over $z$ representing a soft assignment\n",
        "\n",
        "$$\n",
        "p(z) = (w_1, \\cdots, w_K)^{T}, \\quad \\text{with} \\quad 0\\leq w_i \\leq 1 \\quad \\text{and} \\quad \\sum_{i=1}^{K}w_i = 1.\n",
        "$$\n",
        "\n",
        "## PDF of a GMM\n",
        "\n",
        "With our distribution weights $w_i$ for the $K$ clusters, we can compute the probability density for a GMM at any point $\\mathbf{x}$ via the formula\n",
        "\n",
        "$$\n",
        "p(\\mathbf{x}) = \\sum_{i=1}^{K} w_i \\cdot f(\\mathbf{x}\\vert \\mu_i, \\Sigma_i).\n",
        "$$\n",
        "\n",
        "If this is feeling a bit abstract, let's see how this applies to our the initial example where we were clustering a person as rich or poor based on their age.\n",
        "\n",
        "## Rich vs Poor Example\n",
        "\n",
        "We have our single data point $x=25$, and we have 2 Gaussians representing our clusters, $C_1$ and $C_2$, with parameters $\\mu_1 = 37, \\sigma_1^{2}=14$ and $\\mu_2 = 45, \\sigma_2^{2}=11$, respectively.\n",
        "\n",
        "Our latent variables are the probabilities that we classify someone as either rich or poor, i.e., $w_1 = P(z = 1) = P(\\text{rich})$ and $w_2 = P(z = 2) = P(\\text{poor})$.\n",
        "\n",
        "\n",
        "Our interest in this problem is computing the posterior probability, which is, $P(C_1 | x) = P(z=1 \\vert x) P(\\text{rich} \\vert \\text{age 25})$\n",
        "\n",
        ":::: {.fragment}\n",
        "How do we determine these posterior probabilities $P(C_i \\vert x) = P(z=1 \\vert x)$?\n",
        "::::\n",
        "\n",
        ":::: {.fragment}\n",
        "The answer is Bayes' Rule.\n",
        "::::\n",
        "\n",
        "\n",
        "## Rich vs Poor Example Part 2\n",
        "\n",
        "If we know the parameters of the Gaussians, we can determine the value of $P(x | C_1)$ when $x=25$. This is shown with the red dot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "x = np.linspace(norm.ppf(0.001, loc = 37, scale = 14), norm.ppf(0.999, loc = 37, scale = 14), 100)\n",
        "plt.plot(x, norm.pdf(x, loc = 37, scale = 14),'b-', lw = 5, alpha = 0.6, label = 'poor')\n",
        "x = np.linspace(norm.ppf(0.001, loc = 45, scale = 11), norm.ppf(0.999, loc = 45, scale = 11), 100)\n",
        "plt.plot(x, norm.pdf(x, loc = 45, scale = 11),'g-', lw = 5, alpha = 0.6, label = 'rich')\n",
        "plt.plot(25, norm.pdf(25, loc = 45, scale = 11), 'ro')\n",
        "plt.xlim([15, 70])\n",
        "plt.xlabel('Age', size=14)\n",
        "plt.legend(loc = 'best')\n",
        "plt.title('Age Distributions')\n",
        "plt.ylabel(r'$p(x)$', size=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bayes' Rule then allows us to compute \n",
        "\n",
        "$$\n",
        "P(C_1 \\vert x)=\\frac{P(x\\vert C_1)}{P(x)}P(C_1).\n",
        "$$\n",
        "\n",
        "We can always use the law of total probability to compute\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "P(x) &= P(x \\vert C_1)P(C_1) + P(x\\vert C_2)P(C_2), \\\\\n",
        "&=  P(z=1) P(x \\vert z=1) + P(z=2) P(x\\vert z=2), \\\\\n",
        "&=  w_1 P(x \\vert z=1) + w_2 P(x\\vert z=2), \\\\\n",
        "&= \\sum_{i=1}^{2} w_i \\cdot f(\\mathbf{x}\\vert \\mu_i, \\sigma_i).\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "::: {.content-hidden when-profile=\"slides\"}\n",
        "The final formula is illustrated in the following figure.\n",
        ":::\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure()\n",
        "x = np.linspace(norm.ppf(0.001, loc = 37, scale = 14), norm.ppf(0.999, loc = 37, scale = 14), 100)\n",
        "plt.plot(x, norm.pdf(x, loc = 37, scale = 14),'b-', lw = 5, alpha = 0.6, label = 'poor')\n",
        "plt.plot(25, norm.pdf(25, loc = 37, scale = 14), 'ko', markersize = 8)\n",
        "x = np.linspace(norm.ppf(0.001, loc = 45, scale = 11), norm.ppf(0.999, loc = 45, scale = 11), 100)\n",
        "plt.plot(x, norm.pdf(x, loc = 45, scale = 11),'g-', lw = 5, alpha = 0.6, label = 'rich')\n",
        "plt.plot(25, norm.pdf(25, loc = 45, scale = 11), 'ro', markersize = 8)\n",
        "plt.xlim([15, 70])\n",
        "plt.xlabel('Age', size=14)\n",
        "plt.legend(loc = 'best')\n",
        "plt.title('Age Distributions')\n",
        "plt.ylabel(r'$p(x)$', size=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$ P(\\text{rich}\\,|\\,\\text{age 25}) = \\frac{\\text{red}}{\\text{red} \\cdot P(\\text{rich}) + \\text{black} \\cdot P(\\text{poor})} \\cdot P(\\text{rich}).$$\n",
        "\n",
        "## GMM Likelihood\n",
        "\n",
        "Let's gather for all $i=1, \\ldots, K$, the coefficients $w_i,\\boldsymbol{\\mu}_i, \\Sigma_i$ into a vector $\\boldsymbol{\\theta}$.\n",
        "\n",
        "The likelihood function for a Gaussian mixture model with $N$ data points\n",
        "\n",
        "$$\n",
        "L(\\boldsymbol{\\theta}, x_1, \\ldots, x_n) = \\prod_{n=1}^{N}\\sum_{k=1}^{K}w_k \\frac{1}{(2\\pi)^{d/2}\\vert \\Sigma_k \\vert^{1/2}}e^{-\\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)^T\\Sigma_{k}^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_{k})}\n",
        "$$\n",
        "\n",
        "The log-likelihood is\n",
        "\n",
        "$$\n",
        "\\log{\\left(L(\\boldsymbol{\\theta}, x_1, \\ldots, x_n)\\right)} = \\sum_{n=1}^{N}\\log{\\left(\\sum_{k=1}^{K}w_k \\frac{1}{(2\\pi)^{d/2}\\vert \\Sigma_k \\vert^{1/2}}e^{-\\frac{1}{2}(\\mathbf{x}_n-\\boldsymbol{\\mu}_k)^T\\Sigma_{k}^{-1}(\\mathbf{x}_n-\\boldsymbol{\\mu}_{k})}\\right)}.\n",
        "$$\n",
        "\n",
        "Unlike the log of a product, the log of a sum does not simplify nicely. As a result, the partial derivatives with respect to $\\boldsymbol{\\mu}_k$, depend on the covariances and mixture weights (similarly for the other partial derivatives of $\\Sigma_k$ and $w_k$).\n",
        "\n",
        "To solve this problem we turn to __Expectation Maximization__.\n",
        "\n",
        "## Expectation Maximization\n",
        "\n",
        "This is another famous algorithm, in the same \"super-algorithm\" league as $k$-means.\n",
        "\n",
        "EM is formulated using a probabilistic model for data. It can solve a problem like\n",
        "\n",
        "> Given a set of data points and a parameter $K$, find the $(w_k, \\mu_k, \\Sigma_k)~k = 1,\\dots,K$ that __maximizes the likelihood of the data__ assuming a GMM with those parameters.\n",
        "\n",
        "It can also solve lots of other problems involving maximizing likelihood of data under a different model.\n",
        "\n",
        "Similar to $k$-means, this problem is NP-hard.  \n",
        "\n",
        "Furthermore, EM only guarantees that we will find a __local__ optimum of the objective function.\n",
        "\n",
        "## Expectation Maximization for GMM -- The Algorithm\n",
        "\n",
        "::: {.content-hidden when-profile=\"slides\"}\n",
        "Here is the EM algorithm.\n",
        ":::\n",
        "\n",
        "__Step 1 Initialization__\n",
        "\n",
        "Initialize the parameters $w_k, \\boldsymbol{\\mu}_k, \\Sigma_k$ for $k=1, \\dots, K$. The final result will be sensitive to this choice, so a good (and fast) initialization procedure is $k$-means.\n",
        "\n",
        "__Step 2 Expectation__ \n",
        "\n",
        "Use the current values for $w_k, \\boldsymbol{\\mu}_k, \\Sigma_k$ and for each of the $N$ data points $x_n$, compute the posterior probabilities\n",
        "\n",
        "$$\n",
        "r_{nk} = \\frac{w_k f(\\mathbf{x}_n \\vert \\boldsymbol{\\mu}_{k}, \\Sigma_k)}{\\sum_{i=1}^{K}w_i f(\\mathbf{x}_n \\vert \\boldsymbol{\\mu}_{j}, \\Sigma_j)},\n",
        "$$\n",
        "\n",
        "where $f(\\mathbf{x}_n \\vert \\boldsymbol{\\mu}_{k}, \\Sigma_k)$ is the multivariate Gaussian.\n",
        "\n",
        "__Step 3 Maximization__ \n",
        "\n",
        "Using the values $r_{nk}$ for $n=1,\\ldots, N$ and $k=1, \\ldots, K$. First compute $N_{k} = \\sum_{n=1}^{N} r_{nk}$. Then compute updated $w_k, \\boldsymbol{\\mu}_k, \\Sigma_k$ according to the formulas\n",
        "\n",
        "$$\n",
        "\\boldsymbol{\\mu}_{k}= \\frac{1}{N_k}\\sum_{n=1}^{N} r_{nk} \\mathbf{x}_n, \\quad \n",
        "\\Sigma_{k} = \\frac{1}{N_k}\\sum_{n=1}^{N} (\\mathbf{x}_n \\boldsymbol{\\mu}_k)^{T}(\\mathbf{x}_n -\\boldsymbol{\\mu}_k), \\quad\n",
        "w_k = \\frac{N_k}{N}.\n",
        "$$\n",
        "\n",
        "__Step 4__ Stop if convergence criterion is satisfied. Otherwise repeat Steps 2 and 3.\n",
        "\n",
        "## Storage and Computational Costs\n",
        "\n",
        "It is important to be aware of the computational costs of our algorithm.\n",
        "\n",
        "Storage costs:\n",
        "\n",
        "- There are $N$ $d$-dimensional points\n",
        "- There are $K$ clusters\n",
        "- There are $N\\times K$ coefficients $r_{nk}$\n",
        "- There are $K$ $d$-dimensional cluster centers $\\boldsymbol{\\mu}_k$\n",
        "- There are $K$ $d\\times d$ covariance matrices $\\Sigma_k$\n",
        "- There are $K$ weights $w_k$\n",
        "\n",
        "Computational costs:\n",
        "\n",
        "- Computing each $r_{nk}$ requires a sum of $K$ evaluations of the Gaussian PDF.\n",
        "- Updating $\\boldsymbol{\\mu}_k$ requires $N$ vector summations\n",
        "- Updating $\\Sigma_k$ requires $N$ outer products\n",
        "- Updating $w_k$ requires a division (though we must compute $N_k$)\n",
        "\n",
        "## k-means vs GMMs\n",
        "\n",
        "Let's pause for a minute and compare GMM/EM with $k$-means.\n",
        "\n",
        "__GMM/EM__\n",
        "\n",
        "1. Initialize randomly or using some rule\n",
        "1. Compute the probability that each point belongs in each cluster\n",
        "1. Update the clusters (weights, means and variances).\n",
        "1. Repeat 2-3 until convergence.\n",
        "\n",
        "__$k$-means__\n",
        "\n",
        "1. Initialize randomly or using some rule\n",
        "1. Assign each point to a single cluster\n",
        "1. Update the clusters (means).\n",
        "1. Repeat 2-3 until convergence.\n",
        "\n",
        "--- \n",
        "\n",
        "From a practical standpoint, the main difference is that in GMMs, data points do not belong to a __single__ cluster, but have some probability of belonging to __each__ cluster.\n",
        "\n",
        "In other words, as stated previously, GMMs use soft assignment.\n",
        "\n",
        "For that reason, GMMs are also sometimes called __soft $k$-means.__\n",
        "\n",
        "---\n",
        "\n",
        "However, there is also an important conceptual difference. \n",
        "\n",
        "The GMM starts by making an __explicit assumption__ about how the data were generated.  \n",
        "\n",
        "It says: \"the data came from a collection of multivariate Gaussians.\"\n",
        "\n",
        "We made no such assumption when we came up with the $k$-means problem. In that case, we simply defined an objective function and declared that it was a good one.\n",
        "\n",
        "Nonetheless, it appears that we were making a sort of Gaussian assumption when we formulated the $k$-means objective function. However, __it wasn't explicitly stated.__\n",
        "\n",
        "The point is that because the GMM makes its assumptions explicit, we can\n",
        "\n",
        "* examine them and think about whether they are valid, and\n",
        "* replace them with different assumptions if we wish.\n",
        "\n",
        "For example, it is perfectly valid to replace the Gaussian assumption with some other probability distribution.  As long as we can estimate the parameters of such distributions from data (e.g., have MLEs), we can use EM in that case as well.\n",
        "\n",
        "## Versatility of EM\n",
        "\n",
        "A final statement about EM generally. EM is a versatile algorithm that can be used in many other settings.  What is the main idea behind it?\n",
        "\n",
        "Notice that the problem definition only required that we find the clusters, $C_i$, meaning that we were to find the $(\\mu_i, \\Sigma_i)$.\n",
        "\n",
        "However, the EM algorithm posited that we should find as well the $P(C_j|x_i) = P(z=j | x_i)$, that is, the probability that each point is a member of each cluster.\n",
        "\n",
        "This is the true heart of what EM does.   \n",
        "\n",
        "The idea is called \"data augmentation.\"\n",
        "\n",
        "By __adding parameters__ to the problem, it actually finds a way to make the problem solvable.\n",
        "\n",
        "These are the latent parameters we introduced earlier. Latent parameters don't show up in the solution.\n",
        "\n",
        "## Example\n",
        "::: {.content-hidden when-profile=\"slides\"}\n",
        "Here is an example using **GMM**.\n",
        ":::\n",
        "\n",
        "We're going to create two clusters, one spherical, and one highly skewed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Number of samples of larger component\n",
        "n_samples = 1000\n",
        "\n",
        "# C is a transfomation that will make a heavily skewed 2-D Gaussian\n",
        "C = np.array([[0.1, -0.1], [1.7, .4]])\n",
        "\n",
        "print(f'The covariance matrix of our skewed cluster will be:\\n {C.T@C}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rng = np.random.default_rng(0)\n",
        "\n",
        "# now we construct a data matrix that has n_samples from the skewed distribution,\n",
        "# and n_samples/2 from a symmetric distribution offset to position (-4, 2)\n",
        "X = np.r_[(rng.standard_normal((n_samples, 2)) @ C),\n",
        "          .7 * rng.standard_normal((n_samples//2, 2)) + np.array([-4, 2])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], s = 10, alpha = 0.8)\n",
        "plt.axis('equal')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Fit a mixture of Gaussians with EM using two components\n",
        "import sklearn.mixture\n",
        "gmm = sklearn.mixture.GaussianMixture(n_components=2, \n",
        "                                      covariance_type='full', \n",
        "                                      init_params = 'kmeans')\n",
        "y_pred = gmm.fit_predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "colors = ['bg'[p] for p in y_pred]\n",
        "plt.title('Clustering via GMM')\n",
        "plt.axis('off')\n",
        "plt.axis('equal')\n",
        "plt.scatter(X[:, 0], X[:, 1], color = colors, s = 10, alpha = 0.8)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for clus in range(2):\n",
        "    print(f'Cluster {clus}:')\n",
        "    print(f' weight: {gmm.weights_[clus]:0.3f}')\n",
        "    print(f' mean: {gmm.means_[clus]}')\n",
        "    print(f' cov: \\n{gmm.covariances_[clus]}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison with k-means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sklearn.cluster\n",
        "kmeans = sklearn.cluster.KMeans(init = 'k-means++', n_clusters = 2, n_init = 100)\n",
        "y_pred_kmeans = kmeans.fit_predict(X)\n",
        "colors = ['bg'[p] for p in y_pred_kmeans]\n",
        "plt.title('Clustering via $k$-means\\n$k$-means centers: red, GMM centers: black')\n",
        "plt.axis('off')\n",
        "plt.axis('equal')\n",
        "plt.scatter(X[:, 0], X[:, 1], color = colors, s = 10, alpha = 0.8)\n",
        "plt.plot(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], 'ro')\n",
        "plt.plot(gmm.means_[:,0], gmm.means_[:, 1], 'ko')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for clus in range(2):\n",
        "    print(f'Cluster {clus}:')\n",
        "    print(f' center: {kmeans.cluster_centers_[clus]}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overlapping Clusters\n",
        "\n",
        "Now, let's construct __overlapping__ clusters.  What will happen?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "source": [
        "X = np.r_[(rng.standard_normal((n_samples, 2)) @ C),\n",
        "          .7 * rng.standard_normal((n_samples//2, 2))]\n",
        "gmm = sklearn.mixture.GaussianMixture(n_components=2, covariance_type='full')\n",
        "y_pred_over = gmm.fit_predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "colors = ['bgrky'[p] for p in y_pred_over]\n",
        "plt.title('GMM for overlapping clusters\\nNote they have nearly the same center')\n",
        "plt.scatter(X[:, 0], X[:, 1], color = colors, s = 10, alpha = 0.8)\n",
        "plt.axis('equal')\n",
        "plt.axis('off')\n",
        "plt.plot(gmm.means_[:,0], gmm.means_[:,1], 'ro')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for clus in range(2):\n",
        "    print(f'Cluster {clus}:')\n",
        "    print(f' weight: {gmm.weights_[clus]:0.3f}')\n",
        "    print(f' mean: {gmm.means_[clus]}\\n')\n",
        "    # print(f' cov: \\n{gmm.covariances_[clus]}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How many parameters are estimated?\n",
        "\n",
        "Most of the parameters in the model are contained in the covariance matrices.\n",
        "\n",
        "In the most general case, for $K$ clusters of points in $d$ dimensions, there are $K$ covariance matrices each of size $d \\times d$.   \n",
        "\n",
        "So we need $Kd^2$ parameters to specify this model.\n",
        "\n",
        "It can happen that you may not have enough data to estimate so many parameters.\n",
        "\n",
        "Also, it can happen that you believe that clusters should have some constraints on their shapes.\n",
        "\n",
        "Here is where the GMM assumptions become __really__ useful.\n",
        "\n",
        "## Clusters with Equal Variance\n",
        "\n",
        "Let's say you believe all the clusters should have the same shape, but the shape can be arbitrary. \n",
        "\n",
        "Then you only need to estimate __one__ covariance matrix - just $d^2$ parameters.\n",
        "\n",
        "This is specified by the GMM parameter `covariance_type='tied'`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = np.r_[np.dot(rng.standard_normal((n_samples, 2)), C),\n",
        "          0.7 * rng.standard_normal((n_samples, 2))]\n",
        "gmm = sklearn.mixture.GaussianMixture(n_components=2, covariance_type='tied')\n",
        "y_pred = gmm.fit_predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "colors = ['bgrky'[p] for p in y_pred]\n",
        "plt.scatter(X[:, 0], X[:, 1], color=colors, s=10, alpha=0.8)\n",
        "plt.title('Covariance type = tied')\n",
        "plt.axis('equal')\n",
        "plt.axis('off')\n",
        "plt.plot(gmm.means_[:,0],gmm.means_[:,1], 'ok')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non-Skewed Clusters\n",
        "\n",
        "Perhaps you believe in even more restricted shapes: all clusters should have their axes aligned with the coordinate axes.\n",
        "\n",
        "That is, clusters are not skewed.\n",
        "\n",
        "Then you only need to estimate the diagonals of the covariance matrices - just $kn$ parameters.\n",
        "\n",
        "This is specified by the GMM parameter `covariance_type='diag'`.\n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = np.r_[np.dot(rng.standard_normal((n_samples, 2)), C),\n",
        "          0.7 * rng.standard_normal((n_samples, 2))]\n",
        "gmm = sklearn.mixture.GaussianMixture(n_components=4, covariance_type='diag')\n",
        "y_pred = gmm.fit_predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "colors = ['bgrky'[p] for p in y_pred]\n",
        "plt.scatter(X[:, 0], X[:, 1], color = colors, s = 10, alpha = 0.8)\n",
        "plt.axis('equal')\n",
        "plt.axis('off')\n",
        "plt.plot(gmm.means_[:,0], gmm.means_[:,1], 'oc')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Round Clusters\n",
        "\n",
        "Finally, if you believe that all clusters should be round, then you only need to estimate the $k$ variances.  \n",
        "\n",
        "This is specified by the GMM parameter `covariance_type='spherical'`.\n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = np.r_[np.dot(rng.standard_normal((n_samples, 2)), C),\n",
        "          0.7 * rng.standard_normal((n_samples, 2))]\n",
        "gmm = sklearn.mixture.GaussianMixture(n_components=2, covariance_type='spherical')\n",
        "y_pred = gmm.fit_predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "colors = ['bgrky'[p] for p in y_pred]\n",
        "plt.scatter(X[:, 0], X[:, 1], color = colors, s = 10, alpha = 0.8)\n",
        "plt.axis('equal')\n",
        "plt.axis('off')\n",
        "plt.plot(gmm.means_[:,0], gmm.means_[:,1], 'oc')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.content-hidden when-profile=\"web\"}\n",
        "## Summary\n",
        "\n",
        "Today we covered:\n",
        "\n",
        "- Maximum likelihood estimators\n",
        "- Gaussian mixture models\n",
        "- Expectation Maximization\n",
        "\n",
        "A major benefit of GMMs is they are soft clustering technique that allows you to capture non-spherical clusters.\n",
        ":::"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/opt/anaconda3/envs/ds701/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}