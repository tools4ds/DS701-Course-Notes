{
  "hash": "1aef6023d305973635825623757b1b24",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Clustering In Practice\njupyter: python3\n---\n\n\n## Clustering in Practice\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/07-Clustering-II-in-practice.ipynb)\n\n\n...featuring $k$-means\n\nToday we'll do an extended example showing $k$-means clustering in practice and in the context of the python libraries\n__`scikit-learn.`__\n\n`scikit-learn` is the main python library for machine learning functions.\n\nOur goals are to learn:\n\n::: {.incremental}\n* How clustering is used in practice\n* Tools for evaluating the quality of a clustering\n* Tools for assigning meaning or labels to a cluster\n* Important visualizations\n* A little bit about feature extraction for text\n:::\n\n## Training wheels: Synthetic data\n\nGenerally, when learning about or developing a new unsupervised method, it's a \ngood idea to try it out on a dataset in which you already know the \"right\" answer.\n\nOne way to do that is to generate synthetic data that has some known properties.\n \nAmong other things, `scikit-learn` contains tools for generating synthetic data\nfor testing.\n\nWe'll use [datasets.make_blobs](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html).\n\n::: {#049e10dd .cell execution_count=2}\n``` {.python .cell-code code-fold=\"false\"}\nimport sklearn.datasets as sk_data\nX, y, gt_centers = sk_data.make_blobs(n_samples=100, centers=3, n_features=30,\n                          center_box=(-10.0, 10.0), random_state=0, return_centers=True)\n```\n:::\n\n\nLet's check the shapes of the returned values:\n\n::: {#1ac4cbd2 .cell execution_count=3}\n``` {.python .cell-code code-fold=\"false\"}\nprint(\"X.shape: \", X.shape)\nprint(\"y.shape: \", y.shape)\nprint(\"gt_centers: \", gt_centers.shape)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nX.shape:  (100, 30)\ny.shape:  (100,)\ngt_centers:  (3, 30)\n```\n:::\n:::\n\n\n::: {.notes}\n`datasets.makeblobs` takes as arguments:\n\n- `n_samples`: the number of samples to generate,\n- `n_features`: the number of features, or in other words the dimensionality of\neach sample\n- `center_box`: the bounds of the cluster centers\n- `random_state`: random seed\n- `return_centers`: We also choose the option to return the centers so we have ground truth\n:::\n\n## Visualize the Data\n\nTo get a sense of the raw data we can inspect it.\n\nFor statistical visualization, a good library is [seaborn](https://seaborn.pydata.org/).\n\nLet's plot the `X` data as a matrix\n[heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html), \nwhere every row is a data point and the columns are the features.\n\n::: {#2d7b4808 .cell execution_count=4}\n``` {.python .cell-code}\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set the figure size to make the plot smaller\nplt.figure(figsize=(7, 5))  # Adjust the width and height as needed\nsns.heatmap(X, xticklabels = False, yticklabels = False, linewidths = 0, cbar = False)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-4-output-1.png){width=540 height=389}\n:::\n:::\n\n\n---\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n::: {#04fca4d1 .cell execution_count=5}\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-5-output-1.png){width=317 height=241}\n:::\n:::\n\n\n:::\n::: {.column width=\"60%\"}\nGeometrically, these points live in a __30 dimensional__ space, so we cannot directly visualize their geometry.  \n\nThis is a __big problem__ that you will run into time and again!\n\nWe will discuss methods for visualizing high dimensional data later on.\n\nFor now, we will use a method that can turn a set of pairwise distances into an approximate 2-D representation __in some cases.__\n:::\n::::\n\n---\n\nSo let's compute the pairwise distances, _in 30 dimensions_, for visualization purposes.\n\nWe can compute all pairwise distances in a single step using the `scikit-learn` [`metrics.euclidean_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html) function:\n\n::: {#362d0635 .cell execution_count=6}\n``` {.python .cell-code code-fold=\"false\"}\nimport sklearn.metrics as metrics\neuclidean_dists = metrics.euclidean_distances(X)\n# euclidean_dists\n```\n:::\n\n\n::: {#33866824 .cell execution_count=7}\n\n::: {.cell-output .cell-output-stdout}\n```\nMatrix shape is:  (100, 100)\n```\n:::\n:::\n\n\nLet's look at the upper left and lower right corners of the distances matrix.\n\n::: {#61ab87b6 .cell execution_count=8}\n\n::: {.cell-output .cell-output-stdout}\n```\nUpper left 3x3:\n[[ 0.   47.74 45.19]\n [47.74  0.   43.67]\n [45.19 43.67  0.  ]]\n\n...\nLower right 3x3:\n[[ 0.    8.19 41.82]\n [ 8.19  0.   43.41]\n [41.82 43.41  0.  ]]\n```\n:::\n:::\n\n\nNote that this produces a $100\\times100$ symmetric matrix where the diagonal\nis all zeros (distance from itself)\n\n---\n\nLet's look at a histogram of the distances.\n\n::: {#f47706e2 .cell execution_count=9}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.metrics as metrics\n\n# Compute the pairwise distances\neuclidean_dists = metrics.euclidean_distances(X)\n\n# Extract the lower triangular part of the matrix, excluding the diagonal\nlower_triangular_values = euclidean_dists[np.tril_indices_from(euclidean_dists, k=-1)]\n\n# Plot the histogram\nplt.figure(figsize=(10, 6))\nplt.hist(lower_triangular_values, bins=30, edgecolor='black')\nplt.title('Histogram of Lower Diagonal Values of Euclidean Distances')\nplt.xlabel('Distance')\nplt.ylabel('Frequency')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-9-output-1.png){width=825 height=523}\n:::\n:::\n\n\n::: {.notes}\nRemember that these are the pairwise distances, _in 30 dimensions_. So at least\nwith this dataset we see a clean separation presumably between inter-cluster\ndistances and intra-cluster distances.\n\nHow would the curse of dimensionality affect this? We discuss the curse of\ndimensionality later in the course.\n:::\n## Visualizing with MDS\n\nThe idea behind [Multidimensional Scaling (MDS)](https://en.wikipedia.org/wiki/Multidimensional_scaling) is: \n\n* given a pairwise distance (or dissimilarity) matrix, \n* find a set of coordinates in 1, 2 or 3-D space that approximates those distances as well as possible.\n* points that are close (or far) in high dimensional space are close (or far) in the reduced dimension space\n\n::: {.fragment}\nNote that there are two forms of MDS:\n\n* Metric MDS, of which Classical MDS is a special case, and has a closed form solution\n  based on the eigenvectors of the centered distance matrix\n* Non-Metric MDS, which tries to find a non-parametric monotonic relationship\n  between the dissimilarities and the target coordinates through an iterative approach\n:::\n\n::: {.fragment}\nAlso note that MDS may not always work well if, for example the dissimilarities\nare not well modeled by a metric like Euclidean distance\n:::\n\n## MDS Visualization Result\n\n::: {#cell-code-mds .cell execution_count=10}\n``` {.python .cell-code}\nimport sklearn.manifold\nimport matplotlib.pyplot as plt\nmds = sklearn.manifold.MDS(n_components=2, max_iter=3000, eps=1e-9, random_state=0,\n                   dissimilarity = \"precomputed\", n_jobs = 1)\nfit = mds.fit(euclidean_dists)\npos = fit.embedding_\nplt.scatter(pos[:, 0], pos[:, 1], s=8)\nplt.axis('square');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/code-mds-output-1.png){#code-mds width=426 height=411}\n:::\n:::\n\n\nSo we can see that, although our data lives in 30 dimensions, we can get a\nsense of how the points are clustered by approximately placing the points into\ntwo dimensions.\n\n---\n\nOut of curiosity, we can visualize the pairwise distance matrix using a heatmap.\n\n::: {#11ccb83c .cell execution_count=11}\n``` {.python .cell-code code-fold=\"false\"}\nsns.heatmap(euclidean_dists, xticklabels=False, yticklabels=False, linewidths=0, \n            square=True );\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-11-output-1.png){width=471 height=397}\n:::\n:::\n\n\n::: {.fragment}\nPerhaps not as useful a visualization however.\n:::\n\n## Applying  $k$-Means\n\nThe Python package `scikit-learn` has a huge set of tools for unsupervised learning generally, and clustering specifically.  \n\nThese are in __`sklearn.cluster.`__\n\nThere are 3 functions in all the clustering classes, \n\n* **`fit()`**: builds the model from the training data\n    * e.g. for $k$-means, find the centroids\n* **`predict()`**: assigns labels to the data after building the models \n    * e.g. cluster number for $k$-means\n* **`fit_predict()`**: does both in a single step \n\n---\n\nLet's go back to the original 30-D synthetic dataset and apply $k$-means and\nshow the cluster numbers.\n\n::: {#2a58b834 .cell execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(init = 'k-means++', n_clusters = 3, n_init = 100, random_state=0)\ny_prime = kmeans.fit_predict(X)\nprint(y_prime)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0 1 2 2 1 0 1 1 0 1 2 0 2 0 1 2 2 0 2 1 2 1 2 0 1 1 0 2 2 2 2 0 2 0 2 1 2\n 1 2 1 1 1 0 2 0 2 0 1 0 2 2 0 0 0 0 2 0 1 1 2 0 2 1 0 0 1 2 0 0 1 1 1 0 0\n 2 0 1 0 1 0 1 1 1 0 2 2 1 0 0 2 0 1 1 2 0 2 2 1 1 2]\n```\n:::\n:::\n\n\nFor comparisons, here are the original cluster numbers.\n\n::: {#7d1574d5 .cell execution_count=13}\n``` {.python .cell-code}\nprint(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[0 2 1 1 2 0 2 2 0 2 1 0 1 0 2 1 1 0 1 2 1 2 1 0 2 2 0 1 1 1 1 0 1 0 1 2 1\n 2 1 2 2 2 0 1 0 1 0 2 0 1 1 0 0 0 0 1 0 2 2 1 0 1 2 0 0 2 1 0 0 2 2 2 0 0\n 1 0 2 0 2 0 2 2 2 0 1 1 2 0 0 1 0 2 2 1 0 1 1 2 2 1]\n```\n:::\n:::\n\n\nNote that the $k$-means labels are different than the ground truth labels. They\nwould have to be remapped with\n\n$$\n\\begin{align*}\n0 &\\rightarrow 0 \\\\\n1 &\\rightarrow 2 \\\\\n2 &\\rightarrow 1\n\\end{align*}\n$$\n\n::: {#995488e6 .cell execution_count=14}\n``` {.python .cell-code}\n# Remap y_prime\nremap = {0: 0, 1: 2, 2: 1}\ny_prime_remapped = np.vectorize(remap.get)(y_prime)\nprint(\"Remapped y_prime:\")\nprint(y_prime_remapped)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRemapped y_prime:\n[0 2 1 1 2 0 2 2 0 2 1 0 1 0 2 1 1 0 1 2 1 2 1 0 2 2 0 1 1 1 1 0 1 0 1 2 1\n 2 1 2 2 2 0 1 0 1 0 2 0 1 1 0 0 0 0 1 0 2 2 1 0 1 2 0 0 2 1 0 0 2 2 2 0 0\n 1 0 2 0 2 0 2 2 2 0 1 1 2 0 0 1 0 2 2 1 0 1 1 2 2 1]\n```\n:::\n:::\n\n\n::: {#5153fd48 .cell execution_count=15}\n``` {.python .cell-code}\n# Calculate the number of mismatched entries\nmismatches = np.sum(y != y_prime_remapped)\n\n# Calculate the percentage of mismatched entries\ntotal_entries = len(y)\npercentage_mismatched = (mismatches / total_entries) * 100\n\nprint(f\"Percentage of mismatched entries: {percentage_mismatched:.2f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPercentage of mismatched entries: 0.00%\n```\n:::\n:::\n\n\nTo reiterate, mismatches are 0.00%.\n\n---\n\nAll the tools in `scikit-learn` are implemented as python objects.\n\nThus, the general sequence for using a tool from `scikit-learn` is:\n\n* Create the object, probably with some hyperparameter settings or intialization,\n* Run the method, generally by using the `fit()` function, and\n* Examine the results, which are generally property variables of the object.\n\n::: {#43fa3cf6 .cell execution_count=16}\n``` {.python .cell-code code-fold=\"false\"}\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_\ninertia = kmeans.inertia_\n```\n:::\n\n\nAnd we see the resulting cluster inertia.\n\n::: {#89900b46 .cell execution_count=17}\n``` {.python .cell-code}\nprint(f'Clustering inertia: {inertia:0.1f}.')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nClustering inertia: 2733.8.\n```\n:::\n:::\n\n\n## Visualizing the Results of Clustering\n\nLet's visualize the results.  We'll do that by reordering the data items according to their cluster.\n\n::: {#87ee7a5a .cell execution_count=18}\n``` {.python .cell-code}\nimport numpy as np\nidx = np.argsort(labels)\nrX = X[idx, :]\nsns.heatmap(rX, xticklabels = False, yticklabels = False, linewidths = 0);\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-18-output-1.png){width=707 height=389}\n:::\n:::\n\n\nYou can clearly see the feature similarities of the samples in the same cluster.\n\n---\n\nWe can also sort the pairwise distance matrix.\n\n::: {#87cd9597 .cell execution_count=19}\n``` {.python .cell-code}\nrearranged_dists = euclidean_dists[idx,:][:,idx]\nsns.heatmap(rearranged_dists, xticklabels = False, yticklabels = False, linewidths = 0, square = True);\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-19-output-1.png){width=471 height=397}\n:::\n:::\n\n\nHere again, you can see the inter-cluster sample distances are small.\n\n## Cluster Evaluation\n\nHow do we know whether the clusters we get represent \"real\" structure in our data?\n\n:::: {.fragment}\nConsider a dataset of 100 points _uniformly distributed_ in the unit square.\n\n::: {#61eb5147 .cell execution_count=20}\n``` {.python .cell-code}\nimport pandas as pd\nnp.random.seed(42)\nunif_X = np.random.default_rng().uniform(0, 1, 500)\nunif_Y = np.random.default_rng().uniform(0, 1, 500)\ndf = pd.DataFrame(np.column_stack([unif_X, unif_Y]), columns = ['X', 'Y'])\ndf.plot('X', 'Y', kind = 'scatter', \n        colorbar = False, xlim = (0, 1), ylim = (0, 1), \n        figsize = (4, 4))\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-20-output-1.png){width=376 height=361}\n:::\n:::\n\n\n::::\n\n---\n\nAfter running $k$-means on this data:\n\n::: {#0640d95f .cell execution_count=21}\n``` {.python .cell-code}\nkmeans = KMeans(init = 'k-means++', n_clusters = 3, n_init = 500, random_state=0)\ndf['label'] = kmeans.fit_predict(df[['X', 'Y']])\ndf.plot('X', 'Y', kind = 'scatter', c = 'label', \n        colormap='viridis', colorbar = False, \n        xlim = (0, 1), ylim = (0, 1), \n        figsize = (4, 4))\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-21-output-1.png){width=376 height=361}\n:::\n:::\n\n\n:::: {.fragment}\nThe point is: many clustering algorithms will usually output some \"clustering\" of the data.\n::::\n\n---\n\nThe question is, does the clustering reflect __real__ structure?\n\n:::: {.fragment}\nGenerally we encounter two problems:\n::::\n\n::: {.incremental}\n* Are there \"real\" clusters in the data?\n* if so, __how many__ clusters are there?\n:::\n\n:::: {.fragment}\nThere is often no definitive answer to either of these questions.\n\nYou will often need to use your judgment in answering them.\n::::\n\n::: {.content-hidden}\n\n## Cluster Evaluations\n\n(Under Development)\n\n### With Ground Truth Data\n\n* How to evaluate clustering algos when you have ground truth data\n    * how do you align labels?\n    * different accuracy measures\n\n### Without Ground Truth Data\n\n* Can you make guesses to the underlying probability distributions to show that\n  it is non-uniform?\n\n## Clustering Metrics\n\n[sklearn clustering examples](https://scikit-learn.org/stable/auto_examples/cluster/index.html)\n\n- [sklearn performance evaluation guide](https://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation)\n- [sklearn metrics](https://scikit-learn.org/stable/api/sklearn.metrics.html)\n    - [sklearn clustering metrics](https://scikit-learn.org/stable/api/sklearn.metrics.html#module-sklearn.metrics.cluster)\n        - [sklearn rand score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.rand_score.html)\n- [sklearn example: adjustment for chance in clustering performance evaluation](https://scikit-learn.org/stable/auto_examples/cluster/plot_adjusted_for_chance_measures.html)\n\n:::\n\n## Clustering Metrics\n\nBroadly separated into two categories:\n\n:::: {.fragment}\n**Ground truth based metrics**\n\nWe'll look at (Adjusted) Rand Index.\n\n::::\n\n:::: {.fragment}\nEven with ground truth labels, we can't apply accuracy measures like we will\nsee in supervised learning approaches since clustering algorithms don't try to\nmatch exact labels, but rather group similar items together.\n\n::::\n\n:::: {.fragment}\n**Internal cluster metrics (no ground truth external labels)**\n\nWe'll look at Silhouette Coefficient.\n::::\n\n\n## Rand Index\n\n::: aside\nNamed after William M. Rand.\n:::\n\nIn some cases we'll have ground truth cluster labels, call them $T$.\n\nIn that case, what we need is a way to compare a proposed clustering, $C$, with $T$.\n\nThe _Rand Index_ counts:\n\n- the number of pairs of points that are in the same cluster in both $T$ and $C$, and\n- the number of pairs of points that are in different clusters in both $T$ and $C$.\n\nnormalized by the combinatoric number of pairs of points, $n \\choose 2$.\n\n## Defining the Rand Index\n\nLet's proceed by way of example.\n\nWe'll create another synthetic dataset of 3 clusters, so we have ground truth\nlabels $T$.\n\n::: {#155d88ef .cell execution_count=22}\n``` {.python .cell-code}\nX_rand, y_rand = sk_data.make_blobs(n_samples=[100, 250, 150], \n                                    centers = [[1, 2],[1.5, 3], [2, 4]], \n                                    n_features = 2,\n                                    center_box = (-10.0, 10.0), \n                                    cluster_std = [.2, .3, .2], \n                                    random_state = 0)\ndf_rand_gt = pd.DataFrame(np.column_stack([X_rand[:, 0], X_rand[:, 1], y_rand]), columns = ['X', 'Y', 'label'])\ndf_rand_clust = df_rand_gt.copy()\nkmeans = KMeans(init = 'k-means++', n_clusters = 3, n_init = 100, random_state=0)\ndf_rand_clust['label'] = kmeans.fit_predict(df_rand_gt[['X', 'Y']])\n```\n:::\n\n\nWe then run $k$-means with 3 clusters to on the datasets to get labels $C$ and plot the results:\n\n::: {#485f92bd .cell execution_count=23}\n``` {.python .cell-code}\nfigs, axs = plt.subplots(1, 2, figsize = (12, 5))\ndf_rand_gt.plot('X', 'Y', \n                kind = 'scatter', \n                c = 'label', \n                colormap='viridis', \n                ax = axs[0],\n                colorbar = False)\naxs[0].set_title('Ground Truth (T)')\naxs[0].set_axis_off()\n\ndf_rand_clust.plot('X', 'Y', \n                    kind = 'scatter', \n                    c = 'label', \n                    colormap='viridis', \n                    ax = axs[1],\n                    colorbar = False)\naxs[1].set_title('Clustering (C)')\naxs[1].set_axis_off();\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-23-output-1.png){width=912 height=409}\n:::\n:::\n\n\n## Rand Index Defined\n\nSpecifically:\n\n::: {.incremental}\n- Let $a$ be the number of pairs that have the same label in $T$ and the same label in $C$. \n- Let $b$ be the number of pairs that have different labels in $T$ and different labels in $C$. \n:::\n\n:::: {.fragment}\nThen the Rand Index is: \n\n$$\n\\text{RI}(T,C) = \\frac{a+b}{n \\choose 2}\n$$\n\n::::\n\n:::: {.fragment}\nHow do we know whether a particular Rand Index (RI) score is significant?\n\nWe might compare it to the RI for a __random__ assignment of points to labels.\n\nThis leads to the __Adjusted Rand Index.__\n::::\n\n## Definition of Adjusted Rand Index\n\nTo \"calibrate\" the Rand Index this way, we use the expected Rand Index of random labelings,\ndenoted $E[\\text{RI}]$.   \n\nThe Expected Rand Index considers $C$ to be a clustering that has the same cluster sizes as $T$, but labels are assigned at random.\n\n:::: {.fragment}\nUsing that, we define the adjusted Rand index as a simple __rescaling__ of RI:\n\n$$\n\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}\n$$\n\nThe computation of the $E[\\text{RI}]$ and the $\\max(\\text{RI})$ are simple combinatorics (we'll omit the derivation).\n::::\n\n## Rand Index Example\n\n:::: {.columns}\n::: {.column width=\"40%\"}\n\n::: {#40c4dc05 .cell execution_count=24}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(4, 3))  # Adjust the width and height as needed\nsns.heatmap(X, xticklabels = False, yticklabels = False, linewidths = 0);\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-24-output-1.png){width=321 height=241}\n:::\n:::\n\n\n:::\n::: {.column width=\"60%\"}\nLet's consider again our 3-cluster dataset with known labels `y`.\n:::\n::::\n\n\nHere is the adjusted Rand Index, when using $k$-means to cluster this dataset for 1 to 10 clusters:\n\n::: {#0746370c .cell execution_count=25}\n``` {.python .cell-code}\ndef ri_evaluate_clusters(X,max_clusters,ground_truth):\n    ri = np.zeros(max_clusters+1)\n    ri[0] = 0;\n    for k in range(1,max_clusters+1):\n        kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10)\n        kmeans.fit_predict(X)\n        ri[k] = metrics.adjusted_rand_score(kmeans.labels_,ground_truth)\n    return ri\n    \nri = ri_evaluate_clusters(X, 10, y)\nplt.plot(range(1,len(ri)), ri[1:], 'o-')\nplt.xlabel('Number of clusters')\nplt.title('$k$-means Clustering Compared to Known Labels')\nplt.ylabel('Adjusted Rand Index');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-25-output-1.png){width=812 height=450}\n:::\n:::\n\n\n## Deciding on the Number of Clusters\n\nThe second question we face in evaluating a clustering is how many clusters are present.\n\nIn practice, to use $k$-means or most other clustering methods, one must choose $k$, the number of clusters, via some process.\n\n## Inspecting Clustering Error\n\nThe first thing you might do is to look at the $k$-means objective function  and see if it levels off after a certain point.\n\nRecall that the $k$-means objective can be considered the clustering \"error\".\n\nIf the error stops going down, that would suggest that the clustering is not improving as the number of clusters is increased.\n\nLet's calculate the error for 1-11 clusters.\n\n::: {#f4ed287e .cell execution_count=26}\n``` {.python .cell-code code-fold=\"false\"}\nerror = np.zeros(11)\nfor k in range(1,11):\n    kmeans = KMeans(init='k-means++', n_clusters = k, n_init = 10)\n    kmeans.fit_predict(X)\n    error[k] = kmeans.inertia_\n```\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Inspecting Clustering Error\n:::\n\nFor our synthetic data, here is the $k$-means objective, as a function of $k$:\n\n::: {#3fadd727 .cell execution_count=27}\n``` {.python .cell-code}\nplt.plot(range(1, len(error)), error[1:], 'o-')\nplt.xlabel('Number of clusters')\nplt.title(r'$k$-means clustering performance of synthetic data')\nplt.ylabel('Error');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-27-output-1.png){width=833 height=450}\n:::\n:::\n\n\n::: {.callout-warning}\nThis synthetic data is not at all typical. You will almost never see such a sharp change in the error function as we see here.\n:::\n\n---\n\nLet's create a function to evaluate clusters for later use.\n\n::: {#7346753e .cell execution_count=28}\n``` {.python .cell-code code-fold=\"false\"}\ndef evaluate_clusters(X,max_clusters):\n    error = np.zeros(max_clusters+1)\n    error[0] = 0;\n    for k in range(1,max_clusters+1):\n        kmeans = KMeans(init='k-means++', n_clusters=k, n_init=10)\n        kmeans.fit_predict(X)\n        error[k] = kmeans.inertia_\n    return error\n```\n:::\n\n\n## Silhouette Coefficient\n\nUsually, the ground truth labels are not known.\n\n:::: {.fragment}\nIn that case, evaluation must be performed using the model itself.\n::::\n\n:::: {.fragment}\nRecall our definition of clustering: \n::::\n\n:::: {.fragment}\n> a grouping of data objects, such that the objects within a group are similar (or near) to one another and dissimilar\n> (or far) from the objects in other groups.\n::::\n\n:::: {.fragment}\nThis suggests a metric that could evaluate a clustering: comparing the distances between points within a cluster, to the\ndistances between points in different clusters.\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## Silhouette Coefficient\n:::\n\nThe Silhouette Coefficient is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a\nmodel with \"better defined\" clusters. \n\nWe'll use [`sklearn.metrics.silhouette_score`](https://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient)\n\n::: {.incremental}\n- Let $a$ be the mean distance between a data point and all other points in the same cluster.\n- Let $b$ be the mean distance between a data point and all other points in the next nearest cluster. \n:::\n\n\n:::: {.fragment}\nThen the **Silhouette Coefficient** for a clustering is:\n\n$$ \ns = \\frac{b-a}{\\max(a, b)} \n$$\n::::\n\n::: {.content-visible when-profile=\"slides\"}\n## Silhouette Coefficient\n:::\n\nWe can calcute the Silhouette Score for our synthetic data:\n\n::: {#29ec3ed1 .cell execution_count=29}\n``` {.python .cell-code}\nsc = metrics.silhouette_score(X, labels, metric='euclidean')\nprint('Silhouette Score:', sc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSilhouette Score: 0.8319348841402535\n```\n:::\n:::\n\n\n::: {.content-visible when-profile=\"slides\"}\n## Silhouette Coefficient\n:::\n\nWe can also evaluate it for 2-10 clusters and plot the results:\n\n::: {#21073aca .cell execution_count=30}\n``` {.python .cell-code}\ndef sc_evaluate_clusters(X, max_clusters, n_init, seed):\n    s = np.zeros(max_clusters+1)\n    s[0] = 0;\n    s[1] = 0;\n    for k in range(2, max_clusters+1):\n        kmeans = KMeans(init='k-means++', n_clusters = k, n_init = n_init, random_state = seed)\n        kmeans.fit_predict(X)\n        s[k] = metrics.silhouette_score(X, kmeans.labels_, metric = 'euclidean')\n    return s\n\ns = sc_evaluate_clusters(X, 10, 10, 1)\nplt.plot(range(2, len(s)), s[2:], 'o-')\nplt.xlabel('Number of Clusters')\nplt.title('$k$-means clustering performance on synthetic data')\nplt.ylabel('Silhouette Score');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-30-output-1.png){width=812 height=450}\n:::\n:::\n\n\n:::: {.fragment}\nAgain, these results are more perfect than typical. \n\nBut the general idea is to look for a local maximum in the Silhouette Coefficient as the potential number of clusters.\n::::\n\n# Real Data Clustering Example\n\n## Taking the Training Wheels Off: Real Data\n\nAs a classic \"real world\" example, we'll use the \n[\"20 Newsgroup\" data](https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset)\nprovided as example data in scikit-learn.\n\nWe borrow code from this sklearn\n[example](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html).\n\nLet's load the data and count the number of documents and categories.\n\n::: {#df5fd856 .cell execution_count=31}\n``` {.python .cell-code}\nfrom sklearn.datasets import fetch_20newsgroups\n\n# just use the following categories\ncategories = ['comp.os.ms-windows.misc', 'sci.space', 'rec.sport.baseball']\n\nnews_data = fetch_20newsgroups(\n    remove = ('headers', 'footers', 'quotes'),\n    subset = 'train', \n    categories = categories,\n    shuffle = True,\n    random_state = 42)\n\nlabels = news_data.target\nunique_labels, category_sizes = np.unique(labels, return_counts=True)\ntrue_k = unique_labels.shape[0]\n\nprint(f\"{len(news_data.data)} documents - {true_k} categories\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1781 documents - 3 categories\n```\n:::\n:::\n\n\n---\n\nHere is an example of one of the documents:\n\n::: {#8d3db6d0 .cell execution_count=32}\n``` {.python .cell-code}\nprint(news_data.data[0])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\nEarly to mid June.\n\n\nIf they think the public wants to see it they will carry it. Why not\nwrite them and ask? You can reach them at:\n\n\n                          F: NATIONAL NEWS MEDIA\n\n\nABC \"World News Tonight\"                 \"Face the Nation\"\n7 West 66th Street                       CBS News\nNew York, NY 10023                       2020 M Street, NW\n212/887-4040                             Washington, DC 20036\n                                         202/457-4321\n\nAssociated Press                         \"Good Morning America\"\n50 Rockefeller Plaza                     ABC News\nNew York, NY 10020                       1965 Broadway\nNational Desk (212/621-1600)             New York, NY 10023\nForeign Desk (212/621-1663)              212/496-4800\nWashington Bureau (202/828-6400)\n                                         Larry King Live TV\n\"CBS Evening News\"                       CNN\n524 W. 57th Street                       111 Massachusetts Avenue, NW\nNew York, NY 10019                       Washington, DC 20001\n212/975-3693                             202/898-7900\n\n\"CBS This Morning\"                       Larry King Show--Radio\n524 W. 57th Street                       Mutual Broadcasting\nNew York, NY 10019                       1755 So. Jefferson Davis Highway\n212/975-2824                             Arlington, VA 22202\n                                         703/685-2175\n\"Christian Science Monitor\"\nCSM Publishing Society                   \"Los Angeles Times\"\nOne Norway Street                        Times-Mirror Square\nBoston, MA 02115                         Los Angeles, CA 90053\n800/225-7090                             800/528-4637\n\nCNN                                      \"MacNeil/Lehrer NewsHour\"\nOne CNN Center                           P.O. Box 2626\nBox 105366                               Washington, DC 20013\nAtlanta, GA 30348                        703/998-2870\n404/827-1500\n                                         \"MacNeil/Lehrer NewsHour\"\nCNN                                      WNET-TV\nWashington Bureau                        356 W. 58th Street\n111 Massachusetts Avenue, NW             New York, NY 10019\nWashington, DC 20001                     212/560-3113\n202/898-7900\n\n\"Crossfire\"                              NBC News\nCNN                                      4001 Nebraska Avenue, NW\n111 Massachusetts Avenue, NW             Washington, DC 20036\nWashington, DC 20001                     202/885-4200\n202/898-7951                             202/362-2009 (fax)\n\n\"Morning Edition/All Things Considered\"  \nNational Public Radio                    \n2025 M Street, NW                        \nWashington, DC 20036                     \n202/822-2000                             \n\nUnited Press International\n1400 Eye Street, NW\nWashington, DC 20006\n202/898-8000\n\n\"New York Times\"                         \"U.S. News & World Report\"\n229 W. 43rd Street                       2400 N Street, NW\nNew York, NY 10036                       Washington, DC 20037\n212/556-1234                             202/955-2000\n212/556-7415\n\n\"New York Times\"                         \"USA Today\"\nWashington Bureau                        1000 Wilson Boulevard\n1627 Eye Street, NW, 7th Floor           Arlington, VA 22229\nWashington, DC 20006                     703/276-3400\n202/862-0300\n\n\"Newsweek\"                               \"Wall Street Journal\"\n444 Madison Avenue                       200 Liberty Street\nNew York, NY 10022                       New York, NY 10281\n212/350-4000                             212/416-2000\n\n\"Nightline\"                              \"Washington Post\"\nABC News                                 1150 15th Street, NW\n47 W. 66th Street                        Washington, DC 20071\nNew York, NY 10023                       202/344-6000\n212/887-4995\n\n\"Nightline\"                              \"Washington Week In Review\"\nTed Koppel                               WETA-TV\nABC News                                 P.O. Box 2626\n1717 DeSales, NW                         Washington, DC 20013\nWashington, DC 20036                     703/998-2626\n202/887-7364\n\n\"This Week With David Brinkley\"\nABC News\n1717 DeSales, NW\nWashington, DC 20036\n202/887-7777\n\n\"Time\" magazine\nTime Warner, Inc.\nTime & Life Building\nRockefeller Center\nNew York, NY 10020\n212/522-1212\n\n```\n:::\n:::\n\n\n## Feature Extraction\n\nWe've discussed a bit the challenges of feature engineering.   \n\nOne of the most basic issues concerns how to encode categorical or text data in a form usable by algorithms that expect\nnumeric input.\n\nThe starting point is to note that one can encode a set using a binary vector with one component for each potential set\nmember.  \n\n---\n\n\nThe so-called _bag of words_ encoding for a document is to treat the document as a multiset of words.\n\n::: aside\nA _multiset_ is like a set but allows for multiple instances of each element.\n:::\n\nThat is, we simply count how many times each word occurs. It is a \"bag\" because all the order of the words in the\ndocument is lost.\n\nSurprisingly, we can still tell a lot about the document even without knowing its word ordering.\n\n---\n\nCounting the number of times each word occurs in a document yields a vector of __term frequencies.__\n\nHowever, simply using the \"bag of words\" directly has a number of drawbacks. First of all, large documents will have\nmore words than small documents.   \n\nHence it often makes sense to normalize the frequency vectors.\n\n$\\ell_1$ or $\\ell_2$ normalization are common.\n\n---\n\nNext, as noted in __scikit-learn__:\n\n> In a large text corpus, some words will be very [frequent] (e.g. “the”, “a”, “is” in English) hence carrying very\n> little meaningful information about the actual contents of the document. \n>\n> If we were to feed the direct count data directly to a classifier those very frequent terms would overshadow the\n> frequencies of rarer yet more interesting terms.\n>\n> In order to re-weight the count features into floating point values suitable for usage by a classifier it is very\n> common to use the **tf–idf transform.**\n>\n>__Tf__ means __term-frequency__ while __tf–idf__ means __term-frequency times inverse document-frequency.__\n>\n>This is a originally a term weighting scheme developed for information retrieval (as a ranking function for search\n> engines results), that has also found good use in document classification and clustering.\n\nThe idea is that rare words are more informative than common words.  \n\n(This has connections to information theory).\n\n---\n\nHence, the definition of tf-idf is as follows.\n\nFirst:\n\n$$\n\\text{tf}(t,d) = \\text{Number of times term }t \\text{ occurs in document } d\n$$\n\nNext, if $N$ is the total number of documents in the corpus $D$ then:\n\n$$\n\\text{idf}(t,D)=\\frac{N}{|\\{d\\in D : t\\in d \\}|}\n$$\n\nwhere the denominator is the number of documents in which the term $t$ appears.\n\nAnd finally:\n\n$$\n\\text{tf-idf}(t,d)=\\text{tf}(t,d)\\times \\text{idf}(t,D)\n$$\n\n::: {#c55c998b .cell execution_count=33}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(stop_words = 'english', min_df = 4, max_df = 0.8)\ndata = vectorizer.fit_transform(news_data.data)\n```\n:::\n\n\n## Getting to know the Data\n\n::: {#2d204ea9 .cell execution_count=34}\n``` {.python .cell-code}\nprint(f'The datas type is {type(data)} and the shape is {data.shape}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe datas type is <class 'scipy.sparse._csr.csr_matrix'> and the shape is (1781, 6613)\n```\n:::\n:::\n\n\nFor what it's worth we can look at the data feature matrix:\n\n::: {#4d91c28a .cell execution_count=35}\n``` {.python .cell-code}\nfig, ax1 = plt.subplots(1,1,figsize=(8,4))\ndum = sns.heatmap(data[1:100,1:200].todense(), xticklabels=False, yticklabels=False, \n            linewidths=0, cbar=False, ax=ax1)\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-35-output-1.png){width=614 height=315}\n:::\n:::\n\n\n::: {#10cd6579 .cell execution_count=36}\n``` {.python .cell-code}\nprint(news_data.target)\nprint(news_data.target_names)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[2 0 0 ... 2 1 2]\n['comp.os.ms-windows.misc', 'rec.sport.baseball', 'sci.space']\n```\n:::\n:::\n\n\n## Selecting the Number of Clusters\n\nNow let's look at the different cluster measures versus number of clusters.\n\n::: {#454332e3 .cell execution_count=37}\n``` {.python .cell-code}\nerror = evaluate_clusters(data, 10)\nplt.plot(range(1, len(error)), error[1:])\nplt.title('$k$-means Clustering Performance on Newsgroup Articles')\nplt.xlabel('Number of clusters')\nplt.ylabel('Error');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-37-output-1.png){width=825 height=450}\n:::\n:::\n\n\n---\n\n::: {#b7af416a .cell execution_count=38}\n``` {.python .cell-code}\nri = ri_evaluate_clusters(data, 10, news_data.target)\nplt.plot(range(1, len(ri)), ri[1:], 'o-')\nplt.xlabel('Number of clusters')\nplt.title('$k$-means Clustering Compared to Known Labels\\nNewsgroup Articles')\nplt.ylabel('Adjusted Rand Index');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-38-output-1.png){width=821 height=469}\n:::\n:::\n\n\n---\n\n::: {#081e05dc .cell execution_count=39}\n``` {.python .cell-code}\ns = sc_evaluate_clusters(data, 10, 100, 3)\nplt.plot(range(2, len(s)), s[2:], 'o-')\nplt.xlabel('Number of Clusters')\nplt.title('$k$-means clustering performance on Newsgroup Articles')\nplt.ylabel('Silhouette Score');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-39-output-1.png){width=829 height=450}\n:::\n:::\n\n\n## Looking into the clusters\n\nRun $k$-means with 4 clusters:\n\n::: {#ffe03ba2 .cell execution_count=40}\n``` {.python .cell-code}\nk = 4\nkmeans = KMeans(n_clusters = k, init = 'k-means++', max_iter = 100, n_init = 25, random_state = 3)\nkmeans.fit_predict(data)\n```\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\narray([2, 1, 1, ..., 3, 2, 3], dtype=int32)\n```\n:::\n:::\n\n\nFind the top 10 terms per cluster:\n\n::: {#6b42829f .cell execution_count=41}\n``` {.python .cell-code}\nasc_order_centroids = kmeans.cluster_centers_.argsort()#[:, ::-1]\norder_centroids = asc_order_centroids[:,::-1]\nterms = vectorizer.get_feature_names_out()\nfor i in range(k):\n    print(f'Cluster {i}:')\n    for ind in order_centroids[i, :10]:\n        print(f' {terms[ind]}')\n    print('')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCluster 0:\n ax\n max\n b8f\n g9v\n a86\n 145\n 1d9\n pl\n 2di\n 0t\n\nCluster 1:\n windows\n file\n dos\n files\n thanks\n use\n drivers\n card\n driver\n problem\n\nCluster 2:\n year\n think\n just\n like\n game\n team\n good\n don\n baseball\n games\n\nCluster 3:\n space\n nasa\n moon\n launch\n orbit\n shuttle\n earth\n like\n people\n lunar\n\n```\n:::\n:::\n\n\n## Pairwise Distances Matrix\n\nLet's calculate the pairwise distances matrix.\n\n::: {#471b1acf .cell execution_count=42}\n``` {.python .cell-code}\neuclidean_dists = metrics.euclidean_distances(data)\nlabels = kmeans.labels_\nidx = np.argsort(labels)\nclustered_dists = euclidean_dists[idx][:,idx]\nfig, ax1 = plt.subplots(1,1,figsize=(6,6))\ndum = sns.heatmap(clustered_dists, xticklabels=False, yticklabels=False, linewidths=0, square=True,cbar=False, ax=ax1)\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-42-output-1.png){width=463 height=463}\n:::\n:::\n\n\n## MDS Embedding\n\nLet's visualize with MDS.   \n\nNote that MDS is a slow algorithm and we can't do all 1700+ data points quickly, so we will\ntake a random sample.\n\n::: {#a8c5d930 .cell execution_count=43}\n``` {.python .cell-code}\nimport random\nn_items = euclidean_dists.shape[0]\nsubset = random.sample(range(n_items),500)\n\nfit = mds.fit(euclidean_dists[subset][:,subset])\npos = fit.embedding_\n```\n:::\n\n\nWe have the labels:\n\n::: {#8234f405 .cell execution_count=44}\n``` {.python .cell-code}\nlabels\n```\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\narray([2, 1, 1, ..., 3, 2, 3], dtype=int32)\n```\n:::\n:::\n\n\n## MDS Embedding\n\n::: {#eaa045f5 .cell execution_count=45}\n``` {.python .cell-code}\ncols = [['y', 'b', 'g', 'r', 'c'][l] for l in labels[subset]]\nplt.scatter(pos[:, 0], pos[:, 1], s = 12, c = cols)\nplt.title('MDS Embedding of Newsgroup Articles');\n```\n\n::: {.cell-output .cell-output-display}\n![](07-Clustering-II-in-practice_files/figure-revealjs/cell-45-output-1.png){width=805 height=431}\n:::\n:::\n\n\n## Recap and Next\n\nWe've covered\n\n* How clustering is used in practice\n* Tools for evaluating the quality of a clustering\n* Tools for assigning meaning or labels to a cluster\n* Important visualizations\n* A little bit about feature extraction for text\n\nNext time, we'll look at \n\n* Hierarchical clustering\n* Gaussian mixture models\n\n",
    "supporting": [
      "07-Clustering-II-in-practice_files"
    ],
    "filters": [],
    "includes": {}
  }
}