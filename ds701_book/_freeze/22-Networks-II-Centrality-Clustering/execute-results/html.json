{
  "hash": "467aefe12d9be0886e403065dbd2b4f5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Network Centrality and Clustering\njupyter: python3\n---\n\n\n\n\n[![](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tools4ds/DS701-Course-Notes/blob/main/ds701_book/jupyter_notebooks/22-Networks-II-Centrality-Clustering.ipynb)\n\n::: {#c7a3b75c .cell hide_input='true' slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport scipy as sp\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib as mp\nimport sklearn\nimport networkx as nx\nfrom IPython.display import Image, HTML\n\nimport laUtilities as ut\n\n%matplotlib inline\n```\n:::\n\n\nNow we turn to two important concepts in the analysis of networks:\n* Important __nodes__, and\n* Important __groups of nodes.__\n\nThe question of important nodes leads to the notion of __centrality,__\n\nAnd the question of important groups of nodes leads to the notion of __clustering.__\n\nIn both cases, we will draw connections between graphs and linear algebra.\n\n## Centrality\n\nAn common question in the analysis of networks is to understand the relative \"importance\" of the nodes in the network.\n\nFor example:\n\n* in a social network, who are the most influential individuals?\n* in the Web, which pages are more informative?\n* in road network, which intersections are most heavily used?\n\n\nThe key idea is that the __structure__ of the network should give us some information about the relative importance of the nodes in the network.\n\nTo introduce concepts here, we'll look at a very famous dataset in the history of network analysis: Zachary's karate club.\n\nThe back story: from 1970 to 1972 the anthropologist Wayne Zachary studied the social relationships inside a university karate club.\n\nWhile he was studying the club, a factional division led to a splitting of the club in two.   \n\nThe club became split between those who rallied around the club president, and those who rallied around the karate instructor.\n\n\n\n\n```{note}\nYou can read the story of the Karate club [here](http://www1.ind.ku.dk/complexLearning/zachary1977.pdf).  This dataset has become so famous that it has spawned [its own academic traditions](https://networkkarate.tumblr.com).\n```\n\n\n\n\nHere's a view of the social network of the karate club. \n\n::: {#1bef7184 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=2}\n``` {.python .cell-code}\nGk = nx.karate_club_graph()\nnp.random.seed(9)\nfig = plt.figure(figsize = (12, 6))\nax1 = fig.add_subplot(121)\nnx.draw_networkx(Gk, ax = ax1, pos = nx.circular_layout(Gk), \n                 with_labels = False, node_color='skyblue')\nplt.title('Circular Layout')\nplt.axis('off')\nax2 = fig.add_subplot(122)\nnx.draw_networkx(Gk, ax = ax2, pos = nx.spring_layout(Gk), \n                 with_labels = False, node_color='skyblue')\nplt.title('Spring Layout')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-3-output-1.png){width=912 height=483}\n:::\n:::\n\n\nDo some nodes in the network have a special role?\n\nAre some nodes more \"important\" than others?\n\nThese are often termed questions of __centrality__ (or __prestige__).\n\nToday we'll study metrics for these notions.   We'll work with a graph $G$.\n\nWe will study three basic notions of centrality:\n    \n1. __Closeness__ Centrality:  A central node is close to all others.\n2. __Betweenness__ Centrality: A central node is on many paths through the network.\n3. __Status__ Centrality: A central node is connected to other central nodes.\n\n### Closeness Centrality\n\nThe closeness centrality of a node $i$ is an indicator of the proximity between $i$ and all the other nodes in the graph.\n\nWe assume that $G$ is connected; let $d(i,j)$ be the shortest path distance between $i$ and $j$ in $G$.   \n\n(Although other dissimilarity measures could be used.)\n\nThen the standard way of formulating closeness centrality is the reciprocal of the total distance to all other nodes.\n\n$$ \\text{closeness}(i) = \\frac{1}{\\sum_{j \\in V} d(i,j)}.$$\n\n::: {#ebd31832 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=3}\n``` {.python .cell-code}\nGk = nx.karate_club_graph()\ncent = list(nx.closeness_centrality(Gk).values())\nnp.random.seed(9)\nfig = plt.figure(figsize = (12, 6))\nax1 = fig.add_subplot(121)\nnx.draw_networkx(Gk, ax = ax1, pos = nx.circular_layout(Gk), \n                 node_color = cent,\n                 cmap = plt.cm.plasma,\n                 with_labels = False)\nplt.title('Closeness Centrality')\nplt.axis('off')\nax2 = fig.add_subplot(122)\nnx.draw_networkx(Gk, ax = ax2, pos = nx.spring_layout(Gk), \n                 node_color = cent,\n                 cmap = plt.cm.plasma,\n                 with_labels = False)\nplt.title('Closeness Centrality')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-4-output-1.png){width=912 height=483}\n:::\n:::\n\n\nIn this graph, most nodes are close to most other nodes.   \n\nHowever we can see that some nodes are slightly more central than others.\n\n::: {#4059c052 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=4}\n``` {.python .cell-code}\nplt.figure(figsize = (6, 4))\nplt.hist(cent, bins=np.linspace(0, 1, 30))\nplt.xlabel('Closeness Centrality', size = 14)\nplt.ylabel('Number of Nodes', size = 14)\nplt.title('Distribution of Closeness Centrality', size = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-5-output-1.png){width=516 height=386}\n:::\n:::\n\n\n### Betweenness Centrality\n\nAnother way to think about centrality is \"is the node on many paths?\"\n\nIf we picture the network as a conduit for information, then betweenness captures how important a node is to the communication process (or \"how much\" information passes through the node).\n\nFirst, let's consider the case in which there is only one shortest path between any pair of nodes.\n\nThen, the betweenness centrality of node $i$ is the __number of shortest paths that pass through $i$.__\n\nMathematically:\n\n$$ \\text{betweenness}(i) = \\sum_{i \\neq j \\neq k \\in V} \\left\\{\\begin{array}{ll}1&\\text{if path from }j\\text{ to }k\\text{ goes through }i\\\\0&\\text{otherwise}\\end{array}\\right. $$\n\nWe can convert this to a value between 0 and 1 by dividing by ${n \\choose 2} = n(n-1)/2$.\n\nNow, in a general graph, there may be __multiple__ shortest paths between $j$ and $k$.  \n\nTo handle this, we define:\n* $\\sigma(i \\mid j,k)$ is the number of shortest paths between $j$ and $k$ that pass through $i$, and \n* $\\sigma(j,k)$ is the total number of shortest paths between $j$ and $k$.\n\nThen we define the _dependency_ of $i$ on the paths between $j$ and $k$:\n\n$$ \\text{dependency}(i \\mid j,k) = \\frac{\\sigma(i \\mid j,k )}{\\sigma(j,k)} $$\n\nYou can think of this as \"the probability that a shortest path between $j$ and $k$ goes through $i$.\"\n\nAnd finally:\n    \n$$ \\text{betweenness}(i) = \\sum_{i \\neq j \\neq k \\in V} \\text{dependency}(i \\mid j, k) $$\n\n::: {#376e6146 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=5}\n``` {.python .cell-code}\nGk = nx.karate_club_graph()\ncent = list(nx.betweenness_centrality(Gk).values())\nnp.random.seed(9)\nfig = plt.figure(figsize = (12, 6))\nax1 = fig.add_subplot(121)\nnx.draw_networkx(Gk, ax = ax1, pos = nx.circular_layout(Gk), \n                 node_color = cent,\n                 cmap = plt.cm.plasma,\n                 with_labels = False)\nplt.title('Betweenness Centrality')\nplt.axis('off')\nax2 = fig.add_subplot(122)\nnx.draw_networkx(Gk, ax = ax2, pos = nx.spring_layout(Gk), \n                 node_color = cent,\n                 cmap = plt.cm.plasma,\n                 with_labels = False)\nplt.title('Betweenness Centrality')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-6-output-1.png){width=912 height=483}\n:::\n:::\n\n\nWe start to see with this metric the \"importance\" of two or three key members of the karate club.\n\nNote that many nodes will have a betweenness centrality of zero -- no shortest paths go through them.\n\n::: {#40443466 .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=6}\n``` {.python .cell-code}\nplt.figure(figsize = (6, 4))\nplt.hist(cent, bins=np.linspace(0, 1, 30))\nplt.xlabel('Betweenness Centrality', size = 14)\nplt.ylabel('Number of Nodes', size = 14)\nplt.title('Distribution of Betweenness Centrality', size = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-7-output-1.png){width=516 height=386}\n:::\n:::\n\n\n### Adjacency Matrices\n\nTo define the next centrality, we need to start thinking about graphs as __matrices.__\n\nGiven an $n$-node undirected graph $G = (V, E)$, its adjacency matrix $A$ is defined as:\n\n$$ A_{ij} = \\left\\{ \\begin{array}{ll}1 & \\text{if }(i, j) \\in E\\\\\n                                    0 & \\text{otherwise} \\\\\n    \\end{array}\\right. $$\n\nHere is what the adjacency matrix of the karate club graph looks like:\n\n::: {#b4eed0a5 .cell hide_input='true' slideshow='{\"slide_type\":\"-\"}' tags='[\"hide-input\"]' execution_count=7}\n``` {.python .cell-code}\nfrom PIL import Image, ImageFont, ImageDraw\nfrom contextlib import contextmanager\n\n@contextmanager\ndef show_complete_array():\n    oldoptions = np.get_printoptions()\n    np.set_printoptions(threshold = np.inf)\n    np.set_printoptions(linewidth = 200)\n    try:\n        yield\n    finally:\n        np.set_printoptions(**oldoptions)\n        \nA = nx.adjacency_matrix(Gk).astype('int').todense()\nwith show_complete_array():\n    img = Image.new('RGB', (440, 530), color = (255,255,255))\n    #fnt = ImageFont.truetype(\"Pillow/Tests/fonts/FreeMono.ttf\", 30)\n    ImageDraw.Draw(img).text((0,0), str(A), fill=(0,0,0))\nimg\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\nAn important way to think about adjacency matrices: __column $j$ holds $j$'s neighbors.__\n\nNote that the adjacency matrix has some important algebraic properties:\n* It is nonnegative, and \n* It is symmetric.\n\n### Status Centrality\n\nThe third notion of centrality is more subtle.   \n\nIn this context, one often talks of \"prestige\" rather than \"centrality.\"  But the concepts are related.\n\nThe idea of status centrality is that __\"high status\" nodes are those that are connected to \"high status\" nodes.__\n\nIf you think this definition is circular, you are right!\n\nNonetheless, it leads to some metrics that are quite well defined and not hard to compute.\n\nLet's make this definition more precise: __the centrality of a node is proportional to the sum of the centrality of its neighbors.__\n\nThen we would say that:\n    \n$$ \\mathbf{e}_i = 1/\\lambda \\sum_{(i, j) \\in E} \\mathbf{e}_j $$\n\nThis encodes our recursive definition for $\\mathbf{e}_i$, based on a calculation that may or may not converge, depending on $\\lambda.$\n\nNow, we can write this equation in terms of the adjacency matrix $A$:\n    \n$$ \\mathbf{e}_i = 1/\\lambda \\sum_j A_{ij} \\mathbf{e}_j $$\n\nNotice that this is just the definition of a matrix-vector multiplication.   \n\nSo we can write even more concisely:\n    \n$$ A\\mathbf{e} = \\lambda\\mathbf{e} $$\n\n... which means that $\\mathbf{e}$ must be an eigenvector of $A$.\n\nFor this reason, status centrality is often called __eigenvector centrality.__\n\nWhich eigenvector of $A$ should we choose?   \n\nLet's assume that the graph $G$ is connected.\n\nThen there is a good argument for choosing the largest eigenvalue of $A$, in which case all of the entries in $\\mathbf{e}$ will be nonnegative.\n\n(This fact comes from the Perron-Frobenius theorem, which tells us that if $G$ is connected, then the largest eigenvalue of $A$ is positive and the corresponding eigenvector is nonnegative.)\n\n::: {#2d3cea7f .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=8}\n``` {.python .cell-code}\nGk = nx.karate_club_graph()\ncent = list(nx.eigenvector_centrality(Gk).values())\nnp.random.seed(9)\nfig = plt.figure(figsize = (12, 6))\nax1 = fig.add_subplot(121)\nnx.draw_networkx(Gk, ax = ax1, pos = nx.circular_layout(Gk), \n                 node_color = cent,\n                 cmap = plt.cm.plasma,\n                 with_labels = False)\nplt.title('Eigenvector Centrality')\nplt.axis('off')\nax2 = fig.add_subplot(122)\nnx.draw_networkx(Gk, ax = ax2, pos = nx.spring_layout(Gk), \n                 node_color = cent,\n                 cmap = plt.cm.plasma,\n                 with_labels = False)\nplt.title('Eigenvector Centrality')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-9-output-1.png){width=912 height=483}\n:::\n:::\n\n\n::: {#47ee5fdb .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=9}\n``` {.python .cell-code}\nplt.figure(figsize = (6, 4))\nplt.hist(cent, bins=np.linspace(0, 1, 30))\nplt.xlabel('Eigenvector Centrality', size = 14)\nplt.ylabel('Number of Nodes', size = 14)\nplt.title('Distribution of Eigenvector Centrality', size = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-10-output-1.png){width=516 height=386}\n:::\n:::\n\n\nAs a more detailed example, we can ask whether this definition of \"prestige\" applies to NCAA Division 1A football teams, based on who they play against.\n\n(Remember this data is from 2000!)\n\n::: {#df637f8d .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=10}\n``` {.python .cell-code}\n# data from http://www-personal.umich.edu/~mejn/netdata/\nfootball = nx.readwrite.gml.read_gml('data/football.gml')\nec = nx.eigenvector_centrality(football)\ncent = [ec[i] for i in football.nodes()]\nnp.random.seed(1)\nfig = plt.figure(figsize = (14, 14))\nnx.draw_networkx(football, \n                 pos = nx.spring_layout(football), \n                 node_color = cent,\n                 cmap = plt.cm.plasma)\nplt.title('Eigenvector Centrality on Football Network')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-11-output-1.png){width=1061 height=1074}\n:::\n:::\n\n\nSo let's compare the three versions of centrality we've looked at:\n\n::: {#d1b014a9 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=11}\n``` {.python .cell-code}\nGk = nx.karate_club_graph()\nfn = [nx.closeness_centrality, nx.betweenness_centrality, nx.eigenvector_centrality]\ntitle = ['Closeness Centrality', 'Betweenness Centrality', 'Eigenvector Centrality']\n#\nfig, axs = plt.subplots(2, 3, figsize = (14, 8))\nfor i in range(3):\n    cent = list(fn[i](Gk).values())\n    np.random.seed(9)\n    nx.draw_networkx(Gk, ax = axs[0, i], \n                 pos = nx.spring_layout(Gk), \n                 node_color = cent,\n                 cmap = plt.cm.plasma,\n                 with_labels = False)\n    axs[0, i].set_title(title[i], size = 14)\n    axs[0, i].axis('off')\n    #\n    axs[1, i].hist(cent, bins=np.linspace(0, 1, 30))\n    axs[1, i].set_ylim([0, 27])\n    axs[1, i].set_xlabel(title[i], size = 14)\naxs[1, 0].set_ylabel('Number of Nodes', size = 14);\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-12-output-1.png){width=1111 height=679}\n:::\n:::\n\n\n::: {#a5d2e84a .cell slideshow='{\"slide_type\":\"skip\"}' tags='[\"hide-input\"]' execution_count=12}\n``` {.python .cell-code}\n# consider discussing node ranking\n# this is a fun paper: http://stat.wharton.upenn.edu/~steele/Courses/956/Ranking/RankingFootballSIAM93.pdf\n# cite: https://epubs.siam.org/doi/10.1137/1035004\n# The Perronâ€“Frobenius Theorem and the Ranking of Football Teams\n# and there is useful data here: https://www.sports-reference.com/cfb/years/2020-schedule.html\n```\n:::\n\n\n## Clustering and Partitioning\n\nWe now turn to the question of finding important __groups__ of nodes.\n\nWhy might we want to cluster graph nodes?\n\n* Assiging computations to processors in a parallel computer\n* Segmenting images (finding boundaries between objects)\n* Clustering words found together in documents, or documents with similar words\n* Divide and conquer algorithms\n* Circuit layout in VLSI\n* Community detection in social networks\n\n### Min $s$-$t$ cut\n\nWe'll start with a problem that is fundamental to many other problems in graph analysis.\n\nLet's say we have a graph $G$ and two nodes in mind, $s$ and $t$.  \n\nWe would like to __isolate__ $s$ from $t$.  What is the cheapest way to do it?\n\n<!-- Image credit: On the history of the transportation and maximum flow problems. -->\n<!--     Alexander Schrijver in Math Programming, 91: 3, 2002. -->\n    \n<center>\n    \n<img src=\"figs/L23-max-flow-soviet-rail-1955.png\" width=\"75%\">\n    \n</center> \n\n\n\n\n```{note}\nFor an interesting historical perspective on the min-cut problem and its relation to the Cold War, see\n [\"On the history of the transportation and maximum flow problems,\"](https://link.springer.com/article/10.1007%2Fs101070100259) by Alexander Schrijver, in Mathematical Programming 91.3 (2002): 437-445.\n ```\n\n\n\n\nA min $s$-$t$ cut problem is as follows.\n\nWe are given a weighted graph $G = (V,E)$.\n\nAn $s$-$t$ cut $C$ of $G$ is a partition of $V$ into $(U, V-U)$ such that $s \\in U$ and $t \\in V-U$.\n\nThe __cost__ of a cut is the total weight of the edges that go between the two parts:\n\n$$ \\text{Cost}(C) = \\sum_{e(u,v),\\, u\\in U,\\, v\\in V-U} w(e)$$\n\nThis is a very famous problem that can be solved in time that is polynomial in $|V|$ and $|E|$. \n\nIncreasingly better solutions have been found over the past 60+ years.  \n\nWhat can a min $s$-$t$ cut tell us about a graph?\n\nLet's look at the karate club, in which I've highlighted the president and the instructor:  \n\n::: {#e80046c9 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=13}\n``` {.python .cell-code}\nG=nx.karate_club_graph()\nnp.random.seed(9)\npos = nx.spring_layout(G)\ncut_edges = nx.minimum_edge_cut(G, s=0, t=33)\n#\nfig = plt.figure(figsize=(12,6))\nnode_color = 34 * ['skyblue']\nnode_color[0] = 'tomato'\nnode_color[33] = 'dodgerblue'\nnx.draw_networkx(G, pos=pos, \n                 with_labels=True, node_size=1000,\n                 node_color = node_color,\n                 font_size=16)\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-14-output-1.png){width=912 height=463}\n:::\n:::\n\n\nAs mentioned, when Wayne Zachary studied the club, a conflict arose between the instructor and the president (nodes 0 and 33).  \n\nZachary predicted the way the club would split based on an $s$-$t$ min cut.   \n\nIn fact, he __correctly__ predicted every single member's eventual association except for node 8!\n\n::: {#15b9961d .cell hide_input='true' tags='[\"hide-input\"]' execution_count=14}\n``` {.python .cell-code}\nGcopy = G.copy()\nGcopy.remove_edges_from(cut_edges)\ncc = nx.connected_components(Gcopy)\nnode_set = {node: i for i, s in enumerate(cc) for node in s}\ncolors = ['dodgerblue', 'tomato']\nnode_colors = [colors[node_set[v]-1] for v in G.nodes()]\nfig = plt.figure(figsize=(12,6))\nnx.draw_networkx(G, node_color=node_colors, pos=pos, \n                 with_labels='True', node_size=1000, font_size=16)\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-15-output-1.png){width=912 height=463}\n:::\n:::\n\n\n### Minimum Cuts\n\nNow, in partitioning a graph, we may not have any particular $s$ and $t$ in mind.  \n\nRather, we may want to simply find the \"cheapest\" way to disconnect the graph.\n\nClearly, we can do this using $s$-$t$ min cut, by simply trying all $s$ and $t$ pairs.\n\n<center>\n    \n<img src=\"figs/L23-min-cut.png\" width=\"35%\">\n    \n</center> \n\nLet's try this approach of finding the minimum $s-t$ cut over all possibilities in the karate club graph:\n\n::: {#6028f24f .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=15}\n``` {.python .cell-code}\nGcopy = G.copy()\nGcopy.remove_edges_from(nx.minimum_edge_cut(G))\ncc = nx.connected_components(Gcopy)\nnode_set = {node: i for i, s in enumerate(cc) for node in s}\n#\ncolors = ['tomato', 'dodgerblue']\nnode_colors = [colors[node_set[v]] for v in G]\nfig = plt.figure(figsize=(12,6))\nnx.draw_networkx(G, node_color=node_colors, pos=pos, with_labels='True', \n                 node_size=1000, font_size=16)\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-16-output-1.png){width=912 height=463}\n:::\n:::\n\n\nThis is in fact the minimum cut: node 11 only has one edge to the rest of the graph, so the min cut is 1.\n\nAs this example shows, minimum cut is not, in general, a good approach for clustering or partitioning.\n\nTo get a more useful partition, we need to define a new goal:  finding a __balanced cut.__\n\n### Balanced Cuts\n\nThe idea to avoid the problem above is to normalize the cut by the size of the __smaller__ of the two components.\n\nThe problem above would be avoided because the smaller of the two cuts is just a single node.\n\nThis leads us to define the __isoperimetric ratio__:\n    \n$$ \\alpha = \\frac{E(U, V\\setminus U)}{\\min(|U|, |V\\setminus U|)} $$\n\nAnd the __isoperimetric number of G:__\n    \n$$ \\alpha(G) = \\min_U \\frac{E(U, V\\setminus U)}{\\min(|U|, |V\\setminus U|)} $$\n\nThe idea is that finding $\\alpha(G)$ gives a _balanced cut_ -- one that maximizes the number of disconnected nodes per edge removed.\n\nHow easily can we compute this?  \n\nUnfortunately, it's not computable in polynomial time.\n\nHowever, we can make good approximations, which we'll look at now.\n\nTo do so, we'll return to using linear algebra for graphs, and introduce __spectral graph theory.__\n\n## Spectral Graph Theory\n\n\n\n\n```{note}\nIf you want to study this in more detail, some excellent references are\n* [Allow Me to Introduce\nSpectral and Isoperimetric Graph Partitioning](https://people.eecs.berkeley.edu/~jrs/papers/partnotes.pdf) by Jonathan Shewchuck, which has outstanding visualizations and physical intuition.\n* [Spectral and Algebraic Graph Theory](http://cs-www.cs.yale.edu/homes/spielman/sagt/sagt.pdf) by Daniel Spielman which provides proofs and much more detail.\n```\n\n\n\n\nSpectral graph theory is the use of linear algebra to study the properties of graphs.\n\nTo introduce spectral graph theory, we define some terms.\n\nFor a undirected graph $G$ having $n$ nodes, we define the $n\\times n$ matrix $D$ as a diagonal matrix of node degrees.\n\nI.e., $D = \\text{diag}(d_1, d_2, d_3, \\dots)$ where $d_i$ is the degree of node $i$.\n\nThen assuming $G$ has adjacency matrix $A$, we define the __Laplacian__ of $G$ as:\n\n$$ L = D - A $$\n\nHere is the Laplacian matrix $L$ for the karate club network, shown as a heatmap:\n\n::: {#46d3f20c .cell hide_input='true' tags='[\"hide-input\"]' execution_count=16}\n``` {.python .cell-code}\nL = nx.laplacian_matrix(nx.karate_club_graph()).todense()\nplt.figure(figsize = (7, 7))\nsns.heatmap(L, cmap = plt.cm.tab20)\nplt.axis('equal')\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-17-output-1.png){width=514 height=537}\n:::\n:::\n\n\nNow let us think about an $n$-component vector $\\mathbf{x} \\in \\mathbb{R}^n$ as an __assignment of values__ to nodes in the graph $G$.   \n\nFor example, $\\mathbf{x}$ could encode node \"importance\" or \"strength\" or even a more concrete notion like \"temperature\" or \"altitude.\"\n\nThen here is an amazing fact about the Laplacian of $G$.\n\n(For a proof you can see the notes.)\n    \nThe quadratic form\n\n$$ \\mathbf{x}^TL\\mathbf{x}$$\n\nis exactly the same as\n\n$$  \\sum_{(i,j)\\in E} (x_i - x_j)^2 $$\n\n\n\n\n```{note}\nTo see that \n\n$$ \\mathbf{x}^TL\\mathbf{x} = \\sum_{(i,j)\\in E} (x_i - x_j)^2, $$\n\nfirst consider $\\mathbf{x}^TL\\mathbf{x}$.  Writing out the quadratic form explicitly, we have that\n\n$$ \\mathbf{x}^TL\\mathbf{x} = \\sum_{i, j} L_{ij}x_i x_j. $$\n\nNow, taking into account the values in $L$, we see that in the sum we will have the term $d_i x_i^2$ for each $i$, and also 2 terms of $-x_ix_j$ whenever $(i,j)\\in E$.  \n\nTurning to \n\n$$\\sum_{(i,j)\\in E} (x_i - x_j)^2 = \\sum_{(i,j)\\in E} x_i^2 - 2x_ix_j + x_j^2, $$\n\nwe have the same set of terms in the sum. \n```\n\n\n\n\nThat is, the quadratic form $ \\mathbf{x}^TL\\mathbf{x}$ is the sum of squared differences of $\\mathbf{x}$ __over the edges in $G$.__\n\nIn other words, \"When nodes have the values in $\\mathbf{x}$, how much do __adjacent__ nodes vary?\"\n\nNow, let's think about vectors $\\mathbf{x}$ that __minimize__ the differences over the edges in the graph.\n\nWe can think of these as \"smooth\" functions on the graph -- neighboring nodes don't differ too much.\n\nTo find such \"smooth\" vectors, we would solve this optimization:\n\n$$ \\min_{\\Vert \\mathbf{x}\\Vert = 1}\\sum_{(i,j)\\in E} (x_i - x_j)^2 $$\n\nWe constrain $\\mathbf{x}$ to have a nonzero norm, otherwise $\\mathbf{x} = \\mathbf{0}$ would be a trivial solution.\n\nBut we can express this in terms of the graph Laplacian:\n\n$$ \\min_{\\Vert \\mathbf{x}\\Vert = 1}\\sum_{(i,j)\\in E} (x_i - x_j)^2 = \\min_{\\Vert \\mathbf{x}\\Vert = 1} \\mathbf{x}^TL\\mathbf{x} $$\n\nNow, we know how to solve this constrained minimization!\n\nFrom linear algebra, we know that when\n\n$$ \\lambda = \\min_{\\Vert \\mathbf{x}\\Vert = 1} \\mathbf{x}^TL\\mathbf{x} $$\n\nthen $\\lambda$ is the __smallest eigenvalue of $L$__ and $\\mathbf{x}$ is the corresponding eigenvector.\n\nSo ... we are connecting functions on the graph $G$ with eigenvectors of the matrix $L$.   \n\nQuite remarkable!\n\nWell, what do we know about $L$?\n\n1. $L$ is __symmetric.__  Therefore the eigenvectors of $L$ are orthogonal and its eigenvalues are real.\n2. $L$ is __positive semidefinite.__  Therefore the eigenvalues of $L$ are all positive or zero.  (For a proof see the notes.)\n\nWe can order the eigenvalues from largest to smallest $\\lambda_n  \\geq \\dots \\geq \\lambda_2 \\geq \\lambda_1 \\geq 0.$\n\n\n\n\n```{note}\nHow do we know that $L$ is positive semidefinite?\n\nConsider $ \\sum_{(i,j)\\in E} (x_i - x_j)^2.$  \n\nThis is always a nonnegative quantity.\n\nSo $\\mathbf{x}^T L\\mathbf{x} \\geq 0$ for all $\\mathbf{x}$, which is in fact the definition of positive-semidefiniteness.\n```\n\n\n\n\nNext, let's assume that $G$ is connected.\n\nThen $L$ has a single eigenvalue of value $\\lambda_1 = 0$.   The corresponding eigenvector is $\\mathbf{w}_1 = \\mathbf{1} = [1, 1, 1, \\dots]^T$.\n\nThis is easily seen:\n\n$$L{\\mathbf 1}={\\mathbf 0}.$$\n\nRecall that row $i$ of $L$ consists of $d_i$ on the diagonal, and $d_i$ -1s in other positions.\n\nThe second-smallest eigenvalue of $L$, $\\lambda_2$, is called the __Fiedler value.__\n\nWe know that all of the other eigenvectors of $L$ are orthogonal to $\\mathbf 1$, because $L$ is symmetric.\n\nBecause of that, a definition of the second smallest eigenvalue is: \n\n$$\\lambda_2 = \\min_{\\Vert \\mathbf{x}\\Vert = 1, \\;\\mathbf{x}\\perp {\\mathbf 1}} \\mathbf{x}^TL\\mathbf{x}$$\n\nNote that another way of saying that $\\mathbf{x} \\perp {\\mathbf 1}$ is that the entries of $\\mathbf{x}$ sum to 0.\n\nIn other words, $\\mathbf{x}$ is __mean-centered__ or __zero-mean.__\n\nThe corresponding eigenvector is called the __Fiedler vector.__\n\nIt minimizes:\n\n$$\\mathbf{w}_2 = \\arg \\min_{\\Vert \\mathbf{x}\\Vert=1,\\;\\mathbf{x}\\perp {\\mathbf 1}} \\sum_{(i,j)\\in E} (x_i - x_j)^2$$\n\nLet's look at this closely: \n\nif we think of $x_i$ as a 1-D \"coordinate\" for node $i$ in the graph, \n\nthen choosing $\\mathbf{x} = \\mathbf{w}_2$ (the eigenvector corresponding to $\\lambda_2$) puts each node in a position that minimizes the sum of the \"squared stretching\" of each edge.\n\nNow, perhaps you recall that the energy in a stretched spring is proportional to the square of its stretched length.  \n\nSo imagine that we use the entries in $\\mathbf{w}_2$ to position the nodes of the graph along a single dimension.\n\nThen using the Fiedler vector $\\mathbf{w}_2$ for node coordinates is __exactly the spring layout__ of nodes that we discussed in the last lecture -- __except that it is in one dimension only.__\n\nThis is the basis for the __spectral layout__ that we showed in the last lecture.\n\nIn spectral layout, we use $\\mathbf{w}_2$ for the first dimension, and $\\mathbf{w}_3$ for the second dimension.\n\n$\\mathbf{w}_3$ is the eigenvector corresponding to \n\n$$\\lambda_3 = \\min_{\\Vert \\mathbf{x}\\Vert = 1, \\;\\mathbf{x}\\perp \\{\\mathbf{1}, \\mathbf{w}_2\\}} \\mathbf{x}^TL\\mathbf{x}$$\n\nLet's look again at layouts for the football network:\n\n::: {#33a27d3f .cell hide_input='true' tags='[\"hide-input\"]' execution_count=17}\n``` {.python .cell-code}\nplt.figure(figsize = (12, 6))\nax1 = plt.subplot(121)\nnx.draw_networkx(football, ax = ax1,\n                 node_size=35, \n                 edge_color='gray', \n                 pos = nx.spectral_layout(football),\n                 with_labels=False, alpha=.8, linewidths=2)\nplt.axis('off')\nplt.title('Title 1 Football -- Spectral Layout', size = 16)\nax2 = plt.subplot(122)\nnx.draw_networkx(football, ax = ax2,\n                 node_size=35, \n                 edge_color='gray', \n                 pos = nx.spring_layout(football, seed = 0),\n                 with_labels=False, alpha=.8, linewidths=2)\nplt.axis('off')\nplt.title('Title 1 Football -- Spring Layout', size = 16);\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-18-output-1.png){width=912 height=487}\n:::\n:::\n\n\nSo what is the difference between the spectral layout and the spring layout?\n\nIn one dimension, they are the same, but in multiple dimensions, spectral layout optimizes each dimension separately.\n\n### Spectral Partitioning\n\nThis leads to key ideas in node partitioning.\n\nThe basic idea is to partition nodes according to the Fiedler vector $\\mathbf{w}_2$.\n\nThis can be shown to have provably good performance for the __balanced cut__ problem.\n\n\n\n\n```{note}\nSee [Spectral and Algebraic Graph Theory](http://cs-www.cs.yale.edu/homes/spielman/sagt/sagt.pdf) by Daniel Spielman, Chapter 20, where it is proved that for every $U \\subset V$ with $|U| \\leq |V|/2$, $\\alpha(G) \\geq \\lambda_2 (1-s)$ where $s = |U|/|V|$.   In particular, $\\alpha(G) \\geq \\lambda_2/2.$\n```\n\n\n\n\nThere are a number of options for how to split based on the Fiedler vector.\n\nIf $\\mathbf{w}_2$ is the Fiedler vector, then split nodes according to a value $s$:\n    \n* bisection: $s$ is the median value in $\\mathbf{w}_2$\n* ratio cut: $s$ is the value that maximizes $\\alpha$\n* sign: separate positive and negative vaues ($s = 0$)\n* gap: separate according to the largest gap in the values of $\\mathbf{w}_2$\n\nHere is a spectral parititioning for the karate club graph:\n\n::: {#4b362a51 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=18}\n``` {.python .cell-code}\nG = nx.karate_club_graph()\nf = nx.fiedler_vector(G)\ns = np.zeros(len(f), dtype='int')\ns[f > 0] = 1\n#\nfig = plt.figure(figsize=(12,6))\ncolors = ['tomato', 'dodgerblue']\nnp.random.seed(9)\npos = nx.spring_layout(G)\nnode_colors = [colors[s[v]] for v in G]\nnx.draw_networkx(G, pos=pos, node_color=node_colors, with_labels='True',\n        node_size=1000, font_size=16)\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-19-output-1.png){width=912 height=463}\n:::\n:::\n\n\nInterestingly, this is almost the same as the $s$-$t$ min cut  based on the president and instructor!\n\n### Spectral Clustering\n\nIn many cases we would like to move beyond graph partitioning, to allow for clustering nodes into, say, $k$ clusters.\n\nThe idea of spectral clustering takes the observations about the Fiedler vector and extends them to more than one dimension.\n\nLet's look again at the spectral layout of the football dataset.  \n\nHere we've labelled the nodes according to their conference, which we will think of as ground-truth labels.\n\n::: {#74b46b47 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=19}\n``` {.python .cell-code}\nimport matplotlib.patches as mpatches\nimport re\ncmap = plt.cm.tab20\n#\n# data from http://www-personal.umich.edu/~mejn/netdata/\nfootball = nx.readwrite.gml.read_gml('data/football.gml')\nconf_name = {}\nwith open('data/football.txt', 'r') as fp:\n    for line in fp:\n        m = re.match('\\s*(\\d+)\\s+=\\s*([\\w\\s-]+)\\s*\\n', line)\n        if m:\n            conf_name[int(m.group(1))] = m.group(2)\nconf = [d['value'] for i, d in football.nodes.data()]\n#\n#\nplt.figure(figsize = (12, 12))\nnx.draw_networkx(football,\n                 pos = nx.spectral_layout(football), \n                 node_color = conf,\n                 with_labels = False,\n                 cmap = cmap)\nplt.title('Conference Membership in Football Network')\npatches = [mpatches.Patch(color = cmap(i/11), label = conf_name[i]) for i in range(12)]\nplt.legend(handles = patches)\nplt.axis('off');\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n<>:10: SyntaxWarning: invalid escape sequence '\\s'\n<>:10: SyntaxWarning: invalid escape sequence '\\s'\n/var/folders/ly/jkydg4dj2vs93b_ds7yp5t7r0000gn/T/ipykernel_49048/193594221.py:10: SyntaxWarning: invalid escape sequence '\\s'\n  m = re.match('\\s*(\\d+)\\s+=\\s*([\\w\\s-]+)\\s*\\n', line)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-20-output-2.png){width=912 height=926}\n:::\n:::\n\n\nNow, the key idea is that using spectral layout, we have placed nodes into a Euclidean space.\n\nSo ... we could use a __standard clustering algorithm__ in that space.\n\n::: {#47fbd56a .cell hide_input='true' slideshow='{\"slide_type\":\"fragment\"}' tags='[\"hide-input\"]' execution_count=20}\n``` {.python .cell-code}\nplt.figure(figsize = (12, 12))\nnx.draw_networkx(football,\n                 pos = nx.spectral_layout(football), \n                 node_color = conf,\n                 with_labels = False,\n                 edgelist = [],\n                 cmap = cmap)\nplt.title('Conference Membership in Football Network')\npatches = [mpatches.Patch(color = cmap(i/11), label = conf_name[i]) for i in range(12)]\nplt.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\nplt.legend(handles = patches);\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-21-output-1.png){width=962 height=949}\n:::\n:::\n\n\nNow the above plot shows that many clusters are well-separated in this space, but some still overlap.\n\nTo address this, we can use additional eigenvectors of the Laplacian, ie, $\\mathbf{w}_4, \\mathbf{w}_5, \\dots$.\n\nSo: the idea of spectral clustering is: \n* use enough of the smallest eigenvectors of $L$ to sufficiently \"spread out\" the nodes\n* cluster the nodes in the Euclidean space created by this embedding.\n\nMore specifically: Given a graph $G$:\n\n* Compute $L$, the Laplacian of $G$\n* Compute the smallest $d$ eigenvectors of $L$, __excluding__ the smallest eigenvector (the ones vector)\n* Let $U \\in \\mathbb{R}^{n\\times d}$ be the matrix containing the eigenvectors $\\mathbf{w}_2, \\mathbf{w}_3, \\dots, \\mathbf{w}_{d+1}$ as columns\n* Let the position of each node $i$ be the point in $\\mathbb{R}^d$ given by row $i$ of $U$\n* Cluster the points into $k$ clusters using $k$-means\n\nLet's explore the results of spectral clustering using $d = 2$ dimensions.\n\n::: {#d55d101c .cell hide_input='true' tags='[\"hide-input\"]' execution_count=21}\n``` {.python .cell-code}\n# Here is a complete example of spectral clustering\n#\n# The number of dimensions of spectral layout\nk = 2\n#\n# Obtain the graph\nfootball\n#\n# Compute the eigenvectors of its Laplacian\nL = nx.laplacian_matrix(football).todense()\nw, v = np.linalg.eig(L)\nv = np.array(v)\n# \n# scale each eigenvector by its eigenvalue\nX = v @ np.diag(w)\n#\n# consider the eigenvectors in increasing order of their eigenvalues\nw_order = np.argsort(w)\nX = X[:, w_order]\n#\n# run kmeans using k top eigenvectors as coordinates\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(init='k-means++', n_clusters=11 , n_init=10)\nkmeans.fit_predict(X[:, 1:(k+1)])\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_\nerror = kmeans.inertia_\n#\n# visualize the result\nplt.figure(figsize = (14, 7))\nax1 = plt.subplot(121)\nnx.draw_networkx(football,\n                 ax = ax1,\n                 pos = nx.spectral_layout(football), \n                 node_color = labels,\n                 with_labels = False,\n                 edgelist = [],\n                 cmap = cmap,\n                 node_size = 100)\nplt.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\nplt.title(f'Spectral Clustering, 11 Clusters, dimension = {k}')\nax2 = plt.subplot(122)\nnx.draw_networkx(football,\n                 ax = ax2,\n                 pos = nx.spectral_layout(football), \n                 node_color = conf,\n                 with_labels = False,\n                 edgelist = [],\n                 cmap = cmap,\n                 node_size = 100)\nplt.title('Conference Membership in Football Network')\npatches = [mpatches.Patch(color = cmap(i/11), label = conf_name[i]) for i in range(12)]\nplt.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\nplt.legend(handles = patches, loc='center left', bbox_to_anchor=(1, 0.5));\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-22-output-1.png){width=1275 height=579}\n:::\n:::\n\n\nThis is pretty good, but we can see that in some cases the clustering is not able to separate clusters that overlap in the visualization.\n\nWhich makes sense, as for the case $d = 2$, we are running $k$-means on the points just as we see them in the visualization.\n\nLet's try $d = 3$.   Now there will be another dimension available to the clustering, which we can't see in the visualization.\n\n::: {#207c4a61 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=22}\n``` {.python .cell-code}\n# Here is a complete example of spectral clustering\n#\n# The number of dimensions of spectral layout\nk = 3\n#\n#\n# Compute the eigenvectors of its Laplacian\nL = nx.laplacian_matrix(football).todense()\nw, v = np.linalg.eig(L)\nv = np.array(v)\n# \n# scale each eigenvector by its eigenvalue\nX = v @ np.diag(w)\n#\n# consider the eigenvectors in increasing order of their eigenvalues\nw_order = np.argsort(w)\nX = X[:, w_order]\n#\n# run kmeans using k top eigenvectors as coordinates\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(init='k-means++', n_clusters=11 , n_init=10)\nkmeans.fit_predict(X[:, 1:(k+1)])\ncentroids = kmeans.cluster_centers_\nlabels = kmeans.labels_\nerror = kmeans.inertia_\n#\n# visualize the result\nplt.figure(figsize = (14, 7))\nax1 = plt.subplot(121)\nnx.draw_networkx(football,\n                 ax = ax1,\n                 pos = nx.spectral_layout(football), \n                 node_color = labels,\n                 with_labels = False,\n                 edgelist = [],\n                 cmap = cmap,\n                 node_size = 100)\nplt.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\nplt.title(f'Spectral Clustering, 11 Clusters, dimension = {k}')\nax2 = plt.subplot(122)\nnx.draw_networkx(football,\n                 ax = ax2,\n                 pos = nx.spectral_layout(football), \n                 node_color = conf,\n                 with_labels = False,\n                 edgelist = [],\n                 cmap = cmap,\n                 node_size = 100)\nplt.title('Conference Membership in Football Network')\npatches = [mpatches.Patch(color = cmap(i/11), label = conf_name[i]) for i in range(12)]\nplt.tick_params(left=True, bottom=True, labelleft=True, labelbottom=True)\nplt.legend(handles = patches, loc='center left', bbox_to_anchor=(1, 0.5));\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-23-output-1.png){width=1275 height=579}\n:::\n:::\n\n\nWe can see visually that using 3 dimensions is giving us a better clustering than 2 dimensions.\n\nWhat happens as we increase the dimension further?\n\nTo evaluate this question we can use Adjusted Rand Index:\n\n::: {#1c81bee4 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=23}\n``` {.python .cell-code}\nimport sklearn.metrics as metrics\n#\n# Compute the eigenvectors of its Laplacian\nL = nx.laplacian_matrix(football).todense()\nw, v = np.linalg.eig(L)\nv = np.array(v)\n# \n# scale each eigenvector by its eigenvalue\nX = v @ np.diag(w)\n#\n# consider the eigenvectors in increasing order of their eigenvalues\nw_order = np.argsort(w)\nX = X[:, w_order]\n#\nmax_dimension = 15\nri = np.zeros(max_dimension - 1)\nfor k in range(1, max_dimension):\n    # run kmeans using k top eigenvectors as coordinates\n    kmeans = KMeans(init='k-means++', n_clusters = 11, n_init = 10)\n    kmeans.fit_predict(X[:, 1:(k+1)])\n    ri[k - 1] = metrics.adjusted_rand_score(kmeans.labels_, conf)\n#\nplt.figure(figsize = (8, 6))\nplt.plot(range(1, max_dimension), ri, 'o-')\nplt.xlabel('Number of Dimensions (Eigenvectors)', size = 14)\nplt.title('Spectral Clustering of Football Network Compared to Known Labels', size = 16)\nplt.ylabel('Adjusted Rand Index', size = 14);\n```\n\n::: {.cell-output .cell-output-display}\n![](22-Networks-II-Centrality-Clustering_files/figure-html/cell-24-output-1.png){width=736 height=533}\n:::\n:::\n\n\nBased on this plot, it looks like the football graph is best described as about six-dimensional.\n\nWhen we embed it in six dimensions and cluster there we get an extremely high Adjusted Rand Index.\n\n::: {#793c79b0 .cell hide_input='true' tags='[\"hide-input\"]' execution_count=24}\n``` {.python .cell-code}\nprint(f'Maximum ARI is {np.max(ri):0.3f}, using {1 + np.argmax(ri)} dimensions for spectral embedding.')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMaximum ARI is 0.868, using 6 dimensions for spectral embedding.\n```\n:::\n:::\n\n\n",
    "supporting": [
      "22-Networks-II-Centrality-Clustering_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}